{
    "author": "cyyever",
    "message": "Add Optional to types (#37163)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "8a828a747e65f3b8807d574320f810571a7bdd7e",
    "files": [
        {
            "sha": "ddd718cbb8a63646d39cc12718ca606a4b77405f",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -63,7 +63,7 @@ class FlaxGreedySearchOutput(ModelOutput):\n             The generated sequences.\n     \"\"\"\n \n-    sequences: jnp.ndarray = None\n+    sequences: Optional[jnp.ndarray] = None\n \n \n @flax.struct.dataclass\n@@ -77,7 +77,7 @@ class FlaxSampleOutput(ModelOutput):\n             The generated sequences.\n     \"\"\"\n \n-    sequences: jnp.ndarray = None\n+    sequences: Optional[jnp.ndarray] = None\n \n \n @flax.struct.dataclass\n@@ -93,8 +93,8 @@ class FlaxBeamSearchOutput(ModelOutput):\n             The scores (log probabilities) of the generated sequences.\n     \"\"\"\n \n-    sequences: jnp.ndarray = None\n-    scores: jnp.ndarray = None\n+    sequences: Optional[jnp.ndarray] = None\n+    scores: Optional[jnp.ndarray] = None\n \n \n @flax.struct.dataclass"
        },
        {
            "sha": "344147e6e34c67b1072449885dc81dcc24621809",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -76,7 +76,7 @@ class TFGreedySearchDecoderOnlyOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n     hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n@@ -115,7 +115,7 @@ class TFGreedySearchEncoderDecoderOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n     encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n@@ -146,7 +146,7 @@ class TFSampleDecoderOnlyOutput(ModelOutput):\n             `tf.Tensor` of shape `(num_return_sequences*batch_size, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n     hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n@@ -185,7 +185,7 @@ class TFSampleEncoderDecoderOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size*num_return_sequences, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n     encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None\n@@ -221,7 +221,7 @@ class TFBeamSearchDecoderOnlyOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n@@ -268,7 +268,7 @@ class TFBeamSearchEncoderDecoderOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size*num_beams*num_return_sequences, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n@@ -306,7 +306,7 @@ class TFBeamSampleDecoderOnlyOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size*num_beams, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n@@ -352,7 +352,7 @@ class TFBeamSampleEncoderDecoderOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size*num_beams, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     sequences_scores: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     beam_indices: Optional[tf.Tensor] = None\n@@ -384,7 +384,7 @@ class TFContrastiveSearchDecoderOnlyOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[Tuple[tf.Tensor]]] = None\n     hidden_states: Optional[Tuple[Tuple[tf.Tensor]]] = None\n@@ -422,7 +422,7 @@ class TFContrastiveSearchEncoderDecoderOutput(ModelOutput):\n             `tf.Tensor` of shape `(batch_size, generated_length, hidden_size)`.\n     \"\"\"\n \n-    sequences: tf.Tensor = None\n+    sequences: Optional[tf.Tensor] = None\n     scores: Optional[Tuple[tf.Tensor]] = None\n     encoder_attentions: Optional[Tuple[tf.Tensor]] = None\n     encoder_hidden_states: Optional[Tuple[tf.Tensor]] = None"
        },
        {
            "sha": "e5f900c3b7d693989568e40eee25d1a895d0059c",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -61,11 +61,11 @@ class WatermarkDetectorOutput:\n             Array containing confidence scores of a text being machine-generated for each element in the batch.\n     \"\"\"\n \n-    num_tokens_scored: np.array = None\n-    num_green_tokens: np.array = None\n-    green_fraction: np.array = None\n-    z_score: np.array = None\n-    p_value: np.array = None\n+    num_tokens_scored: Optional[np.array] = None\n+    num_green_tokens: Optional[np.array] = None\n+    green_fraction: Optional[np.array] = None\n+    z_score: Optional[np.array] = None\n+    p_value: Optional[np.array] = None\n     prediction: Optional[np.array] = None\n     confidence: Optional[np.array] = None\n "
        },
        {
            "sha": "b671a1119111353f6569e4ba0f379c8e113de70b",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -573,7 +573,7 @@ def _process_image(\n     def _prepare_input_images(\n         self,\n         images: ImageInput,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         device: Optional[\"torch.device\"] = None,\n     ) -> list[\"torch.Tensor\"]:"
        },
        {
            "sha": "591c556e59f07e39e758f04e29c74871930034d9",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -10,6 +10,8 @@\n # an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n # specific language governing permissions and limitations under the License.\n \n+from typing import Optional\n+\n import torch\n \n from transformers.generation.configuration_utils import GenerationConfig\n@@ -178,8 +180,8 @@ def generate(\n \n def convert_and_export_with_cache(\n     model: PreTrainedModel,\n-    example_input_ids: torch.Tensor = None,\n-    example_cache_position: torch.Tensor = None,\n+    example_input_ids: Optional[torch.Tensor] = None,\n+    example_cache_position: Optional[torch.Tensor] = None,\n ):\n     \"\"\"\n     Convert a `PreTrainedModel` into an exportable module and export it using `torch.export`,"
        },
        {
            "sha": "c7d54dc415146e0f289750c86caffa5021399a92",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -289,7 +289,7 @@ def _flash_attention_forward(\n     sliding_window: Optional[int] = None,\n     use_top_left_mask: bool = False,\n     softcap: Optional[float] = None,\n-    deterministic: bool = None,\n+    deterministic: Optional[bool] = None,\n     cu_seq_lens_q: Optional[torch.LongTensor] = None,\n     cu_seq_lens_k: Optional[torch.LongTensor] = None,\n     max_length_q: Optional[int] = None,"
        },
        {
            "sha": "325f968571ed400903b6c3bb15a4faf10f7e56c9",
            "filename": "src/transformers/modeling_flax_outputs.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_flax_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_outputs.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -40,7 +40,7 @@ class FlaxBaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -59,7 +59,7 @@ class FlaxBaseModelOutputWithNoAttention(ModelOutput):\n             model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n \n \n@@ -79,8 +79,8 @@ class FlaxBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n             model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n-    pooler_output: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n+    pooler_output: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n \n \n@@ -99,7 +99,7 @@ class FlaxImageClassifierOutputWithNoAttention(ModelOutput):\n             called feature maps) of the model at the output of each stage.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n \n \n@@ -127,7 +127,7 @@ class FlaxBaseModelOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Dict[str, jnp.ndarray]] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -158,8 +158,8 @@ class FlaxBaseModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n-    pooler_output: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n+    pooler_output: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -205,8 +205,8 @@ class FlaxBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n             input) to speed up sequential decoding.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n-    pooler_output: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n+    pooler_output: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -252,7 +252,7 @@ class FlaxBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -310,7 +310,7 @@ class FlaxSeq2SeqModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: jnp.ndarray = None\n+    last_hidden_state: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -354,7 +354,7 @@ class FlaxCausalLMOutputWithCrossAttentions(ModelOutput):\n             `past_key_values` input) to speed up sequential decoding.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -382,7 +382,7 @@ class FlaxMaskedLMOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -437,7 +437,7 @@ class FlaxSeq2SeqLMOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -469,7 +469,7 @@ class FlaxNextSentencePredictorOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -495,7 +495,7 @@ class FlaxSequenceClassifierOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -547,7 +547,7 @@ class FlaxSeq2SeqSequenceClassifierOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     decoder_attentions: Optional[Tuple[jnp.ndarray]] = None\n@@ -580,7 +580,7 @@ class FlaxMultipleChoiceModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -606,7 +606,7 @@ class FlaxTokenClassifierOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: jnp.ndarray = None\n+    logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -634,8 +634,8 @@ class FlaxQuestionAnsweringModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    start_logits: jnp.ndarray = None\n-    end_logits: jnp.ndarray = None\n+    start_logits: Optional[jnp.ndarray] = None\n+    end_logits: Optional[jnp.ndarray] = None\n     hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     attentions: Optional[Tuple[jnp.ndarray]] = None\n \n@@ -689,8 +689,8 @@ class FlaxSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    start_logits: jnp.ndarray = None\n-    end_logits: jnp.ndarray = None\n+    start_logits: Optional[jnp.ndarray] = None\n+    end_logits: Optional[jnp.ndarray] = None\n     past_key_values: Optional[Tuple[Tuple[jnp.ndarray]]] = None\n     decoder_hidden_states: Optional[Tuple[jnp.ndarray]] = None\n     decoder_attentions: Optional[Tuple[jnp.ndarray]] = None"
        },
        {
            "sha": "cbc8b3682af608d0db6da02ef36764d0be10ca9c",
            "filename": "src/transformers/modeling_tf_outputs.py",
            "status": "modified",
            "additions": 32,
            "deletions": 32,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_tf_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_outputs.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -44,7 +44,7 @@ class TFBaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -64,7 +64,7 @@ class TFBaseModelOutputWithNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n \n \n@@ -96,8 +96,8 @@ class TFBaseModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    pooler_output: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -119,8 +119,8 @@ class TFBaseModelOutputWithPoolingAndNoAttention(ModelOutput):\n             Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    pooler_output: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n \n \n@@ -164,8 +164,8 @@ class TFBaseModelOutputWithPoolingAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    pooler_output: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    pooler_output: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -202,7 +202,7 @@ class TFBaseModelOutputWithPast(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -235,7 +235,7 @@ class TFBaseModelOutputWithCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n     cross_attentions: Tuple[tf.Tensor] | None = None\n@@ -277,7 +277,7 @@ class TFBaseModelOutputWithPastAndCrossAttentions(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -334,7 +334,7 @@ class TFSeq2SeqModelOutput(ModelOutput):\n             self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor] | None = None\n     decoder_attentions: Tuple[tf.Tensor] | None = None\n@@ -368,7 +368,7 @@ class TFCausalLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -403,7 +403,7 @@ class TFCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -445,7 +445,7 @@ class TFCausalLMOutputWithCrossAttentions(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -476,7 +476,7 @@ class TFMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -530,7 +530,7 @@ class TFSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor] | None = None\n     decoder_attentions: Tuple[tf.Tensor] | None = None\n@@ -565,7 +565,7 @@ class TFNextSentencePredictorOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -594,7 +594,7 @@ class TFSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -645,7 +645,7 @@ class TFSeq2SeqSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor] | None = None\n     decoder_attentions: Tuple[tf.Tensor] | None = None\n@@ -687,7 +687,7 @@ class TFSemanticSegmenterOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -719,7 +719,7 @@ class TFSemanticSegmenterOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n \n \n@@ -745,7 +745,7 @@ class TFImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -776,7 +776,7 @@ class TFMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -805,7 +805,7 @@ class TFTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -836,8 +836,8 @@ class TFQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: tf.Tensor = None\n-    end_logits: tf.Tensor = None\n+    start_logits: Optional[tf.Tensor] = None\n+    end_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n \n@@ -887,8 +887,8 @@ class TFSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: tf.Tensor = None\n-    end_logits: tf.Tensor = None\n+    start_logits: Optional[tf.Tensor] = None\n+    end_logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor] | None = None\n     decoder_attentions: Tuple[tf.Tensor] | None = None\n@@ -927,7 +927,7 @@ class TFSequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -950,7 +950,7 @@ class TFImageClassifierOutputWithNoAttention(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Optional[Tuple[tf.Tensor, ...]] = None\n \n \n@@ -977,7 +977,7 @@ class TFMaskedImageModelingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    reconstruction: tf.Tensor = None\n+    reconstruction: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "c0add5bb6672bf77ca0fca34a5bc194ab80f89f5",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -2530,7 +2530,7 @@ def from_pretrained(\n         local_files_only: bool = False,\n         token: Optional[Union[str, bool]] = None,\n         revision: str = \"main\",\n-        use_safetensors: bool = None,\n+        use_safetensors: Optional[bool] = None,\n         **kwargs,\n     ):\n         r\"\"\""
        },
        {
            "sha": "574735c1890ae2149063b528994add6cff90cfd7",
            "filename": "src/transformers/models/albert/modeling_albert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_albert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -609,8 +609,8 @@ class AlbertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    sop_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    sop_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "6800cfa8d16a84a422e804ada3e0a7d9b9e9ef40",
            "filename": "src/transformers/models/albert/modeling_tf_albert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falbert%2Fmodeling_tf_albert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -164,10 +164,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -749,9 +749,9 @@ class TFAlbertForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: tf.Tensor = None\n-    prediction_logits: tf.Tensor = None\n-    sop_logits: tf.Tensor = None\n+    loss: Optional[tf.Tensor] = None\n+    prediction_logits: Optional[tf.Tensor] = None\n+    sop_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "a007b7a7c6d698977369255162c732502b966269",
            "filename": "src/transformers/models/align/modeling_align.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fmodeling_align.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -198,7 +198,7 @@ class AlignVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -226,7 +226,7 @@ class AlignTextModelOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -254,10 +254,10 @@ class AlignOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n     vision_model_output: BaseModelOutputWithPoolingAndNoAttention = None\n "
        },
        {
            "sha": "6e4c9e650da0ed9b6a01395467988ca8dda289fc",
            "filename": "src/transformers/models/altclip/modeling_altclip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fmodeling_altclip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -182,10 +182,10 @@ class AltCLIPOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "92f0685c3bb8b8640d4c5c71c0b4e7df49611d9d",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -902,7 +902,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1189,7 +1189,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1303,7 +1303,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1424,7 +1424,7 @@ def get_decoder(self):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: torch.FloatTensor = None,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: int = -1,\n     ):\n         patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n@@ -1446,9 +1446,9 @@ def get_image_features(\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "3f38c87b5df8e50f84cde2173de8c65610759749",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1402,7 +1402,7 @@ def get_decoder(self):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: torch.FloatTensor = None,\n+        pixel_mask: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: int = -1,\n     ):\n         patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n@@ -1424,9 +1424,9 @@ def get_image_features(\n     @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_mask: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_mask: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "5a96b6235de937d4d4678e20b641871c8ed4346c",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -85,8 +85,8 @@ class AutoFormerDecoderOutput(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    trend: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    trend: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -153,8 +153,8 @@ class AutoformerModelOutput(ModelOutput):\n             Static features of each time series' in a batch which are copied to the covariates at inference time.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    trend: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    trend: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -305,7 +305,7 @@ def __init__(self, config: AutoformerConfig):\n         self.keepdim = config.keepdim if hasattr(config, \"keepdim\") else True\n \n     def forward(\n-        self, data: torch.Tensor, observed_indicator: torch.Tensor = None\n+        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:"
        },
        {
            "sha": "1e6e76a21075febf213765d9b7ec81091c6aa120",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -182,7 +182,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -355,8 +355,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=AyaVisionCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -370,7 +370,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "b046275a2d3ad65ab3df46c7dd8ffa1af7e4397d",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -212,8 +212,8 @@ def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_m\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -227,7 +227,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, AyaVisionCausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "fd16fc6334f19a8f9228b0b6c086d56b0a7e776c",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1198,7 +1198,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n@@ -1476,7 +1476,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,"
        },
        {
            "sha": "b6cdf9077408a18624432cc249c3cdfcc920d3dd",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -940,7 +940,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(BAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n@@ -1187,7 +1187,7 @@ class BambaForCausalLM(LlamaForCausalLM):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,"
        },
        {
            "sha": "bc3d4dcd936c1a52b70634c41998f1288f82f50a",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -993,7 +993,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1178,7 +1178,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1474,7 +1474,7 @@ def get_decoder(self):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1615,7 +1615,7 @@ def _tie_weights(self):\n     @add_end_docstrings(BART_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1742,7 +1742,7 @@ def __init__(self, config: BartConfig, **kwargs):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1871,7 +1871,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.Tensor = None,\n+        input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2020,7 +2020,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "0d928f21410741ac400d3eb140c0b54bc73c7d28",
            "filename": "src/transformers/models/beit/image_processing_beit.py",
            "status": "modified",
            "additions": 19,
            "deletions": 19,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fimage_processing_beit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -190,15 +190,15 @@ def reduce_label(self, label: ImageInput) -> np.ndarray:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_reduce_labels: bool = None,\n-        do_resize: bool = None,\n+        do_reduce_labels: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -223,14 +223,14 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -268,12 +268,12 @@ def _preprocess_image(\n     def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_reduce_labels: bool = None,\n+        do_reduce_labels: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n         \"\"\"Preprocesses a single segmentation map.\"\"\"\n@@ -317,14 +317,14 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_reduce_labels: Optional[bool] = None,"
        },
        {
            "sha": "38d2c8c8b5a582c7cd94b4cf196015bfc77ee3b7",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -880,8 +880,8 @@ class BertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "ba73faf23cf96f624a924b88b1df5ae4848e4a36",
            "filename": "src/transformers/models/bert/modeling_tf_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_tf_bert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -161,10 +161,10 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -1048,8 +1048,8 @@ class TFBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    prediction_logits: tf.Tensor = None\n-    seq_relationship_logits: tf.Tensor = None\n+    prediction_logits: Optional[tf.Tensor] = None\n+    seq_relationship_logits: Optional[tf.Tensor] = None\n     hidden_states: Optional[Union[Tuple[tf.Tensor], tf.Tensor]] = None\n     attentions: Optional[Union[Tuple[tf.Tensor], tf.Tensor]] = None\n "
        },
        {
            "sha": "86658de5241a7ad89f44c6166bc8ddbba20f9502",
            "filename": "src/transformers/models/bert/tokenization_bert_tf.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert_tf.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1,5 +1,5 @@\n import os\n-from typing import List, Union\n+from typing import List, Optional, Union\n \n import tensorflow as tf\n from tensorflow_text import BertTokenizer as BertTokenizerLayer\n@@ -58,13 +58,13 @@ def __init__(\n         self,\n         vocab_list: List,\n         do_lower_case: bool,\n-        cls_token_id: int = None,\n-        sep_token_id: int = None,\n-        pad_token_id: int = None,\n+        cls_token_id: Optional[int] = None,\n+        sep_token_id: Optional[int] = None,\n+        pad_token_id: Optional[int] = None,\n         padding: str = \"longest\",\n         truncation: bool = True,\n         max_length: int = 512,\n-        pad_to_multiple_of: int = None,\n+        pad_to_multiple_of: Optional[int] = None,\n         return_token_type_ids: bool = True,\n         return_attention_mask: bool = True,\n         use_fast_bert_tokenizer: bool = True,"
        },
        {
            "sha": "c2f6646202b37eac392ae3289fc70449ece41707",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1860,8 +1860,8 @@ class BigBirdForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -1894,9 +1894,9 @@ class BigBirdForQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -1970,7 +1970,7 @@ def set_attention_type(self, value: str):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2268,7 +2268,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=BigBirdForPreTrainingOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2381,7 +2381,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=MaskedLMOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2527,7 +2527,7 @@ def set_output_embeddings(self, new_embeddings):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2666,7 +2666,7 @@ def __init__(self, config):\n     @replace_return_docstrings(output_type=SequenceClassifierOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2800,7 +2800,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -2895,7 +2895,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "3e2d13e47a1e38624b59c838da431c2e4af85bff",
            "filename": "src/transformers/models/big_bird/tokenization_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Ftokenization_big_bird.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -184,7 +184,7 @@ def _decode(\n         self,\n         token_ids: List[int],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         spaces_between_special_tokens: bool = True,\n         **kwargs,\n     ) -> str:"
        },
        {
            "sha": "99333b7c159dd2580b81096edf4666dc63d19717",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -2346,7 +2346,7 @@ def get_decoder(self):\n     # Copied from transformers.models.bart.modeling_bart.BartModel.forward with Bart->BigBirdPegasus\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2489,7 +2489,7 @@ def _tie_weights(self):\n     @add_end_docstrings(BIGBIRD_PEGASUS_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2615,7 +2615,7 @@ def __init__(self, config: BigBirdPegasusConfig, **kwargs):\n     # Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2743,7 +2743,7 @@ def __init__(self, config):\n     # Copied from transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward\n     def forward(\n         self,\n-        input_ids: torch.Tensor = None,\n+        input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2887,7 +2887,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "2b1f307a29fb091ec028fdd01f0bcd0851be73d0",
            "filename": "src/transformers/models/bit/image_processing_bit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fimage_processing_bit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -176,17 +176,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "9ce51bebe70c74b264c5290a96e303573c597d05",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1397,7 +1397,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "7bb4a49bb8e384f26133b52a50e8ea9633940762",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1351,7 +1351,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "9f28b33a66b57315286abb26901ab96a5cf42c76",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -169,7 +169,7 @@ def preprocess(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> PIL.Image.Image:"
        },
        {
            "sha": "1f248ab8bee669cd9f2582c509ec4a1e8e0d06bd",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -87,7 +87,7 @@ class BlipForConditionalGenerationModelOutput(ModelOutput):\n     loss: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -129,7 +129,7 @@ class BlipTextVisionModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -170,7 +170,7 @@ class BlipImageTextMatchingModelOutput(ModelOutput):\n     itm_score: Optional[torch.FloatTensor] = None\n     loss: Optional[torch.FloatTensor] = None\n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     vision_pooler_output: Optional[torch.FloatTensor] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -200,10 +200,10 @@ class BlipOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "9573ca0fbbc553a9fb9943f84dd142be5633d9b2",
            "filename": "src/transformers/models/blip/modeling_tf_blip.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -96,7 +96,7 @@ class TFBlipForConditionalGenerationModelOutput(ModelOutput):\n     loss: Tuple[tf.Tensor] | None = None\n     logits: Tuple[tf.Tensor] | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -138,7 +138,7 @@ class TFBlipTextVisionModelOutput(ModelOutput):\n \n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -179,7 +179,7 @@ class TFBlipImageTextMatchingModelOutput(ModelOutput):\n     itm_score: tf.Tensor | None = None\n     loss: tf.Tensor | None = None\n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     vision_pooler_output: tf.Tensor | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -209,10 +209,10 @@ class TFBlipOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: tf.Tensor = None\n-    logits_per_text: tf.Tensor = None\n-    text_embeds: tf.Tensor = None\n-    image_embeds: tf.Tensor = None\n+    logits_per_image: Optional[tf.Tensor] = None\n+    logits_per_text: Optional[tf.Tensor] = None\n+    text_embeds: Optional[tf.Tensor] = None\n+    image_embeds: Optional[tf.Tensor] = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -309,9 +309,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor."
        },
        {
            "sha": "5e5b6f9f4b4d6d26aaf703530af5c90eea2f0d18",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -106,10 +106,10 @@ class Blip2ImageTextMatchingModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -145,7 +145,7 @@ class Blip2TextModelOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -175,7 +175,7 @@ class Blip2VisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "524b4caa7431002164bc651c6dd991e475dc5e0d",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -149,9 +149,9 @@ class BridgeTowerModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    text_features: torch.FloatTensor = None\n-    image_features: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    text_features: Optional[torch.FloatTensor] = None\n+    image_features: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -182,7 +182,7 @@ class BridgeTowerContrastiveOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     text_embeds: Optional[Tuple[torch.FloatTensor]] = None\n     image_embeds: Optional[Tuple[torch.FloatTensor]] = None\n     cross_embeds: Optional[Tuple[torch.FloatTensor]] = None\n@@ -225,7 +225,7 @@ def attention(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor):\n             key_padding_mask=attention_mask,\n         )[0]\n \n-    def forward(self, hidden_state: torch.Tensor, attention_mask: torch.Tensor = None):\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n         residual_state = hidden_state + self.attention(self.ln_1(hidden_state), attention_mask)\n         hidden_state = self.ln_2(residual_state)\n         for _, layer in self.mlp.items():"
        },
        {
            "sha": "df39d2a49b7df67d363f558c341da3ce5c8aebc1",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -152,8 +152,8 @@ class BrosSpadeOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    initial_token_logits: torch.FloatTensor = None\n-    subsequent_token_logits: torch.FloatTensor = None\n+    initial_token_logits: Optional[torch.FloatTensor] = None\n+    subsequent_token_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "7a699f6ca959ee513feac9e348e0c9d857c3e1d6",
            "filename": "src/transformers/models/canine/modeling_canine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcanine%2Fmodeling_canine.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -85,8 +85,8 @@ class CanineModelOutputWithPooling(ModelOutput):\n             attention softmax, used to compute the weighted average in the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "2d1417a8ee80a6aa8ade8ffd4b1b86e7c2dc448e",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -172,17 +172,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "65ace7cbcc4f6133155f1d8122cb7786b7b2c7ac",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1254,8 +1254,8 @@ def get_image_tokens(self, pixel_values: torch.FloatTensor):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1550,8 +1550,8 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "629de907b147ff47450fb1b46d8bf79c76242568",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -165,17 +165,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "647e8f1c2421261d76a5f1f87dfa10c91e5d2fcb",
            "filename": "src/transformers/models/chinese_clip/modeling_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fmodeling_chinese_clip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -86,10 +86,10 @@ class ChineseCLIPOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n     vision_model_output: BaseModelOutputWithPoolingAndCrossAttentions = None\n "
        },
        {
            "sha": "b2fdf0dd7eeb0143c56e4f9957db57645ff644f6",
            "filename": "src/transformers/models/clap/modeling_clap.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fmodeling_clap.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -154,7 +154,7 @@ class ClapTextModelOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -183,7 +183,7 @@ class ClapAudioModelOutput(ModelOutput):\n     \"\"\"\n \n     audio_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -212,10 +212,10 @@ class ClapOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_audio: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    audio_embeds: torch.FloatTensor = None\n+    logits_per_audio: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    audio_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     audio_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "8c02cd14ebc613b98f511780884640aa3a9e30c7",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -200,17 +200,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "2df825a8ada5cc93f34d41b434b1d54cb1d083c0",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -103,7 +103,7 @@ class CLIPVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -132,7 +132,7 @@ class CLIPTextModelOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -160,10 +160,10 @@ class CLIPOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "6afdadd252954363c4ddbbf522e495586db4aff4",
            "filename": "src/transformers/models/clip/modeling_tf_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_tf_clip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -108,10 +108,10 @@ class TFCLIPOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: tf.Tensor = None\n-    logits_per_text: tf.Tensor = None\n-    text_embeds: tf.Tensor = None\n-    image_embeds: tf.Tensor = None\n+    logits_per_image: Optional[tf.Tensor] = None\n+    logits_per_text: Optional[tf.Tensor] = None\n+    text_embeds: Optional[tf.Tensor] = None\n+    image_embeds: Optional[tf.Tensor] = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -225,9 +225,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor."
        },
        {
            "sha": "a24847471f72bc6617887bf2c6a18e11f7d9b26c",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -81,10 +81,10 @@ class CLIPSegOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -110,7 +110,7 @@ class CLIPSegDecoderOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -127,9 +127,9 @@ class CLIPSegImageSegmentationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    conditional_embeddings: torch.FloatTensor = None\n-    pooled_output: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    conditional_embeddings: Optional[torch.FloatTensor] = None\n+    pooled_output: Optional[torch.FloatTensor] = None\n     vision_model_output: BaseModelOutputWithPooling = None\n     decoder_output: CLIPSegDecoderOutput = None\n "
        },
        {
            "sha": "afbab2938350b2b83a7150851c98b3ac5fa8c44a",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -171,7 +171,7 @@ class ClvpEncoderOutput(ModelOutput):\n     \"\"\"\n \n     embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -211,15 +211,15 @@ class ClvpOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     speech_ids: Optional[torch.LongTensor] = None\n-    logits_per_speech: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    speech_embeds: torch.FloatTensor = None\n+    logits_per_speech: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    speech_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     speech_model_output: BaseModelOutputWithPooling = None\n-    decoder_hidden_states: torch.FloatTensor = None\n-    text_encoder_hidden_states: torch.FloatTensor = None\n-    speech_encoder_hidden_states: torch.FloatTensor = None\n+    decoder_hidden_states: Optional[torch.FloatTensor] = None\n+    text_encoder_hidden_states: Optional[torch.FloatTensor] = None\n+    speech_encoder_hidden_states: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Clvp\n@@ -1737,8 +1737,8 @@ def get_speech_features(\n     @replace_return_docstrings(output_type=ClvpOutput, config_class=ClvpConfig)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        input_features: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n         conditioning_encoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         text_encoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n@@ -1868,8 +1868,8 @@ def forward(\n     @torch.no_grad()\n     def generate(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        input_features: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         generation_config: Optional[GenerationConfig] = None,\n         pad_to_max_mel_tokens: Optional[int] = None,"
        },
        {
            "sha": "f55d5a3f15942397f7550f27a45f89155257a788",
            "filename": "src/transformers/models/codegen/tokenization_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -344,7 +344,7 @@ def decode(\n         self,\n         token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         truncate_before_pattern: Optional[List[str]] = None,\n         **kwargs,\n     ) -> str:"
        },
        {
            "sha": "6ca74ff5326f014c3706736832f284986811ce73",
            "filename": "src/transformers/models/codegen/tokenization_codegen_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Ftokenization_codegen_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -192,7 +192,7 @@ def decode(\n         self,\n         token_ids: Union[int, List[int], \"np.ndarray\", \"torch.Tensor\", \"tf.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         truncate_before_pattern: Optional[List[str]] = None,\n         **kwargs,\n     ) -> str:"
        },
        {
            "sha": "7d2af2bdddd4669db633f4457e4ceba67bd4c9c5",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -550,7 +550,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -827,7 +827,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "3b017a31fa58b8b86488e97fa5688fe497f712d1",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -306,7 +306,7 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "600c14c1dfe53ce16e9a7612786303e3e052747c",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -557,7 +557,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,\n@@ -813,7 +813,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "7c73f3e185d9f0cabd5e1850a10f80482539e8a0",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -454,7 +454,7 @@ def __init__(self, config: Cohere2Config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,"
        },
        {
            "sha": "4782bf7d3038e375b0f1a05378102318d2553fc7",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -111,7 +111,7 @@ class ColPaliForRetrievalOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    embeddings: torch.Tensor = None\n+    embeddings: Optional[torch.Tensor] = None\n     past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -191,8 +191,8 @@ def __init__(self, config: ColPaliConfig):\n     @replace_return_docstrings(output_type=ColPaliForRetrievalOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "923ccc20abe2f1764d8a118b72aa853170c4de3f",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -948,7 +948,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -1264,7 +1264,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "90b7b68bb43142dd3e2c1bbc6d55144460268e15",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -178,8 +178,8 @@ class ConditionalDetrObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -248,9 +248,9 @@ class ConditionalDetrSegmentationOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -788,7 +788,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        object_queries: torch.Tensor = None,\n+        object_queries: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ):\n         \"\"\""
        },
        {
            "sha": "d9318f2b128b3c40f91197c89bfc2df1b7de84ec",
            "filename": "src/transformers/models/convbert/modeling_tf_convbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Fmodeling_tf_convbert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -106,10 +106,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:"
        },
        {
            "sha": "e6b3125167dc708c8204e150975afd3686e56fe5",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -90,7 +90,7 @@ def __init__(\n         self,\n         do_resize: bool = True,\n         size: Dict[str, int] = None,\n-        crop_pct: float = None,\n+        crop_pct: Optional[float] = None,\n         resample: PILImageResampling = PILImageResampling.BILINEAR,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n@@ -187,13 +187,13 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n-        crop_pct: float = None,\n+        crop_pct: Optional[float] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "5c769926d2c1ed07512c90c484ed7ad8f4f8f9a3",
            "filename": "src/transformers/models/convnext/modeling_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fmodeling_convnext.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -347,7 +347,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n@@ -413,7 +413,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "a9d8332ff0a4b7a4b35bd179e2bac6839cdca0b7",
            "filename": "src/transformers/models/convnextv2/modeling_convnextv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fmodeling_convnextv2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -367,7 +367,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n@@ -434,7 +434,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "c2ca9eab5a12c3cc656eed191273a1e831a6cd3c",
            "filename": "src/transformers/models/cvt/modeling_cvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_cvt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -60,8 +60,8 @@ class BaseModelOutputWithCLSToken(ModelOutput):\n             plus the initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    cls_token_value: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    cls_token_value: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n "
        },
        {
            "sha": "74202d880623d983aa7fadb8f6972fa385395edf",
            "filename": "src/transformers/models/cvt/modeling_tf_cvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fmodeling_tf_cvt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -65,8 +65,8 @@ class TFBaseModelOutputWithCLSToken(ModelOutput):\n             the initial embedding outputs.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    cls_token_value: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    cls_token_value: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n \n "
        },
        {
            "sha": "84d6f276a8933caf2c5d3d73e6f756ef044e0ccb",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -174,8 +174,8 @@ class DabDetrObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "47a45f44e9199eca08f49c81df56e5f00c0b969f",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -53,11 +53,11 @@ class DacOutput(ModelOutput):\n             Projected latents (continuous representation of input before quantization).\n     \"\"\"\n \n-    loss: torch.FloatTensor = None\n-    audio_values: torch.FloatTensor = None\n-    quantized_representation: torch.FloatTensor = None\n-    audio_codes: torch.LongTensor = None\n-    projected_latents: torch.FloatTensor = None\n+    loss: Optional[torch.FloatTensor] = None\n+    audio_values: Optional[torch.FloatTensor] = None\n+    quantized_representation: Optional[torch.FloatTensor] = None\n+    audio_codes: Optional[torch.LongTensor] = None\n+    projected_latents: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -74,10 +74,10 @@ class DacEncoderOutput(ModelOutput):\n             Projected latents (continuous representation of input before quantization).\n     \"\"\"\n \n-    loss: torch.FloatTensor = None\n-    quantized_representation: torch.FloatTensor = None\n-    audio_codes: torch.FloatTensor = None\n-    projected_latents: torch.FloatTensor = None\n+    loss: Optional[torch.FloatTensor] = None\n+    quantized_representation: Optional[torch.FloatTensor] = None\n+    audio_codes: Optional[torch.FloatTensor] = None\n+    projected_latents: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -89,7 +89,7 @@ class DacDecoderOutput(ModelOutput):\n             Decoded audio values, obtained using the decoder part of Dac.\n     \"\"\"\n \n-    audio_values: torch.FloatTensor = None\n+    audio_values: Optional[torch.FloatTensor] = None\n \n \n class Snake1d(nn.Module):"
        },
        {
            "sha": "813fad89dc5412c46bcdf21d11391670a3acbe45",
            "filename": "src/transformers/models/data2vec/modeling_tf_data2vec_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_tf_data2vec_vision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -90,8 +90,8 @@ class TFData2VecVisionModelOutputWithPooling(TFBaseModelOutputWithPooling):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    pooler_output: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "258023c60b4d85913debc3df4cbf36582256fd55",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -744,7 +744,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: torch.LongTensor = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         output_router_logits: Optional[bool] = False,"
        },
        {
            "sha": "cad151711323a2eac33f3858bccef0ce285afc9a",
            "filename": "src/transformers/models/deberta/modeling_tf_deberta.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta%2Fmodeling_tf_deberta.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -207,9 +207,9 @@ def call(\n         self,\n         input_tensor: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -318,9 +318,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -408,8 +408,8 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -650,9 +650,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -880,11 +880,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n-        mask: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n+        mask: Optional[tf.Tensor] = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\""
        },
        {
            "sha": "899564eef05d3f8cf88a329d7b7d1f4a15893083",
            "filename": "src/transformers/models/deberta_v2/modeling_tf_deberta_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_tf_deberta_v2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -192,9 +192,9 @@ def call(\n         self,\n         input_tensor: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -306,9 +306,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -485,8 +485,8 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         return_dict: bool = True,\n@@ -718,9 +718,9 @@ def call(\n         self,\n         hidden_states: tf.Tensor,\n         attention_mask: tf.Tensor,\n-        query_states: tf.Tensor = None,\n-        relative_pos: tf.Tensor = None,\n-        rel_embeddings: tf.Tensor = None,\n+        query_states: Optional[tf.Tensor] = None,\n+        relative_pos: Optional[tf.Tensor] = None,\n+        rel_embeddings: Optional[tf.Tensor] = None,\n         output_attentions: bool = False,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n@@ -985,11 +985,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n-        mask: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n+        mask: Optional[tf.Tensor] = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\""
        },
        {
            "sha": "ab8c19077506effb1f57ff78357584af4497f888",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -732,12 +732,12 @@ class DecisionTransformerOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    state_preds: torch.FloatTensor = None\n-    action_preds: torch.FloatTensor = None\n-    return_preds: torch.FloatTensor = None\n-    hidden_states: torch.FloatTensor = None\n-    attentions: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    state_preds: Optional[torch.FloatTensor] = None\n+    action_preds: Optional[torch.FloatTensor] = None\n+    return_preds: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[torch.FloatTensor] = None\n+    attentions: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n \n \n class DecisionTransformerPreTrainedModel(PreTrainedModel):"
        },
        {
            "sha": "2e2ddc2d13f41499b4825a9450de1be19a26c6a0",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -698,7 +698,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(DEEPSEEK_V3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -973,7 +973,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "0a7dd1b06d8078016926e8b9bd55ee031873f06b",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -946,7 +946,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -1262,7 +1262,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "8f78d8a7bfc39f8e46fe2f18ac872c6a9c56e718",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -346,7 +346,7 @@ def prepare_annotation(\n         image: torch.Tensor,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:"
        },
        {
            "sha": "4e177dde1a424d719cb6e4ef530df92209afb7f1",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -140,9 +140,9 @@ class DeformableDetrDecoderOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -192,10 +192,10 @@ class DeformableDetrModelOutput(ModelOutput):\n             Logits of predicted bounding boxes coordinates in the first stage.\n     \"\"\"\n \n-    init_reference_points: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -269,8 +269,8 @@ class DeformableDetrObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     init_reference_points: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -785,7 +785,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n         spatial_shapes_list=None,"
        },
        {
            "sha": "d1eceebcb5c0c302e2d6c8bcea95d9c15f33dfeb",
            "filename": "src/transformers/models/deit/image_processing_deit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fimage_processing_deit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -163,14 +163,14 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample=None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "cb88bca353fc551afe959231476e64c97cd30206",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -905,9 +905,9 @@ class token).\n             the self-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    cls_logits: torch.FloatTensor = None\n-    distillation_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    cls_logits: Optional[torch.FloatTensor] = None\n+    distillation_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "49c95268035a98c55e45071080ef748bce1d17ba",
            "filename": "src/transformers/models/deit/modeling_tf_deit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_tf_deit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -88,9 +88,9 @@ class token).\n             the self-attention heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n-    cls_logits: tf.Tensor = None\n-    distillation_logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n+    cls_logits: Optional[tf.Tensor] = None\n+    distillation_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "0cfdc03e81a3940ade1db13f3822291684219442",
            "filename": "src/transformers/models/deprecated/deta/image_processing_deta.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fimage_processing_deta.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -593,7 +593,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -889,7 +889,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[List[Dict], List[List[Dict]]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "a37fb60a873012f481040ddae72f1bcfd98ee923",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -178,9 +178,9 @@ class DetaDecoderOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -232,10 +232,10 @@ class DetaModelOutput(ModelOutput):\n             Logits of proposal bounding boxes coordinates in the gen_encoder_output_proposals.\n     \"\"\"\n \n-    init_reference_points: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -312,8 +312,8 @@ class DetaObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     init_reference_points: Optional[torch.FloatTensor] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -843,7 +843,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n         level_start_index=None,"
        },
        {
            "sha": "1c42759ed29f920eab1c1fa42ad8bf29edcf584a",
            "filename": "src/transformers/models/deprecated/efficientformer/image_processing_efficientformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fimage_processing_efficientformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -181,7 +181,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "f86656c0b13d1c90592821106b034b1d75946036",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_efficientformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_efficientformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -717,9 +717,9 @@ class token).\n             the self-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    cls_logits: torch.FloatTensor = None\n-    distillation_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    cls_logits: Optional[torch.FloatTensor] = None\n+    distillation_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "76fdaa1f088bc461563004d813750fb11b7f5599",
            "filename": "src/transformers/models/deprecated/efficientformer/modeling_tf_efficientformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fmodeling_tf_efficientformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1087,9 +1087,9 @@ class token).\n             the self-attention heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n-    cls_logits: tf.Tensor = None\n-    distillation_logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n+    cls_logits: Optional[tf.Tensor] = None\n+    distillation_logits: Optional[tf.Tensor] = None\n     hidden_states: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[tf.Tensor]] = None\n "
        },
        {
            "sha": "e32a853ae152a46eda9391393da8e82b9159c87d",
            "filename": "src/transformers/models/deprecated/graphormer/configuration_graphormer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fconfiguration_graphormer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -155,7 +155,7 @@ def __init__(\n         pre_layernorm: bool = False,\n         apply_graphormer_init: bool = False,\n         activation_fn: str = \"gelu\",\n-        embed_scale: float = None,\n+        embed_scale: Optional[float] = None,\n         freeze_embeddings: bool = False,\n         num_trans_layers_to_freeze: int = 0,\n         traceable: bool = False,"
        },
        {
            "sha": "0a59c827cd5aa3930e7f4967afc1a6a53cb8b1ee",
            "filename": "src/transformers/models/deprecated/nat/modeling_nat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnat%2Fmodeling_nat.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -97,7 +97,7 @@ class NatEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -132,7 +132,7 @@ class NatModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -169,7 +169,7 @@ class NatImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "1f76a217719d6d55c1a3c82afdfebce805a2906d",
            "filename": "src/transformers/models/deprecated/nezha/modeling_nezha.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fnezha%2Fmodeling_nezha.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -760,8 +760,8 @@ class NezhaForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "4b3f07d7a854ac2d0e55c22d613e133d6cb18b54",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -552,7 +552,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -710,7 +710,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -883,7 +883,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "b518849cedc2a62841c7b8b9adc9135351cd0cf0",
            "filename": "src/transformers/models/deprecated/realm/modeling_realm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Fmodeling_realm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -663,7 +663,7 @@ class RealmEmbedderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    projected_score: torch.FloatTensor = None\n+    projected_score: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -682,9 +682,9 @@ class RealmScorerOutput(ModelOutput):\n             Candidate score derived from the embedder.\n     \"\"\"\n \n-    relevance_score: torch.FloatTensor = None\n-    query_score: torch.FloatTensor = None\n-    candidate_score: torch.FloatTensor = None\n+    relevance_score: Optional[torch.FloatTensor] = None\n+    query_score: Optional[torch.FloatTensor] = None\n+    candidate_score: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -724,13 +724,13 @@ class RealmReaderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    loss: torch.FloatTensor = None\n-    retriever_loss: torch.FloatTensor = None\n-    reader_loss: torch.FloatTensor = None\n+    loss: Optional[torch.FloatTensor] = None\n+    retriever_loss: Optional[torch.FloatTensor] = None\n+    reader_loss: Optional[torch.FloatTensor] = None\n     retriever_correct: torch.BoolTensor = None\n     reader_correct: torch.BoolTensor = None\n-    block_idx: torch.LongTensor = None\n-    candidate: torch.LongTensor = None\n+    block_idx: Optional[torch.LongTensor] = None\n+    candidate: Optional[torch.LongTensor] = None\n     start_pos: torch.int32 = None\n     end_pos: torch.int32 = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -751,7 +751,7 @@ class RealmForOpenQAOutput(ModelOutput):\n     \"\"\"\n \n     reader_output: dict = None\n-    predicted_answer_ids: torch.LongTensor = None\n+    predicted_answer_ids: Optional[torch.LongTensor] = None\n \n \n class RealmPredictionHeadTransform(nn.Module):"
        },
        {
            "sha": "52afb77885cb847ef26016e545815955a1aaecb0",
            "filename": "src/transformers/models/deprecated/trajectory_transformer/modeling_trajectory_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftrajectory_transformer%2Fmodeling_trajectory_transformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -140,7 +140,7 @@ class TrajectoryTransformerOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "496638e5f2d69bfc10abd82a2887be3cbdd8f2c6",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_tf_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_tf_transfo_xl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -690,7 +690,7 @@ class TFTransfoXLModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     mems: List[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -723,7 +723,7 @@ class TFTransfoXLLMHeadModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    prediction_scores: tf.Tensor = None\n+    prediction_scores: Optional[tf.Tensor] = None\n     mems: List[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n@@ -757,7 +757,7 @@ class TFTransfoXLSequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     mems: List[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None"
        },
        {
            "sha": "abe7e599274cad9a6b3ff03099245c2a66f137d5",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -651,7 +651,7 @@ class TransfoXLSequenceClassifierOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     mems: List[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -687,7 +687,7 @@ class TransfoXLLMHeadModelOutput(ModelOutput):\n     \"\"\"\n \n     losses: Optional[torch.FloatTensor] = None\n-    prediction_scores: torch.FloatTensor = None\n+    prediction_scores: Optional[torch.FloatTensor] = None\n     mems: List[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "2c5f853b2a231848d45d87c78d5c594efb59b21b",
            "filename": "src/transformers/models/deprecated/tvlt/image_processing_tvlt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fimage_processing_tvlt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -220,14 +220,14 @@ def resize(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -277,16 +277,16 @@ def _preprocess_image(\n     def preprocess(\n         self,\n         videos: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         patch_size: List[int] = None,\n         num_frames: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         is_mixed: bool = False,"
        },
        {
            "sha": "561b7f90d147eab30fd0985ab084cdcffab22134",
            "filename": "src/transformers/models/deprecated/tvlt/modeling_tvlt.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fmodeling_tvlt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -75,13 +75,13 @@ class TvltModelOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    last_pixel_hidden_state: torch.FloatTensor = None\n-    last_audio_hidden_state: torch.FloatTensor = None\n-    pixel_label_masks: torch.LongTensor = None\n-    audio_label_masks: torch.LongTensor = None\n-    pixel_ids_restore: torch.LongTensor = None\n-    audio_ids_restore: torch.LongTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    last_pixel_hidden_state: Optional[torch.FloatTensor] = None\n+    last_audio_hidden_state: Optional[torch.FloatTensor] = None\n+    pixel_label_masks: Optional[torch.LongTensor] = None\n+    audio_label_masks: Optional[torch.LongTensor] = None\n+    pixel_ids_restore: Optional[torch.LongTensor] = None\n+    audio_ids_restore: Optional[torch.LongTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -104,7 +104,7 @@ class TvltDecoderOutput(ModelOutput):\n             the self-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -136,9 +136,9 @@ class TvltForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    matching_logits: torch.FloatTensor = None\n-    pixel_logits: torch.FloatTensor = None\n-    audio_logits: torch.FloatTensor = None\n+    matching_logits: Optional[torch.FloatTensor] = None\n+    pixel_logits: Optional[torch.FloatTensor] = None\n+    audio_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "c78790f1340e6b8668611caca2431e233e2d6e68",
            "filename": "src/transformers/models/deprecated/vit_hybrid/image_processing_vit_hybrid.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fimage_processing_vit_hybrid.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -192,17 +192,17 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "cf1abf5bbacc594ef726626546543ea2c9d4edce",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -308,7 +308,7 @@ class XLMProphetNetSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -528,7 +528,7 @@ class XLMProphetNetDecoderLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "c26bf484f50a5ee3eb8ad6bd710ac14c131c4ce8",
            "filename": "src/transformers/models/depth_pro/modeling_depth_pro.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fmodeling_depth_pro.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -62,7 +62,7 @@ class DepthProOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     features: Union[torch.FloatTensor, List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -94,7 +94,7 @@ class DepthProDepthEstimatorOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    predicted_depth: torch.FloatTensor = None\n+    predicted_depth: Optional[torch.FloatTensor] = None\n     field_of_view: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "b2677af85957f69a13f1e561fba26dd2ab7678c3",
            "filename": "src/transformers/models/detr/image_processing_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -928,7 +928,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -1237,7 +1237,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "b6227ce5c5d22d2604d204b3dc1c14e33879ec85",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -371,7 +371,7 @@ def prepare_annotation(\n         image: torch.Tensor,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:"
        },
        {
            "sha": "cb47f58bda00160e18f1dbdcf3fc6ae78211bd53",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -169,8 +169,8 @@ class DetrObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -238,9 +238,9 @@ class DetrSegmentationOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -632,7 +632,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        object_queries: torch.Tensor = None,\n+        object_queries: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ):\n         \"\"\""
        },
        {
            "sha": "07ad84cb0783ac1871277bcd19204ffa59ec8b61",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -794,7 +794,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(DIFFLLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1069,7 +1069,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "0e0121b78dabc7913f74f8dacf4e0cb424f6f1c6",
            "filename": "src/transformers/models/dinat/modeling_dinat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinat%2Fmodeling_dinat.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -97,7 +97,7 @@ class DinatEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -132,7 +132,7 @@ class DinatModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -169,7 +169,7 @@ class DinatImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "239bc54db29154d46b8bfeddead186bd32ca2fc1",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -299,16 +299,16 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_thumbnail: bool = None,\n-        do_align_long_axis: bool = None,\n-        do_pad: bool = None,\n+        do_thumbnail: Optional[bool] = None,\n+        do_align_long_axis: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n         random_padding: bool = False,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "0d44069fc8b8b834564e51dc878de831593828b6",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -78,7 +78,7 @@ class DonutSwinEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -114,7 +114,7 @@ class DonutSwinModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "3ff4aa11523e730dee6dc014aef58479f0c2775a",
            "filename": "src/transformers/models/dpr/modeling_dpr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_dpr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -127,8 +127,8 @@ class DPRReaderOutput(ModelOutput):\n     \"\"\"\n \n     start_logits: torch.FloatTensor\n-    end_logits: torch.FloatTensor = None\n-    relevance_logits: torch.FloatTensor = None\n+    end_logits: Optional[torch.FloatTensor] = None\n+    relevance_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "303b03ec244d61c8698d8115cf8237cdbe1f960b",
            "filename": "src/transformers/models/dpr/modeling_tf_dpr.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpr%2Fmodeling_tf_dpr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -18,7 +18,7 @@\n from __future__ import annotations\n \n from dataclasses import dataclass\n-from typing import Tuple, Union\n+from typing import Optional, Tuple, Union\n \n import tensorflow as tf\n \n@@ -68,7 +68,7 @@ class TFDPRContextEncoderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    pooler_output: tf.Tensor = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -96,7 +96,7 @@ class TFDPRQuestionEncoderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    pooler_output: tf.Tensor = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -127,9 +127,9 @@ class TFDPRReaderOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    start_logits: tf.Tensor = None\n-    end_logits: tf.Tensor = None\n-    relevance_logits: tf.Tensor = None\n+    start_logits: Optional[tf.Tensor] = None\n+    end_logits: Optional[tf.Tensor] = None\n+    relevance_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -155,13 +155,13 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n-        output_attentions: bool = None,\n-        output_hidden_states: bool = None,\n-        return_dict: bool = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         training: bool = False,\n     ) -> Union[TFBaseModelOutputWithPooling, Tuple[tf.Tensor, ...]]:\n         outputs = self.bert_model(\n@@ -226,7 +226,7 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n         output_attentions: bool = False,\n@@ -296,7 +296,7 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,\n@@ -329,7 +329,7 @@ def __init__(self, config: DPRConfig, **kwargs):\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: tf.Tensor | None = None,\n         token_type_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,"
        },
        {
            "sha": "d034ff0a4ff96b8e23253efbd074713b29401581",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -294,18 +294,18 @@ def reduce_label(self, label: ImageInput) -> np.ndarray:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_reduce_labels: bool = None,\n-        do_resize: bool = None,\n+        do_reduce_labels: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        keep_aspect_ratio: bool = None,\n+        keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_pad: bool = None,\n+        do_pad: Optional[bool] = None,\n         size_divisor: Optional[int] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -336,17 +336,17 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        keep_aspect_ratio: bool = None,\n+        keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_pad: bool = None,\n+        do_pad: Optional[bool] = None,\n         size_divisor: Optional[int] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -387,12 +387,12 @@ def _preprocess_image(\n     def _preprocess_segmentation_map(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        keep_aspect_ratio: bool = None,\n+        keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n-        do_reduce_labels: bool = None,\n+        do_reduce_labels: Optional[bool] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n         \"\"\"Preprocesses a single segmentation map.\"\"\"\n@@ -436,17 +436,17 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Optional[int] = None,\n-        keep_aspect_ratio: bool = None,\n+        keep_aspect_ratio: Optional[bool] = None,\n         ensure_multiple_of: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_pad: bool = None,\n+        do_pad: Optional[bool] = None,\n         size_divisor: Optional[int] = None,\n         do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "c69bf618fe1e0ddecf382e3356640e6036de1a4b",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -66,7 +66,7 @@ class BaseModelOutputWithIntermediateActivations(ModelOutput):\n             Intermediate activations that can be used to compute hidden states of the model at various layers.\n     \"\"\"\n \n-    last_hidden_states: torch.FloatTensor = None\n+    last_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_activations: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -99,8 +99,8 @@ class BaseModelOutputWithPoolingAndIntermediateActivations(ModelOutput):\n             Intermediate activations that can be used to compute hidden states of the model at various layers.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     intermediate_activations: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "612ede7086ead59ff28a051faf957a9120fdfa61",
            "filename": "src/transformers/models/efficientnet/image_processing_efficientnet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fimage_processing_efficientnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -212,18 +212,18 @@ def rescale(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample=None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        rescale_offset: bool = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        rescale_offset: Optional[bool] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        include_top: bool = None,\n+        include_top: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "9e0b89072921b47bd42a5febf9cdd576a31e33e1",
            "filename": "src/transformers/models/efficientnet/modeling_efficientnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientnet%2Fmodeling_efficientnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -527,7 +527,7 @@ def __init__(self, config: EfficientNetConfig):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n@@ -591,7 +591,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "7b73f022122d4d251380f80b993514a728726c9d",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -711,7 +711,7 @@ class ElectraForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "6dc3ac8ebf8cb32056aac75586f9f5eb15f58e7e",
            "filename": "src/transformers/models/electra/modeling_tf_electra.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_tf_electra.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -601,10 +601,10 @@ def build(self, input_shape=None):\n     # Copied from transformers.models.bert.modeling_tf_bert.TFBertEmbeddings.call\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:\n@@ -931,7 +931,7 @@ class TFElectraForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "a63269c99ef12e0303fe2c7a0bcf93f4918eb459",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -167,14 +167,14 @@ def __init__(\n     def _preprocess(\n         self,\n         images: Union[ImageInput, VideoInput],\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -308,15 +308,15 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         do_pad: bool = True,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,"
        },
        {
            "sha": "80b17541b91616562a47785db9b65f5961467456",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -706,7 +706,7 @@ def __init__(self, config, in_channels, quant_channels=None):\n             quant_channels=quant_channels,\n         )\n \n-    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor = None):\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: Optional[torch.FloatTensor] = None):\n         hidden_states = self.block_1(hidden_states, quant_states)\n         residual = hidden_states\n         hidden_states = self.attn_norm(hidden_states, quant_states)\n@@ -1379,7 +1379,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(EMU3_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1655,7 +1655,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"Emu3TextConfig\")\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -1875,9 +1875,9 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "031dc26f0a4aef38a587a125f91cad432984e733",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -436,7 +436,7 @@ def __init__(self, config, in_channels, quant_channels=None):\n             quant_channels=quant_channels,\n         )\n \n-    def forward(self, hidden_states: torch.FloatTensor, quant_states: torch.FloatTensor = None):\n+    def forward(self, hidden_states: torch.FloatTensor, quant_states: Optional[torch.FloatTensor] = None):\n         hidden_states = self.block_1(hidden_states, quant_states)\n         residual = hidden_states\n         hidden_states = self.attn_norm(hidden_states, quant_states)\n@@ -1175,9 +1175,9 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        image_sizes: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "f33191862e4891375059b882f8c50f5953dabc1b",
            "filename": "src/transformers/models/encodec/feature_extraction_encodec.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Ffeature_extraction_encodec.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -57,8 +57,8 @@ def __init__(\n         feature_size: int = 1,\n         sampling_rate: int = 24000,\n         padding_value: float = 0.0,\n-        chunk_length_s: float = None,\n-        overlap: float = None,\n+        chunk_length_s: Optional[float] = None,\n+        overlap: Optional[float] = None,\n         **kwargs,\n     ):\n         super().__init__(feature_size=feature_size, sampling_rate=sampling_rate, padding_value=padding_value, **kwargs)"
        },
        {
            "sha": "670ac99e03e03a88c07e0478e5190622403eb77a",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -50,8 +50,8 @@ class EncodecOutput(ModelOutput):\n             Decoded audio values, obtained using the decoder part of Encodec.\n     \"\"\"\n \n-    audio_codes: torch.LongTensor = None\n-    audio_values: torch.FloatTensor = None\n+    audio_codes: Optional[torch.LongTensor] = None\n+    audio_values: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -64,8 +64,8 @@ class EncodecEncoderOutput(ModelOutput):\n             Scaling factor for each `audio_codes` input. This is used to unscale each chunk of audio when decoding.\n     \"\"\"\n \n-    audio_codes: torch.LongTensor = None\n-    audio_scales: torch.FloatTensor = None\n+    audio_codes: Optional[torch.LongTensor] = None\n+    audio_scales: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -76,7 +76,7 @@ class EncodecDecoderOutput(ModelOutput):\n             Decoded audio values, obtained using the decoder part of Encodec.\n     \"\"\"\n \n-    audio_values: torch.FloatTensor = None\n+    audio_values: Optional[torch.FloatTensor] = None\n \n \n class EncodecConv1d(nn.Module):\n@@ -589,7 +589,7 @@ def _encode_frame(\n     def encode(\n         self,\n         input_values: torch.Tensor,\n-        padding_mask: torch.Tensor = None,\n+        padding_mask: Optional[torch.Tensor] = None,\n         bandwidth: Optional[float] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple[torch.Tensor, Optional[torch.Tensor]], EncodecEncoderOutput]:"
        },
        {
            "sha": "559078b1ef1aa45b39bc3b910da84ad7f853a68b",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -700,8 +700,8 @@ class ErnieForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "645c9d16a5c5dcd012cf3d91ca784267ede9c654",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 23,
            "deletions": 23,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -112,29 +112,29 @@ class EsmForProteinFoldingOutput(ModelOutput):\n             Per-sample maximum predicted error.\n     \"\"\"\n \n-    frames: torch.FloatTensor = None\n-    sidechain_frames: torch.FloatTensor = None\n-    unnormalized_angles: torch.FloatTensor = None\n-    angles: torch.FloatTensor = None\n-    positions: torch.FloatTensor = None\n-    states: torch.FloatTensor = None\n-    s_s: torch.FloatTensor = None\n-    s_z: torch.FloatTensor = None\n-    distogram_logits: torch.FloatTensor = None\n-    lm_logits: torch.FloatTensor = None\n-    aatype: torch.FloatTensor = None\n-    atom14_atom_exists: torch.FloatTensor = None\n-    residx_atom14_to_atom37: torch.FloatTensor = None\n-    residx_atom37_to_atom14: torch.FloatTensor = None\n-    atom37_atom_exists: torch.FloatTensor = None\n-    residue_index: torch.FloatTensor = None\n-    lddt_head: torch.FloatTensor = None\n-    plddt: torch.FloatTensor = None\n-    ptm_logits: torch.FloatTensor = None\n-    ptm: torch.FloatTensor = None\n-    aligned_confidence_probs: torch.FloatTensor = None\n-    predicted_aligned_error: torch.FloatTensor = None\n-    max_predicted_aligned_error: torch.FloatTensor = None\n+    frames: Optional[torch.FloatTensor] = None\n+    sidechain_frames: Optional[torch.FloatTensor] = None\n+    unnormalized_angles: Optional[torch.FloatTensor] = None\n+    angles: Optional[torch.FloatTensor] = None\n+    positions: Optional[torch.FloatTensor] = None\n+    states: Optional[torch.FloatTensor] = None\n+    s_s: Optional[torch.FloatTensor] = None\n+    s_z: Optional[torch.FloatTensor] = None\n+    distogram_logits: Optional[torch.FloatTensor] = None\n+    lm_logits: Optional[torch.FloatTensor] = None\n+    aatype: Optional[torch.FloatTensor] = None\n+    atom14_atom_exists: Optional[torch.FloatTensor] = None\n+    residx_atom14_to_atom37: Optional[torch.FloatTensor] = None\n+    residx_atom37_to_atom14: Optional[torch.FloatTensor] = None\n+    atom37_atom_exists: Optional[torch.FloatTensor] = None\n+    residue_index: Optional[torch.FloatTensor] = None\n+    lddt_head: Optional[torch.FloatTensor] = None\n+    plddt: Optional[torch.FloatTensor] = None\n+    ptm_logits: Optional[torch.FloatTensor] = None\n+    ptm: Optional[torch.FloatTensor] = None\n+    aligned_confidence_probs: Optional[torch.FloatTensor] = None\n+    predicted_aligned_error: Optional[torch.FloatTensor] = None\n+    max_predicted_aligned_error: Optional[torch.FloatTensor] = None\n \n \n ESMFOLD_INPUTS_DOCSTRING = r\"\"\""
        },
        {
            "sha": "590786b195d725a0063c272ce0b143227cc17873",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -78,15 +78,15 @@ class FastSpeech2ConformerModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    spectrogram: torch.FloatTensor = None\n+    spectrogram: Optional[torch.FloatTensor] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    duration_outputs: torch.LongTensor = None\n-    pitch_outputs: torch.FloatTensor = None\n-    energy_outputs: torch.FloatTensor = None\n+    duration_outputs: Optional[torch.LongTensor] = None\n+    pitch_outputs: Optional[torch.FloatTensor] = None\n+    energy_outputs: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -133,7 +133,7 @@ class FastSpeech2ConformerWithHifiGanOutput(FastSpeech2ConformerModelOutput):\n             Outputs of the energy predictor.\n     \"\"\"\n \n-    waveform: torch.FloatTensor = None\n+    waveform: Optional[torch.FloatTensor] = None\n \n \n _CONFIG_FOR_DOC = \"FastSpeech2ConformerConfig\""
        },
        {
            "sha": "43ce980aa7b138e4aafc1750fdc0ab47401f9001",
            "filename": "src/transformers/models/flaubert/modeling_tf_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflaubert%2Fmodeling_tf_flaubert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -808,7 +808,7 @@ class TFFlaubertWithLMHeadModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "3b8b8128c88252fb1bced4383da8a3d82d18e3c7",
            "filename": "src/transformers/models/flava/image_processing_flava.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fimage_processing_flava.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -63,7 +63,7 @@ def __init__(\n         mask_group_max_patches: Optional[int] = None,\n         mask_group_min_patches: int = 16,\n         mask_group_min_aspect_ratio: Optional[float] = 0.3,\n-        mask_group_max_aspect_ratio: float = None,\n+        mask_group_max_aspect_ratio: Optional[float] = None,\n     ):\n         if not isinstance(input_size, tuple):\n             input_size = (input_size,) * 2\n@@ -246,7 +246,7 @@ def __init__(\n         # Codebook related params\n         return_codebook_pixels: bool = False,\n         codebook_do_resize: bool = True,\n-        codebook_size: bool = None,\n+        codebook_size: Optional[bool] = None,\n         codebook_resample: int = PILImageResampling.LANCZOS,\n         codebook_do_center_crop: bool = True,\n         codebook_crop_size: Optional[int] = None,\n@@ -389,17 +389,17 @@ def map_pixels(self, image: np.ndarray) -> np.ndarray:\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_map_pixels: bool = None,\n+        do_map_pixels: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[ChannelDimension] = None,\n     ) -> np.ndarray:"
        },
        {
            "sha": "74076eddf27f58bacfc630df6654863fa640b35a",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1803,7 +1803,7 @@ def forward(\n         bool_masked_pos: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         image_attention_mask: Optional[torch.Tensor] = None,\n-        skip_unmasked_multimodal_encoder: bool = None,\n+        skip_unmasked_multimodal_encoder: Optional[bool] = None,\n         mlm_labels: Optional[torch.Tensor] = None,\n         mim_labels: Optional[torch.Tensor] = None,\n         itm_labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "63aaa42e9dc43107e7e3f8ddfde3fbbd9d1d935e",
            "filename": "src/transformers/models/fnet/modeling_fnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Fmodeling_fnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -444,8 +444,8 @@ class FNetForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n "
        },
        {
            "sha": "c113a505ef3795204f7342ed4d2b2e398a24141b",
            "filename": "src/transformers/models/fnet/tokenization_fnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffnet%2Ftokenization_fnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -221,7 +221,7 @@ def _decode(\n         self,\n         token_ids: List[int],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool = None,\n+        clean_up_tokenization_spaces: Optional[bool] = None,\n         spaces_between_special_tokens: bool = False,\n         **kwargs,\n     ) -> str:"
        },
        {
            "sha": "143d4e066be1eb5d2fcecf1c761dac83aa4e354d",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -75,7 +75,7 @@ class FocalNetEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -103,7 +103,7 @@ class FocalNetModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -133,7 +133,7 @@ class FocalNetMaskedImageModelingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    reconstruction: torch.FloatTensor = None\n+    reconstruction: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -162,7 +162,7 @@ class FocalNetImageClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "df53e2bb673636a0a0f87dc8649e17066469bf71",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -482,7 +482,7 @@ def forward(\n         self,\n         input_ids: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: torch.Tensor = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,"
        },
        {
            "sha": "c5fecd0cc780d81a51dd651483e8431169dbe04a",
            "filename": "src/transformers/models/funnel/modeling_funnel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_funnel.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -841,7 +841,7 @@ class FunnelForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "0f8e76b99febb9c2c832e7dcbde0f67517e6d282",
            "filename": "src/transformers/models/funnel/modeling_tf_funnel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Fmodeling_tf_funnel.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1104,7 +1104,7 @@ class TFFunnelForPreTrainingOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "fd19ff7b8d4fc7323593f1bbce371d6c31f9e180",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -227,9 +227,11 @@ def gather_continuous_embeddings(\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        image_patches: torch.Tensor = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n-        image_patches_indices: torch.Tensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        image_patches: Optional[\n+            torch.Tensor\n+        ] = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n+        image_patches_indices: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "8fdcd613c2c179a0c2fb2d7c86fd2cf81c0bee71",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -517,7 +517,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -794,7 +794,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "735c508f6d79605743710dd23e9c18b9bf5e9898",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -369,7 +369,7 @@ def __init__(self, config):\n class GemmaModel(LlamaModel):\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "38e78666f56c466afab354a42c46b50e9b586e0a",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -562,7 +562,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,\n@@ -822,7 +822,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,"
        },
        {
            "sha": "d197d978e8d81656af0a8cb5f125fe82b6e4a3b6",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -404,7 +404,7 @@ def __init__(self, config: Gemma2Config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,\n@@ -576,7 +576,7 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,"
        },
        {
            "sha": "f9156ab1b6066cd054bb5c9d14c1d8c011b24f96",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -102,11 +102,11 @@ def __init__(\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n-        do_pan_and_scan: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        do_pan_and_scan: Optional[bool] = None,\n         pan_and_scan_min_crop_size: Optional[int] = None,\n         pan_and_scan_max_num_crops: Optional[int] = None,\n-        pan_and_scan_min_ratio_to_activate: float = None,\n+        pan_and_scan_min_ratio_to_activate: Optional[float] = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -240,22 +240,22 @@ def _process_images_for_pan_and_scan(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: bool = None,\n-        do_pan_and_scan: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        do_pan_and_scan: Optional[bool] = None,\n         pan_and_scan_min_crop_size: Optional[int] = None,\n         pan_and_scan_max_num_crops: Optional[int] = None,\n-        pan_and_scan_min_ratio_to_activate: float = None,\n+        pan_and_scan_min_ratio_to_activate: Optional[float] = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "6dad88c1bc136755ef283ab684fdeff05a64fd7b",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -86,7 +86,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -913,7 +913,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridCache] = None,\n@@ -1223,8 +1223,8 @@ def get_image_features(self, pixel_values: torch.Tensor):\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,"
        },
        {
            "sha": "c4de8d928d5c6a6f9889879e025a351540736fe7",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -333,7 +333,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -854,8 +854,8 @@ def _update_causal_mask(\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,"
        },
        {
            "sha": "7efdf2d45c48dff51de26edf0e40cefe25ef1141",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -77,7 +77,7 @@ class GitVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "5b40a8addc496b1b1fa579e05b60f1956ced29ee",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -533,7 +533,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GLM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -808,7 +808,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "875c0742b96d7c0cea0049b932c68c4254fe71b2",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -266,7 +266,7 @@ def preprocess(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> PIL.Image.Image:"
        },
        {
            "sha": "1278af4faab5e09a6fc9625f425b2a5db014e7dc",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -320,7 +320,7 @@ class GotOcr2VisionEncoderOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -550,7 +550,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -741,8 +741,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "36d2db007b61551c68c9dadb9277bb7626da8ba8",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -385,8 +385,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "75c148f233acdce7e8363a550d6c06153f1fb3fa",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -532,8 +532,8 @@ class GPT2DoubleHeadsModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     mc_loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    mc_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    mc_logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "5812e42af75c5637091bc63c604fa76b44dbaee1",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -628,8 +628,8 @@ class TFGPT2DoubleHeadsModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n-    mc_logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n+    mc_logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None"
        },
        {
            "sha": "34e6ca2d2555c5a357c3bcdef1e705b83e626e0a",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_tf.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1,5 +1,5 @@\n import os\n-from typing import Dict, List, Union\n+from typing import Dict, List, Optional, Union\n \n import tensorflow as tf\n from keras_nlp.tokenizers import BytePairTokenizer\n@@ -25,7 +25,13 @@ class TFGPT2Tokenizer(keras.layers.Layer):\n         merges (List[str]): Merges list for Byte Pair Tokenizer\n     \"\"\"\n \n-    def __init__(self, vocab: Dict[str, int], merges: List[str], max_length: int = None, pad_token_id: int = None):\n+    def __init__(\n+        self,\n+        vocab: Dict[str, int],\n+        merges: List[str],\n+        max_length: Optional[int] = None,\n+        pad_token_id: Optional[int] = None,\n+    ):\n         super().__init__()\n         self.pad_token_id = pad_token_id\n         self.max_length = max_length\n@@ -88,7 +94,7 @@ def get_config(self):\n             \"pad_token_id\": self.pad_token_id,\n         }\n \n-    def call(self, x, max_length: int = None):\n+    def call(self, x, max_length: Optional[int] = None):\n         input_ids = self.tf_tokenizer(x)\n         attention_mask = tf.ones_like(input_ids)\n "
        },
        {
            "sha": "4345036ef13116f96c891266c5c71f0af2e83355",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -534,7 +534,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GRANITE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -807,7 +807,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "494ab5f1825af7e1b6c1fc1debbfdcdc68f5dc92",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -122,7 +122,7 @@ def __init__(self, config: GraniteConfig):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -232,7 +232,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GraniteForCausalLM(LlamaForCausalLM):\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "f2d4fc7f9ad5def0bb0befa6de77c123d286042b",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -989,7 +989,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GRANITEMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1285,7 +1285,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "a47dd45ba76308c0a3d52def34f633a34fafef64",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -935,7 +935,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(GRANITEMOESHARED_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1311,7 +1311,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "03a6c2e4e3c8b8743c2534f6915aa0cfeaf57b3f",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -986,7 +986,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -1302,7 +1302,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "a238c1dc1d9f11cc0f2e41187f027e3f4a3c465a",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -132,9 +132,9 @@ class GroundingDinoDecoderOutput(ModelOutput):\n             weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n \n@@ -166,8 +166,8 @@ class GroundingDinoEncoderOutput(ModelOutput):\n             multi-scale deformable attention heads.\n     \"\"\"\n \n-    last_hidden_state_vision: torch.FloatTensor = None\n-    last_hidden_state_text: torch.FloatTensor = None\n+    last_hidden_state_vision: Optional[torch.FloatTensor] = None\n+    last_hidden_state_text: Optional[torch.FloatTensor] = None\n     vision_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     text_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n@@ -225,10 +225,10 @@ class GroundingDinoModelOutput(ModelOutput):\n             Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    init_reference_points: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     encoder_last_hidden_state_vision: Optional[torch.FloatTensor] = None\n@@ -314,8 +314,8 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     init_reference_points: Optional[torch.FloatTensor] = None\n@@ -1012,7 +1012,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n         spatial_shapes_list=None,\n@@ -2547,8 +2547,8 @@ def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n         input_ids: torch.LongTensor,\n-        token_type_ids: torch.LongTensor = None,\n-        attention_mask: torch.LongTensor = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n         pixel_mask: Optional[torch.BoolTensor] = None,\n         encoder_outputs: Optional[Union[GroundingDinoEncoderOutput, Tuple]] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "6a5c235cf82a5d9fbd637ad00b7f592e1ce411dd",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -302,11 +302,11 @@ class GroupViTModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    segmentation_logits: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    segmentation_logits: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "a6b62ae70ca16045d7cace55bf4d7a11cc323bde",
            "filename": "src/transformers/models/groupvit/modeling_tf_groupvit.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_tf_groupvit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -253,11 +253,11 @@ class TFGroupViTModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits_per_image: tf.Tensor = None\n-    logits_per_text: tf.Tensor = None\n-    segmentation_logits: tf.Tensor = None\n-    text_embeds: tf.Tensor = None\n-    image_embeds: tf.Tensor = None\n+    logits_per_image: Optional[tf.Tensor] = None\n+    logits_per_text: Optional[tf.Tensor] = None\n+    segmentation_logits: Optional[tf.Tensor] = None\n+    text_embeds: Optional[tf.Tensor] = None\n+    image_embeds: Optional[tf.Tensor] = None\n     text_model_output: TFBaseModelOutputWithPooling = None\n     vision_model_output: TFBaseModelOutputWithPooling = None\n \n@@ -646,9 +646,9 @@ def build(self, input_shape: tf.TensorShape = None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n     ) -> tf.Tensor:\n         \"\"\"\n         Applies embedding based on inputs tensor.\n@@ -898,10 +898,10 @@ def transpose_for_scores(self, tensor: tf.Tensor, batch_size: int) -> tf.Tensor:\n     def call(\n         self,\n         hidden_states: tf.Tensor,\n-        attention_mask: tf.Tensor = None,\n-        causal_attention_mask: tf.Tensor = None,\n-        output_attentions: bool = None,\n-        encoder_hidden_states: tf.Tensor = None,\n+        attention_mask: Optional[tf.Tensor] = None,\n+        causal_attention_mask: Optional[tf.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        encoder_hidden_states: Optional[tf.Tensor] = None,\n         training: bool = False,\n     ) -> Tuple[tf.Tensor]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\""
        },
        {
            "sha": "fdd38da56e072fda61f177cb6ed908ac027a2d08",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -520,7 +520,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(HELIUM_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -795,7 +795,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "14a8dad524f230c6483c85a85f791c4a18a0ce92",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -85,7 +85,7 @@ class HieraEncoderOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -124,10 +124,10 @@ class HieraModelOutput(ModelOutput):\n             include the spatial dimensions.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     pooler_output: Optional[torch.FloatTensor] = None\n     bool_masked_pos: torch.BoolTensor = None\n-    ids_restore: torch.LongTensor = None\n+    ids_restore: Optional[torch.LongTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -163,7 +163,7 @@ class HieraForImageClassificationOutput(ImageClassifierOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -198,9 +198,9 @@ class HieraForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     bool_masked_pos: torch.BoolTensor = None\n-    ids_restore: torch.LongTensor = None\n+    ids_restore: Optional[torch.LongTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "02d5fb3c05a4be6c8b273aac6d84b8715990a867",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -96,7 +96,7 @@ class IdeficsBaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -138,7 +138,7 @@ class IdeficsCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1096,7 +1096,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1556,7 +1556,7 @@ def tie_weights(self):\n     @replace_return_docstrings(output_type=IdeficsCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "057988d9921116a4651146147d9366ea89cdc95d",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -91,7 +91,7 @@ class TFIdeficsBaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     past_key_values: Optional[Tuple[Tuple[tf.Tensor]]] = None\n     hidden_states: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[tf.Tensor]] = None\n@@ -133,7 +133,7 @@ class TFIdeficsCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[tf.Tensor] = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: Optional[List[tf.Tensor]] = None\n     hidden_states: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[tf.Tensor]] = None"
        },
        {
            "sha": "5e9f9b8ad772965852f68b48582cd13511bfdb6a",
            "filename": "src/transformers/models/idefics/vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -55,7 +55,7 @@ class IdeficsVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n "
        },
        {
            "sha": "c01e1c2e1fac039450298f00923a89cb4dd7b8f4",
            "filename": "src/transformers/models/idefics/vision_tf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fvision_tf.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -55,7 +55,7 @@ class TFIdeficsVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[tf.Tensor] = None\n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Optional[Tuple[tf.Tensor]] = None\n     attentions: Optional[Tuple[tf.Tensor]] = None\n "
        },
        {
            "sha": "16ff2873b1a8259187db15242b5abce15baa9ef2",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -76,7 +76,7 @@ class Idefics2BaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder, and optionally by the perceiver\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -114,7 +114,7 @@ class Idefics2CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1101,7 +1101,7 @@ def inputs_merger(\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1294,7 +1294,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=Idefics2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "64193c2a5d72ba584a77b501bedcc6d173843c1a",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -75,7 +75,7 @@ class Idefics3BaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -113,7 +113,7 @@ class Idefics3CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -833,7 +833,7 @@ def inputs_merger(\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1016,7 +1016,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=Idefics3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "07e7604574a6dc6c59b58a8eabe3ce28d87090c8",
            "filename": "src/transformers/models/imagegpt/image_processing_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fimage_processing_imagegpt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -177,10 +177,10 @@ def normalize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_normalize: bool = None,\n+        do_normalize: Optional[bool] = None,\n         do_color_quantize: Optional[bool] = None,\n         clusters: Optional[Union[List[List[int]], np.ndarray]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "a67950233f3da06858a78aee45519832f5fa27eb",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -178,7 +178,7 @@ def __init__(self, config: InformerConfig):\n         self.keepdim = config.keepdim if hasattr(config, \"keepdim\") else True\n \n     def forward(\n-        self, data: torch.Tensor, observed_indicator: torch.Tensor = None\n+        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:"
        },
        {
            "sha": "6c9bf4d4d3ca1ae1d1ce39fa3da874a769403906",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -172,7 +172,7 @@ def preprocess(\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> BatchFeature:\n@@ -290,7 +290,7 @@ def _preprocess_image(\n         do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:"
        },
        {
            "sha": "9a797c81d97abb116c3bf3df2ca2b1bbd708a845",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1233,7 +1233,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(JAMBA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n@@ -1432,7 +1432,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,"
        },
        {
            "sha": "1b87473f5fb278eb82a1baa7657f1f699950b11e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -989,7 +989,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(JETMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1295,7 +1295,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "23a1391fa1f14cecefbffd68e376d3fd4fe2d433",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -309,7 +309,7 @@ class Kosmos2ModelOutput(ModelOutput):\n             input) to speed up sequential decoding.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -367,7 +367,7 @@ class Kosmos2ForConditionalGenerationModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -837,10 +837,10 @@ def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional\n     @torch.no_grad()\n     def forward(\n         self,\n-        input_ids: torch.Tensor = None,\n-        inputs_embeds: torch.Tensor = None,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         past_key_values_length: int = 0,\n-        position_ids: torch.Tensor = None,\n+        position_ids: Optional[torch.Tensor] = None,\n     ):\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()\n@@ -1187,11 +1187,11 @@ def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_em\n     def forward_embedding(\n         self,\n         input_ids,\n-        inputs_embeds: torch.Tensor = None,\n-        image_embeds: torch.Tensor = None,\n-        img_input_mask: torch.Tensor = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        image_embeds: Optional[torch.Tensor] = None,\n+        img_input_mask: Optional[torch.Tensor] = None,\n         past_key_values_length: int = 0,\n-        position_ids: torch.Tensor = None,\n+        position_ids: Optional[torch.Tensor] = None,\n     ):\n         # The argument `inputs_embeds` should be the one without being multiplied by `self.embed_scale`.\n         if inputs_embeds is None:"
        },
        {
            "sha": "c17d0f1ec573d05027a71c32442f1ff04406ea63",
            "filename": "src/transformers/models/layoutlm/modeling_tf_layoutlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -127,11 +127,11 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        bbox: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        bbox: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\""
        },
        {
            "sha": "aa9c737bfaae18b34db73302b94871c99ae0b9d7",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -198,10 +198,10 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        apply_ocr: bool = None,\n+        apply_ocr: Optional[bool] = None,\n         ocr_lang: Optional[str] = None,\n         tesseract_config: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "d324c1ac7d6ef17c3a6218df29ea3fec93a40a57",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -561,7 +561,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -621,7 +621,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,\n@@ -675,7 +675,7 @@ def _batch_encode_plus(\n     def _batch_prepare_for_model(\n         self,\n         batch_text_or_text_pairs,\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[int]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "5d36e9fd2705faa126761a4075d4a8dc49c9a27c",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -312,7 +312,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -449,7 +449,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "246e9dcf1f1bf1c21fc87a402176809b8ad66065",
            "filename": "src/transformers/models/layoutlmv3/image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fimage_processing_layoutlmv3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -225,15 +225,15 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample=None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Union[float, Iterable[float]] = None,\n         image_std: Union[float, Iterable[float]] = None,\n-        apply_ocr: bool = None,\n+        apply_ocr: Optional[bool] = None,\n         ocr_lang: Optional[str] = None,\n         tesseract_config: Optional[str] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "4cdd15d5e46c72c374dacf58823670cc89a28282",
            "filename": "src/transformers/models/layoutlmv3/modeling_tf_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_tf_layoutlmv3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -231,7 +231,7 @@ def create_position_ids(self, input_ids: tf.Tensor, inputs_embeds: tf.Tensor) ->\n     def call(\n         self,\n         input_ids: tf.Tensor | None = None,\n-        bbox: tf.Tensor = None,\n+        bbox: Optional[tf.Tensor] = None,\n         token_type_ids: tf.Tensor | None = None,\n         position_ids: tf.Tensor | None = None,\n         inputs_embeds: tf.Tensor | None = None,"
        },
        {
            "sha": "b88f7b4c1b005863ffd2ca571a5c33e507b994ba",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -691,7 +691,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -752,7 +752,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,\n@@ -807,7 +807,7 @@ def _batch_encode_plus(\n     def _batch_prepare_for_model(\n         self,\n         batch_text_or_text_pairs,\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[int]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "737a50df9f221f479961a03fa8a470c3025d2dd9",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -357,7 +357,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -496,7 +496,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "f72039c884256f4a898bf832ce28f41627740a02",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -593,7 +593,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,\n@@ -647,7 +647,7 @@ def _batch_encode_plus(\n     def _batch_prepare_for_model(\n         self,\n         batch_text_or_text_pairs,\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[int]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "4c16642c57c6cd192b39e676a342e819093639fe",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -436,7 +436,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         boxes: Optional[List[List[List[int]]]] = None,\n         word_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "9f800753382e59800d80a96292ee0950b0c54b4b",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1247,7 +1247,7 @@ class LEDSeq2SeqModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1314,7 +1314,7 @@ class LEDSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1381,7 +1381,7 @@ class LEDSeq2SeqSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -1450,8 +1450,8 @@ class LEDSeq2SeqQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "25f7a2e5f526c9fdfeae2a2cb22bf863c4d39b7b",
            "filename": "src/transformers/models/led/modeling_tf_led.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_tf_led.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1470,7 +1470,7 @@ class TFLEDEncoderBaseModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -1533,7 +1533,7 @@ class TFLEDSeq2SeqModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor, ...] | None = None\n     decoder_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -1600,7 +1600,7 @@ class TFLEDSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     decoder_hidden_states: Tuple[tf.Tensor, ...] | None = None\n     decoder_attentions: Tuple[tf.Tensor, ...] | None = None"
        },
        {
            "sha": "9924ac25dea819bcc1137cb0917eb7aabef6be51",
            "filename": "src/transformers/models/levit/modeling_levit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flevit%2Fmodeling_levit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -68,9 +68,9 @@ class token).\n             plus the initial embedding outputs.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    cls_logits: torch.FloatTensor = None\n-    distillation_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    cls_logits: Optional[torch.FloatTensor] = None\n+    distillation_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -551,7 +551,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, BaseModelOutputWithPoolingAndNoAttention]:\n@@ -618,7 +618,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -710,7 +710,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        pixel_values: torch.FloatTensor = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[Tuple, LevitForImageClassificationWithTeacherOutput]:"
        },
        {
            "sha": "54a2981f6de1b694970d855c8b41f3a1a61bb120",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -522,7 +522,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -797,7 +797,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "37ef079c9187dc5261cc11127f706d8fdf69d643",
            "filename": "src/transformers/models/llava/image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -279,7 +279,7 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_pad: bool = None,\n+        do_pad: Optional[bool] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,\n         resample: Optional[PILImageResampling] = None,"
        },
        {
            "sha": "c1d075b6416e7517fb3c6025d3a4222b7802f7cd",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -78,7 +78,7 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -329,8 +329,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=LlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -344,7 +344,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "c212a549fcf25be5d495d12030c5662331dfd8ce",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -329,14 +329,14 @@ def pad(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n@@ -558,19 +558,19 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         image_grid_pinpoints: List = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: Optional[bool] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "06fc6bfedbb9af935d3e37a24934e0f74b4181a4",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -185,7 +185,7 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -542,8 +542,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=LlavaNextCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "139852324a5e17888236a69c125d5adf5c312eee",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -179,17 +179,17 @@ def resize(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> list[np.ndarray]:\n@@ -279,17 +279,17 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: VideoInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Optional[int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "bf30ff17c0dd586b371dca6e68de3406b3447f33",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -86,7 +86,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -581,9 +581,9 @@ def get_image_features(\n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "8168682ad7ec67ab4a40e8121628c3f715fc1239",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -340,9 +340,9 @@ def get_video_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "23e03483f2f052a4e8d9ae46397e8d60a1dcad6a",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -453,15 +453,15 @@ def _pad_for_batching(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Image.Image:\n@@ -528,17 +528,17 @@ def _preprocess(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         image_grid_pinpoints: List = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: Optional[bool] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "31d5b9edb6c53da94772166325b2db3c9a82f687",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -191,7 +191,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -601,10 +601,10 @@ def get_video_features(\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: torch.FloatTensor = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "14307470e43c22a9f3edef2f1d580f30ef133117",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -109,15 +109,15 @@ def __init__(\n     def _preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> list[np.ndarray]:\n@@ -200,15 +200,15 @@ def _preprocess(\n     def preprocess(\n         self,\n         videos: VideoInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "9eefa02a6687552c5eba8ac55ae07f58f7c6a86b",
            "filename": "src/transformers/models/longformer/modeling_longformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_longformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -128,7 +128,7 @@ class LongformerBaseModelOutputWithPooling(ModelOutput):\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor\n-    pooler_output: torch.FloatTensor = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -174,7 +174,7 @@ class LongformerMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -222,8 +222,8 @@ class LongformerQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -269,7 +269,7 @@ class LongformerSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -317,7 +317,7 @@ class LongformerMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -363,7 +363,7 @@ class LongformerTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     global_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "9280838de0b9da8b3880050825a178b14232e1fd",
            "filename": "src/transformers/models/longformer/modeling_tf_longformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongformer%2Fmodeling_tf_longformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -93,7 +93,7 @@ class TFLongformerBaseModelOutput(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -140,8 +140,8 @@ class TFLongformerBaseModelOutputWithPooling(ModelOutput):\n             in the sequence.\n     \"\"\"\n \n-    last_hidden_state: tf.Tensor = None\n-    pooler_output: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n+    pooler_output: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -187,7 +187,7 @@ class TFLongformerMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -235,8 +235,8 @@ class TFLongformerQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    start_logits: tf.Tensor = None\n-    end_logits: tf.Tensor = None\n+    start_logits: Optional[tf.Tensor] = None\n+    end_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -282,7 +282,7 @@ class TFLongformerSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -330,7 +330,7 @@ class TFLongformerMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None\n@@ -376,7 +376,7 @@ class TFLongformerTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n     global_attentions: Tuple[tf.Tensor, ...] | None = None"
        },
        {
            "sha": "7bc27bcd9c838006dd6ad9d4ca15d6c0767c5e0d",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 16,
            "deletions": 13,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -71,7 +71,7 @@ class BaseLukeModelOutputWithPooling(BaseModelOutputWithPooling):\n             compute the weighted average in the self-attention heads.\n     \"\"\"\n \n-    entity_last_hidden_state: torch.FloatTensor = None\n+    entity_last_hidden_state: Optional[torch.FloatTensor] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -102,7 +102,7 @@ class BaseLukeModelOutput(BaseModelOutput):\n             heads.\n     \"\"\"\n \n-    entity_last_hidden_state: torch.FloatTensor = None\n+    entity_last_hidden_state: Optional[torch.FloatTensor] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n@@ -142,8 +142,8 @@ class LukeMaskedLMOutput(ModelOutput):\n     loss: Optional[torch.FloatTensor] = None\n     mlm_loss: Optional[torch.FloatTensor] = None\n     mep_loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    entity_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    entity_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -174,7 +174,7 @@ class EntityClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -205,7 +205,7 @@ class EntityPairClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -236,7 +236,7 @@ class EntitySpanClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -270,7 +270,7 @@ class LukeSequenceClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -304,7 +304,7 @@ class LukeTokenClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -340,8 +340,8 @@ class LukeQuestionAnsweringModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -377,7 +377,7 @@ class LukeMultipleChoiceModelOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     entity_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -472,7 +472,10 @@ def __init__(self, config: LukeConfig):\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n     def forward(\n-        self, entity_ids: torch.LongTensor, position_ids: torch.LongTensor, token_type_ids: torch.LongTensor = None\n+        self,\n+        entity_ids: torch.LongTensor,\n+        position_ids: torch.LongTensor,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n     ):\n         if token_type_ids is None:\n             token_type_ids = torch.zeros_like(entity_ids)"
        },
        {
            "sha": "c4cbc2192200ef270ba18072eb43c976e497970d",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -143,7 +143,10 @@ def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional\n \n     @torch.no_grad()\n     def forward(\n-        self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        past_key_values_length: int = 0,\n     ):\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()"
        },
        {
            "sha": "affb6b8b670cbfdcae9ee2fad6c4d276025e281e",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -658,7 +658,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -824,7 +824,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1124,7 +1124,7 @@ def resize_decoder_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n     @replace_return_docstrings(output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1360,7 +1360,7 @@ def tie_weights(self):\n     @add_end_docstrings(MARIAN_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.Tensor] = None,\n@@ -1504,7 +1504,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "26ba704150dce852dde581c912bb00d8e40ea6c4",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -646,7 +646,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         xpaths: Optional[List[List[List[int]]]] = None,\n         node_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -706,7 +706,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         xpaths: Optional[List[List[List[int]]]] = None,\n         node_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,\n@@ -760,7 +760,7 @@ def _batch_encode_plus(\n     def _batch_prepare_for_model(\n         self,\n         batch_text_or_text_pairs,\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         xpaths: Optional[List[List[int]]] = None,\n         node_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "55d75e3541640d40beba79d618f0105752774f18",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -421,7 +421,7 @@ def batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         xpaths: Optional[List[List[List[int]]]] = None,\n         node_labels: Optional[Union[List[int], List[List[int]]]] = None,\n         add_special_tokens: bool = True,\n@@ -558,7 +558,7 @@ def _batch_encode_plus(\n             List[TextInputPair],\n             List[PreTokenizedInput],\n         ],\n-        is_pair: bool = None,\n+        is_pair: Optional[bool] = None,\n         xpaths: Optional[List[List[List[int]]]] = None,\n         node_labels: Optional[List[List[int]]] = None,\n         add_special_tokens: bool = True,"
        },
        {
            "sha": "37281afeca145349e2f241d635770d0b38047db4",
            "filename": "src/transformers/models/mask2former/configuration_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -160,7 +160,7 @@ def __init__(\n         init_xavier_std: float = 1.0,\n         use_auxiliary_loss: bool = True,\n         feature_strides: List[int] = [4, 8, 16, 32],\n-        output_auxiliary_logits: bool = None,\n+        output_auxiliary_logits: Optional[bool] = None,\n         backbone: Optional[str] = None,\n         use_pretrained_backbone: bool = False,\n         use_timm_backbone: bool = False,"
        },
        {
            "sha": "5c61431bf025b7632c5f2158ef805dd606e319f1",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -575,13 +575,13 @@ def __call__(self, images, segmentation_maps=None, **kwargs) -> BatchFeature:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -599,13 +599,13 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -641,7 +641,7 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: int = 0,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "e4fba109a071bb3a46a32fa476b8af6b98f7f211",
            "filename": "src/transformers/models/mask2former/modeling_mask2former.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fmodeling_mask2former.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -76,7 +76,7 @@ class Mask2FormerPixelDecoderOutput(ModelOutput):\n     \"\"\"\n \n     multi_scale_features: Tuple[torch.FloatTensor] = None\n-    mask_features: torch.FloatTensor = None\n+    mask_features: Optional[torch.FloatTensor] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -105,7 +105,7 @@ class Mask2FormerMaskedAttentionDecoderOutput(BaseModelOutputWithCrossAttentions\n             layernorm.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[torch.FloatTensor] = None\n     masks_queries_logits: Tuple[torch.FloatTensor] = None\n@@ -137,9 +137,9 @@ class Mask2FormerPixelLevelModuleOutput(ModelOutput):\n             called feature maps) of the model at the output of each stage.\n     \"\"\"\n \n-    encoder_last_hidden_state: torch.FloatTensor = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    decoder_last_hidden_state: torch.FloatTensor = None\n+    decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Tuple[torch.FloatTensor] = None\n \n \n@@ -178,9 +178,9 @@ class Mask2FormerModelOutput(ModelOutput):\n             sequence_length)`. Self attentions weights from transformer decoder.\n     \"\"\"\n \n-    encoder_last_hidden_state: torch.FloatTensor = None\n-    pixel_decoder_last_hidden_state: torch.FloatTensor = None\n-    transformer_decoder_last_hidden_state: torch.FloatTensor = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    pixel_decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    transformer_decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     transformer_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -234,12 +234,12 @@ class Mask2FormerForUniversalSegmentationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    class_queries_logits: torch.FloatTensor = None\n-    masks_queries_logits: torch.FloatTensor = None\n+    class_queries_logits: Optional[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[torch.FloatTensor] = None\n     auxiliary_logits: Optional[List[Dict[str, torch.FloatTensor]]] = None\n-    encoder_last_hidden_state: torch.FloatTensor = None\n-    pixel_decoder_last_hidden_state: torch.FloatTensor = None\n-    transformer_decoder_last_hidden_state: torch.FloatTensor = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    pixel_decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    transformer_decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     transformer_decoder_hidden_states: Optional[torch.FloatTensor] = None\n@@ -1004,7 +1004,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes_list=None,\n         level_start_index=None,\n@@ -1801,11 +1801,11 @@ def __init__(self, config: Mask2FormerConfig):\n \n     def forward(\n         self,\n-        inputs_embeds: torch.Tensor = None,\n-        multi_stage_positional_embeddings: torch.Tensor = None,\n-        pixel_embeddings: torch.Tensor = None,\n-        encoder_hidden_states: torch.Tensor = None,\n-        query_position_embeddings: torch.Tensor = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        multi_stage_positional_embeddings: Optional[torch.Tensor] = None,\n+        pixel_embeddings: Optional[torch.Tensor] = None,\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n+        query_position_embeddings: Optional[torch.Tensor] = None,\n         feature_size_list: List = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "b31d0321881a453f66db72a1f152a0446f7d4726",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -576,13 +576,13 @@ def __call__(self, images, segmentation_maps=None, **kwargs) -> BatchFeature:\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -600,13 +600,13 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -642,7 +642,7 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         size_divisor: int = 0,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "5c1873b4d6663209277ad56d147d1bf835548a8c",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -140,7 +140,7 @@ class MaskFormerPixelDecoderOutput(ModelOutput):\n             weighted average in the self-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -235,9 +235,9 @@ class MaskFormerForInstanceSegmentationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    class_queries_logits: torch.FloatTensor = None\n-    masks_queries_logits: torch.FloatTensor = None\n-    auxiliary_logits: torch.FloatTensor = None\n+    class_queries_logits: Optional[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[torch.FloatTensor] = None\n+    auxiliary_logits: Optional[torch.FloatTensor] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     pixel_decoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     transformer_decoder_last_hidden_state: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "dd3fc11ca1650efa248ed2a07fbae06765058e05",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -61,8 +61,8 @@ class MaskFormerSwinModelOutputWithPooling(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    pooler_output: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    pooler_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     hidden_states_spatial_dimensions: Tuple[Tuple[int, int]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -93,7 +93,7 @@ class MaskFormerSwinBaseModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     hidden_states_spatial_dimensions: Tuple[Tuple[int, int]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "850fde60d146e4405400fefc1e9a68dd152bc2a7",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -967,7 +967,7 @@ def _backward_compatibility_gradient_checkpointing(self):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1153,7 +1153,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1442,7 +1442,7 @@ def _tie_weights(self):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1570,7 +1570,7 @@ def set_output_embeddings(self, new_embeddings):\n     @add_end_docstrings(MBART_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1692,7 +1692,7 @@ def __init__(self, config: MBartConfig, **kwargs):\n     # Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1820,7 +1820,7 @@ def __init__(self, config):\n     # Copied from transformers.models.bart.modeling_bart.BartForQuestionAnswering.forward\n     def forward(\n         self,\n-        input_ids: torch.Tensor = None,\n+        input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1965,7 +1965,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "82f2202f47498a9bf5dade99ebf8a088f8f3fbd0",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -751,8 +751,8 @@ class MegatronBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "858844ad7d211ba94660c1fc53bad0647026767d",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -75,8 +75,8 @@ class MimiOutput(ModelOutput):\n             have their past key value states given to this model).\n     \"\"\"\n \n-    audio_codes: torch.LongTensor = None\n-    audio_values: torch.FloatTensor = None\n+    audio_codes: Optional[torch.LongTensor] = None\n+    audio_values: Optional[torch.FloatTensor] = None\n     encoder_past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None\n     decoder_past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None\n \n@@ -97,7 +97,7 @@ class MimiEncoderOutput(ModelOutput):\n             have their past key value states given to this model).\n     \"\"\"\n \n-    audio_codes: torch.LongTensor = None\n+    audio_codes: Optional[torch.LongTensor] = None\n     encoder_past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None\n \n \n@@ -117,7 +117,7 @@ class MimiDecoderOutput(ModelOutput):\n             have their past key value states given to this model).\n     \"\"\"\n \n-    audio_values: torch.FloatTensor = None\n+    audio_values: Optional[torch.FloatTensor] = None\n     decoder_past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None\n \n \n@@ -897,7 +897,7 @@ def __init__(self, config: MimiConfig):\n \n     def forward(\n         self,\n-        hidden_states: torch.LongTensor = None,\n+        hidden_states: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1599,7 +1599,7 @@ def _encode_frame(\n     def encode(\n         self,\n         input_values: torch.Tensor,\n-        padding_mask: torch.Tensor = None,\n+        padding_mask: Optional[torch.Tensor] = None,\n         num_quantizers: Optional[float] = None,\n         encoder_past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "bd7d38da284fa85e45640aed5281e7f83821a030",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -487,7 +487,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -786,7 +786,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "e27453249b99f613d2bc35c0eca00385c728d84e",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -528,7 +528,7 @@ def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_em\n     @unpack_inputs\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: Optional[tf.Tensor] = None,\n         position_ids: Optional[tf.Tensor] = None,\n         past_key_values: Optional[List[tf.Tensor]] = None,\n@@ -770,7 +770,7 @@ def __init__(self, config: MistralConfig, *inputs, **kwargs):\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING)\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: Optional[tf.Tensor] = None,\n         position_ids: Optional[tf.Tensor] = None,\n         past_key_values: Optional[List[tf.Tensor]] = None,\n@@ -837,7 +837,7 @@ def get_decoder(self):\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: Optional[tf.Tensor] = None,\n         position_ids: Optional[tf.Tensor] = None,\n         past_key_values: Optional[List[tf.Tensor]] = None,\n@@ -962,7 +962,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n         attention_mask: Optional[tf.Tensor] = None,\n         position_ids: Optional[tf.Tensor] = None,\n         past_key_values: Optional[List[tf.Tensor]] = None,"
        },
        {
            "sha": "8ef132846624a600f55d0a7ac38d3a65d5a6f84a",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -160,7 +160,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -379,8 +379,8 @@ def get_image_features(\n     @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -393,7 +393,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "3793bef1831a5284d30b4b97ea3568b98089e0c4",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -139,8 +139,8 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -153,7 +153,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: torch.Tensor = None,\n+        image_sizes: Optional[torch.Tensor] = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\""
        },
        {
            "sha": "013f04ab36b69f389909435151b08b8c36711572",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -609,7 +609,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MIXTRAL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1000,7 +1000,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "d94b581477b58c077924b2aa70983daf783b220f",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -333,7 +333,7 @@ def __init__(self, config: MixtralConfig):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -461,7 +461,7 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "763ac051f9448453d14f8f61e221990b5d201e34",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -197,7 +197,7 @@ def forward(\n         self,\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = None,\n+        output_attentions: Optional[bool] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         query = self.q_proj(hidden_state)\n         key = self.k_proj(hidden_state)\n@@ -237,7 +237,7 @@ def forward(\n         self,\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = None,\n+        output_attentions: Optional[bool] = None,\n     ) -> torch.Tensor:\n         # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n         if output_attentions:\n@@ -302,7 +302,7 @@ def forward(\n         self,\n         hidden_state: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = None,\n+        output_attentions: Optional[bool] = None,\n     ):\n         # Self Attention\n         residual = hidden_state\n@@ -469,7 +469,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        use_cache: bool = None,\n+        use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -537,7 +537,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        use_cache: bool = None,\n+        use_cache: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n@@ -1894,7 +1894,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaTextConfig\")\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         cross_attention_states: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "9801e19ac3ed1c9a172b0c40aeef0d117e0b1966",
            "filename": "src/transformers/models/mobilebert/modeling_mobilebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_mobilebert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -734,8 +734,8 @@ class MobileBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n-    seq_relationship_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n+    seq_relationship_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "e85c079025a4f1b23778aacbfc448eccf267ad94",
            "filename": "src/transformers/models/mobilebert/modeling_tf_mobilebert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Fmodeling_tf_mobilebert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1063,8 +1063,8 @@ class TFMobileBertForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    prediction_logits: tf.Tensor = None\n-    seq_relationship_logits: tf.Tensor = None\n+    prediction_logits: Optional[tf.Tensor] = None\n+    seq_relationship_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "6c30c3413baa58b34a50308725b02acfb6d8ac01",
            "filename": "src/transformers/models/mobilenet_v1/image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v1%2Fimage_processing_mobilenet_v1.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -171,7 +171,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "0107e96402cb6ded4ce951209aadeef2d8035a47",
            "filename": "src/transformers/models/mobilenet_v2/image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilenet_v2%2Fimage_processing_mobilenet_v2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -175,7 +175,7 @@ def preprocess(\n         do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,"
        },
        {
            "sha": "f59c246627c3b48bc2c9d68bc8218f952f548d2b",
            "filename": "src/transformers/models/mobilevit/image_processing_mobilevit.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fimage_processing_mobilevit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -220,14 +220,14 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_center_crop: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_flip_channel_order: bool = None,\n+        do_flip_channel_order: Optional[bool] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -262,9 +262,9 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:\n@@ -302,14 +302,14 @@ def preprocess(\n         self,\n         images: ImageInput,\n         segmentation_maps: Optional[ImageInput] = None,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_center_crop: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_flip_channel_order: bool = None,\n+        do_flip_channel_order: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "3960b17f4fa5017fdefc80e4f251bc50d518c64f",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -214,7 +214,7 @@ def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n         return self.drop(self.norm(self.tok_embeddings(input_ids)))\n \n     def forward(\n-        self, input_ids: torch.LongTensor = None, inputs_embeds: Optional[torch.Tensor] = None\n+        self, input_ids: Optional[torch.LongTensor] = None, inputs_embeds: Optional[torch.Tensor] = None\n     ) -> torch.Tensor:\n         if inputs_embeds is not None:\n             hidden_states = self.drop(self.norm(inputs_embeds))"
        },
        {
            "sha": "932d1025987f66d3010e1ec6b0534778c2c4dd6f",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -477,7 +477,7 @@ def compiled_embeddings(self, input_ids: torch.LongTensor) -> torch.Tensor:\n         return self.drop(self.norm(self.tok_embeddings(input_ids)))\n \n     def forward(\n-        self, input_ids: torch.LongTensor = None, inputs_embeds: Optional[torch.Tensor] = None\n+        self, input_ids: Optional[torch.LongTensor] = None, inputs_embeds: Optional[torch.Tensor] = None\n     ) -> torch.Tensor:\n         if inputs_embeds is not None:\n             hidden_states = self.drop(self.norm(inputs_embeds))"
        },
        {
            "sha": "09c48edbf2441e714d79baf8467a343f34207f4d",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -841,7 +841,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MOONSHINE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "9d6f2c52c50fac520e8cb647bd79052612800953",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -734,7 +734,7 @@ def __init__(self, config: MoonshineConfig):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "fcf49f7fc79365c0ef31b73e822461ff7d757c8b",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -100,7 +100,7 @@ class MoshiConditionalGenerationGenerateOutput(ModelOutput):\n     \"\"\"\n \n     audio_sequences: Optional[torch.Tensor] = None\n-    sequences: torch.LongTensor = None\n+    sequences: Optional[torch.LongTensor] = None\n     sequences_scores: Optional[torch.FloatTensor] = None\n     scores: Optional[Tuple[torch.FloatTensor]] = None\n     logits: Optional[Tuple[torch.FloatTensor]] = None\n@@ -143,8 +143,8 @@ class MoshiCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -193,13 +193,13 @@ class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     depth_loss: Optional[torch.FloatTensor] = None\n-    audio_logits: torch.FloatTensor = None\n+    audio_logits: Optional[torch.FloatTensor] = None\n     depth_past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     depth_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     depth_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -220,10 +220,10 @@ class MoshiUnconditionalInput(ModelOutput):\n             1]`: 1 for tokens that are **not masked**, 0 for tokens that are **masked**.\n     \"\"\"\n \n-    input_ids: torch.LongTensor = None\n-    user_audio_codes: torch.Tensor = None\n-    moshi_audio_codes: torch.Tensor = None\n-    attention_mask: torch.LongTensor = None\n+    input_ids: Optional[torch.LongTensor] = None\n+    user_audio_codes: Optional[torch.Tensor] = None\n+    moshi_audio_codes: Optional[torch.Tensor] = None\n+    attention_mask: Optional[torch.LongTensor] = None\n \n \n # Copied from transformers.models.gemma.modeling_gemma.GemmaRMSNorm with Gemma->Moshi\n@@ -1091,7 +1091,7 @@ def __init__(self, config: MoshiDepthConfig):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        last_hidden_state: torch.LongTensor = None,\n+        last_hidden_state: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.BoolTensor] = None,\n         past_key_values: Tuple[Tuple[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -1484,7 +1484,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MOSHI_DECODER_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1799,7 +1799,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoshiCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "f8fbc8e343780e399fa411dd776f63367f08059a",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -2183,7 +2183,7 @@ def __init__(self, config: MT5Config):\n     # Copied from transformers.models.t5.modeling_t5.T5ForSequenceClassification.forward\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "151c4e89f3bff928a231b3bb6b78b9c874cb29ea",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -87,8 +87,8 @@ class MusicgenUnconditionalInput(ModelOutput):\n     \"\"\"\n \n     encoder_outputs: Tuple[torch.FloatTensor] = None\n-    attention_mask: torch.LongTensor = None\n-    guidance_scale: float = None\n+    attention_mask: Optional[torch.LongTensor] = None\n+    guidance_scale: Optional[float] = None\n \n \n def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n@@ -966,7 +966,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(MUSICGEN_DECODER_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1162,7 +1162,7 @@ def get_decoder(self):\n     @add_start_docstrings_to_model_forward(MUSICGEN_DECODER_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1250,7 +1250,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "70b914313c89bd3aa4122bbacd191bfa5502aeb1",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -97,7 +97,7 @@ class MusicgenMelodyOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -899,7 +899,7 @@ def set_input_embeddings(self, value):\n     # Ignore copy\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1075,7 +1075,7 @@ def get_decoder(self):\n     # Ignore copy\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1162,7 +1162,7 @@ def get_decoder(self):\n     # Ignore copy\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "2dec79729c37e32bee8e0d6c9279c502a1044525",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -790,7 +790,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -987,7 +987,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1263,7 +1263,7 @@ def set_lightweight_tuning(self):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1401,7 +1401,7 @@ def set_lightweight_tuning(self):\n     @add_end_docstrings(MVP_CONDITIONAL_GENERATION_EXAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1523,7 +1523,7 @@ def set_lightweight_tuning(self):\n     @add_end_docstrings(MVP_SEQUENCE_CLASSIFICATION_SAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1649,7 +1649,7 @@ def set_lightweight_tuning(self):\n     @add_end_docstrings(MVP_QUESTION_ANSWERING_SAMPLE)\n     def forward(\n         self,\n-        input_ids: torch.Tensor = None,\n+        input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1797,7 +1797,7 @@ def set_lightweight_tuning(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "cc1a22ab23bf31eb0f261544d134d7e423a4cc8b",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -767,7 +767,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(NEMOTRON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1044,7 +1044,7 @@ def get_decoder(self):\n     # Ignore copy (doc string different)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,"
        },
        {
            "sha": "8a2e735f9fe26bd9531dfe264ffd1a0108a933aa",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -188,7 +188,10 @@ def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional\n \n     @torch.no_grad()\n     def forward(\n-        self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        past_key_values_length: int = 0,\n     ):\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()"
        },
        {
            "sha": "d5251d4ff12c4979645d60ec5834c7fdfea23d0b",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -360,16 +360,16 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_crop_margin: bool = None,\n-        do_resize: bool = None,\n+        do_crop_margin: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_thumbnail: bool = None,\n-        do_align_long_axis: bool = None,\n-        do_pad: bool = None,\n-        do_rescale: bool = None,\n+        do_thumbnail: Optional[bool] = None,\n+        do_align_long_axis: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        do_rescale: Optional[bool] = None,\n         rescale_factor: Union[int, float] = None,\n-        do_normalize: bool = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "ca395e261affe7ba5dbfc7ac22320e299cb78170",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -50,16 +50,16 @@ def __call__(\n         self,\n         images=None,\n         text=None,\n-        do_crop_margin: bool = None,\n-        do_resize: bool = None,\n+        do_crop_margin: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: \"PILImageResampling\" = None,  # noqa: F821\n-        do_thumbnail: bool = None,\n-        do_align_long_axis: bool = None,\n-        do_pad: bool = None,\n-        do_rescale: bool = None,\n+        do_thumbnail: Optional[bool] = None,\n+        do_align_long_axis: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        do_rescale: Optional[bool] = None,\n         rescale_factor: Union[int, float] = None,\n-        do_normalize: bool = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[\"ChannelDimension\"] = \"channels_first\",  # noqa: F821"
        },
        {
            "sha": "d6b772a1aff5f211c98cbb0797b9a4a2b400cc9b",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -498,7 +498,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(OLMO_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -773,7 +773,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "4cb4a9fe72740d864998ce15cdc10c4c6ede7f69",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -499,7 +499,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(OLMO2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -774,7 +774,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "a557530663e82416127ae83c221f2bbe1a34dd61",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -898,7 +898,7 @@ def set_input_embeddings(self, value):\n     # Ignore copy\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n@@ -1191,7 +1191,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "67143cff68c7daacd18bd0299231cef215ec23f1",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -68,7 +68,7 @@ class OmDetTurboEncoderOutput(ModelOutput):\n             The extracted states from the Feature Pyramid Network (FPN) and Path Aggregation Network (PAN) of the encoder.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     extracted_states: Tuple[torch.FloatTensor] = None\n@@ -104,14 +104,14 @@ class OmDetTurboDecoderOutput(ModelOutput):\n             weighted average in the self-attention, cross-attention and multi-scale deformable attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    decoder_coords: torch.FloatTensor = None\n-    decoder_classes: torch.FloatTensor = None\n-    encoder_coord_logits: torch.FloatTensor = None\n+    decoder_coords: Optional[torch.FloatTensor] = None\n+    decoder_classes: Optional[torch.FloatTensor] = None\n+    encoder_coord_logits: Optional[torch.FloatTensor] = None\n     encoder_class_logits: Tuple[torch.FloatTensor] = None\n-    init_reference_points: torch.FloatTensor = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Tuple[Tuple[torch.FloatTensor]] = None\n \n \n@@ -157,14 +157,14 @@ class OmDetTurboObjectDetectionOutput(ModelOutput):\n             The number of queried classes for each image.\n     \"\"\"\n \n-    loss: torch.FloatTensor = None\n-    decoder_coord_logits: torch.FloatTensor = None\n-    decoder_class_logits: torch.FloatTensor = None\n-    init_reference_points: torch.FloatTensor = None\n+    loss: Optional[torch.FloatTensor] = None\n+    decoder_coord_logits: Optional[torch.FloatTensor] = None\n+    decoder_class_logits: Optional[torch.FloatTensor] = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n-    encoder_coord_logits: torch.FloatTensor = None\n+    encoder_coord_logits: Optional[torch.FloatTensor] = None\n     encoder_class_logits: Tuple[torch.FloatTensor] = None\n-    encoder_extracted_states: torch.FloatTensor = None\n+    encoder_extracted_states: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -579,7 +579,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n     ):\n         \"\"\""
        },
        {
            "sha": "956bd3e7e2fa7af482e79ec8d785b853757a1f1e",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -582,12 +582,12 @@ def __call__(self, images, task_inputs=None, segmentation_maps=None, **kwargs) -\n     def _preprocess(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -603,12 +603,12 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -643,7 +643,7 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:"
        },
        {
            "sha": "7c3ecb3611b3cfce06f477815569e9b86458b8b2",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -358,7 +358,7 @@ def __init__(\n         num_points: int,\n         oversample_ratio: float,\n         importance_sample_ratio: float,\n-        contrastive_temperature: float = None,\n+        contrastive_temperature: Optional[float] = None,\n     ):\n         \"\"\"\n         This class computes the losses using the class predictions, mask predictions and the contrastive queries.\n@@ -754,10 +754,10 @@ class OneFormerTransformerDecoderOutput(BaseModelOutput):\n             Tuple of class and mask predictions from each layer of the transformer decoder.\n     \"\"\"\n \n-    object_queries: torch.FloatTensor = None\n+    object_queries: Optional[torch.FloatTensor] = None\n     contrastive_logits: Optional[torch.FloatTensor] = None\n-    prediction_masks: torch.FloatTensor = None\n-    prediction_class: torch.FloatTensor = None\n+    prediction_masks: Optional[torch.FloatTensor] = None\n+    prediction_class: Optional[torch.FloatTensor] = None\n     auxiliary_predictions: Optional[Tuple[Dict[str, torch.FloatTensor]]] = None\n \n \n@@ -782,7 +782,7 @@ class OneFormerPixelDecoderOutput(ModelOutput):\n     \"\"\"\n \n     multi_scale_features: Tuple[torch.FloatTensor] = None\n-    mask_features: torch.FloatTensor = None\n+    mask_features: Optional[torch.FloatTensor] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -806,7 +806,7 @@ class OneFormerPixelLevelModuleOutput(ModelOutput):\n \n     encoder_features: List[torch.FloatTensor] = None\n     decoder_features: List[torch.FloatTensor] = None\n-    decoder_last_feature: torch.FloatTensor = None\n+    decoder_last_feature: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -849,13 +849,13 @@ class OneFormerModelOutput(ModelOutput):\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     pixel_decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     transformer_decoder_hidden_states: Optional[torch.FloatTensor] = None\n-    transformer_decoder_object_queries: torch.FloatTensor = None\n+    transformer_decoder_object_queries: Optional[torch.FloatTensor] = None\n     transformer_decoder_contrastive_queries: Optional[torch.FloatTensor] = None\n-    transformer_decoder_mask_predictions: torch.FloatTensor = None\n-    transformer_decoder_class_predictions: torch.FloatTensor = None\n+    transformer_decoder_mask_predictions: Optional[torch.FloatTensor] = None\n+    transformer_decoder_class_predictions: Optional[torch.FloatTensor] = None\n     transformer_decoder_auxiliary_predictions: Optional[Tuple[Dict[str, torch.FloatTensor]]] = None\n     text_queries: Optional[torch.FloatTensor] = None\n-    task_token: torch.FloatTensor = None\n+    task_token: Optional[torch.FloatTensor] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -912,19 +912,19 @@ class OneFormerForUniversalSegmentationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    class_queries_logits: torch.FloatTensor = None\n-    masks_queries_logits: torch.FloatTensor = None\n+    class_queries_logits: Optional[torch.FloatTensor] = None\n+    masks_queries_logits: Optional[torch.FloatTensor] = None\n     auxiliary_predictions: List[Dict[str, torch.FloatTensor]] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     pixel_decoder_hidden_states: Optional[List[torch.FloatTensor]] = None\n     transformer_decoder_hidden_states: Optional[torch.FloatTensor] = None\n-    transformer_decoder_object_queries: torch.FloatTensor = None\n+    transformer_decoder_object_queries: Optional[torch.FloatTensor] = None\n     transformer_decoder_contrastive_queries: Optional[torch.FloatTensor] = None\n-    transformer_decoder_mask_predictions: torch.FloatTensor = None\n-    transformer_decoder_class_predictions: torch.FloatTensor = None\n+    transformer_decoder_mask_predictions: Optional[torch.FloatTensor] = None\n+    transformer_decoder_class_predictions: Optional[torch.FloatTensor] = None\n     transformer_decoder_auxiliary_predictions: Optional[List[Dict[str, torch.FloatTensor]]] = None\n     text_queries: Optional[torch.FloatTensor] = None\n-    task_token: torch.FloatTensor = None\n+    task_token: Optional[torch.FloatTensor] = None\n     attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n \n \n@@ -1085,7 +1085,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n         level_start_index=None,\n@@ -2609,7 +2609,7 @@ def __init__(\n         width: int,\n         layers: int,\n         heads: int,\n-        attn_mask: torch.Tensor = None,\n+        attn_mask: Optional[torch.Tensor] = None,\n         use_checkpoint=False,\n         layer_norm_eps=1e-05,\n     ):"
        },
        {
            "sha": "595d3e93735c153f1088a427bdae432d44f3a739",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -318,8 +318,8 @@ class OpenAIGPTDoubleHeadsModelOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     mc_loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    mc_logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    mc_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "3856711d10625495db9591736306b9f689898182",
            "filename": "src/transformers/models/openai/modeling_tf_openai.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_tf_openai.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -429,8 +429,8 @@ class TFOpenAIGPTDoubleHeadsModelOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n-    mc_logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n+    mc_logits: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor] | None = None\n     attentions: Tuple[tf.Tensor] | None = None\n "
        },
        {
            "sha": "01639f5b66fbc400a7f5e65b5aff5a88db3cdde6",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -767,7 +767,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1007,7 +1007,7 @@ def get_decoder(self):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1086,7 +1086,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "56c8075ef0ee085dd81412284d9bb50d65f2b15b",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -369,12 +369,12 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_pad: bool = None,\n-        do_resize: bool = None,\n+        do_pad: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "7d83ab0004d878e7488d636c0f0d55cb9ede6f79",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -85,10 +85,10 @@ class Owlv2Output(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -205,12 +205,12 @@ class Owlv2ObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    objectness_logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n-    class_embeds: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    objectness_logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    class_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -255,12 +255,12 @@ class Owlv2ImageGuidedObjectDetectionOutput(ModelOutput):\n             The output of the [`Owlv2VisionModel`].\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n-    query_image_embeds: torch.FloatTensor = None\n-    target_pred_boxes: torch.FloatTensor = None\n-    query_pred_boxes: torch.FloatTensor = None\n-    class_embeds: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    query_image_embeds: Optional[torch.FloatTensor] = None\n+    target_pred_boxes: Optional[torch.FloatTensor] = None\n+    query_pred_boxes: Optional[torch.FloatTensor] = None\n+    class_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "2eb0114cf11f9cc1cd9d0caa7302cf392baa87a1",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -85,10 +85,10 @@ class OwlViTOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -202,11 +202,11 @@ class OwlViTObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n-    class_embeds: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    class_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n \n@@ -250,12 +250,12 @@ class OwlViTImageGuidedObjectDetectionOutput(ModelOutput):\n             The output of the [`OwlViTVisionModel`].\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n-    query_image_embeds: torch.FloatTensor = None\n-    target_pred_boxes: torch.FloatTensor = None\n-    query_pred_boxes: torch.FloatTensor = None\n-    class_embeds: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    query_image_embeds: Optional[torch.FloatTensor] = None\n+    target_pred_boxes: Optional[torch.FloatTensor] = None\n+    query_pred_boxes: Optional[torch.FloatTensor] = None\n+    class_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "ef92378e0b0e78a0a2d54c43c774be4862d85a48",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -55,7 +55,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     cache_position: torch.Tensor,\n     batch_size: int,\n     is_training: bool = False,\n-    token_type_ids: torch.Tensor = None,\n+    token_type_ids: Optional[torch.Tensor] = None,\n     **kwargs,\n ):\n     \"\"\"\n@@ -145,7 +145,7 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -339,7 +339,7 @@ def _update_causal_mask(\n         past_key_values=None,\n         cache_position=None,\n         input_tensor=None,\n-        is_training: bool = None,\n+        is_training: Optional[bool] = None,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n@@ -421,8 +421,8 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n     @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,"
        },
        {
            "sha": "ca88a84a399ef11729898ed22c5fc884e9c23e70",
            "filename": "src/transformers/models/patchtsmixer/modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 18,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtsmixer%2Fmodeling_patchtsmixer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1153,7 +1153,7 @@ def __init__(self, config: PatchTSMixerConfig):\n         self.keepdim = config.keepdim if hasattr(config, \"keepdim\") else True\n \n     def forward(\n-        self, data: torch.Tensor, observed_indicator: torch.Tensor = None\n+        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n@@ -1181,7 +1181,7 @@ class PatchTSMixerEncoderOutput(ModelOutput):\n             Hidden-states of the model at the output of each layer.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -1283,9 +1283,9 @@ class PatchTSMixerModelOutput(ModelOutput):\n             enabled.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    patch_input: torch.FloatTensor = None\n+    patch_input: Optional[torch.FloatTensor] = None\n     mask: Optional[torch.FloatTensor] = None\n     loc: Optional[torch.FloatTensor] = None\n     scale: Optional[torch.FloatTensor] = None\n@@ -1402,8 +1402,8 @@ class PatchTSMixerForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_outputs: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    prediction_outputs: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -1521,11 +1521,11 @@ class PatchTSMixerForPredictionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_outputs: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    prediction_outputs: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    loc: torch.FloatTensor = None\n-    scale: torch.FloatTensor = None\n+    loc: Optional[torch.FloatTensor] = None\n+    scale: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -1539,7 +1539,7 @@ class SamplePatchTSMixerPredictionOutput(ModelOutput):\n             Sampled values from the chosen distribution.\n     \"\"\"\n \n-    sequences: torch.FloatTensor = None\n+    sequences: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -1553,7 +1553,7 @@ class SamplePatchTSMixerRegressionOutput(ModelOutput):\n                 Sampled values from the chosen distribution.\n     \"\"\"\n \n-    sequences: torch.FloatTensor = None\n+    sequences: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll\n@@ -1817,8 +1817,8 @@ class PatchTSMixerForTimeSeriesClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_outputs: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    prediction_outputs: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -1859,7 +1859,7 @@ def __init__(self, config: PatchTSMixerConfig):\n     def forward(\n         self,\n         past_values: torch.Tensor,\n-        target_values: torch.Tensor = None,\n+        target_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,\n@@ -1948,8 +1948,8 @@ class PatchTSMixerForRegressionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    regression_outputs: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    regression_outputs: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -2049,7 +2049,7 @@ def __init__(self, config: PatchTSMixerConfig):\n     def forward(\n         self,\n         past_values: torch.Tensor,\n-        target_values: torch.Tensor = None,\n+        target_values: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = False,\n         return_loss: bool = True,\n         return_dict: Optional[bool] = None,"
        },
        {
            "sha": "ae09d410ace06cd8ea0ebb95e0cf87ecab0b2a77",
            "filename": "src/transformers/models/patchtst/modeling_patchtst.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpatchtst%2Fmodeling_patchtst.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -813,13 +813,13 @@ class PatchTSTModelOutput(ModelOutput):\n             Patched input to the Transformer\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    mask: torch.FloatTensor = None\n-    loc: torch.FloatTensor = None\n-    scale: torch.FloatTensor = None\n-    patch_input: torch.FloatTensor = None\n+    mask: Optional[torch.FloatTensor] = None\n+    loc: Optional[torch.FloatTensor] = None\n+    scale: Optional[torch.FloatTensor] = None\n+    patch_input: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -846,7 +846,7 @@ class PatchTSTForPretrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_output: torch.FloatTensor = None\n+    prediction_output: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -875,7 +875,7 @@ class PatchTSTForRegressionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    regression_outputs: torch.FloatTensor = None\n+    regression_outputs: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -908,11 +908,11 @@ class PatchTSTForPredictionOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_outputs: torch.FloatTensor = None\n+    prediction_outputs: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    loc: torch.FloatTensor = None\n-    scale: torch.FloatTensor = None\n+    loc: Optional[torch.FloatTensor] = None\n+    scale: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -940,7 +940,7 @@ class PatchTSTForClassificationOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    prediction_logits: torch.FloatTensor = None\n+    prediction_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n@@ -956,7 +956,7 @@ class SamplePatchTSTOutput(ModelOutput):\n                 Sampled values from the chosen distribution.\n     \"\"\"\n \n-    sequences: torch.FloatTensor = None\n+    sequences: Optional[torch.FloatTensor] = None\n \n \n # Copied from transformers.models.time_series_transformer.modeling_time_series_transformer.nll\n@@ -1095,7 +1095,7 @@ def __init__(self, config: PatchTSTConfig):\n         self.keepdim = config.keepdim if hasattr(config, \"keepdim\") else True\n \n     def forward(\n-        self, data: torch.Tensor, observed_indicator: torch.Tensor = None\n+        self, data: torch.Tensor, observed_indicator: Optional[torch.Tensor] = None\n     ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Parameters:\n@@ -1457,7 +1457,7 @@ def __init__(self, config: PatchTSTConfig):\n     def forward(\n         self,\n         past_values: torch.Tensor,\n-        target_values: torch.Tensor = None,\n+        target_values: Optional[torch.Tensor] = None,\n         past_observed_mask: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1910,7 +1910,7 @@ def __init__(self, config: PatchTSTConfig):\n     def forward(\n         self,\n         past_values: torch.Tensor,\n-        target_values: torch.Tensor = None,\n+        target_values: Optional[torch.Tensor] = None,\n         past_observed_mask: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "b4c5a0acdcd8b8394459b244e845be7b53f63485",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1478,7 +1478,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n     # Copied from transformers.models.bart.modeling_bart.BartForCausalLM.forward with Bart->Pegasus, facebook/bart-base->google/pegasus-large\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "047fd5b6bafecb2f1b8908b597042aaa682fc756",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -77,8 +77,8 @@ class PerceiverModelOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    last_hidden_state: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -98,7 +98,7 @@ class PerceiverDecoderOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n \n \n@@ -127,7 +127,7 @@ class PerceiverMaskedLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -159,7 +159,7 @@ class PerceiverClassifierOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -2862,7 +2862,7 @@ def forward(\n         batch_size: int,\n         device: torch.device,\n         dtype: torch.dtype,\n-        pos: torch.FloatTensor = None,\n+        pos: Optional[torch.FloatTensor] = None,\n     ) -> torch.FloatTensor:\n         pos = _check_or_build_spatial_positions(pos, index_dims, batch_size)\n         fourier_pos_enc = generate_fourier_features("
        },
        {
            "sha": "111ca314339ab84a304600c522ff67e490d3fcae",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -557,7 +557,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(PERSIMMON_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -849,7 +849,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "1bd445366e8713898d3c6fc274855be418e55222",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -495,7 +495,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(PHI_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -767,7 +767,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "cb9b15833822419843b5c6c69de3ce8d23ef7038",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -181,7 +181,7 @@ def __init__(self, config: PhiConfig):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "24e86bca14799755ba14043abc8aa40bf9546907",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -560,7 +560,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -859,7 +859,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "ed38b65ef7d3b9bdd5f3b8e4a162fd7fe673b3d6",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1842,7 +1842,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -2149,7 +2149,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=Phi4MultimodalConfig)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "901cfa27b063ff3dabc52208daf5f3badf780b91",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1549,7 +1549,7 @@ def __init__(self, config: Phi4MultimodalConfig):\n     @add_start_docstrings_to_model_forward(PHI4_MULTIMODAL_MODEL_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1684,7 +1684,7 @@ def __init__(self, config):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=Phi4MultimodalConfig)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "9cbfe776bbd14c613dc4189e74ac13dbcd0eb83b",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1036,7 +1036,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(PHIMOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1366,7 +1366,7 @@ def get_decoder(self):\n     # Ignore copy\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "e9db5175b2ebd3a11b48790a8714677c30743756",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -349,7 +349,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         header_text: Optional[str] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         do_normalize: Optional[bool] = None,\n         max_patches: Optional[int] = None,\n         patch_size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "074a8d1076a5112796be5eece74c824f6c95811c",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -319,16 +319,16 @@ def _pad_for_batching(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         patch_size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "e625c20251e87d6817b2879966bbec00bf027632",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -701,7 +701,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -887,7 +887,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1421,7 +1421,7 @@ def __init__(self, config: PLBartConfig, **kwargs):\n     # Copied from transformers.models.bart.modeling_bart.BartForSequenceClassification.forward\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -1570,7 +1570,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "cd4c4bb77086a53ef7583d2569c24ff62c79cb20",
            "filename": "src/transformers/models/poolformer/image_processing_poolformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpoolformer%2Fimage_processing_poolformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -213,15 +213,15 @@ def resize(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         crop_pct: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_center_crop: bool = None,\n+        do_center_crop: Optional[bool] = None,\n         crop_size: Dict[str, int] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,"
        },
        {
            "sha": "73ff1b9d7b9653b20a2234be277367de7689c271",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1214,7 +1214,7 @@ def get_mel_conditioner_outputs(\n         input_features: torch.FloatTensor,\n         composer: str,\n         generation_config: GenerationConfig,\n-        attention_mask: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n     ):\n         \"\"\"\n         This method is used to concatenate mel conditioner tokens at the front of the input_features in order to"
        },
        {
            "sha": "678a651fee4288068a10e5587780f7567f45c9ce",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -245,7 +245,9 @@ def relative_batch_tokens_ids_to_midi(\n \n     # Taken from the original code\n     # Please see https://github.com/sweetcocoa/pop2piano/blob/fac11e8dcfc73487513f4588e8d0c22a22f2fdc5/midi_tokenizer.py#L257\n-    def relative_tokens_ids_to_notes(self, tokens: np.ndarray, start_idx: float, cutoff_time_idx: float = None):\n+    def relative_tokens_ids_to_notes(\n+        self, tokens: np.ndarray, start_idx: float, cutoff_time_idx: Optional[float] = None\n+    ):\n         \"\"\"\n         Converts relative tokens to notes which will then be used to create Pretty Midi objects.\n "
        },
        {
            "sha": "ddb72211b0e64695820914c42a63a548ceb9a74a",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -308,7 +308,7 @@ class ProphetNetSeq2SeqLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n@@ -528,7 +528,7 @@ class ProphetNetDecoderLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     logits_ngram: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "586d80437203beec247b1493a21ab4d5b6fb014e",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -500,7 +500,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(QWEN2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -799,7 +799,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "21a7b710cf69119c695faf9d1298e07fb7183843",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1114,7 +1114,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1414,7 +1414,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1712,7 +1712,7 @@ def get_rope_index(\n     @replace_return_docstrings(output_type=Qwen2_5_VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "925d59df2fa1f542b2940ecf5714dddb0a294b19",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -585,7 +585,7 @@ def get_rope_index(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "e375272af926dc854f478df916d2a1eec9c1619d",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -83,7 +83,7 @@ class Qwen2AudioCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Cache] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1020,8 +1020,8 @@ def _merge_input_ids_with_audio_features(\n     @replace_return_docstrings(output_type=Qwen2AudioCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        input_features: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         feature_attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "b7075fba3bf137de79486ae2641936adf9ffa346",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -926,7 +926,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(QWEN2MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1251,7 +1251,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=MoeCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "10bc8bc69a938bdc10ce1432f335c7c8b1ebae09",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -169,18 +169,18 @@ def __init__(\n     def _preprocess(\n         self,\n         images: Union[ImageInput, VideoInput],\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ):\n@@ -302,20 +302,20 @@ def preprocess(\n         self,\n         images: ImageInput,\n         videos: VideoInput = None,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "661f5ed8b11cb58032dbe55a34b56b9e4da1a400",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -255,20 +255,20 @@ def preprocess(\n         self,\n         images: ImageInput,\n         videos: VideoInput = None,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         min_pixels: Optional[int] = None,\n         max_pixels: Optional[int] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,"
        },
        {
            "sha": "6680a0cc8baa9658447f2774f628f0e803052d29",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -87,7 +87,7 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -1067,7 +1067,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1598,7 +1598,7 @@ def get_rope_index(\n     @replace_return_docstrings(output_type=Qwen2VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "30abb9701d12d0f1a8e535bee5bbcce406a0c693",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -527,7 +527,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(QWEN3_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n@@ -826,7 +826,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,"
        },
        {
            "sha": "b9ffae15a2143cdeb84fd727a17fea5c2081e7c4",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -622,7 +622,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(QWEN3_MOE_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1013,7 +1013,7 @@ def get_decoder(self):\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "d8a5cc54f243a1bdd363f83942d4cf56bb804442",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -244,7 +244,7 @@ def __init__(self, config):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "c2258d9767a84306ff37cbd07a02e0bbe1c23e7b",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -112,8 +112,8 @@ class RetrievAugLMMarginOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n-    doc_scores: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    doc_scores: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     retrieved_doc_embeds: Optional[torch.FloatTensor] = None\n     retrieved_doc_ids: Optional[torch.LongTensor] = None\n@@ -202,8 +202,8 @@ class RetrievAugLMOutput(ModelOutput):\n             weighted average in the cross-attention heads.\n     \"\"\"\n \n-    logits: torch.FloatTensor = None\n-    doc_scores: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    doc_scores: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     retrieved_doc_embeds: Optional[torch.FloatTensor] = None\n     retrieved_doc_ids: Optional[torch.LongTensor] = None"
        },
        {
            "sha": "9c670683c9927e8d55bc34eddd642ea266b0dd2a",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -115,7 +115,7 @@ class TFRetrievAugLMMarginOutput(ModelOutput):\n     \"\"\"\n \n     loss: tf.Tensor | None = None\n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     doc_scores: tf.Tensor | None = None\n     retrieved_doc_embeds: tf.Tensor | None = None\n@@ -198,7 +198,7 @@ class TFRetrievAugLMOutput(ModelOutput):\n             average in the self-attention heads.\n     \"\"\"\n \n-    logits: tf.Tensor = None\n+    logits: Optional[tf.Tensor] = None\n     past_key_values: List[tf.Tensor] | None = None\n     doc_scores: tf.Tensor | None = None\n     retrieved_doc_embeds: tf.Tensor | None = None"
        },
        {
            "sha": "450da13493d5ea0b42b26c58cb183b1fd4df0fb5",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -492,8 +492,8 @@ def forward(\n         activations: torch.Tensor,\n         position_ids: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        cache_position: torch.Tensor = None,\n-        use_cache: bool = None,\n+        cache_position: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n     ) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:\n         raw_activations = activations\n         inputs_normalized = self.temporal_pre_norm(raw_activations)  # RMSNorm introduces slight slight differences\n@@ -677,7 +677,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(RECURRENTGEMMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "48be78e7d47798ee6875e46022d8e28c7d968319",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1885,7 +1885,7 @@ class ReformerModelWithLMHeadOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_buckets_states: Optional[List[Tuple[torch.LongTensor, torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None"
        },
        {
            "sha": "a8942b1e7c2958d2a536dd75312937b0fdbfd0fe",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -776,7 +776,7 @@ class PreTrainedModel\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -931,7 +931,7 @@ def set_output_embeddings(self, new_embeddings):\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1028,7 +1028,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -1164,7 +1164,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.FloatTensor = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.FloatTensor] = None,\n@@ -1260,7 +1260,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.FloatTensor = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.FloatTensor] = None,\n@@ -1352,7 +1352,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.FloatTensor = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.FloatTensor] = None,\n@@ -1430,7 +1430,7 @@ def __init__(self, config):\n     )\n     def forward(\n         self,\n-        input_ids: torch.FloatTensor = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         token_type_ids: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "4a21ee48d39fd1ca5fc0aef97b5cbb8d25ad479b",
            "filename": "src/transformers/models/rembert/modeling_tf_rembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_tf_rembert.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -106,10 +106,10 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        position_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        position_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         past_key_values_length=0,\n         training: bool = False,\n     ) -> tf.Tensor:"
        },
        {
            "sha": "0f32c04f459c2762a1ec8bdd06e7f2744976cd7e",
            "filename": "src/transformers/models/resnet/modeling_tf_resnet.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_tf_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_tf_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fmodeling_tf_resnet.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -552,10 +552,10 @@ def classifier(self, x: tf.Tensor) -> tf.Tensor:\n     @unpack_inputs\n     def call(\n         self,\n-        pixel_values: tf.Tensor = None,\n-        labels: tf.Tensor = None,\n-        output_hidden_states: bool = None,\n-        return_dict: bool = None,\n+        pixel_values: Optional[tf.Tensor] = None,\n+        labels: Optional[tf.Tensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         training: bool = False,\n     ) -> Union[Tuple[tf.Tensor], TFImageClassifierOutputWithNoAttention]:\n         r\"\"\""
        },
        {
            "sha": "738f8e67e9b9886fc56c71f128ec38c5affc5d1b",
            "filename": "src/transformers/models/roformer/modeling_tf_roformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_tf_roformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -156,9 +156,9 @@ def build(self, input_shape=None):\n \n     def call(\n         self,\n-        input_ids: tf.Tensor = None,\n-        token_type_ids: tf.Tensor = None,\n-        inputs_embeds: tf.Tensor = None,\n+        input_ids: Optional[tf.Tensor] = None,\n+        token_type_ids: Optional[tf.Tensor] = None,\n+        inputs_embeds: Optional[tf.Tensor] = None,\n         training: bool = False,\n     ) -> tf.Tensor:\n         \"\"\""
        },
        {
            "sha": "e458de37949b0f08ac390d2118167f4d15e4d06e",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -477,7 +477,7 @@ def prepare_annotation(\n         image: np.ndarray,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:\n@@ -784,7 +784,7 @@ def preprocess(\n         self,\n         images: ImageInput,\n         annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         do_resize: Optional[bool] = None,\n         size: Optional[Dict[str, int]] = None,"
        },
        {
            "sha": "dd0c54cc631951b860e42ea9cf554aad25e7e75a",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -174,7 +174,7 @@ def prepare_annotation(\n         image: torch.Tensor,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:"
        },
        {
            "sha": "727925fb17c0a63142e41dcaa544bfc0ed743d00",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -137,10 +137,10 @@ class RTDetrDecoderOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -200,17 +200,17 @@ class RTDetrModelOutput(ModelOutput):\n             Extra dictionary for the denoising related values\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    init_reference_points: torch.FloatTensor = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n     enc_topk_logits: Optional[torch.FloatTensor] = None\n     enc_topk_bboxes: Optional[torch.FloatTensor] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n@@ -289,13 +289,13 @@ class RTDetrObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -586,7 +586,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         **kwargs,\n     ):"
        },
        {
            "sha": "fd9913e97e945cc67b881acba0d5054e5ce60dae",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -188,7 +188,7 @@ def prepare_annotation(\n         image: torch.Tensor,\n         target: Dict,\n         format: Optional[AnnotationFormat] = None,\n-        return_segmentation_masks: bool = None,\n+        return_segmentation_masks: Optional[bool] = None,\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> Dict:"
        },
        {
            "sha": "59a00a6e7410c9debfdecc6c6864f69229d401c5",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -486,10 +486,10 @@ class RTDetrV2DecoderOutput(ModelOutput):\n             used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -549,17 +549,17 @@ class RTDetrV2ModelOutput(ModelOutput):\n             Extra dictionary for the denoising related values\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n     encoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     encoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    init_reference_points: torch.FloatTensor = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n     enc_topk_logits: Optional[torch.FloatTensor] = None\n     enc_topk_bboxes: Optional[torch.FloatTensor] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n@@ -638,13 +638,13 @@ class RTDetrV2ObjectDetectionOutput(ModelOutput):\n \n     loss: Optional[torch.FloatTensor] = None\n     loss_dict: Optional[Dict] = None\n-    logits: torch.FloatTensor = None\n-    pred_boxes: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n     auxiliary_outputs: Optional[List[Dict]] = None\n-    last_hidden_state: torch.FloatTensor = None\n-    intermediate_hidden_states: torch.FloatTensor = None\n-    intermediate_logits: torch.FloatTensor = None\n-    intermediate_reference_points: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -798,7 +798,7 @@ def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: torch.Tensor,\n-        position_embeddings: torch.Tensor = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n         **kwargs,\n     ):"
        },
        {
            "sha": "6274464309e3726e548622c584e354e129921e03",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -478,7 +478,7 @@ class RwkvOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     state: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -511,7 +511,7 @@ class RwkvCausalLMOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     state: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None"
        },
        {
            "sha": "5d98674f73e68e2a63c51c367965bb0bb59128f0",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -296,7 +296,7 @@ def _preprocess_image(\n         do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n+        do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,"
        },
        {
            "sha": "549eb8a317ed9c034952cf5140c2b14e089f41e8",
            "filename": "src/transformers/models/sam/modeling_sam.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_sam.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -69,7 +69,7 @@ class SamVisionEncoderOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -103,8 +103,8 @@ class SamImageSegmentationOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    iou_scores: torch.FloatTensor = None\n-    pred_masks: torch.FloatTensor = None\n+    iou_scores: Optional[torch.FloatTensor] = None\n+    pred_masks: Optional[torch.FloatTensor] = None\n     vision_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     vision_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n     mask_decoder_attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n@@ -223,7 +223,9 @@ def _recombine_heads(self, hidden_states: Tensor, point_batch_size: int) -> Tens\n         hidden_states = hidden_states.transpose(1, 2)\n         return hidden_states.reshape(batch // point_batch_size, point_batch_size, n_tokens, n_heads * c_per_head)\n \n-    def forward(self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Tensor = None) -> Tensor:\n+    def forward(\n+        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+    ) -> Tensor:\n         # Input projections\n         query = self.q_proj(query)\n         key = self.k_proj(key)\n@@ -262,7 +264,9 @@ class SamSdpaAttention(SamAttention):\n     def __init__(self, config, downsample_rate=None):\n         super().__init__(config, downsample_rate)\n \n-    def forward(self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Tensor = None) -> Tensor:\n+    def forward(\n+        self, query: Tensor, key: Tensor, value: Tensor, attention_similarity: Optional[Tensor] = None\n+    ) -> Tensor:\n         # Input projections\n         query = self.q_proj(query)\n         key = self.k_proj(key)\n@@ -514,8 +518,8 @@ def forward(\n         dense_prompt_embeddings: torch.Tensor,\n         multimask_output: bool,\n         output_attentions: Optional[bool] = None,\n-        attention_similarity: torch.Tensor = None,\n-        target_embedding: torch.Tensor = None,\n+        attention_similarity: Optional[torch.Tensor] = None,\n+        target_embedding: Optional[torch.Tensor] = None,\n     ) -> Tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Predict masks given image and prompt embeddings."
        },
        {
            "sha": "cbe06e05424a3e59accda6d14b91af447321499d",
            "filename": "src/transformers/models/sam/modeling_tf_sam.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fmodeling_tf_sam.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -71,7 +71,7 @@ class TFSamVisionEncoderOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: tf.Tensor | None = None\n-    last_hidden_state: tf.Tensor = None\n+    last_hidden_state: Optional[tf.Tensor] = None\n     hidden_states: Tuple[tf.Tensor, ...] | None = None\n     attentions: Tuple[tf.Tensor, ...] | None = None\n \n@@ -105,8 +105,8 @@ class TFSamImageSegmentationOutput(ModelOutput):\n             heads.\n     \"\"\"\n \n-    iou_scores: tf.Tensor = None\n-    pred_masks: tf.Tensor = None\n+    iou_scores: Optional[tf.Tensor] = None\n+    pred_masks: Optional[tf.Tensor] = None\n     vision_hidden_states: Tuple[tf.Tensor, ...] | None = None\n     vision_attentions: Tuple[tf.Tensor, ...] | None = None\n     mask_decoder_attentions: Tuple[tf.Tensor, ...] | None = None"
        },
        {
            "sha": "fa7c75844f756a785c67f0989be81efec7906e93",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -1050,7 +1050,10 @@ def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional\n \n     @torch.no_grad()\n     def forward(\n-        self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        past_key_values_length: int = 0,\n     ):\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()\n@@ -1685,7 +1688,7 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1873,7 +1876,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2203,7 +2206,7 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2697,7 +2700,7 @@ def _tie_weights(self):\n     @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2958,7 +2961,7 @@ def _tie_weights(self):\n     @add_start_docstrings_to_model_forward(M4T_SPEECH_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_features: torch.LongTensor = None,\n+        input_features: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -3236,7 +3239,7 @@ def _tie_weights(self):\n     @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -3561,7 +3564,7 @@ def _tie_weights(self):\n     @add_start_docstrings_to_model_forward(M4T_SPEECH_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_features: torch.LongTensor = None,\n+        input_features: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "998008bbf56ba98ffd19b82b9ab669f98590f87f",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -104,7 +104,7 @@ class SeamlessM4Tv2TextToUnitDecoderOutput(ModelOutput):\n             for *masked*\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n     padding_mask: Optional[torch.Tensor] = None\n@@ -153,7 +153,7 @@ class SeamlessM4Tv2TextToUnitOutput(ModelOutput):\n             Language modeling loss.\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     padding_mask: Optional[torch.Tensor] = None\n     decoder_hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     decoder_attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -999,7 +999,10 @@ def get_embedding(num_embeddings: int, embedding_dim: int, padding_idx: Optional\n \n     @torch.no_grad()\n     def forward(\n-        self, input_ids: torch.Tensor = None, inputs_embeds: torch.Tensor = None, past_key_values_length: int = 0\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n+        past_key_values_length: int = 0,\n     ):\n         if input_ids is not None:\n             bsz, seq_len = input_ids.size()\n@@ -1799,7 +1802,7 @@ def __init__(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1988,7 +1991,7 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -2248,9 +2251,9 @@ def set_input_embeddings(self, value):\n \n     def forward(\n         self,\n-        char_input_ids: torch.LongTensor = None,\n-        char_count_per_id: torch.LongTensor = None,\n-        encoder_hidden_states: torch.FloatTensor = None,\n+        char_input_ids: Optional[torch.LongTensor] = None,\n+        char_count_per_id: Optional[torch.LongTensor] = None,\n+        encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -2380,8 +2383,8 @@ def __init__(\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        char_input_ids: torch.LongTensor = None,\n-        char_count_per_id: torch.LongTensor = None,\n+        char_input_ids: Optional[torch.LongTensor] = None,\n+        char_count_per_id: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2499,9 +2502,9 @@ def set_input_embeddings(self, value):\n     @add_start_docstrings_to_model_forward(M4T_TEXT_TO_UNITS_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        char_input_ids: torch.LongTensor = None,\n-        char_count_per_id: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        char_input_ids: Optional[torch.LongTensor] = None,\n+        char_count_per_id: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n@@ -2660,7 +2663,7 @@ def __init__(self, embed_dim, hidden_dim, kernel_size, var_pred_dropout):\n         self.ln2 = nn.LayerNorm(hidden_dim)\n         self.proj = nn.Linear(hidden_dim, 1)\n \n-    def forward(self, hidden_states: Tensor, padding_mask: Tensor = None) -> Tensor:\n+    def forward(self, hidden_states: Tensor, padding_mask: Optional[Tensor] = None) -> Tensor:\n         # Input: B x T x C; Output: B x T\n         if padding_mask is not None:\n             hidden_states = hidden_states.masked_fill(~padding_mask.bool().unsqueeze(-1), 0.0)\n@@ -2977,7 +2980,7 @@ def _tie_weights(self):\n     @add_start_docstrings_to_model_forward(M4T_TEXT_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -3247,7 +3250,7 @@ def _tie_weights(self):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.forward\n     def forward(\n         self,\n-        input_features: torch.LongTensor = None,\n+        input_features: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -3536,7 +3539,7 @@ def _tie_weights(self):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.forward with SeamlessM4T->SeamlessM4Tv2\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n@@ -3902,7 +3905,7 @@ def _tie_weights(self):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.forward with SeamlessM4T->SeamlessM4Tv2\n     def forward(\n         self,\n-        input_features: torch.LongTensor = None,\n+        input_features: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "b978f701657a02685f79ea4ba2f4319150ba2f2f",
            "filename": "src/transformers/models/segformer/image_processing_segformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -219,12 +219,12 @@ def _preprocess(\n     def _preprocess_image(\n         self,\n         image: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -260,8 +260,8 @@ def _preprocess_image(\n     def _preprocess_mask(\n         self,\n         segmentation_map: ImageInput,\n-        do_reduce_labels: bool = None,\n-        do_resize: bool = None,\n+        do_reduce_labels: Optional[bool] = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> np.ndarray:"
        },
        {
            "sha": "84a90e5e623881947016ec4aee148c037561e1e2",
            "filename": "src/transformers/models/segformer/modeling_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodeling_segformer.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -73,7 +73,7 @@ class SegFormerImageClassifierOutput(ImageClassifierOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        },
        {
            "sha": "3981a1f54dac7b4b86e36f0f8fec86e9c0b18916",
            "filename": "src/transformers/models/shieldgemma2/modeling_shieldgemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fmodeling_shieldgemma2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -117,7 +117,7 @@ class ShieldGemma2ImageClassifierOutputWithNoAttention(ImageClassifierOutputWith\n     Args:\n     \"\"\"\n \n-    probabilities: torch.Tensor = None\n+    probabilities: Optional[torch.Tensor] = None\n \n \n class ShieldGemma2ForImageClassification(PreTrainedModel):\n@@ -154,8 +154,8 @@ def tie_weights(self):\n     @add_start_docstrings_to_model_forward(SHIELDGEMMA2_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n-        pixel_values: torch.FloatTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,"
        },
        {
            "sha": "7ec6c36d39ca83dda2516f13548a0853e8f8afb1",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -89,7 +89,7 @@ def __init__(\n         do_normalize: bool = True,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -111,18 +111,18 @@ def __init__(\n     def preprocess(\n         self,\n         images: ImageInput,\n-        do_resize: bool = None,\n+        do_resize: Optional[bool] = None,\n         size: Dict[str, int] = None,\n         resample: PILImageResampling = None,\n-        do_rescale: bool = None,\n-        rescale_factor: float = None,\n-        do_normalize: bool = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n         image_mean: Optional[Union[float, List[float]]] = None,\n         image_std: Optional[Union[float, List[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        do_convert_rgb: bool = None,\n+        do_convert_rgb: Optional[bool] = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images."
        },
        {
            "sha": "6c488262a50d3d22e7d56ab78a74913c10f8ce10",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -169,7 +169,7 @@ class SiglipVisionModelOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -199,7 +199,7 @@ class SiglipTextModelOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -228,10 +228,10 @@ class SiglipOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "b47874394978fcc1398469dd6d48232649bd51e7",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -75,7 +75,7 @@ class Siglip2VisionOutput(ModelOutput):\n     \"\"\"\n \n     image_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -104,7 +104,7 @@ class Siglip2TextOutput(ModelOutput):\n     \"\"\"\n \n     text_embeds: Optional[torch.FloatTensor] = None\n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n     attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n@@ -132,10 +132,10 @@ class Siglip2Output(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits_per_image: torch.FloatTensor = None\n-    logits_per_text: torch.FloatTensor = None\n-    text_embeds: torch.FloatTensor = None\n-    image_embeds: torch.FloatTensor = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n     text_model_output: BaseModelOutputWithPooling = None\n     vision_model_output: BaseModelOutputWithPooling = None\n "
        },
        {
            "sha": "e88e23776b33c4304078c76768917ad1d06d5e1b",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -537,7 +537,7 @@ class SmolVLMBaseModelOutputWithPast(ModelOutput):\n             image_hidden_states of the model produced by the vision encoder\n     \"\"\"\n \n-    last_hidden_state: torch.FloatTensor = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -764,7 +764,7 @@ def inputs_merger(\n     )\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -924,7 +924,7 @@ class SmolVLMCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    logits: torch.FloatTensor = None\n+    logits: Optional[torch.FloatTensor] = None\n     past_key_values: Optional[List[torch.FloatTensor]] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n@@ -987,7 +987,7 @@ def set_output_embeddings(self, new_embeddings):\n     @replace_return_docstrings(output_type=SmolVLMCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "051bdfaf5afc52ac91c55cd171b526bfdfad2ff0",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -184,7 +184,7 @@ def inputs_merger(\n \n     def forward(\n         self,\n-        input_ids: torch.LongTensor = None,\n+        input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,"
        },
        {
            "sha": "174e766598a0cedcab53c2250db2ab9978649fd7",
            "filename": "src/transformers/models/splinter/modeling_splinter.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a828a747e65f3b8807d574320f810571a7bdd7e/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Fmodeling_splinter.py?ref=8a828a747e65f3b8807d574320f810571a7bdd7e",
            "patch": "@@ -964,8 +964,8 @@ class SplinterForPreTrainingOutput(ModelOutput):\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n-    start_logits: torch.FloatTensor = None\n-    end_logits: torch.FloatTensor = None\n+    start_logits: Optional[torch.FloatTensor] = None\n+    end_logits: Optional[torch.FloatTensor] = None\n     hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n     attentions: Optional[Tuple[torch.FloatTensor]] = None\n "
        }
    ],
    "stats": {
        "total": 3263,
        "additions": 1647,
        "deletions": 1616
    }
}