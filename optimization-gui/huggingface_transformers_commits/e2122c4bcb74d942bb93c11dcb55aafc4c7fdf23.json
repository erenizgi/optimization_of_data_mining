{
    "author": "SunMarc",
    "message": "remove ray_scope and check_quantized_param (#41587)\n\nremove",
    "sha": "e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23",
    "files": [
        {
            "sha": "46bd19eda0cfa04c2d16bcfdb07c34bccc1190f4",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23",
            "patch": "@@ -190,14 +190,6 @@ def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str,\n         \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n         return max_memory\n \n-    def check_quantized_param(self, *args, **kwargs) -> bool:\n-        \"\"\"DEPRECATED -> remove in v5\"\"\"\n-        logger.warning_once(\n-            \"`check_quantized_param` is deprecated in favor of `param_needs_quantization`, which is a much \"\n-            \"more self.explanatory name for what the method achieves. It will be removed in v5\"\n-        )\n-        return self.param_needs_quantization(*args, **kwargs)\n-\n     def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n         \"\"\"\n         Check whether a given param needs quantization as defined by `create_quantized_param`."
        },
        {
            "sha": "7a8ade0aa1e1de8eef0760bc3010787fa5649acd",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=e2122c4bcb74d942bb93c11dcb55aafc4c7fdf23",
            "patch": "@@ -1291,20 +1291,6 @@ class TrainingArguments:\n             )\n         },\n     )\n-    ray_scope: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"This argument is deprecated and will be removed in v5.2. Set env var RAY_SCOPE instead.\"\n-                'The scope to use when doing hyperparameter search with Ray. By default, `\"last\"` will be used. Ray'\n-                \" will then use the last checkpoint of all trials, compare those, and select the best one. However,\"\n-                \" other options are also available. See the Ray documentation\"\n-                \" (https://docs.ray.io/en/latest/tune/api_docs/analysis.html\"\n-                \"#ray.tune.ExperimentAnalysis.get_best_trial)\"\n-                \" for more options.\"\n-            )\n-        },\n-    )\n     ddp_timeout: int = field(\n         default=1800,\n         metadata={"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 0,
        "deletions": 22
    }
}