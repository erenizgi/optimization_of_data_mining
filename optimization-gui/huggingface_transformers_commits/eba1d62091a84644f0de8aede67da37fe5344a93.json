{
    "author": "Cyrilvallez",
    "message": "[FA2] Fix it finally - revert fa kwargs preparation (#40161)\n\nrevert",
    "sha": "eba1d62091a84644f0de8aede67da37fe5344a93",
    "files": [
        {
            "sha": "3c4142f20d7e8e5356ed970552b82c64210360e9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/eba1d62091a84644f0de8aede67da37fe5344a93/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eba1d62091a84644f0de8aede67da37fe5344a93/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=eba1d62091a84644f0de8aede67da37fe5344a93",
            "patch": "@@ -46,7 +46,6 @@\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..integrations.fsdp import is_fsdp_managed_module\n from ..masking_utils import create_masks_for_generate\n-from ..modeling_flash_attention_utils import prepare_fa_kwargs_from_position_ids\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n@@ -678,24 +677,12 @@ def prepare_inputs_for_generation(\n         if encoder_attention_mask is not None:\n             model_inputs[\"attention_mask\"] = encoder_attention_mask\n \n-        # 7. Prepare kwargs for flash attention to avoid recomputations\n-        if \"flash\" in self.config._attn_implementation and self._supports_attention_backend:\n-            (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(\n-                model_inputs[\"position_ids\"], is_packed_sequence=False\n-            )\n-            model_inputs.update(\n-                cu_seq_lens_q=cu_seq_lens_q.to(self.device),\n-                cu_seq_lens_k=cu_seq_lens_k.to(self.device),\n-                max_length_q=max_length_q,\n-                max_length_k=max_length_k,\n-            )\n-\n-        # 8. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n+        # 7. Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:\n                 model_inputs[key] = value\n \n-        # 9. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n+        # 8. Remove unexpected `generate` inputs (TODO @joao: fix trainer and examples)\n         model_inputs.pop(\"labels\", None)\n         return model_inputs\n "
        }
    ],
    "stats": {
        "total": 17,
        "additions": 2,
        "deletions": 15
    }
}