{
    "author": "ydshieh",
    "message": "fix failing `test_sdpa_can_dispatch_on_flash` (#39259)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "24f771a043871a109d8a969bf92730746e085f3b",
    "files": [
        {
            "sha": "9692f705f2d4e268ce59030d23d59d6bee390bf7",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/24f771a043871a109d8a969bf92730746e085f3b/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24f771a043871a109d8a969bf92730746e085f3b/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=24f771a043871a109d8a969bf92730746e085f3b",
            "patch": "@@ -993,16 +993,23 @@ def check_model_inputs(func):\n \n     @wraps(func)\n     def wrapper(self, *args, **kwargs):\n-        use_cache = kwargs.get(\"use_cache\", getattr(self.config, \"use_cache\", False))\n-        return_dict = kwargs.pop(\"return_dict\", getattr(self.config, \"return_dict\", True))\n-        all_args = kwargs.copy()\n+        use_cache = kwargs.get(\"use_cache\", None)\n+        if use_cache is None:\n+            use_cache = getattr(self.config, \"use_cache\", False)\n+\n+        return_dict = kwargs.pop(\"return_dict\", None)\n+        if return_dict is None:\n+            return_dict = getattr(self.config, \"return_dict\", True)\n \n         if getattr(self, \"gradient_checkpointing\", False) and self.training and use_cache:\n             logger.warning_once(\n                 \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n             )\n-            kwargs[\"use_cache\"] = False\n+            use_cache = False\n \n+        kwargs[\"use_cache\"] = use_cache\n+\n+        all_args = kwargs.copy()\n         if \"kwargs\" in all_args:\n             for k, v in all_args[\"kwargs\"].items():\n                 all_args[k] = v"
        },
        {
            "sha": "0020c5c78edcd2ee3f35afd8e09301ffa2be7aa1",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/24f771a043871a109d8a969bf92730746e085f3b/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24f771a043871a109d8a969bf92730746e085f3b/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=24f771a043871a109d8a969bf92730746e085f3b",
            "patch": "@@ -311,7 +311,7 @@ def create_and_check_with_lm_head(\n             decoder_attention_mask=decoder_attention_mask,\n             labels=lm_labels,\n         )\n-        self.parent.assertEqual(len(outputs), 4)\n+        self.parent.assertEqual(len(outputs), 5)\n         self.parent.assertEqual(outputs[\"logits\"].size(), (self.batch_size, self.seq_length, self.vocab_size))\n         self.parent.assertEqual(outputs[\"loss\"].size(), ())\n \n@@ -1067,7 +1067,7 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n \n                 for i in range(num_decoder_layers):\n                     if is_legacy_cache:\n-                        self.assertEqual(len(past_kv[0]), 4)  # legacy check: confirm number of elements in tuple\n+                        self.assertEqual(len(past_kv[0]), 5)  # legacy check: confirm number of elements in tuple\n \n                     # Self attention\n                     self_attention_layer_key_cache = (\n@@ -1687,7 +1687,7 @@ def build_model_and_check_forward_pass(self, **kwargs):\n             labels=lm_labels,\n         )\n         # outputs = model(*inputs)\n-        assert len(outputs) == 4\n+        assert len(outputs) == 5\n         assert outputs[\"logits\"].size() == (tester.batch_size, tester.seq_length, tester.vocab_size)\n         assert outputs[\"loss\"].size() == ()\n         return model.model"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 14,
        "deletions": 7
    }
}