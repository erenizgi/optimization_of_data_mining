{
    "author": "zucchini-nlp",
    "message": "[chat template] update when \"push_to_hub\" (#39815)\n\n* update templates push to hub\n\n* rvert jinja suffix and move it to processor file",
    "sha": "313afcc468d5e2500bb1eeb210ece0151166a1ee",
    "files": [
        {
            "sha": "bd08b8d17d540a63b1c4a2bd9cc6ab815deea0ac",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=313afcc468d5e2500bb1eeb210ece0151166a1ee",
            "patch": "@@ -776,6 +776,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, legacy_seri\n                 Additional key word arguments passed along to the [`~utils.PushToHubMixin.push_to_hub`] method.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n+        save_jinja_files = kwargs.pop(\"save_jinja_files\", True)\n \n         if use_auth_token is not None:\n             warnings.warn(\n@@ -803,8 +804,6 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, legacy_seri\n             configs.append(self)\n             custom_object_save(self, save_directory, config=configs)\n \n-        save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n-\n         for attribute_name in self.attributes:\n             # Save the tokenizer in its own vocab file. The other attributes are saved as part of `processor_config.json`\n             if attribute_name == \"tokenizer\":\n@@ -840,7 +839,6 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, legacy_seri\n         # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n         # to avoid serializing chat template in json config file. So let's get it from `self` directly\n         if self.chat_template is not None:\n-            save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n             is_single_template = isinstance(self.chat_template, str)\n             if save_jinja_files and is_single_template:\n                 # New format for single templates is to save them as chat_template.jinja\n@@ -999,6 +997,7 @@ def get_processor_dict(\n                         cache_dir=cache_dir,\n                         token=token,\n                     ):\n+                        template = template.removesuffix(\".jinja\")\n                         additional_chat_template_files[template] = f\"{CHAT_TEMPLATE_DIR}/{template}.jinja\"\n                 except EntryNotFoundError:\n                     pass  # No template dir means no template files"
        },
        {
            "sha": "89612915797ebb3d954c72ccfb673b33f85a891a",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=313afcc468d5e2500bb1eeb210ece0151166a1ee",
            "patch": "@@ -2512,6 +2512,7 @@ def save_pretrained(\n             A tuple of `str`: The files saved.\n         \"\"\"\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n+        save_jinja_files = kwargs.pop(\"save_jinja_files\", True)\n \n         if use_auth_token is not None:\n             warnings.warn(\n@@ -2560,7 +2561,6 @@ def save_pretrained(\n             tokenizer_config[\"extra_special_tokens\"] = self.extra_special_tokens\n             tokenizer_config.update(self.extra_special_tokens)\n \n-        save_jinja_files = kwargs.get(\"save_jinja_files\", True)\n         tokenizer_config, saved_raw_chat_template_files = self.save_chat_templates(\n             save_directory, tokenizer_config, filename_prefix, save_jinja_files\n         )"
        },
        {
            "sha": "e8d1e48ba345be4f0cea8cfacc26568b2d878cf8",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313afcc468d5e2500bb1eeb210ece0151166a1ee/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=313afcc468d5e2500bb1eeb210ece0151166a1ee",
            "patch": "@@ -163,7 +163,7 @@ def list_repo_templates(\n     local_files_only: bool,\n     revision: Optional[str] = None,\n     cache_dir: Optional[str] = None,\n-    token: Union[bool, str, None] = None,\n+    token: Optional[Union[str, bool]] = None,\n ) -> list[str]:\n     \"\"\"List template files from a repo.\n "
        },
        {
            "sha": "1cad7e15b9f182e418376e776d5fe571b2c7da19",
            "filename": "tests/models/auto/test_processor_auto.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/313afcc468d5e2500bb1eeb210ece0151166a1ee/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313afcc468d5e2500bb1eeb210ece0151166a1ee/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_processor_auto.py?ref=313afcc468d5e2500bb1eeb210ece0151166a1ee",
            "patch": "@@ -34,7 +34,10 @@\n     AutoProcessor,\n     AutoTokenizer,\n     BertTokenizer,\n+    LlamaTokenizer,\n+    LlavaProcessor,\n     ProcessorMixin,\n+    SiglipImageProcessor,\n     Wav2Vec2Config,\n     Wav2Vec2FeatureExtractor,\n     Wav2Vec2Processor,\n@@ -57,6 +60,7 @@\n \n \n SAMPLE_PROCESSOR_CONFIG = get_tests_dir(\"fixtures/dummy_feature_extractor_config.json\")\n+SAMPLE_VOCAB_LLAMA = get_tests_dir(\"fixtures/test_sentencepiece.model\")\n SAMPLE_VOCAB = get_tests_dir(\"fixtures/vocab.json\")\n SAMPLE_PROCESSOR_CONFIG_DIR = get_tests_dir(\"fixtures\")\n \n@@ -503,3 +507,43 @@ def test_push_to_hub_dynamic_processor(self):\n                 new_processor = AutoProcessor.from_pretrained(tmp_repo.repo_id, trust_remote_code=True)\n                 # Can't make an isinstance check because the new_processor is from the CustomProcessor class of a dynamic module\n                 self.assertEqual(new_processor.__class__.__name__, \"CustomProcessor\")\n+\n+    def test_push_to_hub_with_chat_templates(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            tokenizer = LlamaTokenizer(SAMPLE_VOCAB_LLAMA, keep_accents=True)\n+            image_processor = SiglipImageProcessor()\n+            chat_template = \"default dummy template for testing purposes only\"\n+            processor = LlavaProcessor(\n+                tokenizer=tokenizer, image_processor=image_processor, chat_template=chat_template\n+            )\n+            self.assertEqual(processor.chat_template, chat_template)\n+\n+            existing_tokenizer_template = getattr(processor.tokenizer, \"chat_template\", None)\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                processor.save_pretrained(\n+                    tmp_dir, repo_id=tmp_repo.repo_id, token=self._token, push_to_hub=True, save_jinja_files=False\n+                )\n+                reloaded_processor = LlavaProcessor.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+                # When we don't use single-file chat template saving, processor and tokenizer chat templates\n+                # should remain separate\n+                self.assertEqual(\n+                    getattr(reloaded_processor.tokenizer, \"chat_template\", None), existing_tokenizer_template\n+                )\n+\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                processor.save_pretrained(tmp_dir, repo_id=tmp_repo.repo_id, token=self._token, push_to_hub=True)\n+                reloaded_processor = LlavaProcessor.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+                # When we save as single files, tokenizers and processors share a chat template, which means\n+                # the reloaded tokenizer should get the chat template as well\n+                self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)\n+\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                processor.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+                processor.save_pretrained(tmp_dir, repo_id=tmp_repo.repo_id, token=self._token, push_to_hub=True)\n+                reloaded_processor = LlavaProcessor.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+                # When we save as single files, tokenizers and processors share a chat template, which means\n+                # the reloaded tokenizer should get the chat template as well\n+                self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)"
        },
        {
            "sha": "49adac8a1ad64e722ee0668b306578cbdf4f40e6",
            "filename": "tests/utils/test_tokenization_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/313afcc468d5e2500bb1eeb210ece0151166a1ee/tests%2Futils%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313afcc468d5e2500bb1eeb210ece0151166a1ee/tests%2Futils%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_tokenization_utils.py?ref=313afcc468d5e2500bb1eeb210ece0151166a1ee",
            "patch": "@@ -131,6 +131,32 @@ def test_push_to_hub(self):\n             new_tokenizer = BertTokenizer.from_pretrained(tmp_repo.repo_id)\n             self.assertDictEqual(new_tokenizer.vocab, tokenizer.vocab)\n \n+    def test_push_to_hub_chat_templates(self):\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            vocab_file = os.path.join(tmp_dir, \"vocab.txt\")\n+            with open(vocab_file, \"w\", encoding=\"utf-8\") as vocab_writer:\n+                vocab_writer.write(\"\".join([x + \"\\n\" for x in self.vocab_tokens]))\n+            tokenizer = BertTokenizer(vocab_file)\n+            tokenizer.chat_template = \"test template\"\n+\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                tokenizer.save_pretrained(\n+                    tmp_repo.repo_id, token=self._token, push_to_hub=True, save_jinja_files=False\n+                )\n+                reloaded_tokenizer = BertTokenizer.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                tokenizer.save_pretrained(tmp_repo.repo_id, token=self._token, push_to_hub=True)\n+                reloaded_tokenizer = BertTokenizer.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+\n+            with TemporaryHubRepo(token=self._token) as tmp_repo:\n+                tokenizer.chat_template = {\"default\": \"a\", \"secondary\": \"b\"}\n+                tokenizer.save_pretrained(tmp_repo.repo_id, token=self._token, push_to_hub=True)\n+                reloaded_tokenizer = BertTokenizer.from_pretrained(tmp_repo.repo_id)\n+                self.assertEqual(tokenizer.chat_template, reloaded_tokenizer.chat_template)\n+\n     def test_push_to_hub_via_save_pretrained(self):\n         with TemporaryHubRepo(token=self._token) as tmp_repo:\n             with tempfile.TemporaryDirectory() as tmp_dir:"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 74,
        "deletions": 5
    }
}