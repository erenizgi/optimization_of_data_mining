{
    "author": "starcatmeow",
    "message": "chore(pixtral): emit block attention mask when using flash attention (#38741)\n\n* chore(pixtral): emit block attention mask when using flash attention\n\nSince flash_attention_2 relies solely on position_ids, emitting the block attention mask avoids unnecessary memory usage and prevents OOM on large inputs.\n\n* remove unnecessary attention_mask assignment",
    "sha": "1dcb022e8f893a382722a7268861c6dbdc1c0c24",
    "files": [
        {
            "sha": "c416179131c371d51bdef74e40302a0c05e5ed7e",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1dcb022e8f893a382722a7268861c6dbdc1c0c24/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1dcb022e8f893a382722a7268861c6dbdc1c0c24/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=1dcb022e8f893a382722a7268861c6dbdc1c0c24",
            "patch": "@@ -214,7 +214,6 @@ def forward(\n         # Since we use packing, if flash_attention_2 is selected we rely on position_ids\n         if self.config._attn_implementation == \"flash_attention_2\":\n             kwargs[\"position_ids\"] = kwargs[\"position_ids\"].to(hidden_states.device, non_blocking=True)\n-            attention_mask = None\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -508,9 +507,13 @@ def forward(\n \n         position_embeddings = self.patch_positional_embedding(patch_embeds, position_ids)\n \n-        attention_mask = generate_block_attention_mask(\n-            [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n-        )\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            # We only rely on position_ids when using flash_attention_2\n+            attention_mask = None\n+        else:\n+            attention_mask = generate_block_attention_mask(\n+                [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n+            )\n \n         return self.transformer(\n             patch_embeds,"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 7,
        "deletions": 4
    }
}