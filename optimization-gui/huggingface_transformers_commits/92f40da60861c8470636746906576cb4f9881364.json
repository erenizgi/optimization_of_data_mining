{
    "author": "ahnjj",
    "message": "Update model card for gpt neox japanese (#39862)\n\n* Update GPT-NeoX-Japanese model card\n\n* Apply suggestions from code review\n\n* Update gpt_neox_japanese.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "92f40da60861c8470636746906576cb4f9881364",
    "files": [
        {
            "sha": "a9c59519cfbb1b729d0195a24fd2baed8ee224c7",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 85,
            "deletions": 29,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/92f40da60861c8470636746906576cb4f9881364/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/92f40da60861c8470636746906576cb4f9881364/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=92f40da60861c8470636746906576cb4f9881364",
            "patch": "@@ -13,53 +13,109 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColorF=white\">\n+\n+    </div>\n+</div>\n+\n *This model was released on 2022-07-27 and added to Hugging Face Transformers on 2022-09-14.*\n \n # GPT-NeoX-Japanese\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-</div>\n+GPT-NeoX-Japanese, a Japanese language model based on [GPT-NeoX](./gpt_neox).\n+Japanese uses three types of characters (hiragana, katakana, kanji) and has a huge vocabulary. This model uses [BPEEncoder V2](https://github.com/tanreinama/Japanese-BPEEncoder_V2), a sub-word tokenizer to handle the different characters.\n \n-## Overview\n \n-We introduce GPT-NeoX-Japanese, which is an autoregressive language model for Japanese, trained on top of [https://github.com/EleutherAI/gpt-neox](https://github.com/EleutherAI/gpt-neox).\n-Japanese is a unique language with its large vocabulary and a combination of hiragana, katakana, and kanji writing scripts.\n-To address this distinct structure of the Japanese language, we use a [special sub-word tokenizer](https://github.com/tanreinama/Japanese-BPEEncoder_V2). We are very grateful to *tanreinama* for open-sourcing this incredibly helpful tokenizer.\n-Following the recommendations from Google's research on [PaLM](https://ai.googleblog.com/2022/04/pathways-language-model-palm-scaling-to.html), we have removed bias parameters from transformer blocks, achieving better model performance. Please refer [this article](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4) in detail.\n \n-Development of the model was led by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](https://github.com/spider-man-tm), [Anuj Arora](https://github.com/Anuj040), and [Kyo Hattori](https://github.com/go5paopao) from [ABEJA, Inc.](https://www.abejainc.com/). For more information on this model-building activity, please refer [here (ja)](https://tech-blog.abeja.asia/entry/abeja-gpt-project-202207).\n+The model also removes some bias parameters for better performance.\n \n-### Usage example\n+You can find all the original GPT-NeoX-Japanese checkpoints under the [ABEJA](https://huggingface.co/abeja/models?search=gpt-neo-x) organization.\n \n-The `generate()` method can be used to generate text using GPT NeoX Japanese model.\n+> [!TIP]\n+> This model was contributed by [Shinya Otani](https://github.com/SO0529), [Takayoshi Makabe](https://github.com/spider-man-tm), [Anuj Arora](https://github.com/Anuj040), and [Kyo Hattori](https://github.com/go5paopao) from [ABEJA, Inc.](https://www.abejainc.com/).\n+>\n+> Click on the GPT-NeoX-Japanese models in the right sidebar for more examples of how to apply GPT-NeoX-Japanese to different language tasks.\n \n-```python\n->>> from transformers import GPTNeoXJapaneseForCausalLM, GPTNeoXJapaneseTokenizer\n+The example below demonstrates how to generate text with [`Pipeline`] or the [`AutoModel`], and from the command line.\n \n->>> model = GPTNeoXJapaneseForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n->>> tokenizer = GPTNeoXJapaneseTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n->>> prompt = \"人とAIが協調するためには、\"\n+```py\n+import torch\n+from transformers import pipeline\n+pipeline = pipeline(task=\"text-generation\", \n+                    model=\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device=0)\n+pipeline(\"人とAIが協調するためには、\")\n+```\n \n->>> input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n->>> gen_tokens = model.generate(\n-...     input_ids,\n-...     do_sample=True,\n-...     temperature=0.9,\n-...     max_length=100,\n-... )\n->>> gen_text = tokenizer.batch_decode(gen_tokens, skip_special_tokens=True)[0]\n+```py\n+import torch  \n+from transformers import AutoModelForCausalLM, AutoTokenizer  \n \n->>> print(gen_text)\n-人とAIが協調するためには、AIと人が共存し、AIを正しく理解する必要があります。\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  \n+model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device_map=\"auto\").to(device)  \n+tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")  \n+input_ids = tokenizer(\"人とAIが協調するためには、\", return_tensors=\"pt\").input_ids.to(device)  \n+outputs = model.generate(input_ids)  \n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))  \n ```\n \n-## Resources\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+```bash\n+echo -e \"人とAIが協調するためには、\" | transformers run --task text-generation --model abeja/gpt-neox-japanese-2.7b --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits.\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_use_double_quant=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=\"float16\"\n+)\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"abeja/gpt-neox-japanese-2.7b\",\n+    quantization_config=quantization_config,\n+    device_map=\"auto\"\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n+input_ids = tokenizer.encode(\"人とAIが協調するためには、\", return_tensors=\"pt\").to(\"cuda\")\n+output = model.generate(input_ids)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+Use the [AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139) to better understand what tokens the model can and cannot attend to.\n+\n+```py\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n+\n+visualizer = AttentionMaskVisualizer(\"abeja/gpt-neox-japanese-2.7b\")\n+visualizer(\"<img>What is shown in this image?\")\n+```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/gpt_neox_japanese-attn-mask.png\"/>\n+</div>\n \n-- [Causal language modeling task guide](../tasks/language_modeling)\n+## Resources\n+Refer to the [Training a better GPT model: Learnings from PaLM](https://medium.com/ml-abeja/training-a-better-gpt-2-93b157662ae4) blog post for more details about how ABEJA trained GPT-NeoX-Japanese.\n \n ## GPTNeoXJapaneseConfig\n "
        }
    ],
    "stats": {
        "total": 114,
        "additions": 85,
        "deletions": 29
    }
}