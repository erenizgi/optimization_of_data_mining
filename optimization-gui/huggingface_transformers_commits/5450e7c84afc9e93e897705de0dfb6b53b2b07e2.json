{
    "author": "simonreise",
    "message": "ðŸ”´ ðŸ”´ ðŸ”´  Added `segmentation maps` support for DPT image processor (#34345)\n\n* Added `segmentation_maps` support for DPT image processor\n\n* Added tests for dpt image processor\n\n* Moved preprocessing into separate functions\n\n* Added # Copied from statements\n\n* Fixed # Copied from statements\n\n* Added `segmentation_maps` support for DPT image processor\n\n* Added tests for dpt image processor\n\n* Moved preprocessing into separate functions\n\n* Added # Copied from statements\n\n* Fixed # Copied from statements",
    "sha": "5450e7c84afc9e93e897705de0dfb6b53b2b07e2",
    "files": [
        {
            "sha": "3c2162409c578af88e0bc0375137bcd70550efa4",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 196,
            "deletions": 36,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/5450e7c84afc9e93e897705de0dfb6b53b2b07e2/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5450e7c84afc9e93e897705de0dfb6b53b2b07e2/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=5450e7c84afc9e93e897705de0dfb6b53b2b07e2",
            "patch": "@@ -139,6 +139,11 @@ class DPTImageProcessor(BaseImageProcessor):\n         size_divisor (`int`, *optional*):\n             If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n             DINOv2 paper, which uses the model in combination with DPT.\n+        do_reduce_labels (`bool`, *optional*, defaults to `False`):\n+            Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0 is\n+            used for background, and background itself is not included in all classes of a dataset (e.g. ADE20k). The\n+            background label will be replaced by 255. Can be overridden by the `do_reduce_labels` parameter in the\n+            `preprocess` method.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -157,6 +162,7 @@ def __init__(\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = False,\n         size_divisor: int = None,\n+        do_reduce_labels: bool = False,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -174,6 +180,7 @@ def __init__(\n         self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n         self.do_pad = do_pad\n         self.size_divisor = size_divisor\n+        self.do_reduce_labels = do_reduce_labels\n \n     def resize(\n         self,\n@@ -275,10 +282,160 @@ def _get_pad(size, size_divisor):\n \n         return pad(image, ((pad_size_left, pad_size_right), (pad_size_top, pad_size_bottom)), data_format=data_format)\n \n+    # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.reduce_label\n+    def reduce_label(self, label: ImageInput) -> np.ndarray:\n+        label = to_numpy_array(label)\n+        # Avoid using underflow conversion\n+        label[label == 0] = 255\n+        label = label - 1\n+        label[label == 254] = 255\n+        return label\n+\n+    def _preprocess(\n+        self,\n+        image: ImageInput,\n+        do_reduce_labels: bool = None,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        keep_aspect_ratio: bool = None,\n+        ensure_multiple_of: int = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: bool = None,\n+        size_divisor: int = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        if do_reduce_labels:\n+            image = self.reduce_label(image)\n+\n+        if do_resize:\n+            image = self.resize(\n+                image=image,\n+                size=size,\n+                resample=resample,\n+                keep_aspect_ratio=keep_aspect_ratio,\n+                ensure_multiple_of=ensure_multiple_of,\n+                input_data_format=input_data_format,\n+            )\n+\n+        if do_rescale:\n+            image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+        if do_normalize:\n+            image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+\n+        if do_pad:\n+            image = self.pad_image(image=image, size_divisor=size_divisor, input_data_format=input_data_format)\n+\n+        return image\n+\n+    def _preprocess_image(\n+        self,\n+        image: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        keep_aspect_ratio: bool = None,\n+        ensure_multiple_of: int = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_pad: bool = None,\n+        size_divisor: int = None,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> np.ndarray:\n+        \"\"\"Preprocesses a single image.\"\"\"\n+        # All transformations expect numpy arrays.\n+        image = to_numpy_array(image)\n+        if do_rescale and is_scaled_image(image):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(image)\n+\n+        image = self._preprocess(\n+            image,\n+            do_reduce_labels=False,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            ensure_multiple_of=ensure_multiple_of,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_pad=do_pad,\n+            size_divisor=size_divisor,\n+            input_data_format=input_data_format,\n+        )\n+        if data_format is not None:\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+        return image\n+\n+    def _preprocess_segmentation_map(\n+        self,\n+        segmentation_map: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        keep_aspect_ratio: bool = None,\n+        ensure_multiple_of: int = None,\n+        do_reduce_labels: bool = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        \"\"\"Preprocesses a single segmentation map.\"\"\"\n+        # All transformations expect numpy arrays.\n+        segmentation_map = to_numpy_array(segmentation_map)\n+        # Add an axis to the segmentation maps for transformations.\n+        if segmentation_map.ndim == 2:\n+            segmentation_map = segmentation_map[None, ...]\n+            added_dimension = True\n+            input_data_format = ChannelDimension.FIRST\n+        else:\n+            added_dimension = False\n+            if input_data_format is None:\n+                input_data_format = infer_channel_dimension_format(segmentation_map, num_channels=1)\n+        segmentation_map = self._preprocess(\n+            image=segmentation_map,\n+            do_reduce_labels=do_reduce_labels,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+            keep_aspect_ratio=keep_aspect_ratio,\n+            ensure_multiple_of=ensure_multiple_of,\n+            do_normalize=False,\n+            do_rescale=False,\n+            input_data_format=input_data_format,\n+        )\n+        # Remove extra axis if added\n+        if added_dimension:\n+            segmentation_map = np.squeeze(segmentation_map, axis=0)\n+        segmentation_map = segmentation_map.astype(np.int64)\n+        return segmentation_map\n+\n+    # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.__call__\n+    def __call__(self, images, segmentation_maps=None, **kwargs):\n+        # Overrides the `__call__` method of the `Preprocessor` class such that the images and segmentation maps can both\n+        # be passed in as positional arguments.\n+        return super().__call__(images, segmentation_maps=segmentation_maps, **kwargs)\n+\n     @filter_out_non_signature_kwargs()\n     def preprocess(\n         self,\n         images: ImageInput,\n+        segmentation_maps: Optional[ImageInput] = None,\n         do_resize: bool = None,\n         size: int = None,\n         keep_aspect_ratio: bool = None,\n@@ -291,6 +448,7 @@ def preprocess(\n         image_std: Optional[Union[float, List[float]]] = None,\n         do_pad: bool = None,\n         size_divisor: int = None,\n+        do_reduce_labels: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -302,6 +460,8 @@ def preprocess(\n             images (`ImageInput`):\n                 Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n                 passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            segmentation_maps (`ImageInput`, *optional*):\n+                Segmentation map to preprocess.\n             do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n                 Whether to resize the image.\n             size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n@@ -326,6 +486,10 @@ def preprocess(\n                 Image mean.\n             image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n                 Image standard deviation.\n+            do_reduce_labels (`bool`, *optional*, defaults to `self.do_reduce_labels`):\n+                Whether or not to reduce all label values of segmentation maps by 1. Usually used for datasets where 0\n+                is used for background, and background itself is not included in all classes of a dataset (e.g.\n+                ADE20k). The background label will be replaced by 255.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -357,9 +521,13 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n+        do_reduce_labels = do_reduce_labels if do_reduce_labels is not None else self.do_reduce_labels\n \n         images = make_list_of_images(images)\n \n+        if segmentation_maps is not None:\n+            segmentation_maps = make_list_of_images(segmentation_maps, expected_ndims=2)\n+\n         if not valid_images(images):\n             raise ValueError(\n                 \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n@@ -377,55 +545,47 @@ def preprocess(\n             size=size,\n             resample=resample,\n         )\n-        # All transformations expect numpy arrays.\n-        images = [to_numpy_array(image) for image in images]\n \n-        if do_rescale and is_scaled_image(images[0]):\n-            logger.warning_once(\n-                \"It looks like you are trying to rescale already rescaled images. If the input\"\n-                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+        images = [\n+            self._preprocess_image(\n+                image=img,\n+                do_resize=do_resize,\n+                do_rescale=do_rescale,\n+                do_normalize=do_normalize,\n+                do_pad=do_pad,\n+                size=size,\n+                resample=resample,\n+                keep_aspect_ratio=keep_aspect_ratio,\n+                ensure_multiple_of=ensure_multiple_of,\n+                rescale_factor=rescale_factor,\n+                image_mean=image_mean,\n+                image_std=image_std,\n+                size_divisor=size_divisor,\n+                data_format=data_format,\n+                input_data_format=input_data_format,\n             )\n+            for img in images\n+        ]\n \n-        if input_data_format is None:\n-            # We assume that all images have the same channel dimension format.\n-            input_data_format = infer_channel_dimension_format(images[0])\n+        data = {\"pixel_values\": images}\n \n-        if do_resize:\n-            images = [\n-                self.resize(\n-                    image=image,\n+        if segmentation_maps is not None:\n+            segmentation_maps = [\n+                self._preprocess_segmentation_map(\n+                    segmentation_map=segmentation_map,\n+                    do_reduce_labels=do_reduce_labels,\n+                    do_resize=do_resize,\n                     size=size,\n                     resample=resample,\n                     keep_aspect_ratio=keep_aspect_ratio,\n                     ensure_multiple_of=ensure_multiple_of,\n                     input_data_format=input_data_format,\n                 )\n-                for image in images\n+                for segmentation_map in segmentation_maps\n             ]\n \n-        if do_rescale:\n-            images = [\n-                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n+            data[\"labels\"] = segmentation_maps\n \n-        if do_normalize:\n-            images = [\n-                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        if do_pad:\n-            images = [\n-                self.pad_image(image=image, size_divisor=size_divisor, input_data_format=input_data_format)\n-                for image in images\n-            ]\n-\n-        images = [\n-            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n-        ]\n-\n-        data = {\"pixel_values\": images}\n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.beit.image_processing_beit.BeitImageProcessor.post_process_semantic_segmentation with Beit->DPT"
        },
        {
            "sha": "713c722a4c2b5d99de64f9de34428fe05cc89351",
            "filename": "tests/models/dpt/test_image_processing_dpt.py",
            "status": "modified",
            "additions": 156,
            "deletions": 1,
            "changes": 157,
            "blob_url": "https://github.com/huggingface/transformers/blob/5450e7c84afc9e93e897705de0dfb6b53b2b07e2/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5450e7c84afc9e93e897705de0dfb6b53b2b07e2/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py?ref=5450e7c84afc9e93e897705de0dfb6b53b2b07e2",
            "patch": "@@ -17,14 +17,20 @@\n import unittest\n \n import numpy as np\n+from datasets import load_dataset\n \n-from transformers.file_utils import is_vision_available\n+from transformers.file_utils import is_torch_available, is_vision_available\n from transformers.testing_utils import require_torch, require_vision\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_vision_available():\n+    from PIL import Image\n+\n     from transformers import DPTImageProcessor\n \n \n@@ -42,6 +48,7 @@ def __init__(\n         do_normalize=True,\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n+        do_reduce_labels=False,\n     ):\n         super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n@@ -56,6 +63,7 @@ def __init__(\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n+        self.do_reduce_labels = do_reduce_labels\n \n     def prepare_image_processor_dict(self):\n         return {\n@@ -64,6 +72,7 @@ def prepare_image_processor_dict(self):\n             \"do_normalize\": self.do_normalize,\n             \"do_resize\": self.do_resize,\n             \"size\": self.size,\n+            \"do_reduce_labels\": self.do_reduce_labels,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -81,6 +90,28 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n \n \n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_single_inputs\n+def prepare_semantic_single_inputs():\n+    dataset = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n+\n+    image = Image.open(dataset[0][\"file\"])\n+    map = Image.open(dataset[1][\"file\"])\n+\n+    return image, map\n+\n+\n+# Copied from transformers.tests.models.beit.test_image_processing_beit.prepare_semantic_batch_inputs\n+def prepare_semantic_batch_inputs():\n+    ds = load_dataset(\"hf-internal-testing/fixtures_ade20k\", split=\"test\", trust_remote_code=True)\n+\n+    image1 = Image.open(ds[0][\"file\"])\n+    map1 = Image.open(ds[1][\"file\"])\n+    image2 = Image.open(ds[2][\"file\"])\n+    map2 = Image.open(ds[3][\"file\"])\n+\n+    return [image1, image2], [map1, map2]\n+\n+\n @require_torch\n @require_vision\n class DPTImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n@@ -105,6 +136,7 @@ def test_image_processor_properties(self):\n         self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n         self.assertTrue(hasattr(image_processing, \"do_pad\"))\n         self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_reduce_labels\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n         image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n@@ -138,3 +170,126 @@ def test_keep_aspect_ratio(self):\n         pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n \n         self.assertEqual(list(pixel_values.shape), [1, 3, 512, 672])\n+\n+    # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_call_segmentation_maps\n+    def test_call_segmentation_maps(self):\n+        # Initialize image_processor\n+        image_processor = self.image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        maps = []\n+        for image in image_inputs:\n+            self.assertIsInstance(image, torch.Tensor)\n+            maps.append(torch.zeros(image.shape[-2:]).long())\n+\n+        # Test not batched input\n+        encoding = image_processor(image_inputs[0], maps[0], return_tensors=\"pt\")\n+        self.assertEqual(\n+            encoding[\"pixel_values\"].shape,\n+            (\n+                1,\n+                self.image_processor_tester.num_channels,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(\n+            encoding[\"labels\"].shape,\n+            (\n+                1,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+        # Test batched\n+        encoding = image_processor(image_inputs, maps, return_tensors=\"pt\")\n+        self.assertEqual(\n+            encoding[\"pixel_values\"].shape,\n+            (\n+                self.image_processor_tester.batch_size,\n+                self.image_processor_tester.num_channels,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(\n+            encoding[\"labels\"].shape,\n+            (\n+                self.image_processor_tester.batch_size,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+        # Test not batched input (PIL images)\n+        image, segmentation_map = prepare_semantic_single_inputs()\n+\n+        encoding = image_processor(image, segmentation_map, return_tensors=\"pt\")\n+        self.assertEqual(\n+            encoding[\"pixel_values\"].shape,\n+            (\n+                1,\n+                self.image_processor_tester.num_channels,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(\n+            encoding[\"labels\"].shape,\n+            (\n+                1,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+        # Test batched input (PIL images)\n+        images, segmentation_maps = prepare_semantic_batch_inputs()\n+\n+        encoding = image_processor(images, segmentation_maps, return_tensors=\"pt\")\n+        self.assertEqual(\n+            encoding[\"pixel_values\"].shape,\n+            (\n+                2,\n+                self.image_processor_tester.num_channels,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(\n+            encoding[\"labels\"].shape,\n+            (\n+                2,\n+                self.image_processor_tester.size[\"height\"],\n+                self.image_processor_tester.size[\"width\"],\n+            ),\n+        )\n+        self.assertEqual(encoding[\"labels\"].dtype, torch.long)\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 255)\n+\n+    # Copied from transformers.tests.models.beit.test_image_processing_beit.BeitImageProcessingTest.test_reduce_labels\n+    def test_reduce_labels(self):\n+        # Initialize image_processor\n+        image_processor = self.image_processing_class(**self.image_processor_dict)\n+\n+        # ADE20k has 150 classes, and the background is included, so labels should be between 0 and 150\n+        image, map = prepare_semantic_single_inputs()\n+        encoding = image_processor(image, map, return_tensors=\"pt\")\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 150)\n+\n+        image_processor.do_reduce_labels = True\n+        encoding = image_processor(image, map, return_tensors=\"pt\")\n+        self.assertTrue(encoding[\"labels\"].min().item() >= 0)\n+        self.assertTrue(encoding[\"labels\"].max().item() <= 255)"
        }
    ],
    "stats": {
        "total": 389,
        "additions": 352,
        "deletions": 37
    }
}