{
    "author": "vasqu",
    "message": "[`Ernie 4.5 VL Moe`] Fix non contiguous params (#43134)\n\n* fix\n\n* add test\n\n* aya\n\n* skip timm backbone",
    "sha": "828c039e08dd91af7ce453567ab1da3370df1f04",
    "files": [
        {
            "sha": "734521bb2bf98c163ac9b64418d54469a7b4ec3e",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/828c039e08dd91af7ce453567ab1da3370df1f04/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828c039e08dd91af7ce453567ab1da3370df1f04/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=828c039e08dd91af7ce453567ab1da3370df1f04",
            "patch": "@@ -432,7 +432,7 @@ def convert(\n             tensor = input_dict.get(key, [])\n             if len(tensor) != 1:\n                 raise ValueError(f\"Transpose conversion requires exactly one tensor, found {len(tensor)}.\")\n-            output[target_pattern] = torch.transpose(tensor[0], dim0=self.dim0, dim1=self.dim1)\n+            output[target_pattern] = torch.transpose(tensor[0], dim0=self.dim0, dim1=self.dim1).contiguous()\n         return output\n \n     @property"
        },
        {
            "sha": "7e9b2da3627d2b051642fc753403b8c78a5ef7e6",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/828c039e08dd91af7ce453567ab1da3370df1f04/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828c039e08dd91af7ce453567ab1da3370df1f04/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=828c039e08dd91af7ce453567ab1da3370df1f04",
            "patch": "@@ -155,6 +155,10 @@ def test_from_pretrained_no_checkpoint(self):\n     def test_save_load(self):\n         pass\n \n+    @unittest.skip(reason=\"Only checkpoints on timm can be loaded into TimmBackbone\")\n+    def test_load_contiguous_weights(self):\n+        pass\n+\n     @unittest.skip(reason=\"TimmBackbone uses its own `from_pretrained` without device_map support\")\n     def test_can_load_with_device_context_manager(self):\n         pass"
        },
        {
            "sha": "061638bc5273ba172b88b9d1296fcae21d232db8",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/828c039e08dd91af7ce453567ab1da3370df1f04/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/828c039e08dd91af7ce453567ab1da3370df1f04/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=828c039e08dd91af7ce453567ab1da3370df1f04",
            "patch": "@@ -933,6 +933,23 @@ def test_save_load_keys_to_ignore_on_save(self):\n                     )\n                     self.assertTrue(len(load_result.unexpected_keys) == 0)\n \n+    def test_load_contiguous_weights(self):\n+        \"\"\"\n+        Checks whether the loaded weights are contiguous or not; inherently checking whether a conversion\n+        operation from `core_model_loading` may have affected the original weights.\n+        \"\"\"\n+        for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            model = model_class(config)\n+            self.assertTrue(all(param.is_contiguous() for param in list(model.parameters())))\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                model = model_class.from_pretrained(tmpdirname)\n+                self.assertTrue(all(param.is_contiguous() for param in list(model.parameters())))\n+\n     def test_gradient_checkpointing_backward_compatibility(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        }
    ],
    "stats": {
        "total": 23,
        "additions": 22,
        "deletions": 1
    }
}