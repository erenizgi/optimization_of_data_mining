{
    "author": "manueldeprada",
    "message": "Align assisted generate for unified signature in decoding methods (#40657)\n\n* Squashed previous branch\n\n* unify assisted generate to common decoding method signature\n\n* move checks to validate steps where possible\n\n* fix csm and other models that override _sample\n\n* ops dia you again\n\n* opsie\n\n* joao review",
    "sha": "acd820561f1438d60e74733ab3edbcacb63e55a6",
    "files": [
        {
            "sha": "3ac83b854978255f616e1dec130e74ef12fe8fdb",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 89,
            "deletions": 94,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/acd820561f1438d60e74733ab3edbcacb63e55a6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acd820561f1438d60e74733ab3edbcacb63e55a6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=acd820561f1438d60e74733ab3edbcacb63e55a6",
            "patch": "@@ -129,6 +129,19 @@\n     \"past_buckets_states\",  # reformer\n ]\n \n+GENERATION_MODES_MAPPING = {\n+    GenerationMode.SAMPLE: \"_sample\",\n+    GenerationMode.GREEDY_SEARCH: \"_sample\",\n+    GenerationMode.BEAM_SEARCH: \"_beam_search\",\n+    GenerationMode.BEAM_SAMPLE: \"_beam_search\",\n+    GenerationMode.ASSISTED_GENERATION: \"_assisted_decoding\",\n+    # Deprecated methods\n+    GenerationMode.DOLA_GENERATION: \"transformers-community/dola\",\n+    GenerationMode.CONTRASTIVE_SEARCH: \"transformers-community/contrastive-search\",\n+    GenerationMode.GROUP_BEAM_SEARCH: \"transformers-community/group-beam-search\",\n+    GenerationMode.CONSTRAINED_BEAM_SEARCH: \"transformers-community/constrained-beam-search\",\n+}\n+\n \n @dataclass\n class GenerateDecoderOnlyOutput(ModelOutput):\n@@ -1492,12 +1505,25 @@ def compute_transition_scores(\n \n         return transition_scores\n \n-    def _validate_generation_mode(self, generation_mode, generation_mode_kwargs):\n+    def _validate_generation_mode(self, generation_mode, generation_config, generation_mode_kwargs):\n         if generation_mode == GenerationMode.BEAM_SEARCH and \"streamer\" in generation_mode_kwargs:\n             raise ValueError(\n                 \"`streamer` cannot be used with beam search (yet!). Make sure that `num_beams` is set to 1.\"\n             )\n \n+        if generation_mode == GenerationMode.ASSISTED_GENERATION:\n+            if generation_config.num_return_sequences > 1:\n+                raise ValueError(\n+                    \"num_return_sequences has to be 1 when doing assisted generate, \"\n+                    f\"but is {generation_config.num_return_sequences}.\"\n+                )\n+            if self._is_stateful:\n+                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n+                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n+                raise ValueError(\n+                    f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\"\n+                )\n+\n         if (assistant_model := generation_mode_kwargs.get(\"assistant_model\")) is not None:\n             if self.config.is_encoder_decoder and not assistant_model.config.is_encoder_decoder:\n                 attributes_to_check = [\"encoder_attention_heads\", \"encoder_ffn_dim\", \"encoder_layers\"]\n@@ -2136,16 +2162,9 @@ def _get_deprecated_gen_repo(\n         \"\"\"\n         Returns the Hub repo for a deprecated generation mode, if any.\n         \"\"\"\n-        moved_to_hub_modes = {\n-            GenerationMode.DOLA_GENERATION: \"transformers-community/dola\",\n-            GenerationMode.CONTRASTIVE_SEARCH: \"transformers-community/contrastive-search\",\n-            GenerationMode.GROUP_BEAM_SEARCH: \"transformers-community/group-beam-search\",\n-            GenerationMode.CONSTRAINED_BEAM_SEARCH: \"transformers-community/constrained-beam-search\",\n-        }\n-        if custom_generate is not None or generation_mode not in moved_to_hub_modes:\n+        if custom_generate is not None or \"/\" not in (repo := GENERATION_MODES_MAPPING[generation_mode]):\n             return None\n \n-        repo = moved_to_hub_modes[generation_mode]\n         logger.warning_once(\n             f\"{generation_mode.name.replace('_', ' ').title()} was moved to a `custom_generate` repo: https://hf.co/{repo}. \"\n             f\"To prevent loss of backward compatibility, add `custom_generate='{repo}'` \"\n@@ -2175,10 +2194,11 @@ def _extract_generation_mode_kwargs(\n             \"assistant_model\": assistant_model,\n             \"streamer\": streamer,\n         }\n-        if synced_gpus is not None:\n-            generation_mode_kwargs[\"synced_gpus\"] = (\n-                is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)\n-            ) and dist.get_world_size() > 1\n+        generation_mode_kwargs[\"synced_gpus\"] = (\n+            (is_deepspeed_zero3_enabled() or is_fsdp_managed_module(self)) and dist.get_world_size() > 1\n+            if synced_gpus is None\n+            else synced_gpus\n+        )\n         generation_mode_kwargs = {k: v for k, v in generation_mode_kwargs.items() if v is not None}\n         # Custom_generate callables can have their own set of arguments\n         # To extract them, we compare the signature with the standard _sample method\n@@ -2338,15 +2358,20 @@ def generate(\n             generation_config, use_model_defaults, **kwargs\n         )\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n+        if isinstance(custom_generate, Callable):\n+            decoding_method = custom_generate\n+        else:\n+            # type() required to access the unbound class-level method\n+            decoding_method = getattr(type(self), GENERATION_MODES_MAPPING[generation_mode])\n \n         self._validate_model_kwargs(model_kwargs.copy())\n-        self._validate_generation_mode(generation_mode, generation_mode_kwargs)\n+        self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n \n         # Deprecation-related step: set Hub repo for deprecated strategies.\n         # NOTE: This must come after initializing generation_config, since we need it to determine if this is a deprecated mode.\n         # It must also be before any preparation steps, since Hub repos expect to be loaded before preparation steps.\n         # TODO joao, manuel: remove this in v4.62.0\n-        if deprecate_mode_repo := self._get_deprecated_gen_repo(generation_mode, trust_remote_code, custom_generate):\n+        if deprecated_mode_repo := self._get_deprecated_gen_repo(generation_mode, trust_remote_code, custom_generate):\n             return GenerationMixin.generate(\n                 self,\n                 inputs=inputs,\n@@ -2358,7 +2383,7 @@ def generate(\n                 negative_prompt_ids=negative_prompt_ids,\n                 negative_prompt_attention_mask=negative_prompt_attention_mask,\n                 use_model_defaults=use_model_defaults,\n-                custom_generate=deprecate_mode_repo,\n+                custom_generate=deprecated_mode_repo,\n                 trust_remote_code=trust_remote_code,\n                 **generation_mode_kwargs,\n                 **kwargs,\n@@ -2376,6 +2401,9 @@ def generate(\n         inputs_tensor, model_input_name, model_kwargs = self._prepare_model_inputs(\n             inputs, generation_config.bos_token_id, model_kwargs\n         )\n+        # Some generation modes (e.g. assisted) need `inputs_tensor` to rerun encoder.forward()\n+        if \"inputs_tensor\" in inspect.signature(decoding_method).parameters.keys():\n+            generation_mode_kwargs[\"inputs_tensor\"] = inputs_tensor\n         batch_size = inputs_tensor.shape[0]\n \n         device = inputs_tensor.device\n@@ -2511,80 +2539,16 @@ def generate(\n         # Set model_kwargs `use_cache` so we can use it later in forward runs\n         model_kwargs[\"use_cache\"] = generation_config.use_cache\n \n-        # 9. go into different generation modes\n-        if isinstance(custom_generate, Callable):\n-            result = custom_generate(\n-                self,\n-                input_ids,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                **generation_mode_kwargs,\n-                **model_kwargs,\n-            )\n-        elif generation_mode == GenerationMode.ASSISTED_GENERATION:\n-            if generation_config.num_return_sequences > 1:\n-                raise ValueError(\n-                    \"num_return_sequences has to be 1 when doing assisted generate, \"\n-                    f\"but is {generation_config.num_return_sequences}.\"\n-                )\n-            if batch_size > 1:\n-                raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n-            if not model_kwargs[\"use_cache\"]:\n-                raise ValueError(\"assisted generate requires `use_cache=True`\")\n-            if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"]:\n-                raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n-            if self._is_stateful:\n-                # In assisted generation we need the ability to confirm whether the model would pick certain tokens,\n-                # which is not possible with stateful models (they can't reset to a previous subset of generated text)\n-                raise ValueError(\n-                    f\"assisted generation is not supported with stateful models, such as {self.__class__.__name__}\"\n-                )\n-\n-            # 10. Get the candidate generator, given the parameterization\n-            candidate_generator = self._get_candidate_generator(\n-                generation_config=generation_config,\n-                input_ids=input_ids,\n-                inputs_tensor=inputs_tensor,\n-                assistant_model=generation_mode_kwargs.pop(\"assistant_model\", None),\n-                logits_processor=logits_processor,\n-                target_tokenizer=generation_mode_kwargs.pop(\"tokenizer\", None),\n-                assistant_tokenizer=generation_mode_kwargs.pop(\"assistant_tokenizer\", None),\n-                model_kwargs=model_kwargs,\n-            )\n-\n-            # 11. run assisted generate\n-            result = self._assisted_decoding(\n-                input_ids,\n-                candidate_generator=candidate_generator,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                **generation_mode_kwargs,\n-                **model_kwargs,\n-            )\n-\n-        elif generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n-            # 10. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n-            result = self._sample(\n-                input_ids,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                **generation_mode_kwargs,\n-                **model_kwargs,\n-            )\n-\n-        elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n-            # 10. run beam sample\n-            result = self._beam_search(\n-                input_ids,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                **generation_mode_kwargs,\n-                **model_kwargs,\n-            )\n+        # 9. Call generation mode\n+        result = decoding_method(\n+            self,\n+            input_ids,\n+            logits_processor=prepared_logits_processor,\n+            stopping_criteria=prepared_stopping_criteria,\n+            generation_config=generation_config,\n+            **generation_mode_kwargs,\n+            **model_kwargs,\n+        )\n \n         # Convert to legacy cache format if requested\n         if (\n@@ -3466,12 +3430,15 @@ def _beam_search(\n     def _assisted_decoding(\n         self,\n         input_ids: torch.LongTensor,\n-        candidate_generator: CandidateGenerator,\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n         synced_gpus: bool = False,\n         streamer: Optional[\"BaseStreamer\"] = None,\n+        inputs_tensor: torch.FloatTensor = None,\n+        assistant_model: Optional[\"PreTrainedModel\"] = None,\n+        assistant_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n+        tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         **model_kwargs,\n     ) -> Union[GenerateNonBeamOutput, torch.LongTensor]:\n         r\"\"\"\n@@ -3483,9 +3450,6 @@ def _assisted_decoding(\n         Parameters:\n             input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n-            candidate_generator (`CandidateGenerator`):\n-                A derived instance of [`CandidateGenerator`] that defines how candidate sequences are generated. For\n-                more information, the documentation of [`CandidateGenerator`] should be read.\n             logits_processor (`LogitsProcessorList`):\n                 An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n                 used to modify the prediction scores of the language modeling head applied at each generation step.\n@@ -3500,6 +3464,15 @@ def _assisted_decoding(\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n+            inputs_tensor (`torch.FloatTensor`, *optional*):\n+                The input tensor for generation. For decoder models, usually `input_ids`. For encoder-decoder models,\n+                the tensor that produced `model_kwargs[\"encoder_outputs\"]`.\n+            assistant_model (`PreTrainedModel`, *optional*):\n+                The model used to assist the generation process. If not provided, the main model will be used.\n+            assistant_tokenizer (`PreTrainedTokenizerBase`, *optional*):\n+                The tokenizer used for the assistant model. If not provided, the token space is assumed to be the same.\n+            tokenizer (`PreTrainedTokenizerBase`, *optional*):\n+                The tokenizer used for the main model. If not provided, the token space is assumed to be the same.\n             model_kwargs:\n                 Additional model specific keyword arguments will be forwarded to the `forward` function of the model.\n                 If model is an encoder-decoder model the kwargs should include `encoder_outputs`.\n@@ -3511,6 +3484,26 @@ def _assisted_decoding(\n             `return_dict_in_generate=True` or a [`~generation.GenerateEncoderDecoderOutput`] if\n             `model.config.is_encoder_decoder=True`.\n         \"\"\"\n+        # The cache must be dynamic for assisted generation, and the check must happen AFTER preparing cache\n+        if not model_kwargs[\"use_cache\"]:\n+            raise ValueError(\"assisted generate requires `use_cache=True`\")\n+        if generation_config.cache_implementation in [\"static\", \"hybrid\", \"sliding_window\"] or (\n+            \"past_key_values\" in model_kwargs\n+            and hasattr(model_kwargs[\"past_key_values\"], \"layers\")\n+            and any(getattr(l, \"is_compileable\", False) for l in model_kwargs[\"past_key_values\"].layers)\n+        ):\n+            raise ValueError(\"assisted generate is not supported with Static cache classes`\")\n+        # Get the candidate generator, given the parameterization\n+        candidate_generator = self._get_candidate_generator(\n+            generation_config=generation_config,\n+            input_ids=input_ids,\n+            inputs_tensor=inputs_tensor,\n+            assistant_model=assistant_model,\n+            logits_processor=logits_processor,\n+            target_tokenizer=tokenizer,\n+            assistant_tokenizer=assistant_tokenizer,\n+            model_kwargs=model_kwargs,\n+        )\n         # init values\n         do_sample = generation_config.do_sample\n         output_attentions = generation_config.output_attentions\n@@ -3535,6 +3528,8 @@ def _assisted_decoding(\n \n         # keep track of which sequences are already finished\n         batch_size, cur_len = input_ids.shape[:2]\n+        if batch_size > 1:\n+            raise ValueError(\"assisted generate is only supported for batch_size = 1\")\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n         model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n "
        },
        {
            "sha": "45ee66d39a97a8f5d2164ce2d3ae89901d360715",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/acd820561f1438d60e74733ab3edbcacb63e55a6/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/acd820561f1438d60e74733ab3edbcacb63e55a6/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=acd820561f1438d60e74733ab3edbcacb63e55a6",
            "patch": "@@ -278,7 +278,7 @@ def _main_generate_loop(\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n \n         self._validate_model_kwargs(model_kwargs.copy())\n-        self._validate_generation_mode(generation_mode, generation_mode_kwargs)\n+        self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n \n         # 2. Set generation parameters if not already defined\n         if synced_gpus is None:"
        }
    ],
    "stats": {
        "total": 185,
        "additions": 90,
        "deletions": 95
    }
}