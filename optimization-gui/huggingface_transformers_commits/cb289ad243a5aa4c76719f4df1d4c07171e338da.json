{
    "author": "pco111",
    "message": "feat(tokenization): add encode_message to tokenize messages one by one (#39507)\n\n* feat(tokenization): add encode_message to tokenize messages one by one\n\n* Fix the `encode_message` method, remove the `add_generation_prompt` parameter and add the corresponding error handling. Update the document to reflect this change and verify the error handling in the test.\n\n* Optimize the `encode_message` method, improve the processing logic of the empty dialogue history, and ensure that the chat template can be applied correctly when the dialogue history is empty. Update the document to reflect these changes.\n\n* The `_encode_message` method is deleted, the message coding logic is simplified, and the functional integrity of the `encode_message` method is ensured. Update the document to reflect these changes.\n\n* Docs fix\n\n* Revert changes in docstring of pad()\n\n* Revert changes in docstring\n\n* Update src/transformers/tokenization_utils_base.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Repair the call of the `encode_message` method, update it to `encode_message_with_chat_template` to support the chat template, and adjust the relevant test cases to reflect this change.\n\n* Optimize the call format of the `apply_chat_template` method, and merge multi-line calls into a single line to improve code readability.\n\n---------\n\nCo-authored-by: pco111 <15262555+pco111@user.noreply.gitee.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "cb289ad243a5aa4c76719f4df1d4c07171e338da",
    "files": [
        {
            "sha": "8f91ca6fcddfbeaf650c35aeb90b2881b3cea742",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 56,
            "deletions": 0,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb289ad243a5aa4c76719f4df1d4c07171e338da/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb289ad243a5aa4c76719f4df1d4c07171e338da/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=cb289ad243a5aa4c76719f4df1d4c07171e338da",
            "patch": "@@ -1694,6 +1694,62 @@ def apply_chat_template(\n         else:\n             return rendered_chat\n \n+    def encode_message_with_chat_template(\n+        self,\n+        message: dict[str, str],\n+        conversation_history: Optional[list[dict[str, str]]] = None,\n+        **kwargs,\n+    ) -> list[int]:\n+        \"\"\"\n+        Tokenize a single message. This method is a convenience wrapper around `apply_chat_template` that allows you\n+        to tokenize messages one by one. This is useful for things like token-by-token streaming.\n+        This method is not guaranteed to be perfect. For some models, it may be impossible to robustly tokenize\n+        single messages. For example, if the chat template adds tokens after each message, but also has a prefix that\n+        is added to the entire chat, it will be impossible to distinguish a chat-start-token from a message-start-token.\n+        In these cases, this method will do its best to find the correct tokenization, but it may not be perfect.\n+        **Note:** This method does not support `add_generation_prompt`. If you want to add a generation prompt,\n+        you should do it separately after tokenizing the conversation.\n+        Args:\n+            message (`dict`):\n+                A dictionary with \"role\" and \"content\" keys, representing the message to tokenize.\n+            conversation_history (`list[dict]`, *optional*):\n+                A list of dicts with \"role\" and \"content\" keys, representing the chat history so far. If you are\n+                tokenizing messages one by one, you should pass the previous messages in the conversation here.\n+            **kwargs:\n+                Additional kwargs to pass to the `apply_chat_template` method.\n+        Returns:\n+            `list[int]`: A list of token ids representing the tokenized message.\n+        \"\"\"\n+        if \"add_generation_prompt\" in kwargs:\n+            raise ValueError(\n+                \"`encode_message_with_chat_template` does not support `add_generation_prompt`. Please add the generation prompt \"\n+                \"separately.\"\n+            )\n+\n+        if conversation_history is None or len(conversation_history) == 0:\n+            return self.apply_chat_template([message], add_generation_prompt=False, tokenize=True, **kwargs)\n+\n+        conversation = conversation_history + [message]\n+        tokens = self.apply_chat_template(conversation, add_generation_prompt=False, tokenize=True, **kwargs)\n+\n+        prefix_tokens = self.apply_chat_template(\n+            conversation_history, add_generation_prompt=False, tokenize=True, **kwargs\n+        )\n+        # It's possible that the prefix tokens are not a prefix of the full list of tokens.\n+        # For example, if the prefix is `<s>User: Hi` and the full conversation is `<s>User: Hi</s><s>Assistant: Hello`.\n+        # In this case, we can't simply find the prefix, so we have to do something a bit more subtle.\n+        # We look for the first place where the tokens differ, and that's our split point.\n+        # This is not perfect, but it's the best we can do without a token-level API.\n+        # To make this more robust, we could do a diff and find the longest common subsequence, but this is\n+        # a good first approximation.\n+        # This is particularly important for models like Llama3 that have changed their chat template to include\n+        # EOS tokens after user messages.\n+        min_len = min(len(prefix_tokens), len(tokens))\n+        for i in range(min_len):\n+            if prefix_tokens[i] != tokens[i]:\n+                return tokens[i:]\n+        return tokens[min_len:]\n+\n     def get_chat_template(self, chat_template: Optional[str] = None, tools: Optional[list[dict]] = None) -> str:\n         \"\"\"\n         Retrieve the chat template string used for tokenizing chat messages. This template is used"
        },
        {
            "sha": "fc74223110f8973cbcf3f60afeadd01b3c51fdbc",
            "filename": "tests/tokenization/test_tokenization_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb289ad243a5aa4c76719f4df1d4c07171e338da/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb289ad243a5aa4c76719f4df1d4c07171e338da/tests%2Ftokenization%2Ftest_tokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftokenization%2Ftest_tokenization_utils.py?ref=cb289ad243a5aa4c76719f4df1d4c07171e338da",
            "patch": "@@ -24,6 +24,7 @@\n import numpy as np\n \n from transformers import (\n+    AutoTokenizer,\n     BatchEncoding,\n     BertTokenizer,\n     BertTokenizerFast,\n@@ -375,3 +376,32 @@ def test_training_new_tokenizer_edge_cases(self):\n         tokenizer = PreTrainedTokenizerFast(tokenizer_object=_tokenizer)\n         toy_text_iterator = (\"a\" for _ in range(1000))\n         tokenizer.train_new_from_iterator(text_iterator=toy_text_iterator, length=1000, vocab_size=50)\n+\n+    def test_encode_message(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n+            {\"role\": \"user\", \"content\": \"Hey there, how are you?\"},\n+            {\"role\": \"assistant\", \"content\": \"Thank you for asking, I am doing well\"},\n+            {\"role\": \"user\", \"content\": \"What's the weather like today?\"},\n+            {\"role\": \"assistant\", \"content\": \"Today the weather is nice\"},\n+        ]\n+\n+        # First, test the default case, where we encode the whole conversation at once\n+        whole_conversation_tokens = tokenizer.apply_chat_template(conversation, tokenize=True)\n+\n+        # Now, test the message-by-message encoding\n+        tokens = []\n+        for i, message in enumerate(conversation):\n+            tokens += tokenizer.encode_message_with_chat_template(message, conversation_history=conversation[:i])\n+\n+        self.assertEqual(whole_conversation_tokens, tokens)\n+\n+    def test_encode_message_raises_on_add_generation_prompt(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-beta\")\n+        conversation = [\n+            {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n+            {\"role\": \"user\", \"content\": \"Hey there, how are you?\"},\n+        ]\n+        with self.assertRaises(ValueError):\n+            tokenizer.encode_message_with_chat_template(conversation[0], add_generation_prompt=True)"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 86,
        "deletions": 0
    }
}