{
    "author": "faaany",
    "message": "[tests] enable bnb tests on xpu (#36233)\n\n* fix failed test\n\n* fix device\n\n* fix more device cases\n\n* add more cases\n\n* fix empty cache\n\n* Update test_4bit.py\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "4dbf17c17f5834eb68f296457acc605a8c533b5a",
    "files": [
        {
            "sha": "a024b801e8bc898a9230f6c9c1083a502ae8f8fa",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -591,12 +591,12 @@ def test_batched_generation(self):\n \n         test_text = \"A sequence: 1, 2\"  # should generate the rest of the sequence\n \n-        unpadded_inputs = tokenizer([test_text], return_tensors=\"pt\").to(\"cuda:0\")\n+        unpadded_inputs = tokenizer([test_text], return_tensors=\"pt\").to(f\"{torch_device}:0\")\n         unpadded_gen_out = model.generate(**unpadded_inputs, max_new_tokens=20)\n         unpadded_gen_text = tokenizer.batch_decode(unpadded_gen_out, skip_special_tokens=True)\n \n         dummy_text = \"This is a longer text \" * 2  # forces left-padding on `test_text`\n-        padded_inputs = tokenizer([test_text, dummy_text], return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n+        padded_inputs = tokenizer([test_text, dummy_text], return_tensors=\"pt\", padding=True).to(f\"{torch_device}:0\")\n         padded_gen_out = model.generate(**padded_inputs, max_new_tokens=20)\n         padded_gen_text = tokenizer.batch_decode(padded_gen_out, skip_special_tokens=True)\n "
        },
        {
            "sha": "f48584d61251c202149220172c6d4741bd0de845",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -35,6 +35,7 @@\n     require_bitsandbytes,\n     require_peft,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -440,7 +441,7 @@ def test_peft_from_pretrained_kwargs(self):\n                 # dummy generation\n                 _ = peft_model.generate(input_ids=torch.LongTensor([[0, 1, 2, 3, 4, 5, 6, 7]]).to(torch_device))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_save_quantized(self):\n         \"\"\"\n@@ -479,7 +480,7 @@ def test_peft_save_quantized(self):\n                     self.assertTrue(\"pytorch_model.bin\" not in os.listdir(tmpdirname))\n                     self.assertTrue(\"model.safetensors\" not in os.listdir(tmpdirname))\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     def test_peft_save_quantized_regression(self):\n         \"\"\""
        },
        {
            "sha": "ea4d87482be1e670c941506743c927a441e42639",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 11,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -32,6 +32,7 @@\n from transformers.models.opt.modeling_opt import OPTAttention\n from transformers.testing_utils import (\n     apply_skip_if_not_implemented,\n+    backend_empty_cache,\n     is_bitsandbytes_available,\n     is_torch_available,\n     require_accelerate,\n@@ -136,7 +137,7 @@ def tearDown(self):\n         del self.model_4bit\n \n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_quantization_num_parameters(self):\n         r\"\"\"\n@@ -224,7 +225,7 @@ def test_generate_quality(self):\n         \"\"\"\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = self.model_4bit.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_4bit.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -242,7 +243,7 @@ def test_generate_quality_config(self):\n \n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = model_4bit_from_config.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(model_4bit_from_config.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -261,7 +262,7 @@ def test_generate_quality_dequantize(self):\n \n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = model_4bit.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(model_4bit.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -277,10 +278,10 @@ def test_device_assignment(self):\n         self.assertEqual(self.model_4bit.device.type, \"cpu\")\n         self.assertAlmostEqual(self.model_4bit.get_memory_footprint(), mem_before)\n \n-        if torch.cuda.is_available():\n+        if torch_device in [\"cuda\", \"xpu\"]:\n             # Move back to CUDA device\n-            self.model_4bit.to(\"cuda\")\n-            self.assertEqual(self.model_4bit.device.type, \"cuda\")\n+            self.model_4bit.to(torch_device)\n+            self.assertEqual(self.model_4bit.device.type, torch_device)\n             self.assertAlmostEqual(self.model_4bit.get_memory_footprint(), mem_before)\n \n     def test_device_and_dtype_assignment(self):\n@@ -323,11 +324,13 @@ def test_device_and_dtype_assignment(self):\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n \n         self.model_fp16 = self.model_fp16.to(torch.float32)\n-        _ = self.model_fp16.generate(input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10)\n+        _ = self.model_fp16.generate(\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_fp16.device), max_new_tokens=10\n+        )\n \n-        if torch.cuda.is_available():\n+        if torch_device in [\"cuda\", \"xpu\"]:\n             # Check that this does not throw an error\n-            _ = self.model_fp16.cuda()\n+            _ = self.model_fp16.to(torch_device)\n \n         # Check this does not throw an error\n         _ = self.model_fp16.to(\"cpu\")\n@@ -617,7 +620,7 @@ class BaseSerializationTest(unittest.TestCase):\n \n     def tearDown(self):\n         gc.collect()\n-        torch.cuda.empty_cache()\n+        backend_empty_cache(torch_device)\n \n     def test_serialization(self, quant_type=\"nf4\", double_quant=True, safe_serialization=True):\n         r\"\"\""
        },
        {
            "sha": "634a2eb16b7e165edd8d8d5d34bb8e11fdcd93b7",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dbf17c17f5834eb68f296457acc605a8c533b5a/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=4dbf17c17f5834eb68f296457acc605a8c533b5a",
            "patch": "@@ -274,7 +274,7 @@ def test_generate_quality(self):\n         \"\"\"\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = self.model_8bit.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_8bit.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -292,7 +292,7 @@ def test_generate_quality_config(self):\n \n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = model_8bit_from_config.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(model_8bit_from_config.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -311,7 +311,7 @@ def test_generate_quality_dequantize(self):\n \n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n         output_sequences = model_8bit.generate(\n-            input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+            input_ids=encoded_input[\"input_ids\"].to(model_8bit.device), max_new_tokens=10\n         )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -362,7 +362,9 @@ def test_device_and_dtype_assignment(self):\n         encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n \n         self.model_fp16 = self.model_fp16.to(torch.float32)\n-        _ = self.model_fp16.generate(input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10)\n+        _ = self.model_fp16.generate(\n+            input_ids=encoded_input[\"input_ids\"].to(self.model_fp16.device), max_new_tokens=10\n+        )\n \n         # Check this does not throw an error\n         _ = self.model_fp16.to(\"cpu\")\n@@ -402,7 +404,7 @@ def test_int8_serialization(self):\n             # generate\n             encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n             output_sequences = model_from_saved.generate(\n-                input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+                input_ids=encoded_input[\"input_ids\"].to(model_from_saved.device), max_new_tokens=10\n             )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n@@ -429,7 +431,7 @@ def test_int8_serialization_regression(self):\n             # generate\n             encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n             output_sequences = model_from_saved.generate(\n-                input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10\n+                input_ids=encoded_input[\"input_ids\"].to(model_from_saved.device), max_new_tokens=10\n             )\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 27,
        "deletions": 21
    }
}