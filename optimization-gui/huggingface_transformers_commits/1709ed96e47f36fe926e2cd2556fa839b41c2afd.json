{
    "author": "lashahub",
    "message": "[models] Add AudioFlamingo3 integration (#40290)\n\n* Audio Flamingo 3 initial integration\n\n* Added local Qwen\n\n* Moving to AF3\n\n* Loading directly from HF\n\n* Formatting\n\n* add snapshot_download\n\n* Loading from hub\n\n* Import gating\n\n* Pass audio arrays directly\n\n* Remove requires_backend\n\n* Move constants to config.json\n\n* Remove redundancies\n\n* Separate tokenizer, cleaner from_pretrained\n\n* Remove LlavaMetaModel\n\n* Remove sound tower wrapper\n\n* Merged BasicSoundEncoder\n\n* Some improvements\n\n* Towards AudioFlamingo3\n\n* Migrate LlavaConfig\n\n* Merge LlavaMetaForCausalLM into AudioFlamingo3ForConditionalGeneration\n\n* Remove redundant lines\n\n* Add AudioFlamingo3PreTrainedModel\n\n* Unified model.safetensors\n\n* Inline MM projector\n\n* Tokenizer in root dir\n\n* Default processor from_pretrained\n\n* Remove tokenizer from modeling\n\n* Added types\n\n* Cleanup\n\n* Docs & license\n\n* device handling\n\n* Change year\n\n* Remove redundant methods\n\n* Use BatchFeature\n\n* Streamline audio feature handling\n\n* Batch inference\n\n* Reorder alphabetically\n\n* Make style check\n\n* Make fixup\n\n* Avoid calls to separate functions\n\n* Remove forward_tower()\n\n* Rename encode_sound to get_audio_features for clarity\n\n* Add batch decoding method to AudioFlamingo3Processor\n\n* Use tensors instead of lists\n\n* Move end embed token eval\n\n* Prepare audio_features_mask in the processor\n\n* No hardcoded 750 and 3000\n\n* Remove _load_sound_mask completely and use WhisperFeatureExtractor\n\n* Compute embeddings separately\n\n* MM Projector is audio adaptor\n\n* Simplify AudioFlamingo3Config initialization with default encoder_config\n\n* Add modular\n\n* Clean up\n\n* make fixup\n\n* Cleanup processing, add params to encoder config\n\n* Remove redundant methods\n\n* update config references, improve method names, and enhance logging in processor\n\n* processor: move FE args to audio_kwargs, use common_kwargs for return_tensors\n\n* Qwen-like processor\n\n* Simplified AudioFlamingo3Processor\n\n* Extract common code from generate() and forward()\n\n* Add conversion script for AudioFlamingo3 to Hugging Face format\n\n* Use save_pretrained()\n\n* Don't overwrite gen config\n\n* Use AutoTokenizer and FE to convert the processor\n\n* minor formatting\n\n* Finalize processor, do token expansion inside\n\n* AudioFlamingo3: refactor docs, types, and audio‚Äìtext feature merge\n\n* AudioFlamingo3 Docs\n\n* Add AudioFlamingo3Processor to AutoProcessor\n\n* Processor tests\n\n* Use audio_config instead of encoder_config\n\n* Add audio_token_id to config\n\n* Cleanup & new keys\n\n* Add links\n\n* Improved processor\n\n* Handle conversational input\n\n* Make processing consistent.\n\n* Add fallback for no sound token, default left padding.\n\n* Cleanup\n\n* Replace manual 4D mask with masking_utils; dtype/device from inputs\n\n* Text only mode\n\n* Finalize processor\n\n* Export processor directly\n\n* Add push_to_hub to converter\n\n* Add model_input_names property to AudioFlamingo3Processor to pass tests\n\n* Processor chat template support\n\n* Added Jinja processor chat template with audio support\n\n* Processor tests\n\n* Model tests\n\n* Added docs\n\n* Don't use common_kwargs in __call__\n\n* Pass 'test_left_padding_compatibility' by never treating padding as content\n\n* Updated docs\n\n* Cleanup docs\n\n* Standardization\n\n* Update conversion script weight mapping.\n\n* Flatten _build_square_attn_mask\n\n* Make style\n\n* Small dim and attn mask fix\n\n* Fix processor padding side bug\n\n* Error handling in converter\n\n* Use position_ids\n\n* Cleanup generation config\n\n* Use precomputed position embeddings in AudioFlamingo3 encoder\n\n* Added usage examples\n\n* Fix generation config\n\n* Integration tests\n\n* Simplify modeling and shift part of mask preparation to processor. And update tests.\n\n* Updated docs\n\n* ASR convenience method\n\n* Fixed tests\n\n* make fixup\n\n* Shift encoder mask preparation to the encoder's forward.\n\n* Change to HF profiles.\n\n* Integration test standardization.\n\n* Clean up before integration test setup.\n\n* Remove strict float32, more similar to Qwen2Audio.\n\n* Use HF dataset links\n\n* Keep weights in BF16\n\n* New audio in tests\n\n* Processor conventions.\n\n* Standardize audio token expansion in processor.\n\n* Add 'strip_prefix' to batch_decode\n\n* Batch decode nits.\n\n* Remove dtype casting.\n\n* Read token ids from tokenizer\n\n* diverse changes according to review\n\n* add training example\n\n* Add missing docstring.\n\n* Fix typos.\n\n* Add audio token docstring.\n\n* Fix fill type.\n\n* Fix docs\n\n* Save converted weights in bf16\n\n* Fix tests\n\n* Keep model in bf16 for tests.\n\n* Update expected results for single.\n\n* Fix integration tests from runner.\n\n* Update reproducer, and dtype nits.\n\n---------\n\nCo-authored-by: Eric B <ebezzam@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>",
    "sha": "1709ed96e47f36fe926e2cd2556fa839b41c2afd",
    "files": [
        {
            "sha": "c92fed507a6dac7d69bf992bc82f6d7efe0af43c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -1008,6 +1008,8 @@\n         title: AltCLIP\n       - local: model_doc/aria\n         title: Aria\n+      - local: model_doc/audioflamingo3\n+        title: AudioFlamingo3\n       - local: model_doc/aya_vision\n         title: AyaVision\n       - local: model_doc/blip"
        },
        {
            "sha": "6afe985973d7f9a612db399a26e15ad8076d9390",
            "filename": "docs/source/en/model_doc/audioflamingo3.md",
            "status": "added",
            "additions": 402,
            "deletions": 0,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,402 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+*This model was released on 2025-07-10 and added to Hugging Face Transformers on 2025-11-11.*\n+\n+# Audio Flamingo 3\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## Overview\n+\n+Audio Flamingo 3 (AF3) is a fully open large audio‚Äìlanguage model designed for robust understanding and reasoning over speech, environmental sounds, and music. AF3 pairs a Whisper-style audio encoder with a causal language model and performs replace-in-place audio‚Äìtext fusion: the processor aligns post-pool audio frames to a dedicated placeholder token and the model replaces those token slots with projected audio embeddings during the forward pass.\n+\n+The model checkpoint is available at: [nvidia/audio-flamingo-3-hf](https://huggingface.co/nvidia/audio-flamingo-3-hf)\n+\n+Highlights:\n+\n+- Unified audio encoder across speech, sound, and music.\n+- **Long-audio support via windowing and post-pool alignment (up to 10 minutes maximum).** The model processes audio in 30-second windows with a hard limit of 20 windows (10 minutes total). Audio longer than 10 minutes will be truncated.\n+- Deterministic fusion that preserves sequence length by replacing audio placeholder tokens with audio embeddings.\n+\n+This model was contributed by [Lasha Koroshinadze](https://huggingface.co/lashahub) and [Eric Bezzam](https://huggingface.co/bezzam).\n+\n+### Paper\n+\n+[Audio Flamingo 3](https://huggingface.co/papers/2507.08128): Advancing Audio Intelligence with Fully Open Large Audio Language Models  \n+A. Goel, S. Ghosh, J. Kim, S. Kumar, Z. Kong, S. Lee, C.-H. H. Yang, R. Duraiswami, D. Manocha, R. Valle, B. Catanzaro  \n+NVIDIA and University of Maryland  \n+Project: https://research.nvidia.com/labs/adlr/AF3/\n+\n+## Usage\n+\n+### Audio Instruct Mode\n+\n+The model supports audio-text instructions, including multi-turn interactions, all processed in batches.\n+\n+‚û°Ô∏è audio + text instruction\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+            {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/WhDJDIviAOg_120_10.mp3\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+‚û°Ô∏è multi-turn:\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"text\",\n+                \"text\": \"Instruction: How does the tone of female speech change throughout the audio? Choose the correct option among the options below: (A) Sad to happy (B) Happy to sad (C) Neutral to happy (D) Happy to neutral.\",\n+            },\n+            {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/000000786159.31.wav\"},\n+        ],\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": [{\"type\": \"text\", \"text\": \"(A) Sad to happy\"}],\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"Why do you think so?\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+‚û°Ô∏è text only:\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"What is the capital of France?\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+‚û°Ô∏è audio only:\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/WhDJDIviAOg_120_10.mp3\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+‚û°Ô∏è batched inference!\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+conversations = [\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+                {\n+                    \"type\": \"audio\",\n+                    \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/t_837b89f2-26aa-4ee2-bdf6-f73f0dd59b26.wav\",\n+                },\n+            ],\n+        }\n+    ],\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\n+                    \"type\": \"text\",\n+                    \"text\": \"This track feels really peaceful and introspective. What elements make it feel so calming and meditative?\",\n+                },\n+                {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/FPSbCAANfbJLVSwD.mp3\"},\n+            ],\n+        }\n+    ],\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversations,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+).to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+print(decoded_outputs)\n+```\n+\n+‚û°Ô∏è Training:\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+model.train()\n+\n+conversation = [\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+                {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/WhDJDIviAOg_120_10.mp3\"},\n+            ],\n+        },\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": [{\"type\": \"text\", \"text\": \"The transcription of the audio is 'summer follows spring the days grow longer and the nights are warm'.\"}],\n+        }\n+    ],\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\n+                    \"type\": \"text\",\n+                    \"text\": \"This track feels really peaceful and introspective. What elements make it feel so calming and meditative?\",\n+                },\n+                {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/FPSbCAANfbJLVSwD.mp3\"},\n+            ],\n+        },\n+        {\n+            \"role\": \"assistant\",\n+            \"content\": [{\"type\": \"text\", \"text\": \"The transcription of the audio is 'some transcription of the audio'.\"}],\n+        }\n+\n+    ]\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    tokenize=True,\n+    add_generation_prompt=True,\n+    return_dict=True,\n+    output_labels=True,\n+).to(model.device)\n+\n+loss = model(**inputs).loss\n+loss.backward()\n+```\n+\n+‚û°Ô∏è transcription shortcut\n+\n+```python\n+from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+model_id = \"nvidia/audio-flamingo-3-hf\"\n+processor = AutoProcessor.from_pretrained(model_id)\n+model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+inputs = processor.apply_transcription_request(audio=\"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/t_837b89f2-26aa-4ee2-bdf6-f73f0dd59b26.wav\").to(model.device)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True, strip_prefix=True)\n+\n+print(decoded_outputs)\n+```\n+\n+The model is trained to emit transcriptions prefixed with assistant framing such as `The spoken content of the audio is \"<text>\".`. Use `strip_prefix=True` (as shown above) to remove the fixed assistant sentence and surrounding quotes so that only the transcription remains.\n+\n+## How the model works\n+\n+### Architecture\n+\n+* **AudioFlamingo3Encoder**\n+  Whisper-style feature extractor + encoder ‚Üí average-pool over time (stride 2) ‚Üí LayerNorm.\n+  Produces per-frame hidden states at the post-pool rate.\n+\n+* **AudioFlamingo3MultiModalProjector**\n+  A small MLP that maps encoder features to the language model‚Äôs hidden size.\n+\n+* **AudioFlamingo3ForConditionalGeneration**\n+  A causal language model that accepts text embeddings where each audio placeholder token slot is replaced, in place, by an audio frame embedding. No sequence-length change is introduced by fusion.\n+\n+### Processor-level alignment\n+\n+1. Each raw waveform is split into fixed-length windows based on the feature extractor‚Äôs `chunk_length` (seconds) and `sampling_rate` (Hz).\n+2. For each window, the processor computes the number of post-pool frames `post_pool_len` that the encoder will output (matching the conv/pool schedule).\n+3. The processor expands the audio placeholder token by the total number of post-pool frames across all windows.\n+4. The model later replaces those token positions with the corresponding projected audio embeddings.\n+\n+## Usage patterns\n+\n+### Transcription shortcut\n+\n+For automatic speech recognition you can skip writing the default instruction each time and call\n+[`~transformers.AudioFlamingo3Processor.apply_transcription_request`]:\n+\n+```python\n+inputs = processor.apply_transcription_request(audio=audio_array)\n+```\n+\n+Pass `prompt=\"Transcribe the input speech.\"` (or a list of prompts for batch audio) to customize the instruction while\n+keeping the audio placeholder handling.\n+\n+`audio` accepts in-memory arrays, local file paths, or URLs. Any processor kwargs (`text_kwargs`, `audio_kwargs`, etc.)\n+are forwarded, so you can tweak padding or tensor formats just like when calling `processor(...)`.\n+\n+## Long audio and windowing\n+\n+**Important: Maximum audio length is 10 minutes.** Audio longer than this will be truncated.\n+\n+* The default setup processes 30-second windows at 16 kHz mono.\n+* **The processor enforces a hard limit of 20 windows per sample, resulting in a maximum of 10 minutes of audio (20 windows √ó 30 seconds).**\n+* For each window:\n+\n+  * `mel_len` is the padded mel length.\n+  * A conv stack reduces time as `conv_output_len = (mel_len - 1) // 2 + 1`.\n+  * Post-pool frames per window: `post_pool_len = (conv_output_len - 2) // 2 + 1`.\n+  * An audio placeholder token is expanded to the sum of `post_pool_len` across all windows.\n+\n+## Padding, attention, and caching\n+\n+* **Left padding vs right padding**\n+  For generation with mixed prompt lengths in a batch, left padding is usually preferable.\n+  For training, right padding is common; AF3‚Äôs fusion mechanism itself is padding-agnostic because it replaces in place.\n+* **Attention masks**\n+  The processor returns `attention_mask` (text) and `input_features_mask` (audio). The model builds an internal 4-D mask on the encoder‚Äôs pre-pool axis with negative infinity at pad positions.\n+* **Caching**\n+  During generation, `input_features` and `input_features_mask` are only passed on the first step. Subsequent steps use cached keys/values from the language model.\n+\n+## Troubleshooting\n+\n+* Empty or truncated outputs when batching\n+  Use left padding for batched generation and decode only the new tokens after the prompt length, as shown in the quickstart.\n+\n+## AudioFlamingo3Config\n+\n+[[autodoc]] AudioFlamingo3Config\n+\n+## AudioFlamingo3EncoderConfig\n+\n+[[autodoc]] AudioFlamingo3EncoderConfig\n+\n+## AudioFlamingo3Processor\n+\n+[[autodoc]] AudioFlamingo3Processor\n+\n+## AudioFlamingo3Encoder\n+\n+[[autodoc]] AudioFlamingo3Encoder\n+    - forward\n+\n+## AudioFlamingo3ForConditionalGeneration\n+\n+[[autodoc]] AudioFlamingo3ForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "3534ce6719d0a4d2f9ea210326a8cf667d72de75",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -25,6 +25,7 @@\n     from .arcee import *\n     from .aria import *\n     from .audio_spectrogram_transformer import *\n+    from .audioflamingo3 import *\n     from .auto import *\n     from .autoformer import *\n     from .aya_vision import *"
        },
        {
            "sha": "1b6d73558d444de1bf0eaa0da74c7856df4baa1a",
            "filename": "src/transformers/models/audioflamingo3/__init__.py",
            "status": "added",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2F__init__.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,31 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_audioflamingo3 import *\n+    from .modeling_audioflamingo3 import *\n+    from .processing_audioflamingo3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6fcd6381309c93d730e8b575975d6d313ec697d0",
            "filename": "src/transformers/models/audioflamingo3/configuration_audioflamingo3.py",
            "status": "added",
            "additions": 210,
            "deletions": 0,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconfiguration_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,210 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class AudioFlamingo3EncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of an [`AudioFlamingo3Encoder`]. It is used to instantiate an\n+    AudioFlamingo3 audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the audio encoder of the AudioFlamingo3\n+    architecture.\n+\n+    e.g. [nvidia/audio-flamingo-3-hf](https://huggingface.co/nvidia/audio-flamingo-3-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        num_mel_bins (`int`, *optional*, defaults to 128):\n+            Number of mel features used per input features. Should correspond to the value used in the\n+            `AudioFlamingo3Processor` class.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of encoder layers.\n+        num_attention_heads (`int`, *optional*, defaults to 20):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        intermediate_size (`int`, *optional*, defaults to 5120):\n+            Dimensionality of the \"intermediate\" (often named feed-forward) layer in encoder.\n+        layerdrop (`float`, *optional*, defaults to 0.0):\n+            The LayerDrop probability for the encoder. See the [LayerDrop paper](https://huggingface.co/papers/1909.11556)\n+            for more details.\n+        activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the layers.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for all fully connected layers in the embeddings, encoder, and pooler.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        scale_embedding (`bool`, *optional*, defaults to `False`):\n+            Scale embeddings by dividing by sqrt(hidden_size).\n+        max_source_positions (`int`, *optional*, defaults to 1500):\n+            The maximum sequence length of log-mel filter-bank features that this model might ever be used with.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import AudioFlamingo3EncoderConfig, AudioFlamingo3Encoder\n+\n+    >>> # Initializing an AudioFlamingo3EncoderConfig\n+    >>> configuration = AudioFlamingo3EncoderConfig()\n+\n+    >>> # Initializing an AudioFlamingo3Encoder (with random weights)\n+    >>> model = AudioFlamingo3Encoder(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"audioflamingo3_encoder\"\n+\n+    attribute_map = {\n+        \"d_model\": \"hidden_size\",\n+        \"encoder_layers\": \"num_hidden_layers\",\n+        \"encoder_attention_heads\": \"num_attention_heads\",\n+        \"encoder_ffn_dim\": \"intermediate_size\",\n+        \"encoder_layerdrop\": \"layerdrop\",\n+    }\n+\n+    def __init__(\n+        self,\n+        num_mel_bins=128,\n+        num_hidden_layers=32,\n+        num_attention_heads=20,\n+        intermediate_size=5120,\n+        layerdrop=0.0,\n+        activation_function=\"gelu\",\n+        hidden_size=1280,\n+        dropout=0.0,\n+        attention_dropout=0.0,\n+        activation_dropout=0.0,\n+        initializer_range=0.02,\n+        scale_embedding=False,\n+        max_source_positions=1500,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.num_mel_bins = num_mel_bins\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.activation_dropout = activation_dropout\n+        self.activation_function = activation_function\n+        self.initializer_range = initializer_range\n+        self.layerdrop = layerdrop\n+        self.num_hidden_layers = num_hidden_layers\n+        self.scale_embedding = scale_embedding\n+        self.max_source_positions = max_source_positions\n+\n+\n+class AudioFlamingo3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of an [`AudioFlamingo3ForConditionalGeneration`]. It is used to instantiate an\n+    AudioFlamingo3 model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the AudioFlamingo3.\n+\n+    e.g. [nvidia/audio-flamingo-3-hf](https://huggingface.co/nvidia/audio-flamingo-3-hf)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        audio_config (`Union[AudioFlamingo3EncoderConfig, dict]`, *optional*, defaults to `AudioFlamingo3EncoderConfig`):\n+            The config object or dictionary of the audio backbone.\n+        text_config (`Union[AutoConfig, dict]`, *optional*, defaults to `Qwen2Config`):\n+            The config object or dictionary of the text backbone.\n+        audio_token_id (`int`, *optional*, defaults to 151669):\n+            The audio token index to encode the audio prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            Activation function used in the projector.\n+        projector_bias (`bool`, *optional*, defaults to `True`):\n+            Whether to include bias terms in the projector.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import AudioFlamingo3ForConditionalGeneration, AudioFlamingo3Config, AudioFlamingo3EncoderConfig, Qwen2Config\n+\n+    >>> # Initializing an AudioFlamingo3Encoder config\n+    >>> audio_config = AudioFlamingo3EncoderConfig()\n+\n+    >>> # Initializing a Qwen2 config\n+    >>> text_config = Qwen2Config()\n+\n+    >>> # Initializing an AudioFlamingo3 configuration\n+    >>> configuration = AudioFlamingo3Config(audio_config, text_config)\n+\n+    >>> # Initializing a model from the audioflamingo3 style configuration\n+    >>> model = AudioFlamingo3ForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"audioflamingo3\"\n+    sub_configs = {\n+        \"audio_config\": AudioFlamingo3EncoderConfig,\n+        \"text_config\": AutoConfig,\n+    }\n+\n+    def __init__(\n+        self,\n+        audio_config=None,\n+        text_config=None,\n+        audio_token_id=151669,\n+        projector_hidden_act=\"gelu\",\n+        projector_bias=True,\n+        **kwargs,\n+    ):\n+        self.audio_token_id = audio_token_id\n+\n+        if isinstance(audio_config, dict):\n+            audio_config[\"model_type\"] = audio_config.get(\"model_type\", \"audioflamingo3_encoder\")\n+            audio_config = CONFIG_MAPPING[audio_config[\"model_type\"]](**audio_config)\n+        elif audio_config is None:\n+            audio_config = CONFIG_MAPPING[\"audioflamingo3_encoder\"]()\n+\n+        self.audio_config = audio_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config.get(\"model_type\", \"qwen2\")\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"qwen2\"]()\n+\n+        self.text_config = text_config\n+        self.projector_hidden_act = projector_hidden_act\n+        self.projector_bias = projector_bias\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"AudioFlamingo3Config\", \"AudioFlamingo3EncoderConfig\"]"
        },
        {
            "sha": "8f5725c3fb072adab0fb47277ffe008e58cbd316",
            "filename": "src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py",
            "status": "added",
            "additions": 286,
            "deletions": 0,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fconvert_audioflamingo3_to_hf.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,286 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"Convert AudioFlamingo3 checkpoints into a Hugging Face repository layout.\"\"\"\n+\n+from __future__ import annotations\n+\n+import argparse\n+import json\n+import logging\n+from collections import defaultdict\n+from pathlib import Path\n+from typing import Any\n+\n+import torch\n+from safetensors.torch import safe_open\n+\n+from transformers import (\n+    AudioFlamingo3Config,\n+    AudioFlamingo3ForConditionalGeneration,\n+    AudioFlamingo3Processor,\n+    AutoTokenizer,\n+    GenerationConfig,\n+    Qwen2Config,\n+    WhisperFeatureExtractor,\n+)\n+\n+\n+logger = logging.getLogger(__name__)\n+logging.basicConfig(level=logging.INFO, format=\"%(levelname)s: %(message)s\")\n+\n+\n+def _load_json(p: Path):\n+    if not p.is_file():\n+        raise FileNotFoundError(f\"Missing JSON: {p}\")\n+    with p.open(\"r\", encoding=\"utf-8\") as f:\n+        return json.load(f)\n+\n+\n+def write_processor(src_root: Path, dst_root: Path):\n+    llm_dir = src_root / \"llm\"\n+\n+    # fmt: off\n+    tokenizer_chat_template = (\n+        \"{% if messages[0]['role'] != 'system' %}\"\n+            \"{{ '<|im_start|>system\\\\nYou are a helpful assistant.<|im_end|>\\\\n' }}\"\n+        \"{% endif %}\"\n+        \"{% for message in messages if message['content'] is not none %}\"\n+            \"{{ '<|im_start|>' + message['role'] + '\\\\n' + message['content'] + '<|im_end|>' + '\\\\n' }}\"\n+        \"{% endfor %}\"\n+        \"{% if add_generation_prompt %}\"\n+            \"{{ '<|im_start|>assistant\\\\n' }}\"\n+        \"{% endif %}\"\n+    )\n+    # fmt: on\n+\n+    # fmt: off\n+    processor_chat_template = (\n+        \"{% if messages[0]['role'] != 'system' %}\"\n+            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n+        \"{% endif %}\"\n+        \"{% for m in messages if m['content'] is not none %}\"\n+            \"<|im_start|>{{ m['role'] }}\\n\"\n+            \"{% if m['content'] is string %}\"\n+                \"{{ m['content'] }}\"\n+            \"{% else %}\"\n+                \"{% set audio = namespace(found=False) %}\"\n+                \"{% set text_buf = namespace(v='') %}\"\n+                \"{% for c in m['content'] %}\"\n+                    \"{% if c.get('type') == 'audio' or 'audio' in c %}\"\n+                        \"{% set audio.found = True %}\"\n+                    \"{% elif c.get('type') == 'text' or 'text' in c %}\"\n+                        \"{% set text_buf.v = text_buf.v + c['text'] %}\"\n+                    \"{% endif %}\"\n+                \"{% endfor %}\"\n+                \"{% if audio.found %}{{ '<sound>' }}{% endif %}{{ text_buf.v }}\"\n+            \"{% endif %}\"\n+            \"<|im_end|>\\n\"\n+        \"{% endfor %}\"\n+        \"{% if add_generation_prompt %}\"\n+            \"<|im_start|>assistant\\n\"\n+        \"{% endif %}\"\n+    )\n+    # fmt: on\n+\n+    processor = AudioFlamingo3Processor(\n+        feature_extractor=WhisperFeatureExtractor(feature_size=128, return_attention_mask=True),\n+        tokenizer=AutoTokenizer.from_pretrained(str(llm_dir), chat_template=tokenizer_chat_template, use_fast=True),\n+        chat_template=processor_chat_template,\n+    )\n+    processor.save_pretrained(str(dst_root))\n+\n+    logger.info(\"processor (tokenizer + preprocessor)\")\n+    return processor\n+\n+\n+PREFIX_MAP = {\n+    \"llm\": \"language_model\",\n+    \"sound_tower\": \"audio_tower\",\n+    \"sound_mm_projector\": \"multi_modal_projector\",\n+}\n+\n+\n+def _resolve_component_dir(dirpath: Path):\n+    if not dirpath.is_dir():\n+        return None\n+    idx = dirpath / \"model.safetensors.index.json\"\n+    mono = dirpath / \"model.safetensors\"\n+    if idx.exists():\n+        wm = _load_json(idx).get(\"weight_map\") or {}\n+        by_shard: dict[str, list[str]] = defaultdict(list)\n+        for k, shard in wm.items():\n+            by_shard[shard].append(k)\n+        return (\"sharded\", dirpath, {k: sorted(v) for k, v in sorted(by_shard.items())})\n+    if mono.exists():\n+        return (\"file\", mono)\n+    cands = sorted([x for x in dirpath.iterdir() if x.suffix == \".safetensors\"])\n+    return (\"file\", cands[0]) if len(cands) == 1 else None\n+\n+\n+def merge_and_shard_weights(src_root: Path, dst_root: Path, processor: AudioFlamingo3Processor):\n+    state: dict[str, Any] = {}\n+    for tag in PREFIX_MAP.keys():\n+        comp = _resolve_component_dir(src_root / tag)\n+        if not comp:\n+            continue\n+\n+        out_prefix = PREFIX_MAP.get(tag, tag)\n+\n+        if comp[0] == \"file\":\n+            fp: Path = comp[1]\n+            with safe_open(str(fp), framework=\"pt\", device=\"cpu\") as f:\n+                for k in f.keys():\n+                    if k == \"__metadata__\":\n+                        continue\n+                    state[f\"{out_prefix}.{k}\"] = f.get_tensor(k)\n+        else:\n+            base: Path = comp[1]\n+            shard_map: dict[str, list[str]] = comp[2]\n+            for shard, keys in shard_map.items():\n+                sp = base / shard\n+                with safe_open(str(sp), framework=\"pt\", device=\"cpu\") as f:\n+                    for k in keys:\n+                        state[f\"{out_prefix}.{k}\"] = f.get_tensor(k)\n+\n+    if not state:\n+        raise FileNotFoundError(\"No tensors found in llm/, sound_tower/, or sound_mm_projector/.\")\n+\n+    tok = processor.tokenizer\n+\n+    text_config = Qwen2Config(\n+        bos_token_id=tok.bos_token_id,\n+        eos_token_id=tok.eos_token_id,\n+        pad_token_id=tok.pad_token_id,\n+        vocab_size=len(tok),\n+        hidden_size=3584,\n+        intermediate_size=18944,\n+        model_max_length=8192,\n+        num_attention_heads=28,\n+        num_hidden_layers=28,\n+        num_key_value_heads=4,\n+        rope_theta=1000000.0,\n+        use_cache=False,\n+    )\n+    config = AudioFlamingo3Config(text_config=text_config, audio_token_id=tok.get_vocab()[\"<sound>\"])\n+    model = AudioFlamingo3ForConditionalGeneration(config).to(dtype=torch.bfloat16)\n+\n+    # Update state dict to new key names if necessary\n+    projector_key_mapping = {\n+        \"multi_modal_projector.layers.0.weight\": \"multi_modal_projector.linear_1.weight\",\n+        \"multi_modal_projector.layers.0.bias\": \"multi_modal_projector.linear_1.bias\",\n+        \"multi_modal_projector.layers.2.weight\": \"multi_modal_projector.linear_2.weight\",\n+        \"multi_modal_projector.layers.2.bias\": \"multi_modal_projector.linear_2.bias\",\n+    }\n+    for old_key, new_key in projector_key_mapping.items():\n+        if old_key in state:\n+            state[new_key] = state.pop(old_key)\n+\n+    # Load weights into the instantiated model so we can push via `push_to_hub` later.\n+    load_res = model.load_state_dict(state, strict=True)\n+    # Enforce a clean load\n+    if getattr(load_res, \"missing_keys\", None) and load_res.missing_keys:\n+        mk = load_res.missing_keys\n+        raise ValueError(f\"Missing keys when loading: {mk[:10]}{' ...' if len(mk) > 10 else ''}\")\n+    if getattr(load_res, \"unexpected_keys\", None) and load_res.unexpected_keys:\n+        uk = load_res.unexpected_keys\n+        raise ValueError(f\"Unexpected keys when loading: {uk[:10]}{' ...' if len(uk) > 10 else ''}\")\n+\n+    generation_config = GenerationConfig(\n+        bos_token_id=tok.bos_token_id,\n+        eos_token_id=tok.eos_token_id,\n+        pad_token_id=tok.pad_token_id,\n+        max_new_tokens=2048,\n+    )\n+    model.generation_config = generation_config\n+\n+    model.save_pretrained(save_directory=str(dst_root))\n+    logger.info(\"model.safetensors index and shards\")\n+    return model\n+\n+\n+\"\"\"\n+Reproducible Usage\n+==================\n+\n+1) Download the original AudioFlamingo-3 weights from NVIDIA (requires Git LFS):\n+\n+```\n+git lfs install\n+git clone https://huggingface.co/nvidia/audio-flamingo-3\n+```\n+\n+This will create a folder `audio-flamingo-3/` containing the original components:\n+`llm/`, `sound_tower/`, and `sound_mm_projector/`.\n+\n+2) Convert to the Hugging Face Transformers format (locally):\n+\n+```\n+python src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py \\\n+  --src_dir audio-flamingo-3 \\\n+  --dst_dir audio-flamingo-3-hf\n+```\n+\n+3) Convert and push directly to the Hub (requires `huggingface-cli login` or `HF_TOKEN`):\n+\n+```\n+python src/transformers/models/audioflamingo3/convert_audioflamingo3_to_hf.py \\\n+  --src_dir audio-flamingo-3 \\\n+  --dst_dir audio-flamingo-3-hf \\\n+  --push_to_hub <username-or-org>/audio-flamingo-3\n+```\n+\n+This command uploads both the processor (tokenizer + feature extractor) and the converted\n+model (sharded safetensors + configs) to the specified Hub repository.\n+\"\"\"\n+\n+\n+def main() -> None:\n+    ap = argparse.ArgumentParser(description=\"Convert AudioFlamingo3 to Hugging Face format.\")\n+    ap.add_argument(\"--src_dir\", required=True, help=\"Source model root directory\")\n+    ap.add_argument(\"--dst_dir\", required=True, help=\"Destination directory for converted model\")\n+    ap.add_argument(\n+        \"--push_to_hub\",\n+        default=None,\n+        type=str,\n+        help=(\n+            \"Optional repository ID to push the converted assets to the Hugging Face Hub, \"\n+            \"e.g. 'username/audio-flamingo-3'.\"\n+        ),\n+    )\n+    args = ap.parse_args()\n+\n+    src_root = Path(args.src_dir).resolve()\n+    if not src_root.is_dir():\n+        raise FileNotFoundError(f\"Source directory not found: {src_root}\")\n+\n+    dst_root = Path(args.dst_dir).resolve()\n+    if dst_root.exists():\n+        raise FileExistsError(f\"Destination already exists: {dst_root}\")\n+\n+    processor = write_processor(src_root, dst_root)\n+    model = merge_and_shard_weights(src_root, dst_root, processor)\n+\n+    # Optionally push converted assets using native push_to_hub only\n+    if args.push_to_hub:\n+        logger.info(\"Pushing processor to the Hub ...\")\n+        processor.push_to_hub(args.push_to_hub)\n+        logger.info(\"Pushing model to the Hub ...\")\n+        model.push_to_hub(args.push_to_hub)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "5bf9031002658ac9fe92a5b21e6c1890401bf603",
            "filename": "src/transformers/models/audioflamingo3/modeling_audioflamingo3.py",
            "status": "added",
            "additions": 628,
            "deletions": 0,
            "changes": 628,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodeling_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,628 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/audioflamingo3/modular_audioflamingo3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_audioflamingo3.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from collections.abc import Callable\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, EncoderDecoderCache\n+from ...generation import GenerationMixin\n+from ...masking_utils import eager_mask, padding_mask_function\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_audioflamingo3 import AudioFlamingo3Config, AudioFlamingo3EncoderConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attn_weights = attn_weights + attention_mask[:, :, :, : key.shape[-2]]\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class AudioFlamingo3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int,\n+        num_heads: int,\n+        dropout: float = 0.0,\n+        is_decoder: bool = False,\n+        bias: bool = True,\n+        is_causal: bool = False,\n+        layer_idx: Optional[int] = None,\n+        config: Optional[AudioFlamingo3Config] = None,\n+    ):\n+        super().__init__()\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.dropout = dropout\n+        self.head_dim = embed_dim // num_heads\n+        self.config = config\n+\n+        if (self.head_dim * num_heads) != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n+                f\" and `num_heads`: {num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+        self.is_decoder = is_decoder\n+        self.is_causal = is_causal\n+\n+        if layer_idx is None and is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+        self.layer_idx = layer_idx\n+\n+        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n+        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        key_value_states: Optional[torch.Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        cache_position: Optional[torch.Tensor] = None,\n+        # TODO: we need a refactor so that the different attention modules can get their specific kwargs\n+        # ATM, we have mixed things encoder, decoder, and encoder-decoder attn\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        # if key_value_states are provided this layer is used as a cross-attention layer\n+        # for the decoder\n+        is_cross_attention = key_value_states is not None\n+\n+        # determine input shapes\n+        bsz, tgt_len = hidden_states.shape[:-1]\n+        q_input_shape = (bsz, tgt_len, -1, self.head_dim)\n+\n+        # Scaling is susceptible to floating point arithmetics' inprecisions\n+        # which can lead to different results (this is dependent from model\n+        # to model, e.g. audioflamingo3 is one such case). We therefore keep the\n+        # original order of scaling to follow the original implementation\n+        # and enforce no scaling (1.0) in the attention call below.\n+        query_states = self.q_proj(hidden_states) * self.scaling\n+        query_states = query_states.view(*q_input_shape)\n+        query_states = query_states.transpose(1, 2).contiguous()\n+\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_values is not None and isinstance(past_key_values, EncoderDecoderCache):\n+            is_updated = past_key_values.is_updated.get(self.layer_idx)\n+            if is_cross_attention:\n+                # after the first generated id, we can subsequently re-use all key/value_states from cache\n+                past_key_values.is_updated[self.layer_idx] = True\n+                past_key_values = past_key_values.cross_attention_cache\n+            else:\n+                past_key_values = past_key_values.self_attention_cache\n+\n+        # use key_value_states if cross attention\n+        current_states = key_value_states if key_value_states is not None else hidden_states\n+        if is_cross_attention and past_key_values and is_updated:\n+            # reuse k,v, cross_attentions\n+            key_states = past_key_values.layers[self.layer_idx].keys\n+            value_states = past_key_values.layers[self.layer_idx].values\n+        else:\n+            key_states = self.k_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            value_states = self.v_proj(current_states).view(bsz, -1, self.num_heads, self.head_dim)\n+            key_states = key_states.transpose(1, 2).contiguous()\n+            value_states = value_states.transpose(1, 2).contiguous()\n+            if past_key_values is not None:\n+                # save all key/value_states to cache to be re-used for fast auto-regressive generation\n+                cache_position = cache_position if not is_cross_attention else None\n+                key_states, value_states = past_key_values.update(\n+                    key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n+                )\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=1.0,\n+            output_attentions=output_attentions,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class AudioFlamingo3EncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: AudioFlamingo3Config):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+\n+        self.self_attn = AudioFlamingo3Attention(\n+            embed_dim=self.embed_dim,\n+            num_heads=config.encoder_attention_heads,\n+            dropout=config.attention_dropout,\n+            config=config,\n+        )\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        if hidden_states.dtype == torch.float16:\n+            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+\n+        return hidden_states, attn_weights\n+\n+\n+@auto_docstring\n+class AudioFlamingo3PreTrainedModel(PreTrainedModel):\n+    config: AudioFlamingo3Config\n+    base_model_prefix = \"model\"\n+    input_modalities = [\"audio\", \"text\"]\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"AudioFlamingo3Attention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of AudioFlamingo3 isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.audio_config.initializer_range\n+        )\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The audio model from AudioFlamingo3 without any head or projection on top.\n+    \"\"\"\n+)\n+class AudioFlamingo3Encoder(AudioFlamingo3PreTrainedModel):\n+    \"\"\"\n+    AudioFlamingo3 encoder: Whisper encoder, average pool (time/2), then LayerNorm.\n+    \"\"\"\n+\n+    # Ignore copy\n+    config: AudioFlamingo3EncoderConfig\n+    main_input_name = \"input_features\"\n+    input_modalities = \"audio\"\n+    _no_split_modules = [\"AudioFlamingo3EncoderLayer\"]\n+\n+    def __init__(self, config: AudioFlamingo3EncoderConfig):\n+        super().__init__(config)\n+        self.dropout = config.dropout\n+        self.layerdrop = config.encoder_layerdrop\n+\n+        embed_dim = config.d_model\n+        self.num_mel_bins = config.num_mel_bins\n+        self.padding_idx = config.pad_token_id\n+        self.max_source_positions = config.max_source_positions\n+        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n+\n+        self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=3, padding=1)\n+        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n+\n+        self.embed_positions = nn.Embedding(self.max_source_positions, embed_dim)\n+        self.embed_positions.requires_grad_(False)\n+\n+        self.layers = nn.ModuleList([AudioFlamingo3EncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.layer_norm = nn.LayerNorm(config.d_model)\n+        # Ignore copy\n+        self.avg_pooler = nn.AvgPool1d(2, stride=2)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _freeze_parameters(self):\n+        for param in self.parameters():\n+            param.requires_grad = False\n+        self._requires_grad = False\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.conv1\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.conv1 = value\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n+                Log-Mel features extracted from raw audio. Use the processor/feature extractor to compute and pad\n+                these features from waveform input.\n+            input_features_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+        \"\"\"\n+\n+        # Prepare attention mask for transformer layers\n+        batch_size = input_features.shape[0]\n+        seq_len = (input_features.shape[-1] - 1) // 2 + 1  # After conv2 downsampling\n+\n+        input_features_lengths = input_features_mask.sum(-1)\n+        input_features_lengths = (input_features_lengths - 1) // 2 + 1  # conv2 downsampling\n+        input_features_mask = torch.arange(seq_len, device=input_features.device) < input_features_lengths[:, None]\n+        attention_mask = eager_mask(\n+            batch_size=batch_size,\n+            cache_position=torch.arange(seq_len, device=input_features.device),\n+            kv_length=seq_len,\n+            mask_function=padding_mask_function(input_features_mask),\n+            dtype=self.conv1.weight.dtype,\n+        )\n+\n+        # Conv front-end\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n+\n+        # Add positions, dropout\n+        hidden_states = inputs_embeds + self.embed_positions.weight\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        # Transformer stack\n+        for layer in self.layers:\n+            drop = self.training and torch.rand([]) < self.layerdrop\n+            if not drop:\n+                hidden_states = layer(hidden_states, attention_mask)[0]\n+\n+        # AvgPool (time/2) + LayerNorm\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+        hidden_states = self.avg_pooler(hidden_states).permute(0, 2, 1)\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+    # Ignore copy\n+    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n+        \"\"\"\n+        Computes the output length of the convolutional layers and the output length of the audio encoder\n+        \"\"\"\n+        input_lengths = (input_lengths - 1) // 2 + 1\n+        output_lengths = (input_lengths - 2) // 2 + 1\n+        return input_lengths, output_lengths\n+\n+\n+class AudioFlamingo3MultiModalProjector(nn.Module):\n+    \"\"\"\n+    Audio adaptor (small MLP) that projects AudioFlamingo3Encoder features\n+    to the LLM embedding space so they can replace `<sound>` tokens.\n+    \"\"\"\n+\n+    def __init__(self, config: AudioFlamingo3Config):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(\n+            config.audio_config.hidden_size, config.text_config.hidden_size, bias=config.projector_bias\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.projector_bias\n+        )\n+\n+    def forward(self, audio_features):\n+        hidden_states = self.linear_1(audio_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The AudioFlamingo3 model which consists of a fine-tuned Whisper encoder, a multi-modal projector and a Qwen2 language model.\n+    \"\"\"\n+)\n+class AudioFlamingo3ForConditionalGeneration(AudioFlamingo3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = None\n+    _tp_plan = None\n+    _pp_plan = None\n+    _keep_in_fp32_modules_strict = None\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.audio_tower = AutoModel.from_config(config.audio_config)\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.multi_modal_projector = AudioFlamingo3MultiModalProjector(config)\n+        # Similar to Qwen2Audio\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_audio_features(\n+        self, input_features: torch.FloatTensor, input_features_mask: torch.Tensor\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        This method is used to get the audio embeddings from input features (a log mel spectrogram), meaning inferring the audio encoder and the multi-modal projector.\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+                Mask to avoid performing attention on padded feature indices.\n+\n+        Returns:\n+            `torch.FloatTensor`:\n+                The audio embeddings.\n+        \"\"\"\n+\n+        # Encode audio\n+        encoder_output = self.audio_tower(input_features, input_features_mask=input_features_mask)\n+        audio_embeds = self.multi_modal_projector(encoder_output.last_hidden_state)\n+\n+        # Mask according to avg pooling (which is after attention blocks)\n+        post_lengths = (input_features_mask.sum(-1) - 2) // 2 + 1\n+        valid_mask = torch.arange(audio_embeds.shape[1], device=post_lengths.device)[None, :] < post_lengths[:, None]\n+        audio_embeds = audio_embeds[valid_mask.to(audio_embeds.device)]\n+        return audio_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+            Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+        >>> model_id = \"nvidia/audio-flamingo-3-hf\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+        >>> conversations = [\n+        >>>     [\n+        >>>         {\n+        >>>             \"role\": \"user\",\n+        >>>             \"content\": [\n+        >>>                 {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+        >>>                 {\n+        >>>                     \"type\": \"audio\",\n+        >>>                     \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/t_837b89f2-26aa-4ee2-bdf6-f73f0dd59b26.wav\",\n+        >>>                 },\n+        >>>             ],\n+        >>>         }\n+        >>>     ],\n+        >>>     [\n+        >>>         {\n+        >>>             \"role\": \"user\",\n+        >>>             \"content\": [\n+        >>>                 {\n+        >>>                     \"type\": \"text\",\n+        >>>                     \"text\": \"This track feels really peaceful and introspective. What elements make it feel so calming and meditative?\",\n+        >>>                 },\n+        >>>                 {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/FPSbCAANfbJLVSwD.mp3\"},\n+        >>>             ],\n+        >>>         }\n+        >>>     ],\n+        >>> ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        >>>     conversations,\n+        >>>     tokenize=True,\n+        >>>     add_generation_prompt=True,\n+        >>>     return_dict=True,\n+        >>> ).to(model.device)\n+\n+        >>> outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+        >>> decoded_outputs = processor.batch_decode(\n+        >>>     outputs[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n+        >>> )\n+        >>> print(decoded_outputs)\n+        [\"The spoken content of the audio is...\", \"The track's calming and meditative feel can be attributed to...\"]\n+        ```\"\"\"\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if input_features is not None and input_ids is not None:\n+            audio_embeds = self.get_audio_features(input_features, input_features_mask)\n+\n+            # replace text-audio token placeholders with audio embeddings\n+            audio_token_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n+            inputs_embeds = inputs_embeds.masked_scatter(\n+                audio_token_mask.to(inputs_embeds.device), audio_embeds.to(inputs_embeds.device)\n+            )\n+\n+        outputs: CausalLMOutputWithPast = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return outputs\n+\n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        input_features_mask = kwargs.pop(\"input_features_mask\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            if input_features is not None:\n+                model_inputs[\"input_features\"] = input_features\n+            if input_features_mask is not None:\n+                model_inputs[\"input_features_mask\"] = input_features_mask\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"AudioFlamingo3ForConditionalGeneration\", \"AudioFlamingo3PreTrainedModel\", \"AudioFlamingo3Encoder\"]"
        },
        {
            "sha": "af17db9bc1da84f904abb8bcac3b02e11b580397",
            "filename": "src/transformers/models/audioflamingo3/modular_audioflamingo3.py",
            "status": "added",
            "additions": 307,
            "deletions": 0,
            "changes": 307,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fmodular_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,307 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...masking_utils import eager_mask, padding_mask_function\n+from ...modeling_outputs import BaseModelOutput, CausalLMOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ..qwen2_audio.modeling_qwen2_audio import (\n+    Qwen2AudioEncoder,\n+    Qwen2AudioPreTrainedModel,\n+)\n+from ..voxtral.modeling_voxtral import VoxtralForConditionalGeneration, VoxtralMultiModalProjector\n+from ..whisper.modeling_whisper import WhisperEncoderLayer\n+from .configuration_audioflamingo3 import AudioFlamingo3Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class AudioFlamingo3EncoderLayer(WhisperEncoderLayer):\n+    pass\n+\n+\n+class AudioFlamingo3PreTrainedModel(Qwen2AudioPreTrainedModel):\n+    pass\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The audio model from AudioFlamingo3 without any head or projection on top.\n+    \"\"\"\n+)\n+class AudioFlamingo3Encoder(Qwen2AudioEncoder):\n+    \"\"\"\n+    AudioFlamingo3 encoder: Whisper encoder, average pool (time/2), then LayerNorm.\n+    \"\"\"\n+\n+    @can_return_tuple\n+    def forward(\n+        self,\n+        input_features: torch.Tensor,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            input_features (`torch.FloatTensor` of shape `(batch_size, feature_size, sequence_length)`):\n+                Log-Mel features extracted from raw audio. Use the processor/feature extractor to compute and pad\n+                these features from waveform input.\n+            input_features_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+        \"\"\"\n+\n+        # Prepare attention mask for transformer layers\n+        batch_size = input_features.shape[0]\n+        seq_len = (input_features.shape[-1] - 1) // 2 + 1  # After conv2 downsampling\n+\n+        input_features_lengths = input_features_mask.sum(-1)\n+        input_features_lengths = (input_features_lengths - 1) // 2 + 1  # conv2 downsampling\n+        input_features_mask = torch.arange(seq_len, device=input_features.device) < input_features_lengths[:, None]\n+        attention_mask = eager_mask(\n+            batch_size=batch_size,\n+            cache_position=torch.arange(seq_len, device=input_features.device),\n+            kv_length=seq_len,\n+            mask_function=padding_mask_function(input_features_mask),\n+            dtype=self.conv1.weight.dtype,\n+        )\n+\n+        # Conv front-end\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n+\n+        # Add positions, dropout\n+        hidden_states = inputs_embeds + self.embed_positions.weight\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        # Transformer stack\n+        for layer in self.layers:\n+            drop = self.training and torch.rand([]) < self.layerdrop\n+            if not drop:\n+                hidden_states = layer(hidden_states, attention_mask)[0]\n+\n+        # AvgPool (time/2) + LayerNorm\n+        hidden_states = hidden_states.permute(0, 2, 1)\n+        hidden_states = self.avg_pooler(hidden_states).permute(0, 2, 1)\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+\n+class AudioFlamingo3MultiModalProjector(VoxtralMultiModalProjector):\n+    \"\"\"\n+    Audio adaptor (small MLP) that projects AudioFlamingo3Encoder features\n+    to the LLM embedding space so they can replace `<sound>` tokens.\n+    \"\"\"\n+\n+    def __init__(self, config: AudioFlamingo3Config):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(\n+            config.audio_config.hidden_size, config.text_config.hidden_size, bias=config.projector_bias\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.projector_bias\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The AudioFlamingo3 model which consists of a fine-tuned Whisper encoder, a multi-modal projector and a Qwen2 language model.\n+    \"\"\"\n+)\n+class AudioFlamingo3ForConditionalGeneration(VoxtralForConditionalGeneration):\n+    _tied_weights_keys = None\n+    _tp_plan = None\n+    _pp_plan = None\n+    _keep_in_fp32_modules_strict = None\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        # Similar to Qwen2Audio\n+        if self.language_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n+\n+    def get_audio_features(\n+        self, input_features: torch.FloatTensor, input_features_mask: torch.Tensor\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        This method is used to get the audio embeddings from input features (a log mel spectrogram), meaning inferring the audio encoder and the multi-modal projector.\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+                Mask to avoid performing attention on padded feature indices.\n+\n+        Returns:\n+            `torch.FloatTensor`:\n+                The audio embeddings.\n+        \"\"\"\n+\n+        # Encode audio\n+        encoder_output = self.audio_tower(input_features, input_features_mask=input_features_mask)\n+        audio_embeds = self.multi_modal_projector(encoder_output.last_hidden_state)\n+\n+        # Mask according to avg pooling (which is after attention blocks)\n+        post_lengths = (input_features_mask.sum(-1) - 2) // 2 + 1\n+        valid_mask = torch.arange(audio_embeds.shape[1], device=post_lengths.device)[None, :] < post_lengths[:, None]\n+        audio_embeds = audio_embeds[valid_mask.to(audio_embeds.device)]\n+        return audio_embeds\n+\n+    def get_audio_embeds(self):\n+        raise NotImplementedError(\"This method is not supported for AudioFlamingo3ForConditionalGeneration.\")\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        input_features_mask: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        input_features_mask (`torch.Tensor` of shape `(batch_size, feature_sequence_length)`):\n+            Mask to avoid performing attention on padding feature indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AudioFlamingo3ForConditionalGeneration, AutoProcessor\n+\n+        >>> model_id = \"nvidia/audio-flamingo-3-hf\"\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = AudioFlamingo3ForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\")\n+\n+        >>> conversations = [\n+        >>>     [\n+        >>>         {\n+        >>>             \"role\": \"user\",\n+        >>>             \"content\": [\n+        >>>                 {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+        >>>                 {\n+        >>>                     \"type\": \"audio\",\n+        >>>                     \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/t_837b89f2-26aa-4ee2-bdf6-f73f0dd59b26.wav\",\n+        >>>                 },\n+        >>>             ],\n+        >>>         }\n+        >>>     ],\n+        >>>     [\n+        >>>         {\n+        >>>             \"role\": \"user\",\n+        >>>             \"content\": [\n+        >>>                 {\n+        >>>                     \"type\": \"text\",\n+        >>>                     \"text\": \"This track feels really peaceful and introspective. What elements make it feel so calming and meditative?\",\n+        >>>                 },\n+        >>>                 {\"type\": \"audio\", \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/FPSbCAANfbJLVSwD.mp3\"},\n+        >>>             ],\n+        >>>         }\n+        >>>     ],\n+        >>> ]\n+\n+        >>> inputs = processor.apply_chat_template(\n+        >>>     conversations,\n+        >>>     tokenize=True,\n+        >>>     add_generation_prompt=True,\n+        >>>     return_dict=True,\n+        >>> ).to(model.device)\n+\n+        >>> outputs = model.generate(**inputs, max_new_tokens=500)\n+\n+        >>> decoded_outputs = processor.batch_decode(\n+        >>>     outputs[:, inputs[\"input_ids\"].shape[1]:], skip_special_tokens=True\n+        >>> )\n+        >>> print(decoded_outputs)\n+        [\"The spoken content of the audio is...\", \"The track's calming and meditative feel can be attributed to...\"]\n+        ```\"\"\"\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if input_features is not None and input_ids is not None:\n+            audio_embeds = self.get_audio_features(input_features, input_features_mask)\n+\n+            # replace text-audio token placeholders with audio embeddings\n+            audio_token_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n+            inputs_embeds = inputs_embeds.masked_scatter(\n+                audio_token_mask.to(inputs_embeds.device), audio_embeds.to(inputs_embeds.device)\n+            )\n+\n+        outputs: CausalLMOutputWithPast = self.language_model(\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return outputs\n+\n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        input_features_mask = kwargs.pop(\"input_features_mask\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            if input_features is not None:\n+                model_inputs[\"input_features\"] = input_features\n+            if input_features_mask is not None:\n+                model_inputs[\"input_features_mask\"] = input_features_mask\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"AudioFlamingo3ForConditionalGeneration\", \"AudioFlamingo3PreTrainedModel\", \"AudioFlamingo3Encoder\"]"
        },
        {
            "sha": "99691c811e8b8e04bb078f6523d150eae9041b9b",
            "filename": "src/transformers/models/audioflamingo3/processing_audioflamingo3.py",
            "status": "added",
            "additions": 318,
            "deletions": 0,
            "changes": 318,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudioflamingo3%2Fprocessing_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,318 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import re\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...audio_utils import AudioInput, make_list_of_audio\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import TextInput\n+from ...utils import is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+MAX_AUDIO_LEN = 10 * 60  # 10 minutes\n+DEFAULT_TRANSCRIPTION_PROMPT = \"Transcribe the input speech.\"\n+\n+\n+class AudioFlamingo3ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+        },\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 16000,\n+            \"chunk_length\": 30.0,\n+            \"return_attention_mask\": True,\n+            \"padding\": \"max_length\",\n+        },\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+            \"padding_side\": \"left\",\n+        },\n+    }\n+\n+\n+class AudioFlamingo3Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs an AudioFlamingo3 processor which wraps an AudioFlamingo3 feature extractor and an AudioFlamingo3\n+    tokenizer into a single processor.\n+\n+    [`AudioFlamingo3Processor`] offers all the functionalities of [`WhisperFeatureExtractor`] and\n+    [`Qwen2TokenizerFast`]. See the [`~AudioFlamingo3Processor.__call__`] for more information.\n+\n+    Args:\n+        feature_extractor ([`WhisperFeatureExtractor`]):\n+            The feature extractor is a required input.\n+        tokenizer ([`Qwen2TokenizerFast`]):\n+            The tokenizer is a required input.\n+        chat_template (`Optional[str]`, *optional*):\n+            The Jinja template to use for formatting the conversation. If not provided, the tokenizer's default chat\n+            template will be used.\n+        audio_token (`Optional[str]`, *optional*, defaults to `\"<sound>\"`):\n+            Special token used to represent audio inputs in the chat template.\n+    \"\"\"\n+\n+    attributes = [\"feature_extractor\", \"tokenizer\"]\n+    feature_extractor_class = \"WhisperFeatureExtractor\"\n+    tokenizer_class = \"Qwen2TokenizerFast\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        tokenizer,\n+        chat_template=None,\n+        audio_token=\"<sound>\",\n+    ):\n+        self.audio_token = audio_token\n+        self.audio_token_id = tokenizer.convert_tokens_to_ids(audio_token)\n+        super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, list[TextInput]],\n+        audio: Optional[AudioInput] = None,\n+        output_labels: Optional[bool] = False,\n+        **kwargs: Unpack[AudioFlamingo3ProcessorKwargs],\n+    ) -> BatchFeature:\n+        r\"\"\"\n+        Main method to prepare one or several text sequence(s) and audio waveform(s) for the model. This\n+        method expands `<sound>` placeholders in the text based on the post-pool frame counts of the\n+        audio windows, then tokenizes the provided strings as-is, and extracts log-mel features\n+        with [`WhisperFeatureExtractor`]. If `audio` is `None`, no audio processing is performed and\n+        the text is tokenized as-is (LM-only behavior).\n+\n+        Args:\n+            text (`str` or `list[str]`):\n+                Input sequence or batch of sequences.\n+            audio (`np.ndarray` or `list[np.ndarray]`):\n+                Input audio or batch of audios as NumPy arrays. If provided, there must be as many `text` inputs as\n+                `audio` inputs.\n+            output_labels (bool, *optional*, default=False):\n+                Whether to return labels for training.\n+\n+        Returns:\n+            [`BatchFeature`]: A dictionary with tokenized text (`input_ids`, `attention_mask`) and\n+            audio features (`input_features`, `input_features_mask`).\n+        \"\"\"\n+\n+        # Merge defaults with user kwargs\n+        call_kwargs = self._merge_kwargs(\n+            AudioFlamingo3ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        text_kwargs = call_kwargs[\"text_kwargs\"]\n+        audio_kwargs = call_kwargs[\"audio_kwargs\"]\n+        return_tensors = text_kwargs.get(\"return_tensors\")\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not (isinstance(text, (list, tuple)) and all(isinstance(t, str) for t in text)):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        audio_inputs = {}\n+        if audio is not None:\n+            audio = make_list_of_audio(audio)\n+            if len(text) != len(audio):\n+                raise ValueError(f\"Got {len(text)} text but {len(audio)} audios; they must match 1:1.\")\n+\n+            # Determine number of chunks per sample, and flatten\n+            window_size = int(audio_kwargs[\"sampling_rate\"] * audio_kwargs[\"chunk_length\"])\n+            max_windows = int(MAX_AUDIO_LEN // audio_kwargs[\"chunk_length\"])\n+\n+            per_sample_windows: list[int] = []\n+            flat_chunks: list[np.ndarray] = []\n+\n+            for audio_el in audio:\n+                n_samples = int(audio_el.shape[0])\n+                n_win = max(1, (n_samples + window_size - 1) // window_size)\n+                if n_win > max_windows:\n+                    logger.warning(\n+                        f\"Audio duration ({n_samples / audio_kwargs['sampling_rate']:.1f}s) exceeds {MAX_AUDIO_LEN}s; truncating to first {MAX_AUDIO_LEN}s.\"\n+                    )\n+                    n_win = max_windows\n+                per_sample_windows.append(n_win)\n+\n+                time_cap = min(n_samples, n_win * window_size)\n+                for i in range(n_win):\n+                    start = i * window_size\n+                    end = min((i + 1) * window_size, time_cap)\n+                    flat_chunks.append(audio_el[start:end])\n+\n+            # Feature extraction\n+            audio_inputs = self.feature_extractor(flat_chunks, **audio_kwargs)\n+            padding_mask = audio_inputs.pop(\"attention_mask\")\n+            audio_inputs[\"input_features_mask\"] = padding_mask\n+\n+            # Compute sequence lengths token counting\n+            audio_lenghts = torch.stack([s.sum() for s in torch.split(padding_mask.sum(-1), per_sample_windows)])\n+            conv_output_lengths = (audio_lenghts - 1) // 2 + 1  # After conv2 downsampling\n+            audio_tokens_lengths = (conv_output_lengths - 2) // 2 + 1  # After avg pooling\n+\n+            # expand audio tokens in text\n+            for i, audio_length in enumerate(audio_tokens_lengths):\n+                expanded = re.sub(re.escape(self.audio_token), self.audio_token * audio_length, text[i])\n+                text[i] = expanded\n+\n+        # Tokenize\n+        text_inputs = self.tokenizer(text, **text_kwargs)\n+\n+        data = {**text_inputs, **audio_inputs}\n+        if output_labels:\n+            labels = data[\"input_ids\"].clone()\n+            labels[labels == self.audio_token_id] = -100\n+            labels[labels == self.tokenizer.pad_token_id] = -100\n+            data[\"labels\"] = labels\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    @property\n+    def model_input_names(self) -> list[str]:\n+        tok_names = self.tokenizer.model_input_names\n+        fea_names = self.feature_extractor.model_input_names\n+        return list(dict.fromkeys(tok_names + fea_names + [\"input_features_mask\"]))\n+\n+    def apply_transcription_request(\n+        self,\n+        audio: Union[str, list[str], AudioInput],\n+        prompt: Optional[Union[str, list[str]]] = None,\n+        **kwargs: Unpack[AudioFlamingo3ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepare inputs for automatic speech recognition without manually writing the default transcription prompt.\n+\n+        Args:\n+            audio (`str`, `list[str]`, `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                Audio to transcribe. Strings are interpreted as local paths or URLs and will be loaded automatically by\n+                the chat template loader; NumPy arrays and PyTorch tensors are forwarded directly.\n+            prompt (`str` or `list[str]`, *optional*):\n+                Custom prompt(s) to include in the user turn. A list must be the same length as the batch. When `None`,\n+                each sample uses `\"Transcribe the input speech.\"`.\n+            **kwargs:\n+                Additional keyword arguments forwarded to [`~AudioFlamingo3Processor.apply_chat_template`] (for example\n+                `text_kwargs`, `audio_kwargs`, ...).\n+\n+        Returns:\n+            [`BatchFeature`]: Processor outputs ready to be passed to [`AudioFlamingo3ForConditionalGeneration.generate`].\n+\n+        \"\"\"\n+\n+        if isinstance(audio, str):\n+            audio_items: list[Union[str, np.ndarray]] = [audio]\n+        elif isinstance(audio, (list, tuple)) and audio and all(isinstance(el, str) for el in audio):\n+            audio_items = list(audio)\n+        else:\n+            audio_items = list(make_list_of_audio(audio))\n+            if is_torch_available():\n+                audio_items = [el.detach().cpu().numpy() if isinstance(el, torch.Tensor) else el for el in audio_items]\n+\n+        batch_size = len(audio_items)\n+        if batch_size == 0:\n+            raise ValueError(\"`audio` must contain at least one sample.\")\n+\n+        if prompt is None:\n+            prompts = [DEFAULT_TRANSCRIPTION_PROMPT] * batch_size\n+        elif isinstance(prompt, str):\n+            prompts = [prompt] * batch_size\n+        elif isinstance(prompt, (list, tuple)):\n+            if len(prompt) != batch_size:\n+                raise ValueError(\n+                    f\"Received {len(prompt)} prompt(s) for {batch_size} audio sample(s); counts must match.\"\n+                )\n+            prompts = []\n+            for item in prompt:\n+                if item is None:\n+                    prompts.append(DEFAULT_TRANSCRIPTION_PROMPT)\n+                elif isinstance(item, str):\n+                    prompts.append(item)\n+                else:\n+                    raise TypeError(\"Each prompt must be a string or `None`.\")\n+        else:\n+            raise TypeError(\"`prompt` must be a string, a sequence of strings, or `None`.\")\n+\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"text\", \"text\": prompt_text},\n+                        {\"type\": \"audio\", \"path\": audio_item}\n+                        if isinstance(audio_item, str)\n+                        else {\"type\": \"audio\", \"audio\": audio_item},\n+                    ],\n+                }\n+            ]\n+            for prompt_text, audio_item in zip(prompts, audio_items)\n+        ]\n+\n+        return self.apply_chat_template(\n+            conversations,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+            **kwargs,\n+        )\n+\n+    def batch_decode(self, *args, strip_prefix=False, **kwargs):\n+        \"\"\"\n+        Forward arguments to [`~PreTrainedTokenizer.batch_decode`] and optionally remove the assistant framing the model\n+        was trained to produce.\n+\n+        AF3 transcription requests respond with sentences such as `\"The spoken content of the audio is \\\"...\\\".\"`.\n+        Setting `strip_prefix=True` trims the fixed prefix for just the transcription text.\n+        \"\"\"\n+        decoded = self.tokenizer.batch_decode(*args, **kwargs)\n+        if strip_prefix:\n+            decoded = [self._strip_assistant_prefix_and_quotes(text) for text in decoded]\n+        return decoded\n+\n+    def _strip_assistant_prefix_and_quotes(self, text: str) -> str:\n+        \"\"\"\n+        Remove the assistant prefix and surrounding quotes from a decoded transcription string.\n+        \"\"\"\n+\n+        stripped = text.strip()\n+\n+        for prefix in (\n+            \"The spoken content of the audio is\",\n+            \"The transcription of the audio is\",\n+        ):\n+            if stripped.startswith(prefix):\n+                stripped = stripped[len(prefix) :].strip()\n+                break\n+\n+        if stripped.endswith(\".\"):\n+            stripped = stripped[:-1].strip()\n+\n+        if len(stripped) >= 2 and stripped[0] == stripped[-1] and stripped[0] in {\"'\", '\"'}:\n+            stripped = stripped[1:-1].strip()\n+\n+        return stripped\n+\n+\n+__all__ = [\"AudioFlamingo3Processor\"]"
        },
        {
            "sha": "9a3b2ec5ecc2f99285988e35c402b4a3a4ac2cc2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -45,6 +45,8 @@\n         (\"aria\", \"AriaConfig\"),\n         (\"aria_text\", \"AriaTextConfig\"),\n         (\"audio-spectrogram-transformer\", \"ASTConfig\"),\n+        (\"audioflamingo3\", \"AudioFlamingo3Config\"),\n+        (\"audioflamingo3_encoder\", \"AudioFlamingo3EncoderConfig\"),\n         (\"autoformer\", \"AutoformerConfig\"),\n         (\"aya_vision\", \"AyaVisionConfig\"),\n         (\"bamba\", \"BambaConfig\"),\n@@ -477,6 +479,8 @@\n         (\"aria\", \"Aria\"),\n         (\"aria_text\", \"AriaText\"),\n         (\"audio-spectrogram-transformer\", \"Audio Spectrogram Transformer\"),\n+        (\"audioflamingo3\", \"AudioFlamingo3\"),\n+        (\"audioflamingo3_encoder\", \"AudioFlamingo3Encoder\"),\n         (\"autoformer\", \"Autoformer\"),\n         (\"aya_vision\", \"AyaVision\"),\n         (\"bamba\", \"Bamba\"),\n@@ -960,6 +964,7 @@\n \n SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n     [\n+        (\"audioflamingo3_encoder\", \"audioflamingo3\"),\n         (\"openai-gpt\", \"openai\"),\n         (\"data2vec-audio\", \"data2vec\"),\n         (\"data2vec-text\", \"data2vec\"),"
        },
        {
            "sha": "257fb95fdea765ca41f74aabb55dd31063484ddc",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -53,6 +53,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"aria\", \"AriaModel\"),\n         (\"aria_text\", \"AriaTextModel\"),\n         (\"audio-spectrogram-transformer\", \"ASTModel\"),\n+        (\"audioflamingo3\", \"AudioFlamingo3ForConditionalGeneration\"),\n+        (\"audioflamingo3_encoder\", \"AudioFlamingo3Encoder\"),\n         (\"autoformer\", \"AutoformerModel\"),\n         (\"aya_vision\", \"AyaVisionModel\"),\n         (\"bamba\", \"BambaModel\"),\n@@ -445,6 +447,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n     [\n         # Model for pre-training mapping\n         (\"albert\", \"AlbertForPreTraining\"),\n+        (\"audioflamingo3\", \"AudioFlamingo3ForConditionalGeneration\"),\n         (\"bart\", \"BartForConditionalGeneration\"),\n         (\"bert\", \"BertForPreTraining\"),\n         (\"big_bird\", \"BigBirdForPreTraining\"),\n@@ -1159,6 +1162,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Seq2Seq Causal LM mapping\n+        (\"audioflamingo3\", \"AudioFlamingo3ForConditionalGeneration\"),\n         (\"bart\", \"BartForConditionalGeneration\"),\n         (\"bigbird_pegasus\", \"BigBirdPegasusForConditionalGeneration\"),\n         (\"blenderbot\", \"BlenderbotForConditionalGeneration\"),"
        },
        {
            "sha": "691e4afc96e7d86a29f0ce58416311d05f99541a",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -48,6 +48,7 @@\n         (\"align\", \"AlignProcessor\"),\n         (\"altclip\", \"AltCLIPProcessor\"),\n         (\"aria\", \"AriaProcessor\"),\n+        (\"audioflamingo3\", \"AudioFlamingo3Processor\"),\n         (\"aya_vision\", \"AyaVisionProcessor\"),\n         (\"bark\", \"BarkProcessor\"),\n         (\"blip\", \"BlipProcessor\"),"
        },
        {
            "sha": "e18751ce5904ddd8c4737f3cc50d864dc5443bb7",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -336,7 +336,7 @@ def forward(\n         expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\n         if input_features.shape[-1] != expected_seq_length:\n             raise ValueError(\n-                f\"Qwen2Audio expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n+                f\"Voxtral expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n             )\n \n         input_features = input_features.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)"
        },
        {
            "sha": "e4e4311cd7293194a0ded05811482e243d1d6603",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -86,7 +86,7 @@ def forward(\n         expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\n         if input_features.shape[-1] != expected_seq_length:\n             raise ValueError(\n-                f\"Qwen2Audio expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n+                f\"Voxtral expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n             )\n \n         input_features = input_features.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)"
        },
        {
            "sha": "4dbb107edccb77dcefed6afff85d43203d133b27",
            "filename": "tests/fixtures/audioflamingo3/expected_results_batched.json",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_batched.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_batched.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_batched.json?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1 @@\n+{\"transcriptions\": [\"There is no clear relationship between the barking and the music, as they seem to be independent of each other.\", \"(B) To indicate that language cannot express clearly, satirizing the inversion of black and white in the world\"], \"token_ids\": [[3862, 374, 902, 2797, 5025, 1948, 279, 293, 33452, 323, 279, 4627, 11, 438, 807, 2803, 311, 387, 9489, 315, 1817, 1008, 13, 151645], [5349, 8, 2014, 13216, 429, 4128, 4157, 3158, 9355, 11, 7578, 404, 4849, 279, 46488, 315, 3691, 323, 4158, 304, 279, 1879, 151645, 151671]]}\n\\ No newline at end of file"
        },
        {
            "sha": "be9233467a20eeb068ab6f80bfc0f6b639aed51b",
            "filename": "tests/fixtures/audioflamingo3/expected_results_single.json",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_single.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_single.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ffixtures%2Faudioflamingo3%2Fexpected_results_single.json?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1 @@\n+{\"transcriptions\": [\"The content of the input audio is 'you can ask why over and over and over again forever even if one day we explain every physical interaction and scientific law and hope and dream and regret with a single elegant equation'.\"], \"token_ids\": [[785, 2213, 315, 279, 1946, 7699, 374, 364, 9330, 646, 2548, 3170, 916, 323, 916, 323, 916, 1549, 15683, 1496, 421, 825, 1899, 582, 10339, 1449, 6961, 16230, 323, 12344, 2329, 323, 3900, 323, 7904, 323, 22231, 448, 264, 3175, 25777, 23606, 4427, 151645]]}\n\\ No newline at end of file"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/audioflamingo3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudioflamingo3%2F__init__.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd"
        },
        {
            "sha": "b31f745435a8281f31e2aa5269de6b8880607b53",
            "filename": "tests/models/audioflamingo3/test_modeling_audioflamingo3.py",
            "status": "added",
            "additions": 350,
            "deletions": 0,
            "changes": 350,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudioflamingo3%2Ftest_modeling_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,350 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch AudioFlamingo3 model.\"\"\"\n+\n+import json\n+import tempfile\n+import unittest\n+from pathlib import Path\n+\n+import pytest\n+\n+from transformers import (\n+    AudioFlamingo3Config,\n+    AudioFlamingo3ForConditionalGeneration,\n+    AutoProcessor,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class AudioFlamingo3ModelTester:\n+    \"\"\"\n+    Builds a tiny AudioFlamingo3 config and synthetic inputs that respect AF3's\n+    post-pool token accounting: num <sound> tokens per sample == post-pool frame count.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        parent,\n+        audio_token_id=0,\n+        seq_length=25,\n+        feat_seq_length=60,\n+        text_config=None,\n+        audio_config=None,\n+        is_training=True,\n+    ):\n+        self.parent = parent\n+        self.audio_token_id = audio_token_id\n+        self.seq_length = seq_length\n+        self.feat_seq_length = feat_seq_length\n+        self.is_training = is_training\n+\n+        # Small text backbone (Qwen2-ish)\n+        if text_config is None:\n+            text_config = {\n+                \"model_type\": \"qwen2\",\n+                \"intermediate_size\": 36,\n+                \"initializer_range\": 0.02,\n+                \"hidden_size\": 32,\n+                \"max_position_embeddings\": 52,\n+                \"num_hidden_layers\": 2,\n+                \"num_attention_heads\": 4,\n+                \"num_key_value_heads\": 2,\n+                \"use_labels\": True,\n+                \"use_mrope\": False,\n+                \"vocab_size\": 99,\n+                \"pad_token_id\": 1,  # Ensure pad token != audio token\n+            }\n+        # Small audio encoder (AF3 Whisper-style)\n+        if audio_config is None:\n+            audio_config = {\n+                \"model_type\": \"audioflamingo3_encoder\",\n+                \"hidden_size\": 16,\n+                \"num_attention_heads\": 4,\n+                \"intermediate_size\": 16,\n+                \"num_hidden_layers\": 2,\n+                \"num_mel_bins\": 80,\n+                \"max_source_positions\": 30,\n+                \"initializer_range\": 0.02,\n+            }\n+\n+        self.text_config = text_config\n+        self.audio_config = audio_config\n+\n+        self.batch_size = 3\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.encoder_seq_length = seq_length\n+\n+    def get_config(self):\n+        return AudioFlamingo3Config(\n+            text_config=self.text_config,\n+            audio_config=self.audio_config,\n+            audio_token_id=self.audio_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        # (#windows == batch_size, n_mels, T_mel)\n+        input_features_values = floats_tensor(\n+            [self.batch_size, self.audio_config[\"num_mel_bins\"], self.feat_seq_length]\n+        )\n+        config = self.get_config()\n+        # Per-window mel validity (all ones => full length)\n+        input_features_mask = torch.ones([self.batch_size, self.feat_seq_length], dtype=torch.bool).to(torch_device)\n+        return config, input_features_values, input_features_mask\n+\n+    def _post_pool_tokens_per_window(self, T_mel):\n+        # Mirror AF3 processor math:\n+        pre = (T_mel - 1) // 2 + 1\n+        post = (pre - 2) // 2 + 1\n+        return post\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, input_features_values, input_features_mask = self.prepare_config_and_inputs()\n+        # Every window has same T_mel here\n+        num_audio_tokens_per_sample = self._post_pool_tokens_per_window(input_features_values.shape[-1])\n+\n+        # Build token ids with valid range and K <sound> tokens\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n+        attention_mask = torch.ones_like(input_ids, dtype=torch.long, device=torch_device)\n+        attention_mask[:, :1] = 0  # left padding sentinel\n+\n+        # Fill first K positions (after padding) with the audio token id, for each sample\n+        input_ids[:, 1 : 1 + num_audio_tokens_per_sample] = config.audio_token_id\n+\n+        inputs_dict = {\n+            \"input_features\": input_features_values,\n+            \"input_features_mask\": input_features_mask,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class AudioFlamingo3ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `AudioFlamingo3ForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (AudioFlamingo3ForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"text-to-speech\": AudioFlamingo3ForConditionalGeneration,\n+            \"audio-text-to-text\": AudioFlamingo3ForConditionalGeneration,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = AudioFlamingo3ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=AudioFlamingo3Config, has_text_modality=False)\n+\n+    @unittest.skip(\n+        reason=\"This test does not apply to AudioFlamingo3 since inputs_embeds corresponding to audio tokens are replaced when input features are provided.\"\n+    )\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported for AudioFlamingo3 models\")\n+    @pytest.mark.torch_compile_test\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported for AudioFlamingo3 models\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"AudioFlamingo3 tests avoid right-padding equivalence; fusion is in-place.\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        # AF3 is audio+text composite; verify SDPA toggles propagate to submodules.\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # SDPA (default)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                audio_attn = \"sdpa\" if model.audio_tower._supports_sdpa else \"eager\"\n+\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.audio_tower.config._attn_implementation == audio_attn)\n+\n+                # Eager\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.audio_tower.config._attn_implementation == \"eager\")\n+\n+                for _, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+@require_torch\n+class AudioFlamingo3ForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    \"\"\"\n+    Slow tests against the public checkpoint to validate processor-model alignment and in-place fusion.\n+    \"\"\"\n+\n+    @classmethod\n+    def setUp(cls):\n+        cleanup(torch_device, gc_collect=True)\n+        cls.checkpoint = \"nvidia/audio-flamingo-3-hf\"\n+        cls.processor = AutoProcessor.from_pretrained(cls.checkpoint)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_fixture_single_matches(self):\n+        \"\"\"\n+        reproducer (creates JSON directly in repo): https://gist.github.com/ebezzam/c979f0f1a2b9223fa137faf1c02022d4#file-reproducer-py\n+        \"\"\"\n+        path = Path(__file__).parent.parent.parent / \"fixtures/audioflamingo3/expected_results_single.json\"\n+        with open(path, \"r\", encoding=\"utf-8\") as f:\n+            raw = json.load(f)\n+        exp_ids = torch.tensor(raw[\"token_ids\"])\n+        exp_txt = raw[\"transcriptions\"]\n+\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"Transcribe the input speech.\",\n+                    },\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/Why_do_we_ask_questions_converted.wav\",\n+                    },\n+                ],\n+            }\n+        ]\n+\n+        model = AudioFlamingo3ForConditionalGeneration.from_pretrained(\n+            self.checkpoint, device_map=torch_device, dtype=torch.bfloat16\n+        ).eval()\n+\n+        batch = self.processor.apply_chat_template(\n+            conversation, tokenize=True, add_generation_prompt=True, return_dict=True\n+        ).to(model.device, dtype=model.dtype)\n+        seq = model.generate(**batch)\n+        inp_len = batch[\"input_ids\"].shape[1]\n+        gen_ids = seq[:, inp_len:] if seq.shape[1] >= inp_len else seq\n+\n+        torch.testing.assert_close(gen_ids.cpu(), exp_ids)\n+        txt = self.processor.batch_decode(gen_ids, skip_special_tokens=True)\n+        self.assertListEqual(txt, exp_txt)\n+\n+    @slow\n+    def test_fixture_batched_matches(self):\n+        \"\"\"\n+        reproducer (creates JSON directly in repo): https://gist.github.com/ebezzam/c979f0f1a2b9223fa137faf1c02022d4#file-reproducer-py\n+        \"\"\"\n+        path = Path(__file__).parent.parent.parent / \"fixtures/audioflamingo3/expected_results_batched.json\"\n+        with open(path, \"r\", encoding=\"utf-8\") as f:\n+            raw = json.load(f)\n+        exp_ids = torch.tensor(raw[\"token_ids\"])\n+        exp_txt = raw[\"transcriptions\"]\n+\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"What is surprising about the relationship between the barking and the music?\",\n+                        },\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/dogs_barking_in_sync_with_the_music.wav\",\n+                        },\n+                    ],\n+                }\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"Why is the philosopher's name mentioned in the lyrics? \"\n+                            \"(A) To express a sense of nostalgia \"\n+                            \"(B) To indicate that language cannot express clearly, satirizing the inversion of black and white in the world \"\n+                            \"(C) To add depth and complexity to the lyrics \"\n+                            \"(D) To showcase the wisdom and influence of the philosopher\",\n+                        },\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/Ch6Ae9DT6Ko_00-04-03_00-04-31.wav\",\n+                        },\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        model = AudioFlamingo3ForConditionalGeneration.from_pretrained(\n+            self.checkpoint, device_map=torch_device, dtype=torch.bfloat16\n+        ).eval()\n+\n+        batch = self.processor.apply_chat_template(\n+            conversations, tokenize=True, add_generation_prompt=True, return_dict=True\n+        ).to(model.device, dtype=model.dtype)\n+        seq = model.generate(**batch)\n+        inp_len = batch[\"input_ids\"].shape[1]\n+        gen_ids = seq[:, inp_len:] if seq.shape[1] >= inp_len else seq\n+\n+        torch.testing.assert_close(gen_ids.cpu(), exp_ids)\n+        txt = self.processor.batch_decode(gen_ids, skip_special_tokens=True)\n+        self.assertListEqual(txt, exp_txt)"
        },
        {
            "sha": "537f8b43b6dac90dca566c1804c489a24667404e",
            "filename": "tests/models/audioflamingo3/test_processing_audioflamingo3.py",
            "status": "added",
            "additions": 192,
            "deletions": 0,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2Ftest_processing_audioflamingo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1709ed96e47f36fe926e2cd2556fa839b41c2afd/tests%2Fmodels%2Faudioflamingo3%2Ftest_processing_audioflamingo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudioflamingo3%2Ftest_processing_audioflamingo3.py?ref=1709ed96e47f36fe926e2cd2556fa839b41c2afd",
            "patch": "@@ -0,0 +1,192 @@\n+# coding=utf-8\n+# Copyright 2025 NVIDIA CORPORATION and the HuggingFace Inc. team. All rights\n+# reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import shutil\n+import tempfile\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import (\n+    AudioFlamingo3Processor,\n+    AutoProcessor,\n+    AutoTokenizer,\n+    WhisperFeatureExtractor,\n+)\n+from transformers.testing_utils import require_librosa, require_torch, require_torchaudio\n+\n+from ...test_processing_common import MODALITY_INPUT_DATA, ProcessorTesterMixin\n+\n+\n+class AudioFlamingo3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = AudioFlamingo3Processor\n+\n+    @classmethod\n+    @require_torch\n+    @require_torchaudio\n+    def setUpClass(cls):\n+        cls.checkpoint = \"nvidia/audio-flamingo-3-hf\"\n+        cls.tmpdirname = tempfile.mkdtemp()\n+\n+        processor = AudioFlamingo3Processor.from_pretrained(cls.checkpoint)\n+        processor.save_pretrained(cls.tmpdirname)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    @require_torch\n+    @require_torchaudio\n+    def get_audio_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).audio_processor\n+\n+    @require_torch\n+    @require_torchaudio\n+    def get_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs)\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def test_can_load_various_tokenizers(self):\n+        processor = AudioFlamingo3Processor.from_pretrained(self.checkpoint)\n+        tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n+        self.assertEqual(processor.tokenizer.__class__, tokenizer.__class__)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def test_save_load_pretrained_default(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.checkpoint)\n+        processor = AudioFlamingo3Processor.from_pretrained(self.checkpoint)\n+        feature_extractor = processor.feature_extractor\n+\n+        processor = AudioFlamingo3Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            processor.save_pretrained(tmpdir)\n+            reloaded = AudioFlamingo3Processor.from_pretrained(tmpdir)\n+\n+        self.assertEqual(reloaded.tokenizer.get_vocab(), tokenizer.get_vocab())\n+        self.assertEqual(reloaded.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n+        self.assertIsInstance(reloaded.feature_extractor, WhisperFeatureExtractor)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def test_tokenizer_integration(self):\n+        slow_tokenizer = AutoTokenizer.from_pretrained(self.checkpoint, use_fast=False)\n+        fast_tokenizer = AutoTokenizer.from_pretrained(self.checkpoint, from_slow=True, legacy=False)\n+\n+        prompt = (\n+            \"<|im_start|>system\\nAnswer the questions.<|im_end|>\"\n+            \"<|im_start|>user\\n<sound>What is it?<|im_end|>\"\n+            \"<|im_start|>assistant\\n\"\n+        )\n+        EXPECTED_OUTPUT = [\n+            \"<|im_start|>\",\n+            \"system\",\n+            \"ƒä\",\n+            \"Answer\",\n+            \"ƒ†the\",\n+            \"ƒ†questions\",\n+            \".\",\n+            \"<|im_end|>\",\n+            \"<|im_start|>\",\n+            \"user\",\n+            \"ƒä\",\n+            \"<sound>\",\n+            \"What\",\n+            \"ƒ†is\",\n+            \"ƒ†it\",\n+            \"?\",\n+            \"<|im_end|>\",\n+            \"<|im_start|>\",\n+            \"assistant\",\n+            \"ƒä\",\n+        ]\n+\n+        self.assertEqual(slow_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n+        self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def test_chat_template(self):\n+        processor = AutoProcessor.from_pretrained(self.checkpoint)\n+        expected_prompt = (\n+            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n\"\n+            \"<|im_start|>user\\n<sound>What is surprising about the relationship between the barking and the music?<|im_end|>\\n\"\n+            \"<|im_start|>assistant\\n\"\n+        )\n+\n+        conversations = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"What is surprising about the relationship between the barking and the music?\",\n+                    },\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/dogs_barking_in_sync_with_the_music.wav\",\n+                    },\n+                ],\n+            }\n+        ]\n+\n+        formatted = processor.tokenizer.apply_chat_template(conversations, tokenize=False, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted)\n+\n+    @require_torch\n+    @require_torchaudio\n+    def test_apply_transcription_request_single(self):\n+        processor = AutoProcessor.from_pretrained(self.checkpoint)\n+\n+        audio_url = \"https://huggingface.co/datasets/nvidia/AudioSkills/resolve/main/assets/t_837b89f2-26aa-4ee2-bdf6-f73f0dd59b26.wav\"\n+        helper_outputs = processor.apply_transcription_request(audio=audio_url)\n+\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Transcribe the input speech.\"},\n+                    {\"type\": \"audio\", \"audio\": audio_url},\n+                ],\n+            }\n+        ]\n+        manual_outputs = processor.apply_chat_template(\n+            conversation,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_dict=True,\n+        )\n+\n+        for key in (\"input_ids\", \"attention_mask\", \"input_features\", \"input_features_mask\"):\n+            self.assertIn(key, helper_outputs)\n+            self.assertTrue(helper_outputs[key].equal(manual_outputs[key]))\n+\n+    # Overwrite to remove skip numpy inputs (still need to keep as many cases as parent)\n+    @require_librosa\n+    @parameterized.expand([(1, \"np\"), (1, \"pt\"), (2, \"np\"), (2, \"pt\")])\n+    def test_apply_chat_template_audio(self, batch_size: int, return_tensors: str):\n+        if return_tensors == \"np\":\n+            self.skipTest(\"AudioFlamingo3 only supports PyTorch tensors\")\n+        self._test_apply_chat_template(\n+            \"audio\", batch_size, return_tensors, \"audio_input_name\", \"feature_extractor\", MODALITY_INPUT_DATA[\"audio\"]\n+        )"
        }
    ],
    "stats": {
        "total": 2743,
        "additions": 2741,
        "deletions": 2
    }
}