{
    "author": "raimbekovm",
    "message": "Fix duplicate words typos in comments and docstrings (#43038)",
    "sha": "fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
    "files": [
        {
            "sha": "9dc7a4a1ee9e06c36e84b3855575c131c1d23120",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -430,7 +430,7 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwa\n         initial_match = end_lengths > 0\n \n         # Tokens continue the string if the cumsum() so far is one of the valid positions for that token\n-        # Note that we're actually tracking one cumsum() for for each possible end_length\n+        # Note that we're actually tracking one cumsum() for each possible end_length\n         later_match = torch.any(cumsum[:, :-1, :, None] == valid_positions[:, :, :, :, None], axis=-2)\n \n         # The match vector is a boolean vector that indicates which positions have valid tokens"
        },
        {
            "sha": "cf27d0904d269a3138f0a18f9d61e11600395789",
            "filename": "src/transformers/models/clap/feature_extraction_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Ffeature_extraction_clap.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -71,7 +71,7 @@ class ClapFeatureExtractor(SequenceFeatureExtractor):\n             Truncation pattern for long audio inputs. Two patterns are available:\n                 - `fusion` will use `_random_mel_fusion`, which stacks 3 random crops from the mel spectrogram and a\n                   downsampled version of the entire mel spectrogram.\n-            If `config.fusion` is set to True, shorter audios also need to to return 4 mels, which will just be a copy\n+            If `config.fusion` is set to True, shorter audios also need to return 4 mels, which will just be a copy\n             of the original mel obtained from the padded audio.\n                 - `rand_trunc` will select a random crop of the mel spectrogram.\n         padding (`str`, *optional*, defaults to `\"repeatpad\"`):\n@@ -279,7 +279,7 @@ def __call__(\n                 Truncation pattern for long audio inputs. Two patterns are available:\n                     - `fusion` will use `_random_mel_fusion`, which stacks 3 random crops from the mel spectrogram and\n                       a downsampled version of the entire mel spectrogram.\n-                If `config.fusion` is set to True, shorter audios also need to to return 4 mels, which will just be a\n+                If `config.fusion` is set to True, shorter audios also need to return 4 mels, which will just be a\n                 copy of the original mel obtained from the padded audio.\n                     - `rand_trunc` will select a random crop of the mel spectrogram.\n             padding (`str`, *optional*):"
        },
        {
            "sha": "112f0a50b28dbdf6cb25356d03f52aae238c1a8f",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -74,7 +74,7 @@ class DiaProcessor(ProcessorMixin):\n         tokenizer (`DiaTokenizer`):\n             An instance of [`DiaTokenizer`]. The tokenizer is a required input.\n         audio_tokenizer (`DacModel`):\n-            An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is is a required input.\n+            An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is a required input.\n     \"\"\"\n \n     audio_tokenizer_class = \"DacModel\""
        },
        {
            "sha": "d39bb473a17204d881dad1e8d8e798e796dff985",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -174,7 +174,7 @@ class MaskFormerModelOutput(ModelOutput):\n     custom_intro=\"\"\"\n     Class for outputs of [`MaskFormerForInstanceSegmentation`].\n \n-    This output can be directly passed to [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or or\n+    This output can be directly passed to [`~MaskFormerImageProcessor.post_process_semantic_segmentation`] or\n     [`~MaskFormerImageProcessor.post_process_instance_segmentation`] or\n     [`~MaskFormerImageProcessor.post_process_panoptic_segmentation`] depending on the task. Please, see\n     [`~MaskFormerImageProcessor] for details regarding usage."
        },
        {
            "sha": "29128b77b9ba2facb5830b6f5a05289362f8d1e5",
            "filename": "src/transformers/models/omdet_turbo/configuration_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -68,7 +68,7 @@ class OmDetTurboConfig(PreTrainedConfig):\n         class_embed_dim (`int`, *optional*, defaults to 512):\n             The dimension of the classes embeddings.\n         class_distance_type (`str`, *optional*, defaults to `\"cosine\"`):\n-            The type of of distance to compare predicted classes to projected classes embeddings.\n+            The type of distance to compare predicted classes to projected classes embeddings.\n             Can be `\"cosine\"` or `\"dot\"`.\n         num_queries (`int`, *optional*, defaults to 900):\n             The number of queries."
        },
        {
            "sha": "89d6c68109fe27040acdf4ff5173047b3a4a1ea1",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -111,7 +111,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         router_logits, expert_index = torch.max(router_probs, dim=-1, keepdim=True)\n         expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)\n         token_priority = torch.cumsum(expert_index, dim=-2)\n-        # mask if the token routed to to the expert will overflow\n+        # mask if the token routed to the expert will overflow\n         expert_capacity_mask = token_priority <= self.expert_capacity\n         expert_index = expert_index * expert_capacity_mask\n         router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)"
        },
        {
            "sha": "2aa67b8bd247423a428d8dfdb602edbec651e2b6",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -170,7 +170,7 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.Tensor, torch.Tens\n         router_logits, expert_index = torch.max(router_probs, dim=-1, keepdim=True)\n         expert_index = torch.nn.functional.one_hot(expert_index, num_classes=self.num_experts)\n         token_priority = torch.cumsum(expert_index, dim=-2)\n-        # mask if the token routed to to the expert will overflow\n+        # mask if the token routed to the expert will overflow\n         expert_capacity_mask = token_priority <= self.expert_capacity\n         expert_index = expert_index * expert_capacity_mask\n         router_probs = torch.max(router_probs, dim=-1).values.unsqueeze(-1)"
        },
        {
            "sha": "6b97e5b133b4aaa2c38945ca6b52529bf1a8b57a",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc15b32246e7b8fb450a46761db2b3dd80f6cdfb/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=fc15b32246e7b8fb450a46761db2b3dd80f6cdfb",
            "patch": "@@ -112,7 +112,7 @@ def _process_model_before_weight_loading(\n \n     # NOTE: TP is applied before quantization so this is only to add hooks.\n     # Quantization is incompatible with DTensors, so we have to anyway have\n-    # gathers! But it should be model independant -> figure out where to put\n+    # gathers! But it should be model independent -> figure out where to put\n     # the gather and that's it.\n     def update_tp_plan(self, config):\n         if \"Qwen3\" in config.__class__.__name__:"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}