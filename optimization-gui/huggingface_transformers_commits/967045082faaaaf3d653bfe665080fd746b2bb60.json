{
    "author": "eustlb",
    "message": "Add voxtral (#39429)\n\n* draft\n\n* draft update (conversion working)\n\n* mend\n\n* draft update\n\n* draft update: working generate\n\n* refactor\n\n* VoxtralProcessor draft\n\n* processor update\n\n* update convert_tekken_tokenizer\n\n* refactor processor\n\n* update convert\n\n* make style\n\n* better handle prefil\n\n* make style\n\n* add tests\n\n* add mistral_common audio loading\n\n* processor update\n\n* revert changes\n\n* audio utils update\n\n* add audio to apply chat template mistral update\n\n* voxtral processor update\n\n* fix\n\n* udpate converstion script\n\n* make mistral tokenier from pretrain work from local dir\n\n* fix udpates\n\n* add integration tests\n\n* add batched version\n\n* processor docstring\n\n* make style\n\n* revert convert_tekken_tokenizer changes\n\n* revert processing_qwen2.5 changes\n\n* add multi-turn test\n\n* processor improvements\n\n* address review changes\n\n* Update src/transformers/tokenization_mistral_common.py\n\nCo-authored-by: Julien Denize <40604584+juliendenize@users.noreply.github.com>\n\n* update audio utils\n\n* nits\n\n* integration test update\n\n* correct _support\n\n* update tests\n\n* test update\n\n* update integration tests\n\n* fix\n\n* fix\n\n* fix\n\n* add test_apply_chat_template_with_audio\n\n* add model doc\n\n* model doc\n\n* nit\n\n* doc uptade\n\n* nit\n\n* processor improvement\n\n* ensure default is 3B\n\n* nits\n\n* make\n\n* make\n\n* convert modular\n\n* update checkpoint\n\n* fix test\n\n* make\n\n* make\n\n* autos\n\n* make\n\n* make\n\n* nit\n\n* nit\n\n* nit\n\n---------\n\nCo-authored-by: Julien Denize <40604584+juliendenize@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "967045082faaaaf3d653bfe665080fd746b2bb60",
    "files": [
        {
            "sha": "2c4367982000c92affa5ec656497b70817daa2c6",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -1095,6 +1095,8 @@\n         title: Vision Text Dual Encoder\n       - local: model_doc/visual_bert\n         title: VisualBERT\n+      - local: model_doc/voxtral\n+        title: Voxtral\n       - local: model_doc/xclip\n         title: X-CLIP\n       title: Multimodal models"
        },
        {
            "sha": "365c19b281a999c928d2e434240db4664e60a8aa",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "added",
            "additions": 300,
            "deletions": 0,
            "changes": 300,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,300 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Voxtral\n+\n+Voxtral is an upgrade of [Ministral 3B and Mistral Small 3B](https://mistral.ai/news/ministraux), extending its language capabilities with audio input support. It is designed to handle tasks such as speech transcription, translation, and audio understanding.\n+\n+You can read more in Mistral's [realease blog post](https://mistral.ai/news/voxtral).\n+\n+The model is available in two checkpoints:\n+- 3B: [mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)\n+- 24B: [mistralai/Voxtral-Small-24B-2507](https://huggingface.co/mistralai/Voxtral-Small-24B-2507)\n+\n+## Key Features\n+\n+Voxtral builds on Ministral-3B by adding audio processing capabilities:\n+\n+- **Transcription mode**: Includes a dedicated mode for speech transcription. By default, Voxtral detects the spoken language and transcribes it accordingly.  \n+- **Long-form context**: With a 32k token context window, Voxtral can process up to 30 minutes of audio for transcription or 40 minutes for broader audio understanding.  \n+- **Integrated Q&A and summarization**: Supports querying audio directly and producing structured summaries without relying on separate ASR and language models.  \n+- **Multilingual support**: Automatically detects language and performs well across several widely spoken languages, including English, Spanish, French, Portuguese, Hindi, German, Dutch, and Italian.  \n+- **Function calling via voice**: Can trigger functions or workflows directly from spoken input based on detected user intent.  \n+- **Text capabilities**: Maintains the strong text processing performance of its Ministral-3B foundation.\n+\n+## Usage\n+\n+Let's first load the model!\n+```python\n+from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+import torch\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+processor = AutoProcessor.from_pretrained(repo_id)\n+model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+```\n+\n+### Audio Instruct Mode\n+\n+The model supports audio-text instructions, including multi-turn and multi-audio interactions, all processed in batches.\n+\n+➡️ audio + text instruction\n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"url\": \"https://huggingface.co/datasets/eustlb/audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+            },\n+            {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(conversation)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated response:\")\n+print(\"=\" * 80)\n+print(decoded_outputs[0])\n+print(\"=\" * 80)\n+```\n+\n+➡️ multi-audio + text instruction \n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n+            },\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n+            },\n+            {\"type\": \"text\", \"text\": \"What sport and what nursery rhyme are referenced?\"},\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(conversation)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated response:\")\n+print(\"=\" * 80)\n+print(decoded_outputs[0])\n+print(\"=\" * 80)\n+```\n+\n+➡️ multi-turn:\n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+            },\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+            },\n+            {\"type\": \"text\", \"text\": \"Describe briefly what you can hear.\"},\n+        ],\n+    },\n+    {\n+        \"role\": \"assistant\",\n+        \"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n+    },\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+            },\n+            {\"type\": \"text\", \"text\": \"Ok, now compare this new audio with the previous one.\"},\n+        ],\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(conversation)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated response:\")\n+print(\"=\" * 80)\n+print(decoded_outputs[0])\n+print(\"=\" * 80)\n+```\n+\n+➡️ text only:\n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"text\",\n+                \"text\": \"What if a cyber brain could possibly generate its own ghost, and create a soul all by itself?\",\n+            },\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(conversation)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated response:\")\n+print(\"=\" * 80)\n+print(decoded_outputs[0])\n+print(\"=\" * 80)\n+```\n+\n+➡️ audio only:\n+```python\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"audio\",\n+                \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+            },\n+        ],\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(conversation)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated response:\")\n+print(\"=\" * 80)\n+print(decoded_outputs[0])\n+print(\"=\" * 80)\n+```\n+\n+➡️ batched inference!\n+```python\n+conversations = [\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\n+                    \"type\": \"audio\",\n+                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+                },\n+                {\n+                    \"type\": \"audio\",\n+                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+                },\n+                {\n+                    \"type\": \"text\",\n+                    \"text\": \"Who's speaking in the speach and what city's weather is being discussed?\",\n+                },\n+            ],\n+        }\n+    ],\n+    [\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\n+                    \"type\": \"audio\",\n+                    \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n+                },\n+                {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+            ],\n+        }\n+    ],\n+]\n+\n+inputs = processor.apply_chat_template(conversations)\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated responses:\")\n+print(\"=\" * 80)\n+for decoded_output in decoded_outputs:\n+    print(decoded_output)\n+    print(\"=\" * 80)\n+```\n+\n+### Transcription Mode\n+\n+Use the model to transcribe audio (supports English, Spanish, French, Portuguese, Hindi, German, Dutch, Italian)!\n+\n+```python\n+inputs = processor.apply_transcrition_request(language=\"en\", audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\")\n+inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+outputs = model.generate(**inputs, max_new_tokens=500)\n+decoded_outputs = processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+\n+print(\"\\nGenerated responses:\")\n+print(\"=\" * 80)\n+for decoded_output in decoded_outputs:\n+    print(decoded_output)\n+    print(\"=\" * 80)\n+```\n+\n+This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb).\n+\n+## VoxtralConfig\n+\n+[[autodoc]] VoxtralConfig\n+\n+## VoxtralEncoderConfig\n+\n+[[autodoc]] VoxtralEncoderConfig\n+\n+## VoxtralProcessor\n+\n+[[autodoc]] VoxtralProcessor\n+\n+## VoxtralEncoder\n+\n+[[autodoc]] VoxtralEncoder\n+    - forward\n+\n+## VoxtralForConditionalGeneration\n+\n+[[autodoc]] VoxtralForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "cb607e3fc94f0d0a43541793d7ff51b415da0f6c",
            "filename": "src/transformers/audio_utils.py",
            "status": "modified",
            "additions": 89,
            "deletions": 1,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Faudio_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Faudio_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Faudio_utils.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -16,25 +16,34 @@\n and remove unnecessary dependencies.\n \"\"\"\n \n+import base64\n+import io\n import os\n import warnings\n from io import BytesIO\n-from typing import Optional, Union\n+from typing import Any, Optional, Union\n \n import numpy as np\n import requests\n \n from .utils import (\n     is_librosa_available,\n     is_numpy_array,\n+    is_soundfile_available,\n     is_torch_tensor,\n     requires_backends,\n )\n \n \n+if is_soundfile_available():\n+    import soundfile as sf\n+\n if is_librosa_available():\n     import librosa\n \n+    # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n+    import soxr\n+\n \n def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None) -> np.ndarray:\n     \"\"\"\n@@ -69,6 +78,85 @@ def load_audio(audio: Union[str, np.ndarray], sampling_rate=16000, timeout=None)\n     return audio\n \n \n+def load_audio_as(\n+    audio: str,\n+    return_format: str,\n+    timeout: Optional[int] = None,\n+    force_mono: bool = False,\n+    sampling_rate: Optional[int] = None,\n+) -> Union[str, dict[str, Any], io.BytesIO, None]:\n+    \"\"\"\n+    Load audio from either a local file path or URL and return in specified format.\n+\n+    Args:\n+        audio (`str`): Either a local file path or a URL to an audio file\n+        return_format (`str`): Format to return the audio in:\n+            - \"base64\": Base64 encoded string\n+            - \"dict\": Dictionary with data and format\n+            - \"buffer\": BytesIO object\n+        timeout (`int`, *optional*): Timeout for URL requests in seconds\n+        force_mono (`bool`): Whether to convert stereo audio to mono\n+        sampling_rate (`int`, *optional*): If provided, the audio will be resampled to the specified sampling rate.\n+\n+    Returns:\n+        `Union[str, Dict[str, Any], io.BytesIO, None]`:\n+            - `str`: Base64 encoded audio data (if return_format=\"base64\")\n+            - `dict`: Dictionary with 'data' (base64 encoded audio data) and 'format' keys (if return_format=\"dict\")\n+            - `io.BytesIO`: BytesIO object containing audio data (if return_format=\"buffer\")\n+    \"\"\"\n+    # TODO: @eustlb, we actually don't need librosa but soxr is installed with librosa\n+    requires_backends(load_audio_as, [\"librosa\"])\n+\n+    if return_format not in [\"base64\", \"dict\", \"buffer\"]:\n+        raise ValueError(f\"Invalid return_format: {return_format}. Must be 'base64', 'dict', or 'buffer'\")\n+\n+    try:\n+        # Load audio bytes from URL or file\n+        audio_bytes = None\n+        if audio.startswith((\"http://\", \"https://\")):\n+            response = requests.get(audio, timeout=timeout)\n+            response.raise_for_status()\n+            audio_bytes = response.content\n+        elif os.path.isfile(audio):\n+            with open(audio, \"rb\") as audio_file:\n+                audio_bytes = audio_file.read()\n+        else:\n+            raise ValueError(f\"File not found: {audio}\")\n+\n+        # Process audio data\n+        with io.BytesIO(audio_bytes) as audio_file:\n+            with sf.SoundFile(audio_file) as f:\n+                audio_array = f.read(dtype=\"float32\")\n+                original_sr = f.samplerate\n+                audio_format = f.format\n+                if sampling_rate is not None and sampling_rate != original_sr:\n+                    # Resample audio to target sampling rate\n+                    audio_array = soxr.resample(audio_array, original_sr, sampling_rate, quality=\"HQ\")\n+                else:\n+                    sampling_rate = original_sr\n+\n+        # Convert to mono if needed\n+        if force_mono and audio_array.ndim != 1:\n+            audio_array = audio_array.mean(axis=1)\n+\n+        buffer = io.BytesIO()\n+        sf.write(buffer, audio_array, sampling_rate, format=audio_format.upper())\n+        buffer.seek(0)\n+\n+        if return_format == \"buffer\":\n+            return buffer\n+        elif return_format == \"base64\":\n+            return base64.b64encode(buffer.read()).decode(\"utf-8\")\n+        elif return_format == \"dict\":\n+            return {\n+                \"data\": base64.b64encode(buffer.read()).decode(\"utf-8\"),\n+                \"format\": audio_format.lower(),\n+            }\n+\n+    except Exception as e:\n+        raise ValueError(f\"Error loading audio: {e}\")\n+\n+\n AudioInput = Union[\n     np.ndarray, \"torch.Tensor\", list[np.ndarray], tuple[np.ndarray], list[\"torch.Tensor\"], tuple[\"torch.Tensor\"]  # noqa: F821\n ]"
        },
        {
            "sha": "ec8c4dfc8fd4a7245b838e47f00766e357c481cf",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -335,6 +335,7 @@\n     from .vits import *\n     from .vivit import *\n     from .vjepa2 import *\n+    from .voxtral import *\n     from .wav2vec2 import *\n     from .wav2vec2_bert import *\n     from .wav2vec2_conformer import *"
        },
        {
            "sha": "e97217ab2f0afa2c0bfcfef55f4868efb1685766",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -389,6 +389,8 @@\n         (\"vits\", \"VitsConfig\"),\n         (\"vivit\", \"VivitConfig\"),\n         (\"vjepa2\", \"VJEPA2Config\"),\n+        (\"voxtral\", \"VoxtralConfig\"),\n+        (\"voxtral_encoder\", \"VoxtralEncoderConfig\"),\n         (\"wav2vec2\", \"Wav2Vec2Config\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2BertConfig\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2ConformerConfig\"),\n@@ -798,6 +800,8 @@\n         (\"vits\", \"VITS\"),\n         (\"vivit\", \"ViViT\"),\n         (\"vjepa2\", \"VJEPA2Model\"),\n+        (\"voxtral\", \"Voxtral\"),\n+        (\"voxtral_encoder\", \"Voxtral Encoder\"),\n         (\"wav2vec2\", \"Wav2Vec2\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2-BERT\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2-Conformer\"),\n@@ -864,6 +868,7 @@\n         (\"xclip\", \"x_clip\"),\n         (\"clip_vision_model\", \"clip\"),\n         (\"qwen2_audio_encoder\", \"qwen2_audio\"),\n+        (\"voxtral_encoder\", \"voxtral\"),\n         (\"clip_text_model\", \"clip\"),\n         (\"aria_text\", \"aria\"),\n         (\"gemma3_text\", \"gemma3\"),"
        },
        {
            "sha": "7e43e68c9a95e7ec4669a3a8bd3569e3ba53ff94",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -359,6 +359,8 @@\n         (\"vits\", \"VitsModel\"),\n         (\"vivit\", \"VivitModel\"),\n         (\"vjepa2\", \"VJEPA2Model\"),\n+        (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n+        (\"voxtral_encoder\", \"VoxtralEncoder\"),\n         (\"wav2vec2\", \"Wav2Vec2Model\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2BertModel\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2ConformerModel\"),\n@@ -458,6 +460,7 @@\n         (\"vipllava\", \"VipLlavaForConditionalGeneration\"),\n         (\"visual_bert\", \"VisualBertForPreTraining\"),\n         (\"vit_mae\", \"ViTMAEForPreTraining\"),\n+        (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n         (\"wav2vec2\", \"Wav2Vec2ForPreTraining\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2ConformerForPreTraining\"),\n         (\"xlm\", \"XLMWithLMHeadModel\"),\n@@ -1078,6 +1081,7 @@\n         (\"t5\", \"T5ForConditionalGeneration\"),\n         (\"t5gemma\", \"T5GemmaForConditionalGeneration\"),\n         (\"umt5\", \"UMT5ForConditionalGeneration\"),\n+        (\"voxtral\", \"VoxtralForConditionalGeneration\"),\n         (\"xlm-prophetnet\", \"XLMProphetNetForConditionalGeneration\"),\n     ]\n )"
        },
        {
            "sha": "5b73e1a137c846f97c7329595a27294a419887ac",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -132,6 +132,7 @@\n         (\"vilt\", \"ViltProcessor\"),\n         (\"vipllava\", \"LlavaProcessor\"),\n         (\"vision-text-dual-encoder\", \"VisionTextDualEncoderProcessor\"),\n+        (\"voxtral\", \"VoxtralProcessor\"),\n         (\"wav2vec2\", \"Wav2Vec2Processor\"),\n         (\"wav2vec2-bert\", \"Wav2Vec2Processor\"),\n         (\"wav2vec2-conformer\", \"Wav2Vec2Processor\"),"
        },
        {
            "sha": "fa7fc1b411d35ab272c3217221eead1c0082e7cf",
            "filename": "src/transformers/models/voxtral/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2F__init__.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_voxtral import *\n+    from .modeling_voxtral import *\n+    from .processing_voxtral import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "72c986da3752b9fe7b1759951cab7d054d6aca2d",
            "filename": "src/transformers/models/voxtral/configuration_voxtral.py",
            "status": "added",
            "additions": 203,
            "deletions": 0,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconfiguration_voxtral.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,203 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+\n+\n+class VoxtralEncoderConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VoxtralEncoder`]. It is used to instantiate a\n+    Voxtral audio encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the audio encoder of the Voxtral\n+    architecture.\n+\n+    e.g. [mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 51866):\n+            Vocabulary size of the model.\n+        hidden_size (`int`, *optional*, defaults to 1280):\n+            Dimensionality of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 5120):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 20):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        scale_embedding (`bool`, *optional*, defaults to `False`):\n+            Scale embeddings by dividing by sqrt(hidden_size) if True.\n+        activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, \"gelu\",\n+        num_mel_bins (`int`, *optional*, defaults to 128):\n+            Number of mel features used per input features. Should correspond to the value used in the\n+            `VoxtralProcessor` class.\n+        max_source_positions (`int`, *optional*, defaults to 1500):\n+            The maximum sequence length of log-mel filter-bank features that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    ```python\n+    >>> from transformers import VoxtralEncoderConfig, VoxtralEncoder\n+\n+    >>> # Initializing a VoxtralEncoderConfig\n+    >>> configuration = VoxtralEncoderConfig()\n+\n+    >>> # Initializing a VoxtralEncoder (with random weights)\n+    >>> model = VoxtralEncoder(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"voxtral_encoder\"\n+\n+    attribute_map = {\n+        \"d_model\": \"hidden_size\",\n+        \"encoder_layers\": \"num_hidden_layers\",\n+        \"encoder_attention_heads\": \"num_attention_heads\",\n+        \"encoder_ffn_dim\": \"intermediate_size\",\n+        \"encoder_layerdrop\": \"layerdrop\",\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=51866,\n+        hidden_size=1280,\n+        intermediate_size=5120,\n+        num_hidden_layers=32,\n+        num_attention_heads=20,\n+        scale_embedding=False,\n+        activation_function=\"gelu\",\n+        num_mel_bins=128,\n+        max_source_positions=1500,\n+        initializer_range=0.02,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+\n+        self.num_attention_heads = num_attention_heads\n+        self.scale_embedding = scale_embedding  # scale factor will be sqrt(hidden_size) if True\n+        self.activation_function = activation_function\n+        self.num_mel_bins = num_mel_bins\n+        self.max_source_positions = max_source_positions\n+        self.initializer_range = initializer_range\n+\n+        # TODO: @eustlb, we do not use dropout and layerdrop, yet we need to hardcode them\n+        # to be able to use Whisper with modular (here actually from Qwen2-Audio and copied from).\n+        # After a future Whisper refactor, we should remove this.\n+        self.dropout = 0.0\n+        self.layerdrop = 0.0\n+        self.activation_dropout = 0.0\n+\n+        self.attention_dropout = attention_dropout\n+\n+\n+class VoxtralConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`VoxtralForConditionalGeneration`]. It is used to instantiate an\n+    Voxtral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Voxtral-Mini-3B.\n+\n+    e.g. [mistralai/Voxtral-Mini-3B-2507](https://huggingface.co/mistralai/Voxtral-Mini-3B-2507)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        audio_config (`Union[AutoConfig, dict]`, *optional*):\n+            The config object or dictionary of the audio encoder.\n+        text_config (`Union[AutoConfig, dict]`, *optional*):\n+            The config object or dictionary of the text model.\n+        audio_token_id (`int`, *optional*):\n+            The image token index to encode the image prompt.\n+        projector_hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            The activation function (function or string) in the multi-modal projector.\n+\n+    ```python\n+    >>> from transformers import VoxtralForConditionalGeneration, VoxtralConfig\n+\n+    >>> # Initializing a Voxtral configuration\n+    >>> configuration = VoxtralConfig(audio_token_id=24, projector_hidden_act=\"gelu\")\n+\n+    >>> # Initializing a 3B model with random weights\n+    >>> model = VoxtralForConditionalGeneration(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"voxtral\"\n+    sub_configs = {\"text_config\": AutoConfig, \"audio_config\": AutoConfig}\n+\n+    _default_text_config_kwargs = {\n+        \"vocab_size\": 131072,\n+        \"hidden_size\": 3072,\n+        \"intermediate_size\": 8192,\n+        \"num_hidden_layers\": 30,\n+        \"num_key_value_heads\": 8,\n+        \"max_position_embeddings\": 131072,\n+        \"rms_norm_eps\": 1e-05,\n+        \"use_cache\": True,\n+        \"rope_theta\": 100000000.0,\n+        \"head_dim\": 128,\n+    }\n+\n+    def __init__(\n+        self,\n+        audio_config=None,\n+        text_config=None,\n+        audio_token_id=None,\n+        projector_hidden_act=\"gelu\",\n+        **kwargs,\n+    ):\n+        if isinstance(audio_config, dict):\n+            audio_config[\"model_type\"] = (\n+                audio_config[\"model_type\"] if \"model_type\" in audio_config else \"voxtral_encoder\"\n+            )\n+            audio_config = CONFIG_MAPPING[audio_config[\"model_type\"]](**audio_config)\n+        elif audio_config is None:\n+            audio_config = CONFIG_MAPPING[\"voxtral_encoder\"]()\n+        self.audio_config = audio_config\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](\n+                **{**self._default_text_config_kwargs, **text_config}\n+            )\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"llama\"](**self._default_text_config_kwargs)\n+        self.text_config = text_config\n+\n+        self.vocab_size = text_config.vocab_size\n+        self.hidden_size = text_config.hidden_size\n+        self.audio_token_id = audio_token_id\n+        self.projector_hidden_act = projector_hidden_act\n+\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"VoxtralEncoderConfig\", \"VoxtralConfig\"]"
        },
        {
            "sha": "b2bc8f01629a030ab352d6728621b8af137aed26",
            "filename": "src/transformers/models/voxtral/convert_voxtral_weights_to_hf.py",
            "status": "added",
            "additions": 302,
            "deletions": 0,
            "changes": 302,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fconvert_voxtral_weights_to_hf.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,302 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import json\n+import os\n+import re\n+\n+import torch\n+from safetensors.torch import load_file\n+\n+from transformers import (\n+    MistralCommonTokenizer,\n+    VoxtralConfig,\n+    VoxtralForConditionalGeneration,\n+    VoxtralProcessor,\n+    WhisperFeatureExtractor,\n+)\n+from transformers.models.whisper.modeling_whisper import sinusoids\n+from transformers.utils.hub import cached_file\n+\n+\n+# fmt: off\n+STATE_DICT_MAPPING = {\n+    # Text model keys\n+    r\"^output.weight\":                                                                  r\"language_model.lm_head.weight\",\n+    r\"^norm.weight\":                                                                    r\"language_model.model.norm.weight\",\n+    r\"^tok_embeddings.weight\":                                                          r\"language_model.model.embed_tokens.weight\",\n+    r\"^layers.(\\d+).attention_norm.weight\":                                             r\"language_model.model.layers.\\1.input_layernorm.weight\",\n+    r\"^layers.(\\d+).ffn_norm.weight\":                                                   r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"^layers.(\\d+).attention.w(q|k|v|o).weight\":                                       r\"language_model.model.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w1.weight\":                                            r\"language_model.model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w2.weight\":                                            r\"language_model.model.layers.\\1.mlp.down_proj.weight\",\n+    r\"^layers.(\\d+).feed_forward.w3.weight\":                                            r\"language_model.model.layers.\\1.mlp.up_proj.weight\",\n+\n+    r\"mm_whisper_embeddings.tok_embeddings.weight\":                                     r\"language_model.model.embed_tokens.weight\",\n+\n+    # audio model keys\n+    r\"mm_whisper_embeddings.whisper_encoder\\.conv_layers\\.0\\.(weight|bias)\": r\"audio_tower.conv1.\\1\",\n+    r\"mm_whisper_embeddings.whisper_encoder\\.conv_layers\\.1\\.(weight|bias)\": r\"audio_tower.conv2.\\1\",\n+\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.norm\\.(weight|bias)\": r\"audio_tower.layer_norm.\\1\",\n+\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.attention\\.w([qkv])\\.(weight|bias)\": r\"audio_tower.layers.\\1.self_attn.\\2_proj.\\3\",\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.attention\\.wo\\.(weight|bias)\": r\"audio_tower.layers.\\1.self_attn.out_proj.\\2\",\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.attention_norm\\.(weight|bias)\": r\"audio_tower.layers.\\1.self_attn_layer_norm.\\2\",\n+\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.feed_forward\\.w1\\.(weight|bias)\": r\"audio_tower.layers.\\1.fc1.\\2\",\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.feed_forward\\.w2\\.(weight|bias)\": r\"audio_tower.layers.\\1.fc2.\\2\",\n+\n+    r\"mm_whisper_embeddings.whisper_encoder\\.transformer\\.layers\\.(\\d+)\\.ffn_norm\\.(weight|bias)\": r\"audio_tower.layers.\\1.final_layer_norm.\\2\",\n+\n+    r\"mm_whisper_embeddings.audio_language_projection\\.0\\.weight\":               r\"multi_modal_projector.linear_1.weight\",\n+    r\"mm_whisper_embeddings.audio_language_projection\\.2\\.weight\":               r\"multi_modal_projector.linear_2.weight\",\n+}\n+# fmt: on\n+\n+\n+def convert_config(original_config: dict, max_position_embeddings: int = 131072):\n+    original_audio_config = original_config.pop(\"multimodal\")\n+    original_audio_config = original_audio_config[\"whisper_model_args\"][\"encoder_args\"]\n+    original_text_config = original_config\n+\n+    # Text config\n+    text_key_mapping = {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"intermediate_size\": \"hidden_dim\",\n+        \"num_attention_heads\": \"n_heads\",\n+        \"num_key_value_heads\": \"n_kv_heads\",\n+        \"rms_norm_eps\": \"norm_eps\",\n+    }\n+    similar_text_keys_to_keep = [\n+        \"head_dim\",\n+        \"vocab_size\",\n+        \"rope_theta\",\n+    ]\n+    new_text_config_kwargs = {k: original_text_config[v] for k, v in text_key_mapping.items()}\n+    new_text_config_kwargs.update({k: v for k, v in original_text_config.items() if k in similar_text_keys_to_keep})\n+    # These are not always defined depending on `params.json`\n+    new_text_config_kwargs[\"sliding_window\"] = original_text_config.get(\"sliding_window\", None)\n+    new_text_config_kwargs[\"max_position_embeddings\"] = original_text_config.get(\n+        \"max_seq_len\", max_position_embeddings\n+    )\n+    # This may sometimes be a string in `params.json`\n+    if new_text_config_kwargs[\"sliding_window\"] is not None:\n+        new_text_config_kwargs[\"sliding_window\"] = int(new_text_config_kwargs[\"sliding_window\"])\n+\n+    # Audio config\n+    audio_key_mapping = {\n+        \"hidden_size\": \"dim\",\n+        \"num_hidden_layers\": \"n_layers\",\n+        \"intermediate_size\": \"hidden_dim\",\n+        \"num_attention_heads\": \"n_heads\",\n+        \"num_key_value_heads\": \"n_heads\",\n+    }\n+    similar_audio_keys_to_keep = [\n+        \"head_dim\",\n+        \"vocab_size\",\n+    ]\n+    new_audio_config_kwargs = {k: original_audio_config[v] for k, v in audio_key_mapping.items()}\n+    new_audio_config_kwargs.update({k: v for k, v in original_audio_config.items() if k in similar_audio_keys_to_keep})\n+\n+    new_config = VoxtralConfig(\n+        audio_config=new_audio_config_kwargs,\n+        text_config=new_text_config_kwargs,\n+        audio_token_id=24,\n+        projector_hidden_act=\"gelu\",\n+    )\n+\n+    return new_config\n+\n+\n+def map_old_key_to_new(old_key):\n+    \"\"\"Map of a key of the original state dict to the equivalent key in HF format\"\"\"\n+    for pattern, replacement in STATE_DICT_MAPPING.items():\n+        new_key, n_replace = re.subn(pattern, replacement, old_key)\n+        # Early exit of the loop\n+        if n_replace > 0:\n+            return new_key\n+\n+    raise ValueError(f\"Key: {old_key} could not be mapped (check the mapping).\")\n+\n+\n+def permute_for_rope(tensor, n_heads, dim1, dim2):\n+    \"\"\"Permute the weights for the ROPE formulation.\"\"\"\n+    tensor = tensor.view(n_heads, dim1 // n_heads // 2, 2, dim2)\n+    tensor = tensor.transpose(1, 2)\n+    tensor = tensor.reshape(dim1, dim2)\n+    return tensor\n+\n+\n+def convert_state_dict(original_state_dict, config):\n+    \"\"\"Convert a state dict file, when a single `nn.Module` is never sharded in different files (usual case).\"\"\"\n+    new_dict = {}\n+\n+    num_attention_heads = config.num_attention_heads\n+    hidden_size = config.hidden_size\n+    head_dim = config.head_dim\n+    num_key_value_heads = config.num_key_value_heads\n+    key_value_dim = head_dim * num_key_value_heads\n+    query_dim = head_dim * num_attention_heads\n+\n+    for old_key, tensor in original_state_dict.items():\n+        new_key = map_old_key_to_new(old_key)\n+\n+        if \"audio_tower\" not in new_key:\n+            if \"q_proj\" in new_key:\n+                tensor = tensor.view(num_attention_heads, head_dim, hidden_size).reshape(query_dim, hidden_size)\n+                tensor = permute_for_rope(tensor, num_attention_heads, query_dim, hidden_size)\n+            elif \"k_proj\" in new_key:\n+                tensor = tensor.view(num_key_value_heads, head_dim, hidden_size).reshape(key_value_dim, hidden_size)\n+                tensor = permute_for_rope(tensor, num_key_value_heads, key_value_dim, hidden_size)\n+            elif \"v_proj\" in new_key:\n+                tensor = tensor.view(num_key_value_heads, head_dim, hidden_size).reshape(key_value_dim, hidden_size)\n+\n+        new_dict[new_key] = tensor\n+    return new_dict\n+\n+\n+def write_model(\n+    input_path_or_repo,\n+    model_name,\n+    config_name,\n+    output_dir,\n+    safe_serialization=True,\n+):\n+    print(\"Converting the model.\")\n+    os.makedirs(output_dir, exist_ok=True)\n+\n+    # --------------\n+    # convert config\n+    # --------------\n+\n+    config_path = cached_file(input_path_or_repo, config_name)\n+    with open(config_path, \"r\") as f:\n+        original_config = json.load(f)\n+\n+    config = convert_config(original_config)\n+    model = VoxtralForConditionalGeneration(config)\n+\n+    # ---------------\n+    # convert weights\n+    # ---------------\n+\n+    model_path = cached_file(input_path_or_repo, model_name)\n+    print(f\"Fetching all parameters from the checkpoint at {model_path}...\")\n+    state_dict = load_file(model_path)\n+    print(\"Converting model...\")\n+    converted_state_dict = convert_state_dict(state_dict, config.text_config)\n+\n+    # we need to add embed positions as they are not in the state dict\n+    with torch.no_grad(), torch.device(\"cuda\"):\n+        # TODO: @eustlb, we are here creating on GPU\n+        # vllm initalizes on device, while we save in state dict\n+        embed_positions_weight = sinusoids(config.audio_config.max_source_positions, config.audio_config.hidden_size)\n+    converted_state_dict[\"audio_tower.embed_positions.weight\"] = embed_positions_weight.cpu()\n+\n+    # -------------------------\n+    # load the weights and save\n+    # -------------------------\n+\n+    print(\"Loading the checkpoint in a Voxtral model.\")\n+    with torch.device(\"meta\"):\n+        model = VoxtralForConditionalGeneration(config)\n+    model.load_state_dict(converted_state_dict, strict=True, assign=True)\n+    print(\"Checkpoint loaded successfully.\")\n+    del model.config._name_or_path\n+\n+    del model.generation_config._from_model_config\n+    model.generation_config.pad_token_id = 11\n+\n+    print(\"Saving the model.\")\n+    model.save_pretrained(output_dir, safe_serialization=safe_serialization)\n+    del state_dict, model\n+\n+    # Safety check: reload the converted model\n+    gc.collect()\n+    print(\"Reloading the model to check if it's saved correctly.\")\n+    VoxtralForConditionalGeneration.from_pretrained(output_dir, torch_dtype=torch.bfloat16, device_map=\"auto\")\n+    print(\"Model reloaded successfully.\")\n+\n+\n+def write_processor(input_path_or_repo: str, feature_extractor_path_or_repo: str, output_dir: str):\n+    tokenizer = MistralCommonTokenizer.from_pretrained(input_path_or_repo)\n+    feature_extractor = WhisperFeatureExtractor.from_pretrained(feature_extractor_path_or_repo)\n+\n+    print(\"Creating the processor...\")\n+    # Create the processor and save it\n+    processor = VoxtralProcessor(\n+        feature_extractor=feature_extractor,\n+        tokenizer=tokenizer,\n+    )\n+    processor.save_pretrained(output_dir)\n+    print(\"Processor saved successfully.\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert Voxtral weights to Hugging Face format\")\n+    parser.add_argument(\n+        \"--input_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing Csm weights\",\n+    )\n+    parser.add_argument(\n+        \"--model_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the model in input_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--config_name\",\n+        type=str,\n+        required=True,\n+        help=\"Name of the config in input_path_or_repo\",\n+    )\n+    parser.add_argument(\n+        \"--feature_extractor_path_or_repo\",\n+        type=str,\n+        required=True,\n+        help=\"Path or repo containing the feature extractor\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n+    )\n+    args = parser.parse_args()\n+\n+    write_model(\n+        args.input_path_or_repo,\n+        args.model_name,\n+        args.config_name,\n+        args.output_dir,\n+        safe_serialization=args.safe_serialization,\n+    )\n+\n+    write_processor(\n+        args.input_path_or_repo,\n+        args.feature_extractor_path_or_repo,\n+        args.output_dir,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "b2350310a8ae10760144665f0176253339bbd52b",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "added",
            "additions": 542,
            "deletions": 0,
            "changes": 542,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,542 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/voxtral/modular_voxtral.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_voxtral.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from .configuration_voxtral import VoxtralConfig, VoxtralEncoderConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: Optional[float] = None,\n+    dropout: float = 0.0,\n+    head_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    if scaling is None:\n+        scaling = query.size(-1) ** -0.5\n+\n+    attn_weights = torch.matmul(query, key.transpose(2, 3)) * scaling\n+    if attention_mask is not None and attention_mask.ndim == 4:\n+        attn_weights = attn_weights + attention_mask[:, :, :, : key.shape[-2]]\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1)\n+\n+    if head_mask is not None:\n+        attn_weights = attn_weights * head_mask.view(1, -1, 1, 1)\n+\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class VoxtralAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(\n+        self,\n+        embed_dim: int,\n+        num_heads: int,\n+        dropout: float = 0.0,\n+        is_decoder: bool = False,\n+        bias: bool = True,\n+        is_causal: bool = False,\n+        layer_idx: Optional[int] = None,\n+        config: Optional[VoxtralConfig] = None,\n+    ):\n+        super().__init__()\n+        self.embed_dim = embed_dim\n+        self.num_heads = num_heads\n+        self.dropout = dropout\n+        self.head_dim = embed_dim // num_heads\n+        self.config = config\n+\n+        if (self.head_dim * num_heads) != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim}\"\n+                f\" and `num_heads`: {num_heads}).\"\n+            )\n+        self.scaling = self.head_dim**-0.5\n+        self.is_decoder = is_decoder\n+        self.is_causal = is_causal\n+\n+        if layer_idx is None and is_decoder:\n+            logger.warning_once(\n+                f\"Instantiating a decoder {self.__class__.__name__} without passing `layer_idx` is not recommended and \"\n+                \"will to errors during the forward call, if caching is used. Please make sure to provide a `layer_idx` \"\n+                \"when creating this class.\"\n+            )\n+        self.layer_idx = layer_idx\n+\n+        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=False)\n+        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n+\n+    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n+        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        layer_head_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        **kwargs,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        bsz, tgt_len, _ = hidden_states.size()\n+\n+        # Scaling is susceptible to floating point arithmetics' inprecisions\n+        # which can lead to different results (this is dependent from model\n+        # to model, e.g. whisper is one such case). We therefore keep the\n+        # original order of scaling to follow the original implementation\n+        # and enforce no scaling (1.0) in the attention call below.\n+        query_states = self._shape(self.q_proj(hidden_states) * self.scaling, tgt_len, bsz)\n+        key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n+        value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.dropout,\n+            scaling=1.0,\n+            output_attentions=output_attentions,\n+            head_mask=layer_head_mask,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(bsz, tgt_len, -1).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class VoxtralEncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: VoxtralConfig):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+\n+        self.self_attn = VoxtralAttention(\n+            embed_dim=self.embed_dim,\n+            num_heads=config.encoder_attention_heads,\n+            dropout=config.attention_dropout,\n+            config=config,\n+        )\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        layer_head_mask: torch.Tensor,\n+        output_attentions: bool = False,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            layer_head_mask (`torch.FloatTensor`): mask for attention heads in a given layer of size\n+                `(encoder_attention_heads,)`.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            layer_head_mask=layer_head_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+\n+        if hidden_states.dtype == torch.float16:\n+            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+\n+        return hidden_states, attn_weights\n+\n+\n+@auto_docstring\n+class VoxtralPreTrainedModel(PreTrainedModel):\n+    config: VoxtralConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"VoxtralAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_attention_backend = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of Voxtral isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.audio_config.initializer_range\n+        )\n+\n+        if isinstance(module, (nn.Linear, nn.Conv1d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Voxtral encoder, which is a Whisper encoder.\n+    \"\"\"\n+)\n+class VoxtralEncoder(VoxtralPreTrainedModel):\n+    \"\"\"\n+    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n+    [`VoxtralEncoderLayer`].\n+\n+    Args:\n+        config: VoxtralEncoderConfig\n+    \"\"\"\n+\n+    # Ignore copy\n+    config: VoxtralEncoderConfig\n+    main_input_name = \"input_features\"\n+    _no_split_modules = [\"VoxtralEncoderLayer\"]\n+    _can_record_outputs = {\n+        \"attentions\": VoxtralAttention,\n+        \"hidden_states\": VoxtralEncoderLayer,\n+    }\n+\n+    def __init__(self, config: VoxtralEncoderConfig):\n+        super().__init__(config)\n+        self.dropout = config.dropout\n+        self.layerdrop = config.encoder_layerdrop\n+\n+        embed_dim = config.d_model\n+        self.num_mel_bins = config.num_mel_bins\n+        self.padding_idx = config.pad_token_id\n+        self.max_source_positions = config.max_source_positions\n+        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n+\n+        self.conv1 = nn.Conv1d(self.num_mel_bins, embed_dim, kernel_size=3, padding=1)\n+        self.conv2 = nn.Conv1d(embed_dim, embed_dim, kernel_size=3, stride=2, padding=1)\n+\n+        self.embed_positions = nn.Embedding(self.max_source_positions, embed_dim)\n+        self.embed_positions.requires_grad_(False)\n+\n+        self.layers = nn.ModuleList([VoxtralEncoderLayer(config) for _ in range(config.encoder_layers)])\n+        self.layer_norm = nn.LayerNorm(config.d_model)\n+        # Ignore copy\n+        self.avg_pooler = nn.AvgPool1d(2, stride=2)\n+\n+        self.gradient_checkpointing = False\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def _freeze_parameters(self):\n+        for param in self.parameters():\n+            param.requires_grad = False\n+        self._requires_grad = False\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.conv1\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        self.conv1 = value\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_features,\n+        attention_mask=None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        Args:\n+            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            attention_mask (`torch.Tensor`)`, *optional*):\n+                Voxtral does not support masking of the `input_features`, this argument is preserved for compatibility,\n+                but it is not used. By default the silence in the input log mel spectrogram are ignored.\n+        \"\"\"\n+        expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\n+        if input_features.shape[-1] != expected_seq_length:\n+            raise ValueError(\n+                f\"Qwen2Audio expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n+            )\n+\n+        input_features = input_features.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n+\n+        embed_pos = self.embed_positions.weight\n+        hidden_states = (inputs_embeds + embed_pos).to(inputs_embeds.dtype)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        for idx, encoder_layer in enumerate(self.layers):\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                layer_head_mask=None,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+    # Ignore copy\n+    def _get_feat_extract_output_lengths(self, input_lengths: torch.LongTensor):\n+        \"\"\"\n+        Computes the output length of the convolutional layers and the output length of the audio encoder\n+        \"\"\"\n+        input_lengths = (input_lengths - 1) // 2 + 1\n+        output_lengths = (input_lengths - 2) // 2 + 1\n+        return input_lengths, output_lengths\n+\n+\n+class VoxtralMultiModalProjector(nn.Module):\n+    def __init__(self, config: VoxtralConfig):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(config.audio_config.intermediate_size, config.text_config.hidden_size, bias=False)\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=False)\n+\n+    def forward(self, audio_features):\n+        hidden_states = self.linear_1(audio_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Voxtral model, which consists of Whisper encoder, a multi-modal projector and a LLama language model.\n+    \"\"\"\n+)\n+class VoxtralForConditionalGeneration(VoxtralPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+    _keep_in_fp32_modules_strict = [\"embed_positions\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.audio_tower = AutoModel.from_config(config.audio_config)\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.multi_modal_projector = VoxtralMultiModalProjector(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_audio_embeds(self, input_features: torch.FloatTensor):\n+        \"\"\"\n+        This method is used to get the audio embeddings from input features (a log mel spectrogram), meaning inferring the audio encoder and the multi-modal projector.\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+\n+        Returns:\n+            `torch.FloatTensor`:\n+                The audio embeddings.\n+        \"\"\"\n+        audio_outputs = self.audio_tower(input_features)\n+        audio_hidden_states = audio_outputs.last_hidden_state\n+        audio_hidden_states = audio_hidden_states.reshape(-1, self.config.audio_config.intermediate_size)\n+        audio_embeds = self.multi_modal_projector(audio_hidden_states)\n+        return audio_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+        >>> import torch\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        >>> repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(repo_id)\n+        >>> model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n+        >>> conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(conversation)\n+        >>> inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+        >>> outputs = model.generate(**inputs, max_new_tokens=30)\n+        >>> processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+        [\"This audio is a humorous conversation between two friends, likely in English, where one of them is trying to figure out what the other's tattoo says.\"]\n+        ```\"\"\"\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if input_features is not None:\n+            audio_embeds = self.get_audio_embeds(input_features)\n+\n+            # replace text-audio token placeholders with audio embeddings\n+            audio_token_mask = input_ids == self.config.audio_token_id\n+            inputs_embeds[audio_token_mask] = audio_embeds\n+\n+        outputs: BaseModelOutputWithPast = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return outputs\n+\n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            model_inputs[\"input_features\"] = input_features\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"VoxtralPreTrainedModel\", \"VoxtralEncoder\", \"VoxtralForConditionalGeneration\"]"
        },
        {
            "sha": "fdb9862ad5f479e33061996aed0dcb51f3581416",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "added",
            "additions": 276,
            "deletions": 0,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,276 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from ..auto import AutoModel, AutoModelForCausalLM\n+from ..qwen2_audio.modeling_qwen2_audio import (\n+    Qwen2AudioAttention,\n+    Qwen2AudioEncoder,\n+    Qwen2AudioEncoderLayer,\n+    Qwen2AudioPreTrainedModel,\n+)\n+from .configuration_voxtral import VoxtralConfig\n+\n+\n+class VoxtralAttention(Qwen2AudioAttention):\n+    pass\n+\n+\n+class VoxtralEncoderLayer(Qwen2AudioEncoderLayer):\n+    pass\n+\n+\n+class VoxtralPreTrainedModel(Qwen2AudioPreTrainedModel):\n+    _supports_flex_attn = True\n+    _supports_cache_class = True\n+    _supports_attention_backend = True\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+\n+\n+# TODO: @eustlb, I would really prefer to use WhisperEncoder but it's messing with modular\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Voxtral encoder, which is a Whisper encoder.\n+    \"\"\"\n+)\n+class VoxtralEncoder(Qwen2AudioEncoder):\n+    _can_record_outputs = {\n+        \"attentions\": VoxtralAttention,\n+        \"hidden_states\": VoxtralEncoderLayer,\n+    }\n+\n+    @check_model_inputs\n+    def forward(\n+        self,\n+        input_features,\n+        attention_mask=None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ):\n+        r\"\"\"\n+        Args:\n+            input_features (`torch.LongTensor` of shape `(batch_size, feature_size, sequence_length)`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+            attention_mask (`torch.Tensor`)`, *optional*):\n+                Voxtral does not support masking of the `input_features`, this argument is preserved for compatibility,\n+                but it is not used. By default the silence in the input log mel spectrogram are ignored.\n+        \"\"\"\n+        expected_seq_length = self.config.max_source_positions * self.conv1.stride[0] * self.conv2.stride[0]\n+        if input_features.shape[-1] != expected_seq_length:\n+            raise ValueError(\n+                f\"Qwen2Audio expects the mel input features to be of length {expected_seq_length}, but found {input_features.shape[-1]}. Make sure to pad the input mel features to {expected_seq_length}.\"\n+            )\n+\n+        input_features = input_features.to(dtype=self.conv1.weight.dtype, device=self.conv1.weight.device)\n+        inputs_embeds = nn.functional.gelu(self.conv1(input_features))\n+        inputs_embeds = nn.functional.gelu(self.conv2(inputs_embeds))\n+        inputs_embeds = inputs_embeds.permute(0, 2, 1)\n+\n+        embed_pos = self.embed_positions.weight\n+        hidden_states = (inputs_embeds + embed_pos).to(inputs_embeds.dtype)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        for idx, encoder_layer in enumerate(self.layers):\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask=attention_mask,\n+                layer_head_mask=None,\n+            )\n+            hidden_states = layer_outputs[0]\n+\n+        hidden_states = self.layer_norm(hidden_states)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+        )\n+\n+\n+class VoxtralMultiModalProjector(nn.Module):\n+    def __init__(self, config: VoxtralConfig):\n+        super().__init__()\n+        self.linear_1 = nn.Linear(config.audio_config.intermediate_size, config.text_config.hidden_size, bias=False)\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=False)\n+\n+    def forward(self, audio_features):\n+        hidden_states = self.linear_1(audio_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The Voxtral model, which consists of Whisper encoder, a multi-modal projector and a LLama language model.\n+    \"\"\"\n+)\n+class VoxtralForConditionalGeneration(VoxtralPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+    _keep_in_fp32_modules_strict = [\"embed_positions\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.vocab_size = config.text_config.vocab_size\n+        self.audio_tower = AutoModel.from_config(config.audio_config)\n+        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n+        self.multi_modal_projector = VoxtralMultiModalProjector(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.language_model.get_output_embeddings()\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.language_model.set_output_embeddings(new_embeddings)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.language_model.get_decoder()\n+\n+    def get_audio_embeds(self, input_features: torch.FloatTensor):\n+        \"\"\"\n+        This method is used to get the audio embeddings from input features (a log mel spectrogram), meaning inferring the audio encoder and the multi-modal projector.\n+        Args:\n+            input_features (`torch.FloatTensor`):\n+                Float values of mel features extracted from the raw speech waveform. Raw speech waveform can be\n+                obtained by loading a `.flac` or `.wav` audio file into an array of type `list[float]` or a\n+                `numpy.ndarray`, *e.g.* via the soundfile library (`pip install soundfile`). To prepare the array into\n+                `input_features`, the [`AutoFeatureExtractor`] should be used for extracting the mel features, padding\n+                and conversion into a tensor of type `torch.FloatTensor`. See [`~WhisperFeatureExtractor.__call__`]\n+\n+        Returns:\n+            `torch.FloatTensor`:\n+                The audio embeddings.\n+        \"\"\"\n+        audio_outputs = self.audio_tower(input_features)\n+        audio_hidden_states = audio_outputs.last_hidden_state\n+        audio_hidden_states = audio_hidden_states.reshape(-1, self.config.audio_config.intermediate_size)\n+        audio_embeds = self.multi_modal_projector(audio_hidden_states)\n+        return audio_embeds\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        input_features: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import VoxtralForConditionalGeneration, AutoProcessor\n+        >>> import torch\n+\n+        >>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+        >>> repo_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(repo_id)\n+        >>> model = VoxtralForConditionalGeneration.from_pretrained(repo_id, torch_dtype=torch.bfloat16, device_map=device)\n+\n+        >>> conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"url\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+                ],\n+            }\n+        ]\n+\n+        >>> inputs = processor.apply_chat_template(conversation)\n+        >>> inputs = inputs.to(device, dtype=torch.bfloat16)\n+\n+        >>> outputs = model.generate(**inputs, max_new_tokens=30)\n+        >>> processor.batch_decode(outputs[:, inputs.input_ids.shape[1]:], skip_special_tokens=True)\n+        [\"This audio is a humorous conversation between two friends, likely in English, where one of them is trying to figure out what the other's tattoo says.\"]\n+        ```\"\"\"\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if input_features is not None:\n+            audio_embeds = self.get_audio_embeds(input_features)\n+\n+            # replace text-audio token placeholders with audio embeddings\n+            audio_token_mask = input_ids == self.config.audio_token_id\n+            inputs_embeds[audio_token_mask] = audio_embeds\n+\n+        outputs: BaseModelOutputWithPast = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            labels=labels,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+        return outputs\n+\n+    def prepare_inputs_for_generation(self, *args, **kwargs):\n+        # Overwritten -- we should not pass input_features when we are in cached decoding stage\n+\n+        input_features = kwargs.pop(\"input_features\", None)\n+        cache_position = kwargs.get(\"cache_position\")\n+\n+        model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n+\n+        if cache_position is not None and cache_position[0] == 0:\n+            # input_features should only be passed when we are not in cached decoding stage\n+            model_inputs[\"input_features\"] = input_features\n+\n+        return model_inputs\n+\n+\n+__all__ = [\"VoxtralPreTrainedModel\", \"VoxtralEncoder\", \"VoxtralForConditionalGeneration\"]"
        },
        {
            "sha": "f684466874f9d4abf07909b13244472fce4742c2",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "added",
            "additions": 449,
            "deletions": 0,
            "changes": 449,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,449 @@\n+# coding=utf-8\n+# Copyright 2025 Sesame and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import io\n+from typing import Optional, Union\n+\n+from ...utils import is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_soundfile_available():\n+    import soundfile as sf\n+\n+if is_mistral_common_available():\n+    from mistral_common.protocol.transcription.request import TranscriptionRequest\n+\n+from ...audio_utils import AudioInput, load_audio_as, make_list_of_audio\n+from ...feature_extraction_utils import BatchFeature\n+from ...processing_utils import AllKwargsForChatTemplate, AudioKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class VoxtralAudioKwargs(AudioKwargs, total=False):\n+    max_source_positions: Optional[int]\n+\n+\n+class VoxtralProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": True,\n+        },\n+        \"audio_kwargs\": {\n+            \"sampling_rate\": 16000,\n+            \"padding\": True,\n+            \"truncation\": False,\n+            \"pad_to_multiple_of\": 480000,\n+            \"max_source_positions\": 3000,\n+        },\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"pt\",\n+            \"return_dict\": True,\n+            \"tokenize\": True,\n+        },\n+    }\n+\n+\n+class VoxtralProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Voxtral processor which wraps [`WhisperFeatureExtractor`] and\n+    [`MistralCommonTokenizer`] into a single processor that inherits both the audio feature extraction and\n+    tokenizer functionalities.\n+\n+    Args:\n+        feature_extractor ([`WhisperFeatureExtractor`]):\n+            The feature extractor is a required input.\n+        tokenizer ([`MistralCommonTokenizer`]):\n+            The tokenizer is a required input.\n+    \"\"\"\n+\n+    attributes = [\"feature_extractor\", \"tokenizer\"]\n+    feature_extractor_class = \"WhisperFeatureExtractor\"\n+    tokenizer_class = \"MistralCommonTokenizer\"\n+\n+    def __init__(\n+        self,\n+        feature_extractor,\n+        tokenizer,\n+    ):\n+        self.audio_token_id = 24\n+        self.audio_token = tokenizer.convert_ids_to_tokens(self.audio_token_id)\n+\n+        super().__init__(feature_extractor, tokenizer)\n+\n+    def _retreive_input_features(self, audio, max_source_positions, **kwargs):\n+        \"\"\"\n+        Handles specific logic of Voxtral expected input features: audio arrays should be padded to next multiple of 480000 (duration is a multiple of 30s), see VoxtralProcessorKwargs' default audio_kwargs.\n+        Then mel input features are extracted and stacked along batch dimension, splitting into chunks of max_source_positions.\n+        \"\"\"\n+        input_features_list = []\n+        for audio_array in audio:\n+            audio_inputs = self.feature_extractor(audio_array, **kwargs)\n+\n+            # let's split into chunks of max_source_positions, and then stack them along batch dimension\n+            input_features = audio_inputs[\"input_features\"].reshape(\n+                self.feature_extractor.feature_size, -1, max_source_positions\n+            )\n+            input_features_list.append(input_features.transpose(0, 1))\n+\n+        return torch.cat(input_features_list)\n+\n+    def apply_chat_template(\n+        self,\n+        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n+        **kwargs: Unpack[AllKwargsForChatTemplate],\n+    ) -> str:\n+        \"\"\"\n+        This method applies the model's chat completion template given a conversation. It relies on MistralCommonTokenizer's\n+        [`~MistralCommonTokenizer.apply_chat_template`] to prepare input ids to the model and on WhisperFeatureExtractor's\n+        [`~WhisperFeatureExtractor.__call__`] to prepare input features to the model.\n+\n+        Note that audio is padded to the nearest 30-second multiple prior to mel feature extraction.\n+\n+        A `conversation` is a list of messages, where each message is a dictionary with a `role` and a `content` field.\n+        For Voxtral, `role` can be `\"user\"` or `\"assistant\"`.\n+        The `content` field can be a string or a list of dictionaries with a `type` field. See example below.\n+\n+        ```python\n+        from huggingface_hub import hf_hub_download\n+        from transformers.audio_utils import load_audio_as\n+\n+        audio_url = \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\"\n+        audio_path = hf_hub_download(repo_id=\"hf-internal-testing/dummy-audio-samples\", filename=\"bcn_weather.mp3\", repo_type=\"dataset\")\n+        audio_base64 = load_audio_as(audio_path, return_format=\"base64\", force_mono=True)\n+\n+        # audio + text\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"audio\", \"url\": audio_url},\n+                    {\"type\": \"audio\", \"path\": audio_path},\n+                    {\"type\": \"audio\", \"base64\": audio_base64},\n+                    {\"type\": \"text\", \"text\": \"How many audio do you hear?\"},\n+                ],\n+            },\n+        ]\n+\n+        processor = VoxtralProcessor.from_pretrained(\"mistralai/Voxtral-Mini-3B-2507\")\n+        inputs = processor.apply_chat_template(conversation)\n+        ```\n+\n+        Args:\n+            conversation (`Union[list[Dict, [str, str]], list[list[dict[str, str]]]]`):\n+                The conversation to format.\n+        \"\"\"\n+        if kwargs.get(\"continue_final_message\", False):\n+            if kwargs.get(\"add_generation_prompt\", False):\n+                raise ValueError(\n+                    \"continue_final_message and add_generation_prompt are not compatible. Use continue_final_message when you want the model to continue the final message, and add_generation_prompt when you want to add a header that will prompt it to start a new assistant message instead.\"\n+                )\n+            if kwargs.get(\"return_assistant_tokens_mask\", False):\n+                raise ValueError(\"continue_final_message is not compatible with return_assistant_tokens_mask.\")\n+\n+        # Fill sets of kwargs that should be used by different parts of template\n+        processed_kwargs = {\n+            \"mm_load_kwargs\": {},\n+            \"template_kwargs\": {},\n+        }\n+\n+        for kwarg_type in processed_kwargs:\n+            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__.keys():\n+                kwarg_type_defaults = AllKwargsForChatTemplate.__annotations__[kwarg_type]\n+                default_value = getattr(kwarg_type_defaults, key, None)\n+                value = kwargs.pop(key, default_value)\n+                if value is not None and not isinstance(value, dict):\n+                    processed_kwargs[kwarg_type][key] = value\n+\n+        # Pass unprocessed custom kwargs\n+        processed_kwargs[\"template_kwargs\"].update(kwargs)\n+\n+        if isinstance(conversation, (list, tuple)) and (\n+            isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n+        ):\n+            is_batched = True\n+            conversations = conversation\n+        else:\n+            is_batched = False\n+            conversations = [conversation]\n+\n+        # Check for any overlapping keys between mm_load_kwargs and kwargs\n+        mm_load_kwargs = processed_kwargs[\"mm_load_kwargs\"]\n+        if any(key in kwargs for key in mm_load_kwargs):\n+            overlapping_keys = [key for key in mm_load_kwargs if key in kwargs]\n+            logger.warning(\n+                f\"{overlapping_keys[0] if len(overlapping_keys) == 1 else ', '.join(overlapping_keys)} load multimodal data kwarg{'s' if len(overlapping_keys) > 1 else ''} {'have' if len(overlapping_keys) > 1 else 'has'} been passed to the processor, but {'they are' if len(overlapping_keys) > 1 else 'it is'} not supported for VoxtralProcessor since it relies on mistral_common directly. {'They' if len(overlapping_keys) > 1 else 'It'} will be ignored.\"\n+            )\n+\n+        output_kwargs = self._merge_kwargs(\n+            VoxtralProcessorKwargs,\n+            **kwargs,\n+        )\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        common_kwargs = output_kwargs[\"common_kwargs\"]\n+\n+        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        tokenizer_kwargs = {**processed_kwargs[\"template_kwargs\"], **text_kwargs}\n+        tokenizer_kwargs[\"return_tensors\"] = None  # let's not return tensors here\n+        tokenize = tokenizer_kwargs.pop(\"tokenize\", False)\n+        return_dict = tokenizer_kwargs.pop(\"return_dict\", False)\n+\n+        encoded_instruct_inputs = self.tokenizer.apply_chat_template(\n+            conversations,\n+            tokenize=tokenize,\n+            return_dict=return_dict,\n+            **tokenizer_kwargs,\n+        )\n+\n+        if tokenize:\n+            if return_dict:\n+                audio = encoded_instruct_inputs.pop(\"audio\", None)\n+                data = dict(encoded_instruct_inputs)\n+                if audio is not None:\n+                    max_source_positions = audio_kwargs.pop(\"max_source_positions\")\n+                    data[\"input_features\"] = self._retreive_input_features(audio, max_source_positions, **audio_kwargs)\n+\n+                return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+        if not is_batched:\n+            return encoded_instruct_inputs[0]\n+\n+        return encoded_instruct_inputs\n+\n+    def __call__(\n+        self,\n+        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]],\n+        **kwargs: Unpack[VoxtralProcessorKwargs],\n+    ):\n+        r\"\"\"\n+        Method to prepare text to be fed as input to the model. This method forwards the `text`\n+        arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.__call__`] to encode\n+        the text. Please refer to the docstring of the above methods for more information.\n+        This methods does not support audio. To prepare the audio, please use:\n+        1. `apply_chat_template` [`~VoxtralProcessor.apply_chat_template`] method.\n+        2. `apply_transcrition_request` [`~VoxtralProcessor.apply_transcrition_request`] method.\n+\n+        Args:\n+            text (`str`, `list[str]`, `list[list[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **input_features** -- List of audio values to be fed to a model. Returned when `audio` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+        \"\"\"\n+        if isinstance(text, str):\n+            text = [text]\n+\n+        if any(self.audio_token in t for t in text):\n+            raise ValueError(\n+                f\"{self.audio_token} is present in the provided text which is not supported by VoxtralProcessor. Please use the `apply_chat_template` method instead.\"\n+            )\n+\n+        output_kwargs = self._merge_kwargs(\n+            VoxtralProcessorKwargs,\n+            **kwargs,\n+        )\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+        common_kwargs = output_kwargs[\"common_kwargs\"]\n+\n+        out = self.tokenizer(text, **text_kwargs)\n+\n+        return BatchFeature(data=out, tensor_type=common_kwargs.pop(\"return_tensors\", None))\n+\n+    # TODO: @eustlb, this should be moved to mistral_common + testing\n+    def apply_transcrition_request(\n+        self,\n+        language: Union[str, list[str]],\n+        audio: Union[str, list[str], AudioInput],\n+        model_id: str,\n+        sampling_rate: Optional[int] = None,\n+        format: Optional[Union[str, list[str]]] = None,\n+        **kwargs: Unpack[VoxtralProcessorKwargs],\n+    ):\n+        \"\"\"\n+        This method applies the model's transcription request template given a language and audio.\n+        It relies on MistralCommonTokenizer and WhisperFeatureExtractor to prepare input ids and input features to the model.\n+\n+        ```python\n+        from transformers import VoxtralProcessor\n+\n+        model_id = \"mistralai/Voxtral-Mini-3B-2507\"\n+        processor = VoxtralProcessor.from_pretrained(model_id)\n+\n+        language = \"en\"\n+        audio = \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\"\n+\n+        inputs = processor.apply_transcrition_request(language=language, audio=audio, model_id=model_id)\n+        ```\n+\n+        Args:\n+            language (`str`, `list[str]`):\n+                The language or languages of the audio. If provided as a string, will be applied uniformly to all audio.\n+                If provided as a list, will be applied to each audio individually with a one-to-one mapping.\n+            audio (`str`, `list[str]`, `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+                The audio or batch of audio to be prepared. If provided as a string, it should correspond to the path or url of the audio file.\n+            model_id (`str`:\n+                The hub model id of the model to use for transcription.\n+            sampling_rate (`int`, *optional*):\n+                The sampling rate of the audio. Necessary if it is provided as `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`.\n+                Used to avoid silent errors when passing audio that is not in the expected sampling rate.\n+            format (`str`, `list[str]`, *optional*):\n+                The format of the audio, necessary if is provided as `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            VoxtralProcessorKwargs,\n+            **kwargs,\n+        )\n+        text_kwargs = output_kwargs[\"text_kwargs\"]\n+        audio_kwargs = output_kwargs[\"audio_kwargs\"]\n+        common_kwargs = output_kwargs[\"common_kwargs\"]\n+\n+        is_str = isinstance(audio, str)\n+        is_list_of_str = all(isinstance(el, str) for el in audio)\n+        is_list_of_audio = not (is_str or is_list_of_str)\n+\n+        if is_list_of_audio:\n+            if sampling_rate is None:\n+                logger.warning_once(\n+                    f\"You've provided audio without specifying the sampling rate. It will be assumed to be {audio_kwargs['sampling_rate']}, which can result in silent errors.\"\n+                )\n+            elif sampling_rate != audio_kwargs[\"sampling_rate\"]:\n+                raise ValueError(\n+                    f\"The sampling rate of the audio ({sampling_rate}) does not match the sampling rate of the processor ({audio_kwargs['sampling_rate']}). Please provide resampled the audio to the expected sampling rate.\"\n+                )\n+\n+        sampling_rate = audio_kwargs[\"sampling_rate\"]\n+        return_dict = common_kwargs.pop(\"return_dict\", False)\n+        tokenize = common_kwargs.pop(\"tokenize\", False)\n+\n+        # make sure to remove from text_kwargs and audio_kwargs\n+        for k in (\"return_dict\", \"tokenize\"):\n+            text_kwargs.pop(k, None)\n+            audio_kwargs.pop(k, None)\n+\n+        return_tensors = common_kwargs.pop(\"return_tensors\", None)\n+        if return_tensors != \"pt\":\n+            raise ValueError(f\"{self.__class__.__name__} only supports `return_tensors='pt'`.\")\n+\n+        # validate audio input\n+        if is_str:\n+            audio = [load_audio_as(audio, return_format=\"buffer\", force_mono=True, sampling_rate=sampling_rate)]\n+        elif is_list_of_str:\n+            audio = [\n+                load_audio_as(el, return_format=\"buffer\", force_mono=True, sampling_rate=sampling_rate) for el in audio\n+            ]\n+        else:\n+            audio = make_list_of_audio(audio)\n+            if len(audio) != len(format):\n+                raise ValueError(\n+                    f\"When passed as a list of audio, the length ({len(audio)}) must match the number of format ({len(format)})\"\n+                )\n+            audio_buffers = []\n+            for array, f in zip(audio, format):\n+                # Create new BytesIO object and write audio data to it\n+                buffer = io.BytesIO()\n+                # Convert to mono if needed\n+                if array.ndim == 2:\n+                    array = array.mean(axis=1)\n+                # Write to buffer with default format and sampling rate\n+                sf.write(buffer, array, samplerate=audio_kwargs[\"sampling_rate\"], format=f)\n+                buffer.seek(0)\n+                audio_buffers.append(buffer)\n+            audio = audio_buffers\n+\n+        # validate language input\n+        n_audio = len(audio)\n+        if isinstance(language, str):\n+            language = [language] * n_audio\n+\n+        if len(language) != n_audio:\n+            raise ValueError(\n+                f\"When passed as a list of languages, the length ({len(language)}) must match the number of audio ({n_audio})\"\n+            )\n+\n+        input_ids = []\n+        texts = []\n+        audio_arrays = []\n+        for audio_el, language_el in zip(audio, language):\n+            openai_transcription_request = {\n+                \"model\": model_id,\n+                \"file\": audio_el,\n+                \"language\": language_el,\n+            }\n+\n+            transcription_request = TranscriptionRequest.from_openai(openai_transcription_request)\n+            tokenized_transcription_request = self.tokenizer.tokenizer.encode_transcription(transcription_request)\n+\n+            input_ids.append(tokenized_transcription_request.tokens)\n+            texts.append(tokenized_transcription_request.text)\n+            audio_arrays.extend([el.audio_array for el in tokenized_transcription_request.audios])\n+\n+        if tokenize:\n+            if return_dict:\n+                # text are already tokenized but we need to pad etc\n+                encoding = self.tokenizer(\n+                    input_ids,\n+                    add_special_tokens=False,\n+                    **text_kwargs,\n+                )\n+                data = dict(encoding)\n+\n+                # extract the input features\n+                max_source_positions = audio_kwargs.pop(\"max_source_positions\")\n+                data[\"input_features\"] = self._retreive_input_features(\n+                    audio_arrays, max_source_positions, **audio_kwargs\n+                )\n+\n+                return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+        return texts\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+\n+__all__ = [\"VoxtralProcessor\"]"
        },
        {
            "sha": "c93ed4161d51d4b40bde78a3065095294879177b",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 59,
            "deletions": 4,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -22,6 +22,7 @@\n \n import numpy as np\n \n+from transformers.audio_utils import load_audio_as\n from transformers.tokenization_utils_base import (\n     LARGE_INTEGER,\n     VERY_LARGE_INTEGER,\n@@ -41,11 +42,13 @@\n if is_mistral_common_available():\n     from mistral_common.protocol.instruct.request import ChatCompletionRequest\n     from mistral_common.protocol.instruct.validator import ValidationMode\n-    from mistral_common.tokens.tokenizers.base import SpecialTokenPolicy\n+    from mistral_common.tokens.tokenizers.base import SpecialTokenPolicy, TokenizerVersion\n+    from mistral_common.tokens.tokenizers.image import MultiModalVersion\n     from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n     from mistral_common.tokens.tokenizers.tekken import Tekkenizer\n     from mistral_common.tokens.tokenizers.utils import download_tokenizer_from_hf_hub\n \n+\n if is_torch_available():\n     import torch\n \n@@ -1473,12 +1476,24 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n                     else:\n                         raise ValueError(\"Image content must be specified.\")\n                     normalized_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_content}})\n+                elif content_type == \"audio\":\n+                    maybe_url: Optional[str] = content.get(\"url\")\n+                    maybe_path: Optional[str] = content.get(\"path\")\n+                    maybe_base64: Optional[str] = content.get(\"base64\")\n+                    if maybe_url or maybe_path:\n+                        audio_data = load_audio_as(maybe_url or maybe_path, return_format=\"dict\", force_mono=True)\n+                        normalized_content.append({\"type\": \"input_audio\", \"input_audio\": audio_data})\n+                        continue\n+                    if not maybe_base64:\n+                        raise ValueError(\"Audio content must be specified.\")\n+                    normalized_content.append({\"type\": \"audio_url\", \"audio_url\": {\"url\": maybe_base64}})\n                 else:\n                     normalized_content.append(content)\n             message[\"content\"] = normalized_content\n \n         outputs = []\n         images: list[np.ndarray] = []\n+        audios: list[np.ndarray] = []\n \n         for conversation in conversations:\n             messages: list[dict[str, Union[str, list[dict[str, Union[str, dict[str, Any]]]]]]] = []\n@@ -1498,6 +1513,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             else:\n                 outputs.append(tokenized_request.text)\n             images.extend(tokenized_request.images)\n+            audios.extend([el.audio_array for el in tokenized_request.audios])\n \n         if not is_batched:\n             outputs = outputs[0]\n@@ -1528,6 +1544,13 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n                     else:\n                         raise ValueError(f\"Unsupported return_tensors type: {return_tensors}\")\n                     out.data[\"pixel_values\"] = pixel_values\n+                if audios:\n+                    if return_tensors is not None:\n+                        raise NotImplementedError(\n+                            \"When passing audio content in apply_chat_template, `return_tensors` must be None since we cannot batch the audio inputs. The returned audio will be a list of numpy arrays.\"\n+                        )\n+                    # Transformers convention is audio for plural audio (audio does not take a \"s\")\n+                    out.data[\"audio\"] = audios\n                 return out\n             else:\n                 return out[\"input_ids\"]\n@@ -1735,12 +1758,12 @@ def from_pretrained(\n             raise ValueError(\"`init_inputs` are not supported by `MistralCommonTokenizer.from_pretrained`.\")\n \n         # Handle kwargs and AutoTokenizer case\n-        if kwargs and not kwargs.keys() == {\"_from_auto\"}:\n+        if kwargs and not set(kwargs.keys()).issubset({\"_from_auto\", \"trust_remote_code\"}):\n             raise ValueError(\n                 f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.from_pretrained`.\"\n             )\n \n-        if not os.path.isfile(pretrained_model_name_or_path):\n+        if not os.path.isdir(pretrained_model_name_or_path):\n             tokenizer_path = download_tokenizer_from_hf_hub(\n                 repo_id=pretrained_model_name_or_path,\n                 cache_dir=cache_dir,\n@@ -1750,7 +1773,37 @@ def from_pretrained(\n                 local_files_only=local_files_only,\n             )\n         else:\n-            tokenizer_path = pretrained_model_name_or_path\n+            valid_tokenizer_files = []\n+            tokenizer_file: str\n+\n+            instruct_versions = list(TokenizerVersion.__members__)\n+            mm_versions = list(MultiModalVersion.__members__) + [\"\"]  # allow no mm version\n+            sentencepiece_suffixes = [f\".model.{v}{m}\" for v in instruct_versions for m in mm_versions] + [\".model\"]\n+\n+            for path in os.listdir(pretrained_model_name_or_path):\n+                pathlib_repo_file = Path(path)\n+                file_name = pathlib_repo_file.name\n+                suffix = \"\".join(pathlib_repo_file.suffixes)\n+                if file_name == \"tekken.json\":\n+                    valid_tokenizer_files.append(file_name)\n+                elif suffix in sentencepiece_suffixes:\n+                    valid_tokenizer_files.append(file_name)\n+\n+            if len(valid_tokenizer_files) == 0:\n+                raise ValueError(f\"No tokenizer file found in directory: {pretrained_model_name_or_path}\")\n+            # If there are multiple tokenizer files, we use tekken.json if it exists, otherwise the versioned one.\n+            if len(valid_tokenizer_files) > 1:\n+                if \"tekken.json\" in valid_tokenizer_files:\n+                    tokenizer_file = \"tekken.json\"\n+                else:\n+                    tokenizer_file = sorted(valid_tokenizer_files)[-1]\n+                logger.warning(\n+                    f\"Multiple tokenizer files found in directory: {pretrained_model_name_or_path}. Using {tokenizer_file}.\"\n+                )\n+            else:\n+                tokenizer_file = valid_tokenizer_files[0]\n+\n+            tokenizer_path = os.path.join(pretrained_model_name_or_path, tokenizer_file)\n \n         return cls(\n             tokenizer_path=tokenizer_path,\n@@ -1802,6 +1855,8 @@ def save_pretrained(\n         Returns:\n             A tuple of `str`: The files saved.\n         \"\"\"\n+        # `save_jinja_files`` must be skipped to be able to save from a processor\n+        kwargs.pop(\"save_jinja_files\", None)\n         if kwargs:\n             raise ValueError(\n                 f\"Kwargs {list(kwargs.keys())} are not supported by `MistralCommonTokenizer.save_pretrained`.\""
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/voxtral/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Fmodels%2Fvoxtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Fmodels%2Fvoxtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2F__init__.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60"
        },
        {
            "sha": "96bf971d1b52ca03d31373053079f04f10d139c4",
            "filename": "tests/models/voxtral/test_modeling_voxtral.py",
            "status": "added",
            "additions": 472,
            "deletions": 0,
            "changes": 472,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvoxtral%2Ftest_modeling_voxtral.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60",
            "patch": "@@ -0,0 +1,472 @@\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Voxtral model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    VoxtralConfig,\n+    VoxtralForConditionalGeneration,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    require_torch_sdpa,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class VoxtralModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        ignore_index=-100,\n+        audio_token_id=0,\n+        seq_length=35,\n+        feat_seq_length=60,\n+        text_config={\n+            \"model_type\": \"llama\",\n+            \"intermediate_size\": 36,\n+            \"initializer_range\": 0.02,\n+            \"hidden_size\": 32,\n+            \"max_position_embeddings\": 52,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_key_value_heads\": 2,\n+            \"use_labels\": True,\n+            \"use_mrope\": False,\n+            \"vocab_size\": 99,\n+            \"head_dim\": 8,\n+        },\n+        is_training=True,\n+        audio_config={\n+            \"model_type\": \"voxtral_encoder\",\n+            \"hidden_size\": 16,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 16,\n+            \"num_hidden_layers\": 2,\n+            \"num_mel_bins\": 80,\n+            \"max_source_positions\": 30,\n+            \"initializer_range\": 0.02,\n+        },\n+    ):\n+        self.parent = parent\n+        self.ignore_index = ignore_index\n+        self.audio_token_id = audio_token_id\n+        self.text_config = text_config\n+        self.audio_config = audio_config\n+        self.seq_length = seq_length\n+        self.feat_seq_length = feat_seq_length\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+\n+        self.batch_size = 3\n+        self.encoder_seq_length = seq_length\n+\n+    def get_config(self):\n+        return VoxtralConfig(\n+            text_config=self.text_config,\n+            audio_config=self.audio_config,\n+            ignore_index=self.ignore_index,\n+            audio_token_id=self.audio_token_id,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        input_features_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.audio_config[\"num_mel_bins\"],\n+                self.feat_seq_length,\n+            ]\n+        )\n+        config = self.get_config()\n+        return config, input_features_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_features_values = config_and_inputs\n+        num_audio_tokens_per_batch_idx = 30\n+\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 1) + 1\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        attention_mask[:, :1] = 0\n+\n+        input_ids[:, 1 : 1 + num_audio_tokens_per_batch_idx] = config.audio_token_id\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"input_features\": input_features_values,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class VoxtralForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `VoxtralForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (VoxtralForConditionalGeneration,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"text-to-speech\": VoxtralForConditionalGeneration, \"audio-text-to-text\": VoxtralForConditionalGeneration}\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = VoxtralModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=VoxtralConfig, has_text_modality=False)\n+\n+    @unittest.skip(\n+        reason=\"This test does not apply to Voxtral since inputs_embeds corresponding to audio tokens are replaced when input features are provided.\"\n+    )\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        # overwrite because Voxtral is audio+text model (not vision+text)\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        if not self._is_composite:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                text_attn = \"sdpa\" if model.language_model._supports_sdpa else \"eager\"\n+                vision_attn = \"sdpa\" if model.audio_tower._supports_sdpa else \"eager\"\n+\n+                # `None` as it is the requested one which will be assigned to each sub-config\n+                # Sub-model will dispatch to SDPA if it can (checked below that `SDPA` layers are present)\n+                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model.language_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model.audio_tower.config._attn_implementation == vision_attn)\n+\n+                model_eager = model_class.from_pretrained(tmpdirname, attn_implementation=\"eager\")\n+                model_eager = model_eager.eval().to(torch_device)\n+                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.language_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.audio_tower.config._attn_implementation == \"eager\")\n+\n+                for name, submodule in model_eager.named_modules():\n+                    class_name = submodule.__class__.__name__\n+                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+\n+@require_torch\n+class VoxtralForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.checkpoint_name = \"mistralai/Voxtral-Mini-3B-2507\"\n+        self.dtype = torch.bfloat16\n+        self.processor = AutoProcessor.from_pretrained(self.checkpoint_name)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_mini_single_turn_audio_only(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+                    },\n+                ],\n+            }\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversation)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+        EXPECTED_OUTPUT = [\n+            'The audio is a humorous exchange between two individuals, likely friends or acquaintances, about tattoos. Here\\'s a breakdown:\\n\\n1. **Initial Reaction**: One person (let\\'s call him A) is surprised to see the other person (let\\'s call him B) has a tattoo. A asks if B has a tattoo, and B confirms.\\n\\n2. **Tattoo Interpretation**: A then asks what B\\'s tattoo says, and B responds with \"sweet.\" This exchange is repeated multiple times, with A asking what B\\'s tattoo says, and B always answering \"sweet.\"\\n\\n3. **Confusion**: A seems confused and asks what B\\'s tattoo says multiple times, each time getting the same response. This leads to a humorous back-and-forth.\\n\\n4. **Clarification**: Eventually, B clarifies that A\\'s tattoo says \"dude\" and A\\'s says \"sweet.\" This is the punchline of the joke, as A had been asking about B\\'s tattoo but not his own.\\n\\n5. **Final Exchange**: B then asks what A\\'s tattoo says, and A responds with \"sweet,\" leading to a final round of confusion.\\n\\nThe humor comes from the repetition of the word \"sweet\" and the confusion that arises from A\\'s lack of self-awareness about his own tattoo.'\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_mini_single_turn_text_and_audio(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+                ],\n+            }\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversation)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            \"What can you tell me about this audio?This audio is a farewell address by President Barack Obama, delivered in Chicago. In the speech, he reflects on his eight years in office, highlighting the resilience, hope, and unity of the American people. He expresses gratitude for the conversations he had with the public, which kept him honest and inspired. The president also emphasizes the importance of self-government and civic engagement, encouraging Americans to participate in their democracy actively. He concludes by expressing optimism about the country's future and his commitment to serving as a citizen.\"\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_mini_single_turn_text_and_multiple_audios(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/mary_had_lamb.mp3\",\n+                    },\n+                    {\n+                        \"type\": \"audio\",\n+                        \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"What sport and what nursery rhyme are referenced?\"},\n+                ],\n+            }\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversation)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            'What sport and what nursery rhyme are referenced?The audio references both a nursery rhyme and a baseball game. The nursery rhyme is \"Mary Had a Little Lamb,\" and the baseball game is a playoff game between the Baltimore Orioles and the Oakland Athletics.'\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_mini_single_turn_text_only(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversation = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"Hello, how are you doing today?\"},\n+                ],\n+            }\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversation)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            \"Hello, how are you doing today?Hello! I'm functioning as intended, thank you. How about you? How's your day going?\"\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_mini_single_turn_text_and_multiple_audios_batched(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+                        },\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+                        },\n+                        {\n+                            \"type\": \"text\",\n+                            \"text\": \"Who's speaking in the speach and what city's weather is being discussed?\",\n+                        },\n+                    ],\n+                }\n+            ],\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/winning_call.mp3\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What can you tell me about this audio?\"},\n+                    ],\n+                }\n+            ],\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversations)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            \"Who's speaking in the speach and what city's weather is being discussed?The speaker in the speech is Barack Obama, and the weather being discussed is in Barcelona.\",\n+            'What can you tell me about this audio?This audio is a commentary of a baseball game, specifically a home run hit by Edgar Martinez. Here are some key points:\\n\\n- **Game Context**: The game is likely a playoff or championship game, as the commentator mentions the American League Championship.\\n- **Play Description**: Edgar Martinez hits a home run, which is described as a \"line drive\" and a \"base hit.\"\\n- **Team Involvement**: The team is the Mariners, and the commentator is excited about their chances to win the championship.\\n- **Emotional Tone**: The commentator expresses disbelief and excitement, using phrases like \"I don\\'t believe it\" and \"my, oh my.\"\\n- **Player Involvement**: The commentator mentions Joy and Junior, likely referring to other players or commentators in the broadcast.',\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_mini_multi_turn_text_and_audio(self):\n+        \"\"\"\n+        reproducer: https://gist.github.com/eustlb/c5e0e0a12e84e3d575151ba63d17e4cf\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        conversations = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+                        },\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Describe briefly what you can hear.\"},\n+                    ],\n+                },\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": \"The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.\",\n+                },\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"audio\",\n+                            \"path\": \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/dude_where_is_my_car.wav\",\n+                        },\n+                        {\"type\": \"text\", \"text\": \"Ok, now compare this new audio with the previous one.\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+\n+        inputs = self.processor.apply_chat_template(conversations)\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            'Describe briefly what you can hear.The audio begins with the speaker delivering a farewell address in Chicago, reflecting on his eight years as president and expressing gratitude to the American people. The audio then transitions to a weather report, stating that it was 35 degrees in Barcelona the previous day, but the temperature would drop to minus 20 degrees the following day.Ok, now compare this new audio with the previous one.The new audio is a humorous conversation between two friends, one of whom has a tattoo. The speaker is excited to see the tattoo and asks what it says. The other friend repeatedly says \"sweet\" in response, leading to a playful exchange. The speaker then realizes the joke and says \"your tattoo says dude, your tattoo says sweet, got it?\" The previous audio was a farewell address by a president, reflecting on his time in office and expressing gratitude to the American people. The new audio is a casual, light-hearted conversation in contrast to the serious and reflective tone of the previous audio.'\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)\n+\n+    @slow\n+    def test_transcribe_mode_audio_input(self):\n+        \"\"\"\n+        To test transcribe mode of the model, WER evaluation has been run to compare with the declared model performances.\n+        see https://github.com/huggingface/transformers/pull/39429 PR's descrition.\n+        disclaimer: Perfect token matching cannot be achieved due to floating-point arithmetic differences between vLLM and Transformers implementations.\n+        \"\"\"\n+        model = VoxtralForConditionalGeneration.from_pretrained(\n+            self.checkpoint_name, torch_dtype=self.dtype, device_map=torch_device\n+        )\n+        inputs = self.processor.apply_transcrition_request(\n+            language=\"en\",\n+            audio=\"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/obama.mp3\",\n+            model_id=self.checkpoint_name,\n+        )\n+        inputs = inputs.to(torch_device, dtype=self.dtype)\n+        outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n+\n+        decoded_outputs = self.processor.batch_decode(outputs, skip_special_tokens=True)\n+\n+        EXPECTED_OUTPUT = [\n+            \"lang:enThis week, I traveled to Chicago to deliver my final farewell address to the nation, following in the tradition of presidents before me. It was an opportunity to say thank you. Whether we've seen eye-to-eye or rarely agreed at all, my conversations with you, the American people, in living rooms and schools, at farms and on factory floors, at diners and on distant military outposts, All these conversations are what have kept me honest, kept me inspired, and kept me going. Every day, I learned from you. You made me a better president, and you made me a better man. Over the course of these eight years, I've seen the goodness, the resilience, and the hope of the American people. I've seen neighbors looking out for each other as we rescued our economy from the worst crisis of our lifetimes. I've hugged cancer survivors who finally know the security of affordable health care. I've seen communities like Joplin rebuild from disaster, and cities like Boston show the world that no terrorist will ever break the American spirit. I've seen the hopeful faces of young graduates and our newest military officers. I've mourned with grieving families searching for answers, and I found grace in a Charleston church. I've seen our scientists help a paralyzed man regain his sense of touch, and our wounded warriors walk again. I've seen our doctors and volunteers rebuild after earthquakes and stop pandemics in their tracks. I've learned from students who are building robots and curing diseases and who will change the world in ways we can't even imagine. I've seen the youngest of children remind us of our obligations to care for our refugees, to work in peace, and above all, to look out for each other. That's what's possible when we come together in the slow, hard, sometimes frustrating, but always vital work of self-government. But we can't take our democracy for granted. All of us, regardless of party, should throw ourselves into the work of citizenship. Not just when there's an election. Not just when our own narrow interest is at stake. But over the full span of a lifetime. If you're tired of arguing with strangers on the Internet, try to talk with one in real life. If something needs fixing, lace up your shoes and do some organizing. If you're disappointed by your elected officials, then grab a clipboard, get some signatures, and run for office yourself. Our success depends on our\"\n+        ]\n+        self.assertEqual(decoded_outputs, EXPECTED_OUTPUT)"
        },
        {
            "sha": "150290cb4f6255c80fdd1c7cdaef2f74036737f9",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 72,
            "deletions": 2,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/967045082faaaaf3d653bfe665080fd746b2bb60/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=967045082faaaaf3d653bfe665080fd746b2bb60"
        }
    ],
    "stats": {
        "total": 2813,
        "additions": 2806,
        "deletions": 7
    }
}