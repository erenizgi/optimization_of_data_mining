{
    "author": "Cyrilvallez",
    "message": "Fix accelerate integration (#42264)\n\n* fix\n\n* add sorting\n\n* typo\n\n* fix\n\n* improve doc\n\n* doc\n\n* doc",
    "sha": "a74be0dcb9f2975cea06b6ad0a82f539e7c5763d",
    "files": [
        {
            "sha": "7d6634c70ece2f751718b35404c24cc6e58d91da",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 20,
            "deletions": 61,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/a74be0dcb9f2975cea06b6ad0a82f539e7c5763d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a74be0dcb9f2975cea06b6ad0a82f539e7c5763d/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=a74be0dcb9f2975cea06b6ad0a82f539e7c5763d",
            "patch": "@@ -159,61 +159,6 @@ def wrapper(*args, **kwargs):\n             setattr(torch, torch_function_name, old_torch_function)\n \n \n-def find_tied_parameters(model: \"nn.Module\", **kwargs):\n-    \"\"\"\n-    Find the tied parameters in a given model.\n-\n-    <Tip warning={true}>\n-\n-    The signature accepts keyword arguments, but they are for the recursive part of this function and you should ignore\n-    them.\n-\n-    </Tip>\n-\n-    Args:\n-        model (`torch.nn.Module`): The model to inspect.\n-\n-    Returns:\n-        list[list[str]]: A list of lists of parameter names being all tied together.\n-\n-    Example:\n-\n-    ```py\n-    >>> from collections import OrderedDict\n-    >>> import torch.nn as nn\n-\n-    >>> model = nn.Sequential(OrderedDict([(\"linear1\", nn.Linear(4, 4)), (\"linear2\", nn.Linear(4, 4))]))\n-    >>> model.linear2.weight = model.linear1.weight\n-    >>> find_tied_parameters(model)\n-    [['linear1.weight', 'linear2.weight']]\n-    ```\n-    \"\"\"\n-\n-    # get ALL model parameters and their names\n-    all_named_parameters = dict(model.named_parameters(remove_duplicate=False))\n-\n-    # get ONLY unique named parameters,\n-    # if parameter is tied and have multiple names, it will be included only once\n-    no_duplicate_named_parameters = dict(model.named_parameters(remove_duplicate=True))\n-\n-    # the difference of the two sets will give us the tied parameters\n-    tied_param_names = set(all_named_parameters.keys()) - set(no_duplicate_named_parameters.keys())\n-\n-    # 'tied_param_names' contains the names of parameters that are tied in the model, but we do not know\n-    # which names refer to the same parameter. To identify this, we need to group them together.\n-    tied_param_groups = {}\n-    for tied_param_name in tied_param_names:\n-        tied_param = all_named_parameters[tied_param_name]\n-        for param_name, param in no_duplicate_named_parameters.items():\n-            # compare if parameters are the same, if so, group their names together\n-            if param is tied_param:\n-                if param_name not in tied_param_groups:\n-                    tied_param_groups[param_name] = []\n-                tied_param_groups[param_name].append(tied_param_name)\n-\n-    return [sorted([weight] + list(set(tied))) for weight, tied in tied_param_groups.items()]\n-\n-\n def check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\") -> dict | str | None:\n     from ..modeling_utils import get_torch_context_manager_or_global_device\n \n@@ -271,11 +216,21 @@ def compute_module_sizes(\n     leaves_module_sizes = defaultdict(int)\n \n     if buffers_only:\n-        named_tensors = model.named_buffers(recurse=True)\n+        iterator = model.named_buffers()\n     else:\n-        named_tensors = model.state_dict().items()\n-\n-    for name, param in named_tensors:\n+        # We need parameters + buffers here, as state_dict does not count non-persistent buffers which are taking space\n+        def all_tensors():\n+            yield from model.named_parameters()\n+            yield from model.named_buffers()\n+\n+        iterator = all_tensors()\n+\n+    tied_keys = getattr(model, \"all_tied_weights_keys\", {}).keys()\n+    for name, param in iterator:\n+        # Do not count tied keys (the model is usually not tied yet here, so they will appear in the iterator)\n+        # If the model is already tied, then they simply do not appear in the iterator anyway (remove_duplicates=True by default)\n+        if name in tied_keys:\n+            continue\n         if hf_quantizer is not None:\n             dtype_size = hf_quantizer.param_element_size(model, name)\n         else:\n@@ -591,8 +546,12 @@ def _init_infer_auto_device_map(\n \n     if tied_parameters is None:\n         if len(model.all_tied_weights_keys) > 0:\n-            # create a list of list of tied params\n-            tied_parameters = [list(t) for t in model.all_tied_weights_keys.items()]\n+            # create a list of list of tied params based on unique tied groups\n+            groups = set(model.all_tied_weights_keys.values())\n+            tied_parameters = [\n+                sorted([k for k, v in model.all_tied_weights_keys.items() if v == target] + [target])\n+                for target in groups\n+            ]\n         else:\n             tied_parameters = [[]]\n "
        }
    ],
    "stats": {
        "total": 81,
        "additions": 20,
        "deletions": 61
    }
}