{
    "author": "qubvel",
    "message": "Add V-JEPA for video classification model (#38788)\n\n* adding model and conversion scripts\n\n* add imports to test vjepa conversion\n\n* fix imports and make conversion work\n\n* fix computation for short side\n\n* replace attention with library attention function\n\n* cleanup more attention classes\n\n* remove config overrides\n\n* add test cases, fix some of the failing ones\n\n* fix the model outputs\n\n* fix outputs of the model per review\n\n* fix too big model test case\n\n* fix styling __init__.py\n\n* fix initialization test\n\n* remove all asserts per review\n\n* update sorting unsorting logic as per feedback\n\n* remove is_video per review\n\n* remove another is_video segment\n\n* remove unwanted stuff\n\n* small fixes\n\n* add docstrings for the model\n\n* revert adding vjepa2 config here\n\n* update styling\n\n* add config docstrings (wip)\n\n* fix dpr issue\n\n* removed test failing issues\n\n* update styles\n\n* merge predictor configs into main config\n\n* remove processing code, add video processor\n\n* remove permute which is not necessary now\n\n* fix styles\n\n* updated vjepa2 to be in video_processing_auto\n\n* update comment for preprocessing\n\n* test integration test and fix the outputs\n\n* update test values, change test to look at repeated frames for a given image\n\n* add a simple video processing test\n\n* refactoring pixel_values_videos and upload ckpts to original\n\n* fix torch_fx test cases\n\n* remove unused config\n\n* add all config docstrings\n\n* add more integration tests\n\n* add basic doc\n\n* revert unwanted styling changes\n\n* working make fixup\n\n* Fix model_type in config\n\n* Add ForVideoClassification model\n\n* update attention implementation to fit new hf standards\n\n* fix the preprocessing logic, ensure it matches the original model\n\n* remove use_rope logic, cleanup\n\n* fix docstrings\n\n* Further cleanup, update doc\n\n* Fix model prefix\n\n* fix get_vision_features\n\n* VJEPA2Embeddings style refactor\n\n* nit, style comment\n\n* change modules default values\n\n* Only `str` activation in config\n\n* GradientCheckpointingLayer\n\n* fixup\n\n* fix conversion script\n\n* Remove return_dict\n\n* remove None return typehint\n\n* Refactor VJEPA2Layer, remove use_SiLU\n\n* Fix fx tests\n\n* dpr -> drop_path_rates\n\n* move *ModelOutput on top\n\n* format docs bit\n\n* update docs\n\n* update docs\n\n* update doc example\n\n* remove prune_heads from model\n\n* remove unused config params\n\n* refactor embed signature\n\n* Add vjepa to docs\n\n* Fix config docstring\n\n* attention head\n\n* update defaults\n\n* Update docs/source/en/model_doc/vjepa2.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Update docs/source/en/model_doc/vjepa2.md\n\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\n\n* Fix import\n\n* Min refactoring\n\n* Update HUB_SOURCE and HUB_REPO in conversion script\n\n* Add missing headers\n\n* VJEPA -> V-JEPA in docs\n\n* Add image to doc\n\n* fix style\n\n* fix init weights\n\n* change checkpoint name in modeling tests\n\n* Initial cls head setup\n\n* remove rop attention from head (not needed)\n\n* remove swigluffn - not needed\n\n* Add siglip layer\n\n* Replace with siglip layer\n\n* Rename Siglip - VJEPA2\n\n* remove unused modules\n\n* remove siglip mlp\n\n* nit\n\n* remove MLP\n\n* Refactor head cross attention\n\n* refactor VJEPA2HeadCrossAttentionLayer\n\n* nit renaming\n\n* fixup\n\n* remove commented code\n\n* Add cls head params to config\n\n* depth from config\n\n* move pooler + classifier  to the model\n\n* Update for cls model signature\n\n* move layers, rename a bit\n\n* fix docs\n\n* update weights init\n\n* remove typehint for init\n\n* add to auto-mapping\n\n* enable tests\n\n* Add conversion script\n\n* fixup\n\n* add to docs\n\n* fix docs\n\n* nit\n\n* refactor for mapping\n\n* clean\n\n* Add integration test\n\n* Fixing multi gpu test\n\n* update not-split-modules\n\n* update video cls test tolerance\n\n* Increase test_inference_image tolerance\n\n* Update no-split modules for multi gpu\n\n* Apply suggestions from code review\n\n* fixing multi-gpu\n\n* fix docstring\n\n* Add cls snippet to docs\n\n* Update checkpoint",
    "sha": "9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
    "files": [
        {
            "sha": "b16875339ede7711d2d9535ba09a627634d800ec",
            "filename": "docs/source/en/model_doc/vjepa2.md",
            "status": "modified",
            "additions": 43,
            "deletions": 1,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvjepa2.md?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -38,7 +38,7 @@ This model was contributed by [koustuvs](https://huggingface.co/koustuvs), [yoni\n \n ## Usage example\n \n-The snippet below shows how to load the V-JEPA 2 model using the `AutoModel` class.\n+The snippet below shows how to load the V-JEPA 2 model for feature extraction using the `AutoModel` class.\n \n ```py\n import torch\n@@ -68,6 +68,43 @@ encoder_outputs = outputs.last_hidden_state\n predictor_outputs = outputs.predictor_output.last_hidden_state\n ```\n \n+V-JEPA 2 can also be finetuned for video classification. In the following snippet, we show how use finetuned on Something-Something-V2 video classification model.\n+\n+```python\n+import torch\n+import numpy as np\n+\n+from torchcodec.decoders import VideoDecoder\n+from transformers import AutoVideoProcessor, AutoModelForVideoClassification\n+\n+device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+\n+# Load model and video preprocessor\n+hf_repo = \"facebook/vjepa2-vitl-fpc16-256-ssv2\"\n+\n+model = AutoModelForVideoClassification.from_pretrained(hf_repo).to(device)\n+processor = AutoVideoProcessor.from_pretrained(hf_repo)\n+\n+# To load a video, sample the number of frames according to the model.\n+video_url = \"https://huggingface.co/datasets/nateraw/kinetics-mini/resolve/main/val/bowling/-WH-lxmGJVY_000005_000015.mp4\"\n+vr = VideoDecoder(video_url)\n+frame_idx = np.arange(0, model.config.frames_per_clip, 8) # you can define more complex sampling strategy\n+video = vr.get_frames_at(indices=frame_idx).data  # frames x channels x height x width\n+\n+# Preprocess and run inference\n+inputs = processor(video, return_tensors=\"pt\").to(model.device)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n+logits = outputs.logits\n+\n+print(\"Top 5 predicted class names:\")\n+top5_indices = logits.topk(5).indices[0]\n+top5_probs = torch.softmax(logits, dim=-1).topk(5).values[0]\n+for idx, prob in zip(top5_indices, top5_probs):\n+    text_label = model.config.id2label[idx.item()]\n+    print(f\" - {text_label}: {prob:.2f}\")\n+```\n+\n ## VJEPA2Config\n \n [[autodoc]] VJEPA2Config\n@@ -77,6 +114,11 @@ predictor_outputs = outputs.predictor_output.last_hidden_state\n [[autodoc]] VJEPA2Model\n     - forward\n \n+## VJEPA2ForVideoClassification\n+\n+[[autodoc]] VJEPA2ForVideoClassification\n+    - forward\n+\n ## VJEPA2VideoProcessor\n \n [[autodoc]] VJEPA2VideoProcessor"
        },
        {
            "sha": "75c4cbf3451bbd4643b6b76f61c86af45a29ddbc",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -150,6 +150,7 @@ def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     \"ForQuestionAnswering\": ForQuestionAnsweringLoss,\n     \"ForSequenceClassification\": ForSequenceClassificationLoss,\n     \"ForImageClassification\": ForSequenceClassificationLoss,\n+    \"ForVideoClassification\": ForSequenceClassificationLoss,\n     \"ForTokenClassification\": ForTokenClassification,\n     \"ForSegmentation\": ForSegmentationLoss,\n     \"ForObjectDetection\": ForObjectDetectionLoss,"
        },
        {
            "sha": "1b776b66ce3a403523ded335abd0d69f7c4afc35",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -844,6 +844,7 @@\n         (\"timesformer\", \"TimesformerForVideoClassification\"),\n         (\"videomae\", \"VideoMAEForVideoClassification\"),\n         (\"vivit\", \"VivitForVideoClassification\"),\n+        (\"vjepa2\", \"VJEPA2ForVideoClassification\"),\n     ]\n )\n "
        },
        {
            "sha": "1fd19c4d0782695010e0e889aacd1d2795748262",
            "filename": "src/transformers/models/vjepa2/configuration_vjepa2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconfiguration_vjepa2.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -60,6 +60,10 @@ class VJEPA2Config(PretrainedConfig):\n             `\"relu\"`, `\"selu\"` and `\"gelu_new\"` are supported.\n         initializer_range (`float`, *optional*, defaults to 0.02):\n             The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout probability for attentions.\n+        num_pooler_layers (`int`, *optional*, defaults to 3):\n+            The number of self-attention layers in the pooler.\n         pred_hidden_size (`int`, *optional*, defaults to 384):\n             Dimensionality of the predictor layers\n         pred_num_attention_heads (`int`, *optional*, defaults to 12):\n@@ -107,6 +111,8 @@ def __init__(\n         attention_probs_dropout_prob=0.0,\n         hidden_act=\"gelu\",\n         initializer_range=0.02,\n+        attention_dropout=0.0,\n+        num_pooler_layers=3,\n         # predictor params\n         pred_hidden_size=384,\n         pred_num_attention_heads=12,\n@@ -134,6 +140,8 @@ def __init__(\n         self.hidden_act = hidden_act\n         self.initializer_range = initializer_range\n         self.image_size = crop_size\n+        self.attention_dropout = attention_dropout\n+        self.num_pooler_layers = num_pooler_layers\n         # predictor params\n         self.pred_hidden_size = pred_hidden_size\n         self.pred_num_attention_heads = pred_num_attention_heads"
        },
        {
            "sha": "4e3512f5f9fbea23de42be2323553656f107d270",
            "filename": "src/transformers/models/vjepa2/convert_vjepa2_classifier_to_hf.py",
            "status": "added",
            "additions": 220,
            "deletions": 0,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_classifier_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_classifier_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fconvert_vjepa2_classifier_to_hf.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -0,0 +1,220 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import json\n+import os\n+import re\n+\n+import numpy as np\n+import torch\n+from decord import VideoReader\n+from huggingface_hub import HfApi, hf_hub_download\n+\n+from transformers import VJEPA2ForVideoClassification, VJEPA2VideoProcessor\n+\n+\n+device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+\n+\n+def get_video():\n+    path = hf_hub_download(\n+        repo_id=\"nateraw/kinetics-mini\",\n+        filename=\"val/bowling/-WH-lxmGJVY_000005_000015.mp4\",\n+        repo_type=\"dataset\",\n+    )\n+    video_reader = VideoReader(path)\n+    return video_reader\n+\n+\n+CLASSIFIERS = {\n+    # Something-Something-v2 dataset\n+    \"vjepa2-vitl-fpc16-256-ssv2\": {\n+        \"base_model\": \"facebook/vjepa2-vitl-fpc64-256\",\n+        \"checkpoint\": \"https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitl-16x2x3.pt\",\n+        \"num_labels\": 174,\n+        \"frames_per_clip\": 16,\n+        \"dataset\": \"something-something-v2\",\n+        \"result\": (145, 0.30867, \"Stuffing [something] into [something]\"),\n+    },\n+    \"vjepa2-vitg-fpc64-384-ssv2\": {\n+        \"base_model\": \"facebook/vjepa2-vitg-fpc64-384\",\n+        \"checkpoint\": \"https://dl.fbaipublicfiles.com/vjepa2/evals/ssv2-vitg-384-64x2x3.pt\",\n+        \"frames_per_clip\": 64,\n+        \"num_labels\": 174,\n+        \"dataset\": \"something-something-v2\",\n+        \"result\": (112, 0.26408, \"Putting [something] onto [something]\"),\n+    },\n+    # Diving48 dataset\n+    \"vjepa2-vitl-fpc32-256-diving48\": {\n+        \"base_model\": \"facebook/vjepa2-vitl-fpc64-256\",\n+        \"checkpoint\": \"https://dl.fbaipublicfiles.com/vjepa2/evals/diving48-vitl-256.pt\",\n+        \"num_labels\": 48,\n+        \"frames_per_clip\": 32,\n+        \"dataset\": \"diving48\",\n+        \"result\": (35, 0.32875, \"['Inward', '35som', 'NoTwis', 'TUCK']\"),\n+    },\n+    \"vjepa2-vitg-fpc32-384-diving48\": {\n+        \"base_model\": \"facebook/vjepa2-vitg-fpc64-384\",\n+        \"checkpoint\": \"https://dl.fbaipublicfiles.com/vjepa2/evals/diving48-vitg-384-32x4x3.pt\",\n+        \"frames_per_clip\": 32,\n+        \"num_labels\": 48,\n+        \"dataset\": \"diving48\",\n+        \"result\": (22, 0.35351, \"['Forward', '25som', '2Twis', 'PIKE']\"),\n+    },\n+}\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"module.pooler.query_tokens\":                          r\"pooler.query_tokens\",\n+    r\"module.pooler.cross_attention_block.norm(\\d+).\":      r\"pooler.cross_attention_layer.layer_norm\\1.\",\n+    r\"module.pooler.cross_attention_block.xattn.(q|k|v).\":  r\"pooler.cross_attention_layer.cross_attn.\\1_proj.\",\n+    r\"module.pooler.cross_attention_block.mlp.fc(\\d+).\":    r\"pooler.cross_attention_layer.mlp.fc\\1.\",\n+    r\"module.pooler.blocks.(\\d+).norm(\\d+).\":               r\"pooler.self_attention_layers.\\1.layer_norm\\2.\",\n+    r\"module.pooler.blocks.(\\d+).attn.(q|k|v).\":            r\"pooler.self_attention_layers.\\1.self_attn.\\2_proj.\",\n+    r\"module.pooler.blocks.(\\d+).attn.proj.\":               r\"pooler.self_attention_layers.\\1.self_attn.out_proj.\",\n+    r\"module.pooler.blocks.(\\d+).mlp.fc(\\d+).\":             r\"pooler.self_attention_layers.\\1.mlp.fc\\2.\",\n+    r\"module.linear.\":                                      r\"classifier.\",\n+}\n+# fmt: on\n+\n+\n+def get_id2label_mapping(dataset_name: str) -> dict[int, str]:\n+    path = hf_hub_download(\n+        repo_id=\"huggingface/label-files\",\n+        filename=f\"{dataset_name}-id2label.json\",\n+        repo_type=\"dataset\",\n+    )\n+    with open(path, \"r\") as f:\n+        id2label = json.load(f)\n+    id2label = {int(k): v for k, v in id2label.items()}\n+    return id2label\n+\n+\n+def split_qkv(state_dict):\n+    state_dict = state_dict.copy()\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if \".qkv.\" in key:\n+            tensor = state_dict.pop(key)\n+            q, k, v = torch.chunk(tensor, 3, dim=0)\n+            state_dict[key.replace(\".qkv.\", \".q.\")] = q\n+            state_dict[key.replace(\".qkv.\", \".k.\")] = k\n+            state_dict[key.replace(\".qkv.\", \".v.\")] = v\n+        elif \".kv.\" in key:\n+            tensor = state_dict.pop(key)\n+            k, v = torch.chunk(tensor, 2, dim=0)\n+            state_dict[key.replace(\".kv.\", \".k.\")] = k\n+            state_dict[key.replace(\".kv.\", \".v.\")] = v\n+\n+    return state_dict\n+\n+\n+def convert_old_keys_to_new_keys(state_dict):\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    old_text = \"\\n\".join(state_dict)\n+    new_text = old_text\n+    for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+        if replacement is None:\n+            new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+            continue\n+        new_text = re.sub(pattern, replacement, new_text)\n+    output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+def main(args: argparse.Namespace):\n+    model_params = CLASSIFIERS[args.model_name]\n+    id2label = get_id2label_mapping(model_params[\"dataset\"])\n+\n+    if not len(id2label) == model_params[\"num_labels\"]:\n+        raise ValueError(\n+            f\"Number of labels in id2label mapping ({len(id2label)}) does not \"\n+            f\"match number of labels in model ({model_params['num_labels']})\"\n+        )\n+\n+    model = VJEPA2ForVideoClassification.from_pretrained(\n+        model_params[\"base_model\"],\n+        num_labels=model_params[\"num_labels\"],\n+        id2label=id2label,\n+        frames_per_clip=model_params[\"frames_per_clip\"],\n+    )\n+    processor = VJEPA2VideoProcessor.from_pretrained(model_params[\"base_model\"])\n+\n+    # load and convert classifier checkpoint\n+    checkpoint = torch.hub.load_state_dict_from_url(model_params[\"checkpoint\"])\n+    state_dict = checkpoint[\"classifiers\"][0]\n+\n+    state_dict_qkv_split = split_qkv(state_dict)\n+    key_mapping = convert_old_keys_to_new_keys(state_dict_qkv_split.keys())\n+    converted_state_dict2 = {key_mapping[k]: v for k, v in state_dict_qkv_split.items()}\n+\n+    result = model.load_state_dict(converted_state_dict2, strict=False)\n+    if result.unexpected_keys:\n+        raise ValueError(f\"Error loading state dict: {result.unexpected_keys}\")\n+\n+    if not args.skip_verification:\n+        # get inputs\n+        video_reader = get_video()\n+        frame_indexes = np.arange(0, 128, 128 / model_params[\"frames_per_clip\"])\n+        video = video_reader.get_batch(frame_indexes).asnumpy()\n+        inputs = processor(video, return_tensors=\"pt\").to(device)\n+\n+        # run model\n+        model.to(device).eval()\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # compare results\n+        probs = torch.softmax(outputs.logits, dim=-1)\n+        top_prob, top_idx = probs.topk(1)\n+        top_prob, top_idx = top_prob.item(), top_idx.item()\n+        label = id2label[top_idx]\n+        expected_id, expected_prob, expected_label = model_params[\"result\"]\n+\n+        if not top_idx == expected_id:\n+            raise ValueError(f\"Expected id {expected_id} but got {top_idx}\")\n+        if not label == expected_label:\n+            raise ValueError(f\"Expected label {expected_label} but got {label}\")\n+        if not np.isclose(top_prob, expected_prob, atol=1e-3):\n+            raise ValueError(f\"Expected prob {expected_prob} but got {top_prob}\")\n+        print(\"Verification passed\")\n+\n+    output_dir = os.path.join(args.base_dir, args.model_name)\n+    model.save_pretrained(output_dir)\n+    processor.save_pretrained(output_dir)\n+\n+    if args.push_to_hub:\n+        api = HfApi()\n+        repo_id = f\"{args.repo_org}/{args.model_name}\"\n+        if not api.repo_exists(repo_id):\n+            api.create_repo(repo_id, repo_type=\"model\")\n+        api.upload_folder(folder_path=output_dir, repo_id=repo_id, repo_type=\"model\")\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\"--model_name\", type=str, required=True)\n+    parser.add_argument(\"--base_dir\", type=str, default=\"converted_models/\")\n+    parser.add_argument(\"--repo_org\", type=str, default=\"qubvel-hf\")\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\")\n+    parser.add_argument(\"--skip_verification\", action=\"store_true\")\n+    args = parser.parse_args()\n+\n+    main(args)"
        },
        {
            "sha": "8e13434307cfcf763878774ace574b5af51a02cf",
            "filename": "src/transformers/models/vjepa2/modeling_vjepa2.py",
            "status": "modified",
            "additions": 400,
            "deletions": 48,
            "changes": 448,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvjepa2%2Fmodeling_vjepa2.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -12,14 +12,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+from dataclasses import dataclass\n from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutput, dataclass\n+from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, logging\n from .configuration_vjepa2 import VJEPA2Config\n@@ -536,17 +537,21 @@ def forward(\n         )\n \n \n-def apply_masks(x, masks) -> torch.Tensor:\n+def apply_masks(tensor: torch.Tensor, masks: List[torch.Tensor]) -> torch.Tensor:\n     \"\"\"\n-    :param x: tensor of shape [B (batch-size), N (num-patches), D (feature-dim)]\n-    :param masks: list of tensors of shape [B, K] containing indices of K patches in [N] to keep\n+    Args:\n+        tensor (`torch.Tensor`):\n+            Tensor of shape [batch_size, num_patches, feature_dim]\n+        masks (`List[torch.Tensor]`):\n+            List of tensors of shape [batch_size, num_patches] containing indices of patches to keep\n     \"\"\"\n-    all_x = []\n-    for m in masks:\n-        mask_keep = m.unsqueeze(-1).repeat(1, 1, x.size(-1))\n-        all_x += [torch.gather(x, dim=1, index=mask_keep)]\n+    all_masked_tensors = []\n+    for mask in masks:\n+        mask = mask.to(tensor.device)\n+        mask_keep = mask.unsqueeze(-1).repeat(1, 1, tensor.size(-1))\n+        all_masked_tensors += [torch.gather(tensor, dim=1, index=mask_keep)]\n \n-    return torch.cat(all_x, dim=0)\n+    return torch.cat(all_masked_tensors, dim=0)\n \n \n class VJEPA2PredictorEmbeddings(nn.Module):\n@@ -649,13 +654,18 @@ def __init__(self, config: VJEPA2Config):\n         self.proj = nn.Linear(config.pred_hidden_size, config.hidden_size, bias=True)\n \n     def sort_tokens(self, hidden_states, position_masks, argsort, head_mask=None):\n+        # gather position masks\n+        argsort = argsort.to(position_masks.device)\n         position_masks = torch.gather(position_masks, dim=1, index=argsort)\n-        hidden_states = torch.gather(\n-            hidden_states,\n-            dim=1,\n-            index=argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1)),\n-        )\n+\n+        # gather hidden states\n+        argsort = argsort.to(hidden_states.device)\n+        hidden_states_argsort = argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1))\n+        hidden_states = torch.gather(hidden_states, dim=1, index=hidden_states_argsort)\n+\n+        # gather head mask\n         if head_mask is not None and head_mask[0] is not None:\n+            argsort = argsort.to(head_mask.device)\n             head_mask = head_mask.permute(1, 0, 2, 3, 4)\n             argsort_4d = (\n                 argsort.unsqueeze(1)\n@@ -673,15 +683,14 @@ def sort_tokens(self, hidden_states, position_masks, argsort, head_mask=None):\n             )\n             head_mask = torch.gather(head_mask, dim=4, index=argsort_5d)\n             head_mask = head_mask.permute(1, 0, 2, 3, 4)\n+\n         return hidden_states, position_masks, head_mask\n \n     def unsort_tokens(self, hidden_states, argsort):\n+        argsort = argsort.to(hidden_states.device)\n         reverse_argsort = torch.argsort(argsort, dim=1)\n-        hidden_states = torch.gather(\n-            hidden_states,\n-            dim=1,\n-            index=reverse_argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1)),\n-        )\n+        reverse_argsort = reverse_argsort.unsqueeze(-1).expand(-1, -1, hidden_states.size(-1))\n+        hidden_states = torch.gather(hidden_states, dim=1, index=reverse_argsort)\n         return hidden_states\n \n     @can_return_tuple\n@@ -735,49 +744,304 @@ def forward(\n         )\n \n \n+class VJEPA2PoolerSelfAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+class VJEPA2PoolerCrossAttention(nn.Module):\n+    \"\"\"It's different from other cross-attention layers, doesn't have output projection layer (o_proj)\"\"\"\n+\n+    # in case of modular refactoring - o_proj can be replaces with nn.Identity()\n+\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        queries: torch.Tensor,\n+        keys: torch.Tensor,\n+        values: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, q_seq_length, embed_dim = queries.shape\n+        kv_seq_length = keys.shape[1]\n+\n+        queries = self.q_proj(queries)\n+        keys = self.k_proj(keys)\n+        values = self.v_proj(values)\n+\n+        queries = queries.view(batch_size, q_seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, kv_seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, kv_seq_length, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, q_seq_length, embed_dim).contiguous()\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+# Modified from SiglipEncoderLayer, but we have to propagate proper hidden_size to VJEPA2MLP\n+class VJEPA2PoolerSelfAttentionLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.self_attn = VJEPA2PoolerSelfAttention(config)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = VJEPA2MLP(config, hidden_size=config.hidden_size)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, ...]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class VJEPA2PoolerCrossAttentionLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.layer_norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.cross_attn = VJEPA2PoolerCrossAttention(config)\n+        self.layer_norm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = VJEPA2MLP(config, hidden_size=config.hidden_size)\n+\n+    def forward(\n+        self,\n+        queries: torch.Tensor,\n+        hidden_state: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, ...]:\n+        # Apply cross-attention\n+        residual = queries\n+        hidden_state = self.layer_norm1(hidden_state)\n+        hidden_state, *attn_weights = self.cross_attn(\n+            queries,\n+            hidden_state,\n+            hidden_state,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_state = residual + hidden_state\n+\n+        # Apply MLP\n+        residual = hidden_state\n+        hidden_state = self.layer_norm2(hidden_state)\n+        hidden_state = self.mlp(hidden_state)\n+        hidden_state = residual + hidden_state\n+\n+        outputs = (hidden_state,)\n+        if output_attentions:\n+            outputs += tuple(attn_weights)\n+\n+        return outputs\n+\n+\n+class VJEPA2AttentivePooler(nn.Module):\n+    \"\"\"Attentive Pooler\"\"\"\n+\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__()\n+        self.query_tokens = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n+        self.cross_attention_layer = VJEPA2PoolerCrossAttentionLayer(config)\n+        self.self_attention_layers = nn.ModuleList(\n+            [VJEPA2PoolerSelfAttentionLayer(config) for _ in range(config.num_pooler_layers)]\n+        )\n+\n+    def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n+        for layer in self.self_attention_layers:\n+            hidden_state = layer(hidden_state, attention_mask=None)[0]\n+        queries = self.query_tokens.repeat(hidden_state.shape[0], 1, 1)\n+        hidden_state = self.cross_attention_layer(queries, hidden_state)[0]\n+        return hidden_state.squeeze(1)\n+\n+\n @auto_docstring\n class VJEPA2PreTrainedModel(PreTrainedModel):\n     config_class = VJEPA2Config\n     base_model_prefix = \"vjepa2\"\n     main_input_name = \"pixel_values_videos\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"VJEPA2Layer\"]\n+    _no_split_modules = [\n+        \"VJEPA2Layer\",\n+        \"VJEPA2PoolerSelfAttentionLayer\",\n+        \"VJEPA2PoolerCrossAttentionLayer\",\n+        \"VJEPA2PredictorEmbeddings\",\n+    ]\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n \n-    def _init_weights(\n-        self,\n-        module: Union[\n-            nn.Linear,\n-            nn.Conv2d,\n-            nn.LayerNorm,\n-            VJEPA2Embeddings,\n-            VJEPA2PredictorEmbeddings,\n-        ],\n-    ):\n+    def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv3d)):\n-            # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n-            # `trunc_normal_cpu` not implemented in `half` issues\n-            module.weight.data = nn.init.trunc_normal_(\n-                module.weight.data.to(torch.float32),\n-                mean=0.0,\n-                std=self.config.initializer_range,\n-            ).to(module.weight.dtype)\n+\n+        init_std = self.config.initializer_range\n+\n+        # Upcast the input in `fp32` and cast it back to desired `dtype` to avoid\n+        # `trunc_normal_cpu` not implemented in `half` issues\n+        def trunc_normal_f32_(weight, std):\n+            data_float_32 = weight.data.to(torch.float32)\n+            data_init = nn.init.trunc_normal_(data_float_32, mean=0.0, std=std)\n+            weight.data = data_init.to(weight.dtype)\n+\n+        if isinstance(module, VJEPA2AttentivePooler):\n+            trunc_normal_f32_(module.query_tokens, std=init_std)\n+            for i, layer in enumerate(module.self_attention_layers, 1):\n+                std = init_std / (i**0.5)\n+                trunc_normal_f32_(layer.self_attn.out_proj.weight, std=std)\n+                trunc_normal_f32_(layer.mlp.fc2.weight, std=std)\n+            std = init_std / (len(module.self_attention_layers) + 1) ** 0.5\n+            trunc_normal_f32_(module.cross_attention_layer.mlp.fc2.weight, std=std)\n+        elif isinstance(module, VJEPA2PredictorEmbeddings):\n+            if module.zero_init_mask_tokens:\n+                module.mask_tokens.data.zero_()\n+            else:\n+                trunc_normal_f32_(module.mask_tokens, std=init_std)\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.Conv3d)):\n+            trunc_normal_f32_(module.weight, std=init_std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.LayerNorm):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n-        elif isinstance(module, VJEPA2PredictorEmbeddings):\n-            if not module.zero_init_mask_tokens:\n-                module.mask_token = nn.init.trunc_normal_(\n-                    module.mask_token.to(torch.float32),\n-                    mean=0.0,\n-                    std=self.config.initializer_range,\n-                ).to(module.mask_token.dtype)\n-            else:\n-                module.mask_tokens.data.zero_()\n \n \n def _convert_head_mask_to_5d(head_mask, num_hidden_layers):\n@@ -900,4 +1164,92 @@ def get_vision_features(self, pixel_values_videos) -> torch.Tensor:\n         return encoder_output.last_hidden_state\n \n \n-__all__ = [\"VJEPA2Model\", \"VJEPA2PreTrainedModel\"]\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    V-JEPA 2 Model transformer with a video classification head on top (a linear layer on top of the attentive pooler).\n+    \"\"\"\n+)\n+class VJEPA2ForVideoClassification(VJEPA2PreTrainedModel):\n+    def __init__(self, config: VJEPA2Config):\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+        self.vjepa2 = VJEPA2Model(config)\n+\n+        # Classifier head\n+        self.pooler = VJEPA2AttentivePooler(config)\n+        self.classifier = nn.Linear(config.hidden_size, config.num_labels, bias=True)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values_videos: torch.Tensor,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> Union[Tuple, ImageClassifierOutput]:\n+        r\"\"\"\n+        pixel_values_videos (`torch.Tensor` with shape `[batch size x num_frames x num_channels x height x width]`):\n+            The input video pixels which is processed by VJEPA2VideoProcessor.\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> import numpy as np\n+        >>> from transformers import AutoVideoProcessor, VJEPA2ForVideoClassification\n+\n+        >>> device = \"cuda\"\n+\n+        >>> video_processor = AutoVideoProcessor.from_pretrained(\"facebook/vjepa2-vitl-fpc16-256-ssv2\")\n+        >>> model = VJEPA2ForVideoClassification.from_pretrained(\"facebook/vjepa2-vitl-fpc16-256-ssv2\").to(device)\n+\n+        >>> video = np.ones((64, 256, 256, 3))  # 64 frames, 256x256 RGB\n+        >>> inputs = video_processor(video, return_tensors=\"pt\").to(device)\n+\n+        >>> # For inference\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+        >>> logits = outputs.logits\n+\n+        >>> predicted_label = logits.argmax(-1).item()\n+        >>> print(model.config.id2label[predicted_label])\n+\n+        >>> # For training\n+        >>> labels = torch.ones(1, dtype=torch.long, device=device)\n+        >>> loss = model(**inputs, labels=labels).loss\n+\n+        ```\"\"\"\n+\n+        outputs = self.vjepa2(\n+            pixel_values_videos=pixel_values_videos,\n+            skip_predictor=True,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = outputs.last_hidden_state\n+        pooler_output = self.pooler(last_hidden_state)\n+        logits = self.classifier(pooler_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(pooled_logits=logits, labels=labels, config=self.config)\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"VJEPA2Model\", \"VJEPA2PreTrainedModel\", \"VJEPA2ForVideoClassification\"]"
        },
        {
            "sha": "070159272653c8ce7028868e9d7e39bdb791ac2b",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -56,6 +56,7 @@\n     MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES,\n     MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES,\n     MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES,\n+    MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES,\n     MODEL_FOR_ZERO_SHOT_IMAGE_CLASSIFICATION_MAPPING_NAMES,\n     MODEL_MAPPING_NAMES,\n )\n@@ -194,6 +195,7 @@ def _generate_supported_model_class_names(\n     \"TrOCRDecoder\",\n     \"PeftModelForCausalLM\",\n     \"PeftModelForSeq2SeqLM\",\n+    \"VJEPA2ForVideoClassification\",\n     # TODO: add support for them as it should be quite easy to do so (small blocking issues).\n     # XLNetForQuestionAnswering,\n ]\n@@ -904,6 +906,7 @@ def _generate_dummy_input(\n                 *get_values(MODEL_FOR_NEXT_SENTENCE_PREDICTION_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_MULTIPLE_CHOICE_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES),\n+                *get_values(MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_BACKBONE_MAPPING_NAMES),\n                 *get_values(MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES),\n             ]:"
        },
        {
            "sha": "5a38962771ce12d85c6e4efa781107ffaa9e9ef3",
            "filename": "tests/models/vjepa2/test_modeling_vjepa2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 3,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvjepa2%2Ftest_modeling_vjepa2.py?ref=9bec2654ed5b4ac43e880dc7e3cb2c18aeae70a9",
            "patch": "@@ -40,7 +40,7 @@\n     import torch\n     from torch import nn\n \n-    from transformers import VJEPA2Model\n+    from transformers import VJEPA2ForVideoClassification, VJEPA2Model\n \n \n if is_vision_available():\n@@ -153,7 +153,7 @@ class VJEPA2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     test_torch_exportable = True\n \n-    all_model_classes = (VJEPA2Model,) if is_torch_available() else ()\n+    all_model_classes = (VJEPA2Model, VJEPA2ForVideoClassification) if is_torch_available() else ()\n \n     fx_compatible = True\n \n@@ -267,7 +267,7 @@ def test_inference_image(self):\n             [[-0.0061, -1.8365, 2.7343], [-2.5938, -2.7181, -0.1663], [-1.7993, -2.2430, -1.1388]],\n             device=torch_device,\n         )\n-        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=8e-2, atol=8e-2)\n \n     @slow\n     def test_inference_video(self):\n@@ -343,3 +343,22 @@ def test_predictor_partial_mask(self):\n         # verify the last hidden states\n         expected_shape = torch.Size((1, num_masks, 1024))\n         self.assertEqual(outputs.predictor_output.last_hidden_state.shape, expected_shape)\n+\n+    @slow\n+    def test_video_classification(self):\n+        checkpoint = \"facebook/vjepa2-vitl-fpc16-256-ssv2\"\n+\n+        model = VJEPA2ForVideoClassification.from_pretrained(checkpoint).to(torch_device)\n+        video_processor = AutoVideoProcessor.from_pretrained(checkpoint)\n+\n+        sample_video = np.ones((16, 3, 256, 256))\n+        inputs = video_processor(sample_video, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        self.assertEqual(outputs.logits.shape, (1, 174))\n+\n+        expected_logits = torch.tensor([0.8814, -0.1195, -0.6389], device=torch_device)\n+        resulted_logits = outputs.logits[0, 100:103]\n+        torch.testing.assert_close(resulted_logits, expected_logits, rtol=1e-2, atol=1e-2)"
        }
    ],
    "stats": {
        "total": 750,
        "additions": 698,
        "deletions": 52
    }
}