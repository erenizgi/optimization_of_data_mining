{
    "author": "shuminghu",
    "message": "PerceptionLM (#37878)\n\n* plm template\n\n* A working plm with fixed image features\n\n* hacked processor\n\n* First version that reproduced PLM output using PE from timm.\n\n* Simplify and fix tie_word_embeddings\n\n* Use PIL resize. Simplify converstion.\n\n* First version that works with video input.\n\n* simplifed image preprocessing (not batched)\n\n* Minor fixes after rebasing on main.\n\n* Video processor based on new API.\n\n* Revert to use _preprocess for image processor.\n\n* refactor with modular\n\n* fix tie_word_embedding\n\n* Testing with timm PE\n\n* check in missed converstion from modular to model.py\n\n* First working version of PLM with Eva PE. PLM-1B and 3B outputs are exactly the same as before. PLM-8B output has some differences.\n\n* address review comments\n\n* Fixed batching if video and image examples mixed.\n\n* Simplify PE configuration.\n\n* Enable AutoModel for PerceptionEncoder.\n\n* Update PE config style.\n\n* update all headers\n\n* Minor fixes.\n\n* Move lm_head to PerceptionLMForConditionalGeneration.\nFix vit_G model specification.\n\n* Fix for testing_modeling_perception_lm.py\n\n* Image processing refactoring to use more common parts.\n\n* Fix processor test.\n\n* update tests to use model from hub\n\n* More test fixes.\n\n* integration test GT update after rebasing; probably due to video preprocessing\n\n* update test media path to hub\n\n* Stop tracking local scripts\n\n* address some review comments\n\n* refactor image processing.\n\n* small fixes\n\n* update documentation and minor fixes\n\n* remove scripts\n\n* Minor fix for CI\n\n* Fix image processing\n\n* CI and doc fix\n\n* CI formatting fix\n\n* ruff fix\n\n* ruff formatting\n\n* ran utils/sort_auto_mappings.py\n\n* update docstring\n\n* more docstring udpates\n\n* add vision_input_type default fallback for image processing\n\n* more verbose variable naming\n\n* test update\n\n* Remove PE and PEConfig use AutoModel(TimmWrapper) instead\n\n* Minor cleanup.\n\n* Minor Fix: remove any ref to PE. Ruff format and check.\n\n* fix docstring\n\n* Fix modular/model consistency.Improvex docstringfor  .\n\n* Fix PerceptionLMForConditionalGenerationModelTest\n\n* ruff fix\n\n* fix for check_repo\n\n* minor formatting\n\n* dummy size arg to fix for processor test.\n\n* Update docstring for PerceptionLMConfig\n\n* Minor fixes from review feedback.\n\n* Revert some minor changes per reviewer feedback.\n\n* update base_model_prefix\n\n* address reviewer feedback\n\n* fix comment in modeling file\n\n* address reviewer feedback\n\n* ruff format\n\n* Pre-merge test update.\n\n* reapply modular and fix checkpoint name\n\n* processor test path\n\n* use modular a bit more\n\n* remove dead code\n\n* add token decorator\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
    "files": [
        {
            "sha": "823e10894d3fb477b17ade1d2b41961d7a2cc557",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -1039,6 +1039,8 @@\n         title: PaliGemma\n       - local: model_doc/perceiver\n         title: Perceiver\n+      - local: model_doc/perception_lm\n+        title: PerceptionLM\n       - local: model_doc/phi4_multimodal\n         title: Phi4 Multimodal\n       - local: model_doc/pix2struct"
        },
        {
            "sha": "3982d521b9499f6bbe14c3377998c97b42b9d845",
            "filename": "docs/source/en/model_doc/perception_lm.md",
            "status": "added",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,68 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# PerceptionLM\n+\n+## Overview\n+\n+The PerceptionLM model was proposed in [PerceptionLM: Open-Access Data and Models for Detailed Visual Understanding](https://ai.meta.com/research/publications/perceptionlm-open-access-data-and-models-for-detailed-visual-understanding/) by Jang Hyun Cho et al. It's a fully open, reproducible model for transparent research in image and video understanding. PLM consists of\n+a vision encoder with a small scale (<8B parameters) LLM decoder.\n+\n+The abstract from the paper is the following:\n+\n+*Vision-language models are integral to computer vision research, yet many high-performing models\n+remain closed-source, obscuring their data, design and training recipe. The research community\n+has responded by using distillation from black-box models to label training data, achieving strong\n+benchmark results, at the cost of measurable scientific progress. However, without knowing the details\n+of the teacher model and its data sources, scientific progress remains difficult to measure. In this\n+paper, we study building a Perception Language Model (PLM) in a fully open and reproducible\n+framework for transparent research in image and video understanding. We analyze standard training\n+pipelines without distillation from proprietary models and explore large-scale synthetic data to identify\n+critical data gaps, particularly in detailed video understanding. To bridge these gaps, we release 2.8M\n+human-labeled instances of fine-grained video question-answer pairs and spatio-temporally grounded\n+video captions. Additionally, we introduce PLM–VideoBench, a suite for evaluating challenging video\n+understanding tasks focusing on the ability to reason about “what”, “where”, “when”, and “how” of a\n+video. We make our work fully reproducible by providing data, training recipes, code & models.*\n+\n+\n+This model was contributed by [shumingh](https://huggingface.co/shumingh).\n+The original code can be found [here](https://github.com/facebookresearch/perception_models).\n+\n+\n+## PerceptionLMConfig\n+\n+[[autodoc]] PerceptionLMConfig\n+\n+## PerceptionLMProcessor\n+\n+[[autodoc]] PerceptionLMProcessor\n+\n+## PerceptionLMImageProcessorFast\n+\n+[[autodoc]] PerceptionLMImageProcessorFast\n+\n+## PerceptionLMVideoProcessor\n+\n+[[autodoc]] PerceptionLMVideoProcessor\n+\n+## PerceptionLMModel\n+\n+[[autodoc]] PerceptionLMModel\n+\n+## PerceptionLMForConditionalGeneration\n+\n+[[autodoc]] PerceptionLMForConditionalGeneration\n+    - forward"
        },
        {
            "sha": "1b2de793d4a7c12ac45ce243baa1cb6106a20f37",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -237,6 +237,7 @@\n     from .pegasus import *\n     from .pegasus_x import *\n     from .perceiver import *\n+    from .perception_lm import *\n     from .persimmon import *\n     from .phi import *\n     from .phi3 import *"
        },
        {
            "sha": "e758f2eab81b5b16626880499a9d57696c69673e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -272,6 +272,8 @@\n         (\"pegasus\", \"PegasusConfig\"),\n         (\"pegasus_x\", \"PegasusXConfig\"),\n         (\"perceiver\", \"PerceiverConfig\"),\n+        (\"perception_encoder\", \"TimmWrapperConfig\"),\n+        (\"perception_lm\", \"PerceptionLMConfig\"),\n         (\"persimmon\", \"PersimmonConfig\"),\n         (\"phi\", \"PhiConfig\"),\n         (\"phi3\", \"Phi3Config\"),\n@@ -673,6 +675,8 @@\n         (\"pegasus\", \"Pegasus\"),\n         (\"pegasus_x\", \"PEGASUS-X\"),\n         (\"perceiver\", \"Perceiver\"),\n+        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_lm\", \"PerceptionLM\"),\n         (\"persimmon\", \"Persimmon\"),\n         (\"phi\", \"Phi\"),\n         (\"phi3\", \"Phi3\"),\n@@ -880,6 +884,7 @@\n         (\"llama4_text\", \"llama4\"),\n         (\"blip_2_qformer\", \"blip_2\"),\n         (\"fastspeech2_conformer_with_hifigan\", \"fastspeech2_conformer\"),\n+        (\"perception_encoder\", \"perception_lm\"),\n     ]\n )\n "
        },
        {
            "sha": "c2ee5ffe593486636fa9f136ecca802368ae5696",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -134,6 +134,7 @@\n             (\"owlvit\", (\"OwlViTImageProcessor\", \"OwlViTImageProcessorFast\")),\n             (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n             (\"perceiver\", (\"PerceiverImageProcessor\", \"PerceiverImageProcessorFast\")),\n+            (\"perception_lm\", (\"PerceptionLMImageProcessorFast\",)),\n             (\"phi4_multimodal\", (\"Phi4MultimodalImageProcessorFast\",)),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n             (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n@@ -599,7 +600,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                     raise ValueError(\n                         \"This image processor cannot be instantiated. Please make sure you have `Pillow` installed.\"\n                     )\n-\n         raise ValueError(\n             f\"Unrecognized image processor in {pretrained_model_name_or_path}. Should have a \"\n             f\"`image_processor_type` key in its {IMAGE_PROCESSOR_NAME} of {CONFIG_NAME}, or one of the following \""
        },
        {
            "sha": "68830ccc3d727e9e69ed1a700e439ebf94f2dea6",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -260,6 +260,8 @@\n         (\"pegasus\", \"PegasusModel\"),\n         (\"pegasus_x\", \"PegasusXModel\"),\n         (\"perceiver\", \"PerceiverModel\"),\n+        (\"perception_encoder\", \"PerceptionEncoder\"),\n+        (\"perception_lm\", \"PerceptionLMModel\"),\n         (\"persimmon\", \"PersimmonModel\"),\n         (\"phi\", \"PhiModel\"),\n         (\"phi3\", \"Phi3Model\"),\n@@ -942,6 +944,7 @@\n         (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),\n         (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n+        (\"perception_lm\", \"PerceptionLMForConditionalGeneration\"),\n         (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n         (\"pixtral\", \"LlavaForConditionalGeneration\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLForConditionalGeneration\"),"
        },
        {
            "sha": "2ead4a18007dcf6dad01ff9543bb54010c6b97f1",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -101,6 +101,7 @@\n         (\"owlv2\", \"Owlv2Processor\"),\n         (\"owlvit\", \"OwlViTProcessor\"),\n         (\"paligemma\", \"PaliGemmaProcessor\"),\n+        (\"perception_lm\", \"PerceptionLMProcessor\"),\n         (\"phi4_multimodal\", \"Phi4MultimodalProcessor\"),\n         (\"pix2struct\", \"Pix2StructProcessor\"),\n         (\"pixtral\", \"PixtralProcessor\"),"
        },
        {
            "sha": "81c3ba93bcf470394a99309dbf2f9869cd68d887",
            "filename": "src/transformers/models/perception_lm/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2F__init__.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_perception_lm import *\n+    from .image_processing_perception_lm_fast import *\n+    from .modeling_perception_lm import *\n+    from .processing_perception_lm import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "12352967d7c7751e630185632ad385bdc6710902",
            "filename": "src/transformers/models/perception_lm/configuration_perception_lm.py",
            "status": "added",
            "additions": 88,
            "deletions": 0,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconfiguration_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,88 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PerceptionLM model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..timm_wrapper.configuration_timm_wrapper import TimmWrapperConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PerceptionLMConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`PerceptionLMForConditionalGeneration`]. It is used to instantiate an\n+    PerceptionLM model according to the specified arguments, defining the model architecture.\n+\n+    Example models:\n+    -  [facebook/Perception-LM-1B](https://huggingface.co/facebook/Perception-LM-1B).\n+    -  [facebook/Perception-LM-3B](https://huggingface.co/facebook/Perception-LM-3B).\n+    -  [facebook/Perception-LM-8B](https://huggingface.co/facebook/Perception-LM-8B).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vision_config (`Union[TimmWrapperConfig, dict]`, *optional*, defaults to `TimmWrapperConfig()`):\n+            The config object or dictionary of the vision backbone.\n+        text_config (`Union[PretrainedConfig, dict]`, *optional*, defaults to `LlamaConfig()`):\n+            The config object or dictionary of the text backbone.\n+        vision_use_cls_token (`bool`, *optional*, defaults to `True`):\n+            Whether CLS token is used in the vision backbone. If used, we remove CLS token embedding from vision output.\n+        projector_pooling_ratio (`int`, *optional*, defaults to 1):\n+            The pooling ratio used in the multimodal projector.\n+        image_token_id (`int`, *optional*, defaults to 128002):\n+            The image token index to encode the image prompt.\n+        video_token_id (`int`, *optional*, defaults to 128003):\n+            The video token index to encode the video prompt.\n+    \"\"\"\n+\n+    model_type = \"perception_lm\"\n+    sub_configs = {\"text_config\": AutoConfig, \"vision_config\": TimmWrapperConfig}\n+\n+    def __init__(\n+        self,\n+        vision_config=None,\n+        text_config=None,\n+        vision_use_cls_token=True,\n+        projector_pooling_ratio=1,\n+        image_token_id=128002,\n+        video_token_id=128003,\n+        **kwargs,\n+    ):\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        if isinstance(vision_config, dict):\n+            vision_config = TimmWrapperConfig(**vision_config)\n+        elif isinstance(vision_config, TimmWrapperConfig):\n+            vision_config = vision_config\n+        elif vision_config is None:\n+            vision_config = TimmWrapperConfig()\n+        self.vision_config = vision_config\n+        self.vision_use_cls_token = vision_use_cls_token\n+\n+        if isinstance(text_config, dict):\n+            text_config[\"model_type\"] = text_config[\"model_type\"] if \"model_type\" in text_config else \"llama\"\n+            text_config = CONFIG_MAPPING[text_config[\"model_type\"]](**text_config)\n+        elif text_config is None:\n+            text_config = CONFIG_MAPPING[\"llama\"]()\n+\n+        self.text_config = text_config\n+        self.projector_pooling_ratio = projector_pooling_ratio\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"PerceptionLMConfig\"]"
        },
        {
            "sha": "ee96c86876dd5bd94b026ccf75adb3fef03263f0",
            "filename": "src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py",
            "status": "added",
            "additions": 615,
            "deletions": 0,
            "changes": 615,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fconvert_perception_lm_weights_to_hf.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,615 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+import gc\n+import json\n+import os\n+import tempfile\n+import warnings\n+\n+import torch\n+from timm.models.eva import checkpoint_filter_fn\n+from tokenizers import AddedToken, processors\n+\n+from transformers import (\n+    GenerationConfig,\n+    LlamaConfig,\n+    LlamaTokenizer,\n+    PreTrainedTokenizerFast,\n+)\n+from transformers.convert_slow_tokenizer import TikTokenConverter\n+from transformers.models.auto.modeling_auto import AutoModel\n+from transformers.models.perception_lm.configuration_perception_lm import (\n+    PerceptionLMConfig,\n+)\n+from transformers.models.perception_lm.image_processing_perception_lm_fast import (\n+    PerceptionLMImageProcessorFast,\n+)\n+from transformers.models.perception_lm.modeling_perception_lm import (\n+    PerceptionLMForConditionalGeneration,\n+)\n+from transformers.models.perception_lm.processing_perception_lm import (\n+    PerceptionLMProcessor,\n+)\n+from transformers.models.perception_lm.video_processing_perception_lm import (\n+    PerceptionLMVideoProcessor,\n+)\n+from transformers.models.timm_wrapper.configuration_timm_wrapper import TimmWrapperConfig\n+\n+\n+try:\n+    from transformers import LlamaTokenizerFast\n+except ImportError as e:\n+    warnings.warn(e)\n+    warnings.warn(\n+        \"The converted tokenizer will be the `slow` tokenizer. To use the fast, update your `tokenizers` library and re-run the tokenizer conversion\"\n+    )\n+    LlamaTokenizerFast = None\n+\n+\"\"\"\n+Sample usage:\n+\n+```\n+python src/transformers/models/perception_lm/convert_perception_lm_weights_to_hf.py \\\n+    --input_dir /path/to/downloaded/perception_lm/model_path  --output_dir /output/path\n+```\n+\n+Thereafter, models can be loaded via:\n+\n+```py\n+from transformers import LlamaForCausalLM, LlamaTokenizer\n+\n+model = LlamaForCausalLM.from_pretrained(\"/output/path\")\n+tokenizer = LlamaTokenizer.from_pretrained(\"/output/path\")\n+```\n+\n+Important note: you need to be able to host the whole model in RAM to execute this script (even if the biggest versions\n+come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).\n+\n+If you want your tokenizer to add a bos automatically you should update the tokenizer._tokenizers.post_processor:\n+\n+```py\n+from tokenizers import processors\n+bos = \"<|begin_of_text|>\"\n+tokenizer._tokenizers.post_processor = processors.Sequence(\n+    [\n+        processors.ByteLevel(trim_offsets=False),\n+        processors.TemplateProcessing(\n+            single=f\"{bos}:0 $A:0\",\n+            pair=f\"{bos}:0 $A:0 {bos}:1 $B:1\",\n+            special_tokens=[\n+                (bos, tokenizer.encode(bos)),\n+            ],\n+        ),\n+    ]\n+)\n+```\n+\n+\"\"\"\n+\n+BOS_ADDED_TOKEN = AddedToken(\n+    \"<|begin_of_text|>\",\n+    single_word=False,\n+    lstrip=False,\n+    rstrip=False,\n+    normalized=False,\n+    special=True,\n+)\n+EOS_ADDED_TOKEN = AddedToken(\n+    \"<|end_of_text|>\",\n+    single_word=False,\n+    lstrip=False,\n+    rstrip=False,\n+    normalized=False,\n+    special=True,\n+)\n+EOT_ADDED_TOKEN = AddedToken(\n+    \"<|eot_id|>\",\n+    single_word=False,\n+    lstrip=False,\n+    rstrip=False,\n+    normalized=False,\n+    special=True,\n+)\n+\n+DEFAULT_SPECIAL_TOKENS = {\n+    \"perception_lm\": [\n+        \"<|begin_of_text|>\",\n+        \"<|end_of_text|>\",\n+        \"<|image|>\",\n+        \"<|video|>\",\n+        \"<|reserved_special_token_2|>\",\n+        \"<|reserved_special_token_3|>\",\n+        \"<|start_header_id|>\",\n+        \"<|end_header_id|>\",\n+        \"<|reserved_special_token_4|>\",\n+        \"<|eot_id|>\",  # End of turn\n+    ]\n+    + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)]\n+}\n+\n+CHAT_TEMPLATE = (\n+    \"{{- bos_token }}\"\n+    \"{%- if messages[0]['role'] == 'system' -%}\"\n+    \"    {%- set system_message = messages[0]['content']|trim %}\\n\"\n+    \"    {%- set messages = messages[1:] %}\\n\"\n+    \"{%- else %}\"\n+    \"    {%- set system_message = 'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.' %}\"\n+    \"{%- endif %}\"\n+    \"{{- '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{{- system_message }}\"\n+    \"{{- '<|eot_id|>' }}\"\n+    \"{%- for message in messages %}\"\n+    \"{{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'image') %}\"\n+    \"{{ '<|image|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'video') %}\"\n+    \"{{ '<|video|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'text') %}\"\n+    \"{{- content['text'] | trim }}\"\n+    \"{%- endfor %}\"\n+    \"{{'<|eot_id|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- if add_generation_prompt %}\"\n+    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{%- endif %}\"\n+)\n+\n+\n+def compute_intermediate_size(n, ffn_dim_multiplier=1, multiple_of=256):\n+    return multiple_of * ((int(ffn_dim_multiplier * int(8 * n / 3)) + multiple_of - 1) // multiple_of)\n+\n+\n+def read_json(path):\n+    with open(path, \"r\") as f:\n+        return json.load(f)\n+\n+\n+def write_json(text, path):\n+    with open(path, \"w\") as f:\n+        json.dump(text, f)\n+\n+\n+def write_weights(state_dict, index_dict, param_count, filename):\n+    for k, v in state_dict.items():\n+        index_dict[\"weight_map\"][k] = filename\n+        param_count += v.numel()\n+    torch.save(state_dict, filename)\n+    print(f\"Saved {filename}\")\n+    return param_count\n+\n+\n+def write_model(\n+    model_path,\n+    input_base_path,\n+    params,\n+    image_token_id,\n+    safe_serialization=True,\n+    tokenizer=None,\n+    num_shards=None,\n+    push_to_hub=False,\n+):\n+    print(\"Converting the model.\")\n+    num_shards = 1\n+    model_params = params.get(\"model\", params)\n+    n_layers = model_params[\"n_layers\"]\n+    n_heads = model_params[\"n_heads\"]\n+    dim = model_params[\"dim\"]\n+    dims_per_head = dim // n_heads\n+    base = model_params.get(\"rope_theta\", 10000.0)\n+    inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n+    context_length = model_params[\"max_seqlen\"]\n+    max_position_embeddings = context_length\n+    tie_word_embeddings = model_params.get(\"weight_tying\", False)\n+    projector_pooling_ratio = model_params.get(\"pooling_ratio\", 1)\n+\n+    if model_params.get(\"n_kv_heads\", None) is not None:\n+        num_key_value_heads = model_params[\"n_kv_heads\"]  # for GQA / MQA\n+        key_value_dim = dims_per_head * num_key_value_heads\n+    else:  # compatibility with other checkpoints\n+        num_key_value_heads = n_heads\n+        key_value_dim = dim\n+\n+    # permute for sliced rotary\n+    def permute(w, n_heads, dim1=dim, dim2=dim):\n+        return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n+\n+    with tempfile.TemporaryDirectory() as tmp_model_path:\n+        print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n+        # Load weights\n+        if num_shards == 1:\n+            # Not sharded\n+            # (The sharded implementation would also work, but this is simpler.)\n+            loaded = torch.load(\n+                os.path.join(input_base_path, \"consolidated.pth\"),\n+                map_location=\"cpu\",\n+                weights_only=True,\n+            )\n+        else:\n+            # Sharded\n+            checkpoint_list = sorted([file for file in os.listdir(input_base_path) if file.endswith(\".pth\")])\n+            print(\"Loading in order:\", checkpoint_list)\n+            loaded = [\n+                torch.load(\n+                    os.path.join(input_base_path, file),\n+                    map_location=\"cpu\",\n+                    weights_only=True,\n+                )\n+                for file in checkpoint_list\n+            ]\n+        param_count = 0\n+        index_dict = {\"weight_map\": {}}\n+        for layer_i in range(n_layers):\n+            filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 2}.bin\"\n+            assert num_shards == 1, \"PerceptionLM does not support sharded weights\"\n+            state_dict = {\n+                f\"model.language_model.layers.{layer_i}.self_attn.q_proj.weight\": permute(\n+                    loaded[f\"layers.{layer_i}.attention.wq.weight\"], n_heads=n_heads\n+                ),\n+                f\"model.language_model.layers.{layer_i}.self_attn.k_proj.weight\": permute(\n+                    loaded[f\"layers.{layer_i}.attention.wk.weight\"],\n+                    n_heads=num_key_value_heads,\n+                    dim1=key_value_dim,\n+                ),\n+                f\"model.language_model.layers.{layer_i}.self_attn.v_proj.weight\": loaded[\n+                    f\"layers.{layer_i}.attention.wv.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[\n+                    f\"layers.{layer_i}.attention.wo.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.mlp.gate_proj.weight\": loaded[\n+                    f\"layers.{layer_i}.feed_forward.w1.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.mlp.down_proj.weight\": loaded[\n+                    f\"layers.{layer_i}.feed_forward.w2.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.mlp.up_proj.weight\": loaded[\n+                    f\"layers.{layer_i}.feed_forward.w3.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.input_layernorm.weight\": loaded[\n+                    f\"layers.{layer_i}.attention_norm.weight\"\n+                ],\n+                f\"model.language_model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[\n+                    f\"layers.{layer_i}.ffn_norm.weight\"\n+                ],\n+            }\n+            state_dict[f\"model.language_model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\"] = inv_freq\n+            for k, v in state_dict.items():\n+                index_dict[\"weight_map\"][k] = filename\n+                param_count += v.numel()\n+            torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+            print(f\"Saved {filename}\")\n+\n+        filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 2}.bin\"\n+\n+        state_dict = {\n+            \"model.language_model.embed_tokens.weight\": loaded[\"tok_embeddings.weight\"],\n+            \"model.language_model.norm.weight\": loaded[\"norm.weight\"],\n+            \"model.multi_modal_projector.linear_1.weight\": loaded[\"vision_projector.projector.0.weight\"],\n+            \"model.multi_modal_projector.linear_2.weight\": loaded[\"vision_projector.projector.2.weight\"],\n+            \"model.multi_modal_projector.linear_1.bias\": loaded[\"vision_projector.projector.0.bias\"],\n+            \"model.multi_modal_projector.linear_2.bias\": loaded[\"vision_projector.projector.2.bias\"],\n+        }\n+        if not tie_word_embeddings:\n+            state_dict[\"lm_head.weight\"] = loaded[\"output.weight\"]\n+        for k, v in state_dict.items():\n+            index_dict[\"weight_map\"][k] = filename\n+            param_count += v.numel()\n+        torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+        print(f\"Saved {filename}\")\n+\n+        filename = f\"pytorch_model-{n_layers + 2}-of-{n_layers + 2}.bin\"\n+        state_dict = {k.replace(\"vision_model.\", \"\"): v for k, v in loaded.items() if \"vision_model\" in k}\n+        vision_params = model_params[\"vision_model\"]\n+        if vision_params[\"layers\"] == 23 and vision_params[\"width\"] == 1024:\n+            architecture = \"vit_pe_core_large_patch14_336\"\n+        elif vision_params[\"layers\"] == 47 and vision_params[\"width\"] == 1536:\n+            architecture = \"vit_pe_core_gigantic_patch14_448\"\n+        else:\n+            raise ValueError(\n+                f\"Unsupported PE config: {vision_params['layers']} layers and {vision_params['width']} width\"\n+            )\n+\n+        vision_config = TimmWrapperConfig.from_pretrained(\n+            f\"timm/{architecture}.fb\",\n+            model_args={\n+                \"embed_dim\": vision_params[\"width\"],\n+                \"depth\": vision_params[\"layers\"],\n+                \"img_size\": (vision_params[\"image_size\"], vision_params[\"image_size\"]),\n+                \"global_pool\": \"\",\n+                \"use_post_transformer_norm\": vision_params[\"use_ln_post\"],\n+                \"init_values\": vision_params[\"ls_init_value\"],\n+                \"ref_feat_shape\": (\n+                    vision_params[\"image_size\"] // vision_params[\"patch_size\"],\n+                    vision_params[\"image_size\"] // vision_params[\"patch_size\"],\n+                ),\n+            },\n+        )\n+\n+        perception_encoder = AutoModel.from_config(vision_config)\n+        state_dict = checkpoint_filter_fn(state_dict, perception_encoder)\n+        state_dict = {\"model.vision_tower.timm_model.\" + k: v for k, v in state_dict.items()}\n+        for k, v in state_dict.items():\n+            index_dict[\"weight_map\"][k] = filename\n+            param_count += v.numel()\n+        torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+        print(f\"Saved {filename}\")\n+\n+        # Write configs\n+        index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n+        write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n+        ffn_dim_multiplier = model_params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in model_params else 1\n+        multiple_of = model_params[\"multiple_of\"] if \"multiple_of\" in model_params else 256\n+\n+        bos_token_id = tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")\n+        eos_token_id = [tokenizer.convert_tokens_to_ids(t) for t in [\"<|end_of_text|>\", \"<|eot_id|>\"]]\n+\n+        use_scaled_rope = model_params[\"use_scaled_rope\"]\n+        if use_scaled_rope:\n+            rope_scaling = {\n+                \"factor\": model_params[\"rope_scale_factor\"] * 1.0,\n+                \"low_freq_factor\": model_params.get(\"low_freq_factor\", 1.0) * 1.0,\n+                \"high_freq_factor\": model_params.get(\"high_freq_factor\", 4.0) * 1.0,\n+                \"original_max_position_embeddings\": 8192,\n+                \"rope_type\": \"llama3\",\n+            }\n+        else:\n+            rope_scaling = None\n+\n+        text_config = LlamaConfig(\n+            hidden_size=dim,\n+            intermediate_size=compute_intermediate_size(dim, ffn_dim_multiplier, multiple_of),\n+            num_attention_heads=model_params[\"n_heads\"],\n+            num_hidden_layers=model_params[\"n_layers\"],\n+            rms_norm_eps=model_params[\"norm_eps\"],\n+            num_key_value_heads=num_key_value_heads,\n+            vocab_size=len(tokenizer),\n+            rope_theta=base,\n+            rope_scaling=rope_scaling,\n+            max_position_embeddings=max_position_embeddings,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+        )\n+\n+        config = PerceptionLMConfig(\n+            text_config=text_config.to_dict(),\n+            vision_config=vision_config.to_dict(),\n+            projector_pooling_ratio=projector_pooling_ratio,\n+            vision_use_cls_token=vision_params[\"use_cls_token\"],\n+            image_token_id=tokenizer.image_token_id,\n+            video_token_id=tokenizer.video_token_id,\n+        )\n+\n+        config.save_pretrained(tmp_model_path)\n+\n+        generation_config = GenerationConfig(\n+            do_sample=False,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+        )\n+        generation_config.save_pretrained(tmp_model_path)\n+\n+        # Make space so we can load the model properly now.\n+        del state_dict\n+        # output_weight = loaded.get(\"output.weight\", None)\n+        del loaded\n+        gc.collect()\n+\n+        print(\"Loading the checkpoint in a PerceptionLM model.\")\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(\n+            tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True\n+        )\n+        # if not tie_word_embeddings:\n+        #     if output_weight is None:\n+        #         raise ValueError(\"Output weight/lm_head is not found in the checkpoint.\")\n+        #     model.lm_head.load_state_dict({\"weight\": output_weight})\n+\n+        # Avoid saving this as part of the config.\n+        del model.config._name_or_path\n+        model.config.torch_dtype = torch.bfloat16\n+\n+        print(\"Saving in the Transformers format.\")\n+        if push_to_hub:\n+            print(\"Pushing to the hub.\")\n+            model.push_to_hub(\n+                model_path,\n+                safe_serialization=safe_serialization,\n+                private=True,\n+                use_temp_dir=True,\n+            )\n+        else:\n+            print(\"Saving to disk.\")\n+            model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+\n+\n+class Llama3Converter(TikTokenConverter):\n+    def __init__(\n+        self,\n+        vocab_file,\n+        special_tokens=None,\n+        context_length=11520,\n+        **kwargs,\n+    ):\n+        super().__init__(vocab_file, additional_special_tokens=special_tokens, **kwargs)\n+        tokenizer = self.converted()\n+\n+        self.converted_tokenizer = PreTrainedTokenizerFast(\n+            tokenizer_object=tokenizer,\n+            bos_token=\"<|begin_of_text|>\",\n+            eos_token=\"<|eot_id|>\",\n+            model_input_names=[\"input_ids\", \"attention_mask\"],\n+            model_max_length=context_length,\n+            clean_up_tokenization_spaces=True,\n+            extra_special_tokens={\n+                \"image_token\": \"<|image|>\",\n+                \"video_token\": \"<|video|>\",\n+                \"pad_token\": \"<|end_of_text|>\",\n+            },\n+        )\n+        self.converted_tokenizer.image_token_id = self.converted_tokenizer.encode(\n+            self.converted_tokenizer.image_token, add_special_tokens=False\n+        )[0]\n+        self.converted_tokenizer.video_token_id = self.converted_tokenizer.encode(\n+            self.converted_tokenizer.video_token, add_special_tokens=False\n+        )[0]\n+        self.update_post_processor(self.converted_tokenizer)\n+        # finer special_tokens_map.json\n+        self.converted_tokenizer._bos_token = BOS_ADDED_TOKEN\n+        self.converted_tokenizer._eos_token = EOT_ADDED_TOKEN\n+\n+    # We can't do this while building the tokenizer because we have no easy access to the bos token id\n+    def update_post_processor(self, tokenizer):\n+        tokenizer._tokenizer.post_processor = processors.Sequence(\n+            [\n+                processors.ByteLevel(trim_offsets=False),\n+                processors.TemplateProcessing(\n+                    single=\"<|begin_of_text|> $A\",\n+                    pair=\"<|begin_of_text|>:0 $A:0 <|begin_of_text|>:1 $B:1\",\n+                    special_tokens=[\n+                        (\n+                            \"<|begin_of_text|>\",\n+                            tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\"),\n+                        ),\n+                    ],\n+                ),\n+            ]\n+        )\n+\n+\n+def write_tokenizer(\n+    tokenizer_path,\n+    input_tokenizer_path,\n+    special_tokens=None,\n+    params=None,\n+    push_to_hub=False,\n+):\n+    print(\"Converting the tokenizer.\")\n+    tokenizer_class = LlamaTokenizer if LlamaTokenizerFast is None else LlamaTokenizerFast\n+    context_length = params[\"model\"][\"max_seqlen\"]\n+    tokenizer = Llama3Converter(\n+        input_tokenizer_path,\n+        special_tokens,\n+        context_length,\n+    ).converted_tokenizer\n+\n+    tokenizer.image_token_id = tokenizer.encode(tokenizer.image_token, add_special_tokens=False)[0]\n+    processor_config = {\n+        \"pooling_ratio\": params[\"model\"][\"pooling_ratio\"],\n+        \"patch_size\": params[\"model\"][\"vision_model\"][\"patch_size\"],\n+        \"processor_class\": \"PerceptionLMProcessor\",\n+    }\n+    tile_size = params[\"model\"][\"vision_model\"][\"image_size\"]\n+\n+    image_preprocessor_config = {\n+        \"image_processor_type\": \"PerceptionLMImageProcessorFast\",\n+        \"vision_input_type\": params[\"data\"][\"vision_input_type\"],\n+        \"tile_size\": tile_size,\n+        \"max_num_tiles\": params[\"data\"][\"max_num_tiles\"],\n+        \"max_frame_tiles\": 1,\n+        \"size\": {\"height\": tile_size, \"width\": tile_size},\n+        \"do_resize\": True,\n+        \"do_rescale\": True,\n+        \"do_normalize\": True,\n+        \"image_mean\": [0.5, 0.5, 0.5],\n+        \"image_std\": [0.5, 0.5, 0.5],\n+    }\n+    image_preprocessor = PerceptionLMImageProcessorFast(**image_preprocessor_config)\n+    video_preprocessor_config = {\n+        \"video_processor_type\": \"PerceptionLMVideoProcessor\",\n+        \"size\": {\"height\": tile_size, \"width\": tile_size},\n+    }\n+    video_preprocessor = PerceptionLMVideoProcessor(**video_preprocessor_config)\n+    processor = PerceptionLMProcessor(\n+        image_processor=image_preprocessor,\n+        video_processor=video_preprocessor,\n+        tokenizer=tokenizer,\n+        chat_template=CHAT_TEMPLATE,\n+        **processor_config,\n+    )\n+\n+    if push_to_hub:\n+        print(f\"Pushing a {tokenizer_class.__name__} to the Hub repo - {tokenizer_path}.\")\n+        processor.push_to_hub(tokenizer_path, private=True, use_temp_dir=True)\n+    else:\n+        print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n+        processor.save_pretrained(tokenizer_path)\n+    return tokenizer\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        help=\"Location of Llama weights, which contains tokenizer.model and model folders\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        help=\"Whether or not to push the model to the hub at `output_dir` instead of saving it locally.\",\n+        action=\"store_true\",\n+        default=False,\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\",\n+        action=\"store_true\",\n+        default=True,\n+        help=\"Whether or not to save using `safetensors`.\",\n+    )\n+    parser.add_argument(\n+        \"--num_shards\",\n+        default=None,\n+        type=int,\n+        help=\"The number of individual shards used for the model. Does not have to be the same as the number of consolidated_xx.pth\",\n+    )\n+    parser.add_argument(\n+        \"--special_tokens\",\n+        default=None,\n+        type=list[str],\n+        help=\"The list of special tokens that should be added to the model.\",\n+    )\n+    args = parser.parse_args()\n+    if args.special_tokens is None:\n+        # no special tokens by default\n+        args.special_tokens = DEFAULT_SPECIAL_TOKENS.get(\"perception_lm\", [])\n+\n+    params = read_json(os.path.join(args.input_dir, \"params.json\"))\n+\n+    spm_path = os.path.join(args.input_dir, \"tokenizer.model\")\n+    tokenizer = write_tokenizer(\n+        args.output_dir,\n+        spm_path,\n+        special_tokens=args.special_tokens,\n+        params=params,\n+        push_to_hub=args.push_to_hub,\n+    )\n+    write_model(\n+        model_path=args.output_dir,\n+        input_base_path=args.input_dir,\n+        params=params,\n+        image_token_id=tokenizer.image_token_id,\n+        safe_serialization=args.safe_serialization,\n+        tokenizer=tokenizer,\n+        num_shards=args.num_shards,\n+        push_to_hub=args.push_to_hub,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "8a5fedc50b4236ebad3e002b4ec2a7d94c9e3714",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "added",
            "additions": 306,
            "deletions": 0,
            "changes": 306,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,306 @@\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for PerceptionLM.\"\"\"\n+\n+import math\n+from functools import reduce\n+from typing import Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import (\n+    BatchFeature,\n+)\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    get_image_size,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+    ChannelDimension,\n+    PILImageResampling,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+\n+class PerceptionLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    vision_input_type: str = \"thumb+tile\"\n+    tile_size: int = 448\n+    max_num_tiles: int = 36\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast PerceptionLM image processor.\",\n+)\n+class PerceptionLMImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    size = {\"width\": 448, \"height\": 448}  # for backward compatibility in tests\n+    valid_kwargs = PerceptionLMFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[PerceptionLMFastImageProcessorKwargs]) -> None:\n+        super().__init__(**kwargs)\n+\n+    @staticmethod\n+    def _factors(n: int):\n+        \"\"\"Return all factors of a number.\"\"\"\n+        return set(\n+            reduce(\n+                list.__add__,\n+                ([i, n // i] for i in range(1, int(n**0.5) + 1) if n % i == 0),\n+            )\n+        )\n+\n+    def _find_supported_aspect_ratios(self):\n+        \"\"\"\n+        This function computes all the allowed aspect ratios for a fixed\n+        number of input chunks. The order of returned items matters for the result of `_fit_image_to_canvas` function.\n+        If tie exists in `_fit_image_to_canvas`, the latter in `_find_supported_aspect_ratios` wins.\n+\n+        For example, with `num_tiles=5`, it will return:\n+        {\n+            0.2: [(1, 5)],\n+            5.0: [(5, 1)],\n+            0.25: [(1, 4)],\n+            1.0: [(2, 2), (1, 1)],\n+            4.0: [(4, 1)],\n+            0.3333333333333333: [(1, 3)],\n+            3.0: [(3, 1)],\n+            0.5: [(1, 2)],\n+            2.0: [(2, 1)]\n+        }\n+        \"\"\"\n+        asp_dict = {}\n+        for chunk_size in range(self.max_num_tiles, 0, -1):\n+            _factors = sorted(self._factors(chunk_size))\n+            _asp_ratios = [(x, chunk_size // x) for x in _factors]\n+            for ratio in _asp_ratios:\n+                k = ratio[0] / ratio[1]\n+                if k not in asp_dict:\n+                    asp_dict[k] = [ratio]\n+                else:\n+                    asp_dict[k].append(ratio)\n+        return asp_dict\n+\n+    def _get_image_height_width(\n+        self, image_width: int, image_height: int, target_width: int, target_height: int\n+    ) -> tuple[int, int]:\n+        \"\"\"\n+        Given image width, height and target width, height for the canvas, return the dimensions of how the image would be resized\n+        with aspect ratio preservation.\n+        \"\"\"\n+        scale = image_width / image_height\n+\n+        if scale > 1.0:\n+            # Width is larger than height\n+\n+            # Rescaling factor is the minimum of the two scaling factors. Else one side would be outside of the canvas.\n+            rescaling_factor = min(target_width / image_width, target_height / image_height)\n+\n+            # Set new width to target width and height to the rescaled height.\n+            new_w = rescaling_factor * image_width\n+            new_h = math.floor(new_w / scale)\n+\n+        else:\n+            # Height is larger than width\n+\n+            # Rescaling factor is the minimum of the two scaling factors. Else one side would be outside of the canvas.\n+            rescaling_factor = min(target_width / image_width, target_height / image_height)\n+\n+            # Set new height to target height and width to the rescaled width.\n+            new_h = rescaling_factor * image_height\n+            new_w = math.floor(new_h * scale)\n+\n+        return new_w, new_h\n+\n+    def _fit_image_to_canvas(self, img_width: int, img_height: int, tile_size: int):\n+        \"\"\"\n+        Given an image width, height and target number of chunks this function will see if the image\n+        can be fit into any of the canvases that can be build from arranging the tiles in a grid.\n+        If the image can be fit onto several canvases, it will return the canvas where the shorter edge\n+        of the image will be largest.\n+        \"\"\"\n+        # Initialize the optimal canvas to None. If no canvas is found where image fits, function returns None.\n+        optimal_canvas = None\n+        optimal_image_width_height = None\n+\n+        scale = img_width / img_height\n+\n+        # Gather all potential supported image resolutions and iterate through them to find best match\n+        potential_arrangements = [\n+            item for sublist in self._find_supported_aspect_ratios().values() for item in sublist\n+        ]\n+        for n_w, n_h in potential_arrangements:\n+            # Compute the canvas size\n+            canvas_width, canvas_height = n_w * tile_size, n_h * tile_size\n+\n+            # Check if image can fit into the canvas without downsampling\n+            if canvas_width >= img_width and canvas_height >= img_height:\n+                # If we did not find a good canvas yet, we will use the current one\n+                if optimal_canvas is None:\n+                    # Set optimal canvas and determine the actual image height and width in the canvas with aspect ratio preserving resampling\n+                    optimal_canvas = (n_w, n_h)\n+                    optimal_image_width_height = self._get_image_height_width(\n+                        image_width=img_width,\n+                        image_height=img_height,\n+                        target_width=n_w * tile_size,\n+                        target_height=n_h * tile_size,\n+                    )\n+                else:\n+                    # If we already found an optimal canvas before, we will check if the shorter edge of the image will be larger than the current optimal canvas.\n+                    # This means we can potentially upsample the image resolution which is beneficial to performance.\n+                    image_width_height = self._get_image_height_width(\n+                        image_width=img_width,\n+                        image_height=img_height,\n+                        target_width=n_w * tile_size,\n+                        target_height=n_h * tile_size,\n+                    )\n+                    # Llama3V dynamic tiling. Priortize biggest canvas.\n+                    if (scale < 1.0 and (image_width_height[0] >= optimal_image_width_height[0])) or (\n+                        scale >= 1.0 and (image_width_height[1] >= optimal_image_width_height[1])\n+                    ):\n+                        optimal_canvas = (n_w, n_h)\n+                        optimal_image_width_height = image_width_height\n+        return optimal_canvas\n+\n+    def _find_closest_aspect_ratio(self, img_width: int, img_height: int, tile_size: int) -> tuple:\n+        \"\"\"\n+        Given an image width, height and target number of chunks\n+        this function will find the closest supported aspect ratio.\n+        \"\"\"\n+        target_aspect_ratio = img_width / img_height\n+        asp_dict = self._find_supported_aspect_ratios()\n+        closest_aspect_ratio = None\n+        if target_aspect_ratio >= 1:\n+            closest_aspect_ratio = min(\n+                [k for k in asp_dict.keys() if k <= target_aspect_ratio],\n+                key=lambda x: abs(x - target_aspect_ratio),\n+            )\n+            tiles_given_aspect_ratio = asp_dict[closest_aspect_ratio]\n+            # select largest width\n+            return max(tiles_given_aspect_ratio, key=lambda x: x[0])\n+        else:\n+            closest_aspect_ratio = min(\n+                [k for k in asp_dict.keys() if k > target_aspect_ratio],\n+                key=lambda x: abs(1 / x - 1 / target_aspect_ratio),\n+            )\n+            tiles_given_aspect_ratio = asp_dict[closest_aspect_ratio]\n+            # select largest height\n+            return max(tiles_given_aspect_ratio, key=lambda x: x[1])\n+\n+    def _split(self, image: torch.Tensor, ncw: int, nch: int) -> torch.Tensor:\n+        # Split image into number of required tiles (width x height)\n+        batch_size, num_channels, height, width = image.size()\n+        image = image.view(batch_size, num_channels, nch, height // nch, ncw, width // ncw)\n+        # Permute dimensions to reorder the axes\n+        image = image.permute(0, 2, 4, 1, 3, 5).contiguous()\n+        # Reshape into the desired output shape (batch_size * 4, num_channels, width/2, height/2)\n+        image = image.view(batch_size, ncw * nch, num_channels, height // nch, width // ncw)\n+        return image\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        tile_size: int,\n+        max_num_tiles: int,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ):\n+        height, width = get_image_size(image, channel_dim=input_data_format)\n+        if max_num_tiles > 1:\n+            aspect_ratio = self._fit_image_to_canvas(img_width=width, img_height=height, tile_size=tile_size)\n+            if aspect_ratio is None:\n+                # If we did not find a canvas, we have to find the closest aspect ratio and downsample the image\n+                aspect_ratio = self._find_closest_aspect_ratio(img_width=width, img_height=height, tile_size=tile_size)\n+        else:\n+            aspect_ratio = (1, 1)\n+        new_width, new_height = aspect_ratio[0] * tile_size, aspect_ratio[1] * tile_size\n+        image = F.resize(image, (new_height, new_width), interpolation=resample)\n+        return image, aspect_ratio\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        do_rescale: Optional[bool],\n+        rescale_factor: Optional[Union[int, float]],\n+        do_normalize: Optional[bool],\n+        image_mean: Optional[Union[float, list[float]]],\n+        image_std: Optional[Union[float, list[float]]],\n+        tile_size: int,\n+        max_num_tiles: int,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        disable_grouping: bool,\n+        **kwargs: Unpack[PerceptionLMFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        # Group images by size for batched transformation\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                if self.vision_input_type == \"thumb+tile\":\n+                    thumbnails, _ = self.resize(stacked_images, tile_size, max_num_tiles=1)\n+                    images_for_tiling, (tiles_w, tiles_h) = self.resize(\n+                        stacked_images, tile_size, max_num_tiles=max_num_tiles\n+                    )\n+                    image_tiles = self._split(images_for_tiling, tiles_w, tiles_h)\n+                    stacked_images = torch.cat([thumbnails.unsqueeze(1), image_tiles], dim=1)\n+                else:  # vanilla single tile for low memory devices\n+                    stacked_images, _ = self.resize(stacked_images, tile_size, max_num_tiles=1)\n+\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # Fused rescale and normalize\n+            stacked_images = self.rescale_and_normalize(\n+                stacked_images,\n+                do_rescale,\n+                rescale_factor,\n+                do_normalize,\n+                image_mean,\n+                image_std,\n+            )\n+            processed_images_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"PerceptionLMImageProcessorFast\"]"
        },
        {
            "sha": "4e00bed6d6c1647efcc06a3d1fb865bf16b29507",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "added",
            "additions": 494,
            "deletions": 0,
            "changes": 494,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,494 @@\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+#           This file was automatically generated from src/transformers/models/perception_lm/modular_perception_lm.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_perception_lm.py file directly. One of our CI enforces this.\n+#                🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨🚨\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from dataclasses import dataclass\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...generation import GenerationMixin\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import auto_docstring, can_return_tuple\n+from ..auto import AutoModel\n+from .configuration_perception_lm import PerceptionLMConfig\n+\n+\n+class PerceptionLMAdaptiveAvgPooling(nn.Module):\n+    def __init__(self, pooling_ratio=2):\n+        super().__init__()\n+        self.pooling_ratio = pooling_ratio\n+\n+    def forward(self, hidden_states):\n+        b, num_tokens, c = hidden_states.shape\n+        h = int(math.sqrt(num_tokens))\n+        if h * h != num_tokens:\n+            raise ValueError(f\"num_tokens {num_tokens} is expected to be a square number\")\n+\n+        shape = (h // self.pooling_ratio, h // self.pooling_ratio)\n+        hidden_states = hidden_states.permute(0, 2, 1).reshape(b, -1, h, h)\n+        hidden_states = F.adaptive_avg_pool2d(hidden_states, shape)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        return hidden_states\n+\n+\n+class PerceptionLMMultiModalProjector(nn.Module):\n+    def __init__(self, config: PerceptionLMConfig):\n+        super().__init__()\n+        input_size = config.vision_config.model_args[\"embed_dim\"]\n+        output_size = config.text_config.hidden_size\n+        self.linear_1 = nn.Linear(\n+            in_features=input_size,\n+            out_features=output_size,\n+            bias=True,\n+        )\n+        self.gelu = nn.GELU()\n+        self.linear_2 = nn.Linear(\n+            in_features=output_size,\n+            out_features=output_size,\n+            bias=True,\n+        )\n+        self.pooling = (\n+            PerceptionLMAdaptiveAvgPooling(config.projector_pooling_ratio)\n+            if config.projector_pooling_ratio > 1\n+            else nn.Identity()\n+        )\n+\n+    def forward(self, features):\n+        features = features.permute(1, 0, 2)  # NLD -> LND\n+        features = self.linear_1(features)\n+        features = self.gelu(features)\n+        features = self.linear_2(features)\n+        features = features.permute(1, 0, 2)  # LND -> NLD\n+        features = self.pooling(features)\n+        return features\n+\n+\n+@auto_docstring\n+class PerceptionLMPreTrainedModel(PreTrainedModel):\n+    config_class = PerceptionLMConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_flash_attn_3 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of PerceptionLM isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/PerceptionLM/tree/main/perception_lm should serve for that purpose\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for PerceptionLM outputs, with hidden states and attentions.\n+    \"\"\"\n+)\n+class PerceptionLMModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for PerceptionLM causal language model (or autoregressive) outputs.\n+    \"\"\"\n+)\n+class PerceptionLMCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[list[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring\n+class PerceptionLMModel(PerceptionLMPreTrainedModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: PerceptionLMConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+        self.multi_modal_projector = PerceptionLMMultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_tiles, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_tiles, num_patches, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values.flatten(0, 1))\n+        image_outputs = image_outputs.last_hidden_state\n+        if self.config.vision_use_cls_token:\n+            image_outputs = image_outputs[:, 1:, :]\n+        image_features = self.multi_modal_projector(image_outputs)\n+        return image_features\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[tuple, PerceptionLMModelOutputWithPast]:\n+        \"\"\"\n+        Forward pass of the PerceptionLM model.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`, *optional*):\n+                Indices of input sequence tokens in the vocabulary.\n+            pixel_values (`torch.FloatTensor`, *optional*):\n+                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n+            pixel_values_videos (`torch.FloatTensor`, *optional*):\n+                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Mask to avoid performing attention on padding token indices.\n+            position_ids (`torch.LongTensor`, *optional*):\n+                Indices of positions of each input sequence token in the position embeddings.\n+            past_key_values (`list[torch.FloatTensor]`, *optional*):\n+                Precomputed key and value hidden states for fast autoregressive generation.\n+            inputs_embeds (`torch.FloatTensor`, *optional*):\n+                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n+            use_cache (`bool`, *optional*):\n+                Whether or not to use past key values to speed up decoding.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            cache_position (`torch.LongTensor`, *optional*):\n+                Position indices for caching.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+                Number of logits to keep.\n+            **lm_kwargs:\n+                Additional keyword arguments for the language model.\n+\n+        Returns:\n+            [`PerceptionLMModelOutputWithPast`] or `tuple`:\n+                Model outputs as a `PerceptionLMModelOutputWithPast` if `return_dict=True`, otherwise a tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both (pixel_values or pixel_values_videos) and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_features = None\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values.to(inputs_embeds),\n+            )\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            self.check_mask_feature_size_match(special_image_mask, image_features)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        video_features = None\n+        if pixel_values_videos is not None:\n+            video_features = self.get_image_features(\n+                pixel_values=pixel_values_videos.to(inputs_embeds),\n+            )\n+            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            self.check_mask_feature_size_match(special_video_mask, video_features)\n+            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_features = video_features.to(inputs_embeds)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+        return PerceptionLMModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            hidden_states=outputs.hidden_states,\n+            past_key_values=outputs.past_key_values,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=(video_features if pixel_values_videos is not None else None),\n+        )\n+\n+    def check_mask_feature_size_match(self, media_mask, media_features):\n+        media_token_count = media_mask.sum()\n+        media_feature_size = media_features.size()[:-1].numel()\n+        if media_token_count != media_feature_size:\n+            raise ValueError(\n+                f\"The number of tokens in the media mask ({media_token_count}) does not match the number of features in the media features ({media_feature_size}. Features shape: {media_features.shape})\"\n+            )\n+\n+\n+@auto_docstring\n+class PerceptionLMForConditionalGeneration(PerceptionLMPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {}\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: PerceptionLMConfig):\n+        super().__init__(config)\n+        self.model = PerceptionLMModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[tuple, PerceptionLMCausalLMOutputWithPast]:\n+        \"\"\"\n+        Forward pass for the PerceptionLMForConditionalGeneration model.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`, *optional*):\n+                Indices of input sequence tokens in the vocabulary.\n+            pixel_values (`torch.FloatTensor`, *optional*):\n+                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n+            pixel_values_videos (`torch.FloatTensor`, *optional*):\n+                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Mask to avoid performing attention on padding token indices.\n+            position_ids (`torch.LongTensor`, *optional*):\n+                Indices of positions of each input sequence token in the position embeddings.\n+            past_key_values (`list[torch.FloatTensor]`, *optional*):\n+                Precomputed key and value hidden states for fast autoregressive generation.\n+            inputs_embeds (`torch.FloatTensor`, *optional*):\n+                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n+            labels (`torch.LongTensor`, *optional*):\n+                Labels for computing the language modeling loss.\n+            use_cache (`bool`, *optional*):\n+                Whether or not to use past key values to speed up decoding.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            cache_position (`torch.LongTensor`, *optional*):\n+                Position indices for caching.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+                Number of logits to keep.\n+            **lm_kwargs:\n+                Additional keyword arguments for the language model.\n+\n+        Returns:\n+            [`PerceptionLMCausalLMOutputWithPast`] or `tuple`:\n+                Model outputs as a `PerceptionLMCausalLMOutputWithPast` if `return_dict=True`, otherwise a tuple.\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits,\n+                labels=labels,\n+                vocab_size=self.config.text_config.vocab_size,\n+                **lm_kwargs,\n+            )\n+\n+        return PerceptionLMCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        pixel_values_videos=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+        return model_inputs\n+\n+\n+__all__ = [\"PerceptionLMForConditionalGeneration\", \"PerceptionLMPreTrainedModel\", \"PerceptionLMModel\"]"
        },
        {
            "sha": "c703313d970243a4e69e52bf8d3138231aa2c9bd",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "added",
            "additions": 405,
            "deletions": 0,
            "changes": 405,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,405 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch PerceptionLM model.\"\"\"\n+\n+import math\n+from typing import Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n+from ..auto import AutoModel\n+from ..llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n+    LlavaPreTrainedModel,\n+)\n+from .configuration_perception_lm import PerceptionLMConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PerceptionLMAdaptiveAvgPooling(nn.Module):\n+    def __init__(self, pooling_ratio=2):\n+        super().__init__()\n+        self.pooling_ratio = pooling_ratio\n+\n+    def forward(self, hidden_states):\n+        b, num_tokens, c = hidden_states.shape\n+        h = int(math.sqrt(num_tokens))\n+        if h * h != num_tokens:\n+            raise ValueError(f\"num_tokens {num_tokens} is expected to be a square number\")\n+\n+        shape = (h // self.pooling_ratio, h // self.pooling_ratio)\n+        hidden_states = hidden_states.permute(0, 2, 1).reshape(b, -1, h, h)\n+        hidden_states = F.adaptive_avg_pool2d(hidden_states, shape)\n+        hidden_states = hidden_states.flatten(2).transpose(1, 2)\n+\n+        return hidden_states\n+\n+\n+class PerceptionLMMultiModalProjector(nn.Module):\n+    def __init__(self, config: PerceptionLMConfig):\n+        super().__init__()\n+        input_size = config.vision_config.model_args[\"embed_dim\"]\n+        output_size = config.text_config.hidden_size\n+        self.linear_1 = nn.Linear(\n+            in_features=input_size,\n+            out_features=output_size,\n+            bias=True,\n+        )\n+        self.gelu = nn.GELU()\n+        self.linear_2 = nn.Linear(\n+            in_features=output_size,\n+            out_features=output_size,\n+            bias=True,\n+        )\n+        self.pooling = (\n+            PerceptionLMAdaptiveAvgPooling(config.projector_pooling_ratio)\n+            if config.projector_pooling_ratio > 1\n+            else nn.Identity()\n+        )\n+\n+    def forward(self, features):\n+        features = features.permute(1, 0, 2)  # NLD -> LND\n+        features = self.linear_1(features)\n+        features = self.gelu(features)\n+        features = self.linear_2(features)\n+        features = features.permute(1, 0, 2)  # LND -> NLD\n+        features = self.pooling(features)\n+        return features\n+\n+\n+class PerceptionLMPreTrainedModel(LlavaPreTrainedModel):\n+    base_model_prefix = \"model\"\n+\n+\n+class PerceptionLMModelOutputWithPast(LlavaModelOutputWithPast):\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class PerceptionLMCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@auto_docstring\n+class PerceptionLMModel(LlavaModel):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def __init__(self, config: PerceptionLMConfig):\n+        super().__init__(config)\n+        self.vision_tower = AutoModel.from_config(config.vision_config)\n+        self.multi_modal_projector = PerceptionLMMultiModalProjector(config)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_tiles, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_tiles, num_patches, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values.flatten(0, 1))\n+        image_outputs = image_outputs.last_hidden_state\n+        if self.config.vision_use_cls_token:\n+            image_outputs = image_outputs[:, 1:, :]\n+        image_features = self.multi_modal_projector(image_outputs)\n+        return image_features\n+\n+    def check_mask_feature_size_match(self, media_mask, media_features):\n+        media_token_count = media_mask.sum()\n+        media_feature_size = media_features.size()[:-1].numel()\n+        if media_token_count != media_feature_size:\n+            raise ValueError(\n+                f\"The number of tokens in the media mask ({media_token_count}) does not match the number of features in the media features ({media_feature_size}. Features shape: {media_features.shape})\"\n+            )\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[tuple, PerceptionLMModelOutputWithPast]:\n+        \"\"\"\n+        Forward pass of the PerceptionLM model.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`, *optional*):\n+                Indices of input sequence tokens in the vocabulary.\n+            pixel_values (`torch.FloatTensor`, *optional*):\n+                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n+            pixel_values_videos (`torch.FloatTensor`, *optional*):\n+                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Mask to avoid performing attention on padding token indices.\n+            position_ids (`torch.LongTensor`, *optional*):\n+                Indices of positions of each input sequence token in the position embeddings.\n+            past_key_values (`list[torch.FloatTensor]`, *optional*):\n+                Precomputed key and value hidden states for fast autoregressive generation.\n+            inputs_embeds (`torch.FloatTensor`, *optional*):\n+                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n+            use_cache (`bool`, *optional*):\n+                Whether or not to use past key values to speed up decoding.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            cache_position (`torch.LongTensor`, *optional*):\n+                Position indices for caching.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+                Number of logits to keep.\n+            **lm_kwargs:\n+                Additional keyword arguments for the language model.\n+\n+        Returns:\n+            [`PerceptionLMModelOutputWithPast`] or `tuple`:\n+                Model outputs as a `PerceptionLMModelOutputWithPast` if `return_dict=True`, otherwise a tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both (pixel_values or pixel_values_videos) and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        image_features = None\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values.to(inputs_embeds),\n+            )\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            self.check_mask_feature_size_match(special_image_mask, image_features)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        video_features = None\n+        if pixel_values_videos is not None:\n+            video_features = self.get_image_features(\n+                pixel_values=pixel_values_videos.to(inputs_embeds),\n+            )\n+            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            self.check_mask_feature_size_match(special_video_mask, video_features)\n+            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_features = video_features.to(inputs_embeds)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+        return PerceptionLMModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            hidden_states=outputs.hidden_states,\n+            past_key_values=outputs.past_key_values,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=(video_features if pixel_values_videos is not None else None),\n+        )\n+\n+\n+@auto_docstring\n+class PerceptionLMForConditionalGeneration(LlavaForConditionalGeneration):\n+    _checkpoint_conversion_mapping = {}\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        pixel_values_videos=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+        return model_inputs\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[tuple, PerceptionLMCausalLMOutputWithPast]:\n+        \"\"\"\n+        Forward pass for the PerceptionLMForConditionalGeneration model.\n+\n+        Args:\n+            input_ids (`torch.LongTensor`, *optional*):\n+                Indices of input sequence tokens in the vocabulary.\n+            pixel_values (`torch.FloatTensor`, *optional*):\n+                Input image tensor of shape `(batch_size, num_tiles, channels, height, width)`.\n+            pixel_values_videos (`torch.FloatTensor`, *optional*):\n+                Input video tensor of shape `(batch_size, num_frames, channels, height, width)`.\n+            attention_mask (`torch.Tensor`, *optional*):\n+                Mask to avoid performing attention on padding token indices.\n+            position_ids (`torch.LongTensor`, *optional*):\n+                Indices of positions of each input sequence token in the position embeddings.\n+            past_key_values (`list[torch.FloatTensor]`, *optional*):\n+                Precomputed key and value hidden states for fast autoregressive generation.\n+            inputs_embeds (`torch.FloatTensor`, *optional*):\n+                Optionally, instead of passing `input_ids`, you can choose to directly pass an embedded representation.\n+            labels (`torch.LongTensor`, *optional*):\n+                Labels for computing the language modeling loss.\n+            use_cache (`bool`, *optional*):\n+                Whether or not to use past key values to speed up decoding.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers.\n+            cache_position (`torch.LongTensor`, *optional*):\n+                Position indices for caching.\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*, defaults to 0):\n+                Number of logits to keep.\n+            **lm_kwargs:\n+                Additional keyword arguments for the language model.\n+\n+        Returns:\n+            [`PerceptionLMCausalLMOutputWithPast`] or `tuple`:\n+                Model outputs as a `PerceptionLMCausalLMOutputWithPast` if `return_dict=True`, otherwise a tuple.\n+        \"\"\"\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits,\n+                labels=labels,\n+                vocab_size=self.config.text_config.vocab_size,\n+                **lm_kwargs,\n+            )\n+\n+        return PerceptionLMCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n+        )\n+\n+    def get_image_features(self, **kwargs):\n+        raise AttributeError(\"Not needed for PerceptionLM\")\n+\n+    def language_model(self):\n+        raise AttributeError(\"Not needed for PerceptionLM\")\n+\n+    def vision_tower(self):\n+        raise AttributeError(\"Not needed for PerceptionLM\")\n+\n+    def multi_modal_projector(self):\n+        raise AttributeError(\"Not needed for PerceptionLM\")\n+\n+\n+__all__ = [\n+    \"PerceptionLMForConditionalGeneration\",\n+    \"PerceptionLMPreTrainedModel\",\n+    \"PerceptionLMModel\",\n+]"
        },
        {
            "sha": "7dc1dc1ea371427d8fd659017a1b7ef501feae2f",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "added",
            "additions": 207,
            "deletions": 0,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,207 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for PerceptionLM.\n+\"\"\"\n+\n+from typing import Iterable, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, get_image_size, to_numpy_array\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import logging\n+from ...video_utils import VideoInput\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PerceptionLMProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": False,\n+        },\n+    }\n+\n+\n+class PerceptionLMProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a PerceptionLM processor which wraps a PerceptionLM image processor, a PerceptionLM video processor, and a tokenizer into a single processor.\n+\n+    [`PerceptionLMProcessor`] offers all the functionalities of [`PerceptionLMImageProcessorFast`], [`PerceptionLMVideoProcessor`], and the tokenizer (e.g. [`LlamaTokenizerFast`]). See the\n+    [`~PerceptionLMProcessor.__call__`] and [`~PerceptionLMProcessor.decode`] for more information.\n+\n+    Args:\n+        video_processor ([`PerceptionLMVideoProcessor`], *optional*):\n+            The video processor to process video inputs.\n+        image_processor ([`PerceptionLMImageProcessorFast`], *optional*):\n+            The image processor to process image inputs.\n+        tokenizer ([`LlamaTokenizerFast`] or similar, *optional*):\n+            The tokenizer to process text inputs.\n+        patch_size (`int`, *optional*):\n+            Patch size from the vision tower.\n+        chat_template (`str`, *optional*):\n+            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n+        pooling_ratio (`int`, *optional*, defaults to 2):\n+            Pooling ratio for vision tokens. If not 1, 2D adaptive pooling is applied over projected vision tokens.\n+    \"\"\"\n+\n+    attributes = [\"video_processor\", \"image_processor\", \"tokenizer\"]\n+    image_processor_class = \"AutoImageProcessor\"\n+    video_processor_class = \"AutoVideoProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        video_processor=None,\n+        image_processor=None,\n+        tokenizer=None,\n+        patch_size=None,\n+        chat_template=None,\n+        pooling_ratio=2,\n+        **kwargs,\n+    ):\n+        self.patch_size = patch_size\n+        self.pooling_ratio = pooling_ratio\n+        self.image_token = tokenizer.image_token\n+        self.video_token = tokenizer.video_token\n+        self.image_token_id = tokenizer.image_token_id\n+        self.video_token_id = tokenizer.video_token_id\n+        super().__init__(video_processor, image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos: VideoInput = None,\n+        **kwargs: Unpack[PerceptionLMProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Prepares a batch containing one or more sequences of text and/or images and/or videos.\n+\n+        If `text` is provided, it is tokenized using the tokenizer.\n+        If `images` is provided, they are processed using the image processor.\n+        If `videos` is provided, they are processed using the video processor.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, *optional*):\n+                The image or batch of images to be processed. Each image can be a PIL image, NumPy array, or PyTorch tensor.\n+                Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, *optional*):\n+                The sequence or batch of sequences to be tokenized. Each sequence can be a string.\n+            videos (`Any`, *optional*):\n+                The video or batch of videos to be processed.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is provided.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is provided).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is provided.\n+            - **pixel_values_videos** -- Video pixel values to be fed to a model. Returned when `videos` is provided.\n+        \"\"\"\n+        if text is None:\n+            raise ValueError(\n+                \"You have to specify at least `text` input. Optionally, you can also specify `images` or `videos`.\"\n+            )\n+\n+        output_kwargs = self._merge_kwargs(\n+            PerceptionLMProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        if images is not None:\n+            image_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n+        else:\n+            image_inputs = {}\n+\n+        if videos is not None:\n+            videos_inputs = self.video_processor(videos, **output_kwargs[\"videos_kwargs\"])\n+        else:\n+            videos_inputs = {}\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        # try to expand inputs in processing if we have the necessary parts\n+        prompt_strings = []\n+\n+        pixel_values = iter(image_inputs.get(\"pixel_values\", []))\n+        pixel_values_videos = iter(videos_inputs.get(\"pixel_values_videos\", []))\n+        for sample in text:\n+            # Replace the media token with the expanded media token sequence\n+            sample = self._expand_media_tokens(sample, self.tokenizer.image_token, pixel_values)\n+            sample = self._expand_media_tokens(sample, self.tokenizer.video_token, pixel_values_videos)\n+            prompt_strings.append(sample)\n+\n+        return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+        text_inputs = self.tokenizer(prompt_strings, **output_kwargs[\"text_kwargs\"])\n+        self._check_special_mm_tokens(prompt_strings, text_inputs, modalities=[\"image\", \"video\"])\n+        return BatchFeature(data={**text_inputs, **image_inputs, **videos_inputs}, tensor_type=return_tensors)\n+\n+    def _expand_media_tokens(self, sample, media_token: str, media_iter: Iterable):\n+        media_count = sample.count(media_token)\n+        if media_count > 0:\n+            media_list = [next(media_iter) for _ in range(media_count)]\n+            sample_splits = sample.split(media_token)\n+            media_token_list = []\n+            for media in media_list:\n+                height, width = get_image_size(to_numpy_array(media))\n+                num_tiles = media.shape[0]\n+                num_media_tokens = (\n+                    (height // self.patch_size // self.pooling_ratio)\n+                    * (width // self.patch_size // self.pooling_ratio)\n+                    * num_tiles\n+                )\n+                media_token_list.append(num_media_tokens)\n+            sample = \"\"\n+            for i, num_media_tokens in enumerate(media_token_list):\n+                sample += sample_splits[i]\n+                sample += media_token * num_media_tokens\n+            sample += sample_splits[-1]\n+        return sample\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PerceptionLMTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PerceptionLMTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"PerceptionLMProcessor\"]"
        },
        {
            "sha": "7381045c1d7c2ace20292df0154078859853aa21",
            "filename": "src/transformers/models/perception_lm/video_processing_perception_lm.py",
            "status": "added",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fvideo_processing_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,53 @@\n+# coding=utf-8\n+# Copyright 2025 Meta Platforms, Inc. and the HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Video processor class for PerceptionLM.\"\"\"\n+\n+from ...image_utils import (\n+    IMAGENET_STANDARD_MEAN,\n+    IMAGENET_STANDARD_STD,\n+)\n+from ...processing_utils import Unpack, VideosKwargs\n+from ...utils import is_vision_available\n+from ...utils.import_utils import requires\n+from ...video_processing_utils import (\n+    BaseVideoProcessor,\n+)\n+\n+\n+if is_vision_available():\n+    from ...image_utils import PILImageResampling\n+\n+\n+class PerceptionLMFastVideoProcessorInitKwargs(VideosKwargs): ...\n+\n+\n+@requires(backends=(\"torchvision\",))\n+class PerceptionLMVideoProcessor(BaseVideoProcessor):\n+    resample = PILImageResampling.BICUBIC\n+    image_mean = IMAGENET_STANDARD_MEAN\n+    image_std = IMAGENET_STANDARD_STD\n+    size = {\"height\": 448, \"width\": 448}\n+    do_resize = True\n+    do_center_crop = False\n+    do_rescale = True\n+    do_normalize = True\n+    do_convert_rgb = True\n+    valid_kwargs = PerceptionLMFastVideoProcessorInitKwargs\n+    model_input_names = [\"pixel_values_videos\"]\n+\n+    def __init__(self, **kwargs: Unpack[PerceptionLMFastVideoProcessorInitKwargs]):\n+        super().__init__(**kwargs)\n+\n+\n+__all__ = [\"PerceptionLMVideoProcessor\"]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/perception_lm/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2F__init__.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29"
        },
        {
            "sha": "6cf8a2c4f129978c1dc09a341d4dc451dce2f077",
            "filename": "tests/models/perception_lm/test_image_processing_perception_lm.py",
            "status": "added",
            "additions": 224,
            "deletions": 0,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_image_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_image_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_image_processing_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,224 @@\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    if is_torchvision_available():\n+        from transformers import PerceptionLMImageProcessorFast\n+\n+\n+class PerceptionLMImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        tile_size=16,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+        max_num_tiles=4,\n+        vision_input_type=\"thumb+tile\",\n+        resample=Image.Resampling.BICUBIC,  # dummy value\n+        size={\"shortest_edge\": 20},  # dummy value\n+    ):\n+        super().__init__()\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.tile_size = tile_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+        self.max_num_tiles = max_num_tiles\n+        self.vision_input_type = vision_input_type\n+        self.resample = resample\n+        self.size = size\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"tile_size\": self.tile_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+            \"max_num_tiles\": self.max_num_tiles,\n+            \"vision_input_type\": self.vision_input_type,\n+            \"resample\": self.resample,\n+            \"size\": self.size,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTester.prepare_image_inputs\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class PerceptionLMImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    fast_image_processing_class = PerceptionLMImageProcessorFast if is_torchvision_available() else None\n+    test_slow_image_processor = False\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = PerceptionLMImageProcessingTester(self)\n+\n+    @property\n+    # Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest.image_processor_dict\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"tile_size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"max_num_tiles\"))\n+            self.assertTrue(hasattr(image_processing, \"vision_input_type\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.tile_size, 16)\n+            self.assertEqual(image_processor.max_num_tiles, 4)\n+            self.assertEqual(image_processor.vision_input_type, \"thumb+tile\")\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, tile_size=42, max_num_tiles=9\n+            )\n+            self.assertEqual(image_processor.tile_size, 42)\n+            self.assertEqual(image_processor.max_num_tiles, 9)\n+            self.assertEqual(image_processor.vision_input_type, \"thumb+tile\")\n+\n+    def test_call_pil(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_numpy(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+            # Test not batched input\n+            encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (1, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+    @unittest.skip(reason=\"PerceptionLMImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_nested_input(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test batched as a list of images\n+            encoded_images = image_processing(image_inputs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+\n+            # Test batched as a nested list of images, where each sublist is one batch\n+            image_inputs_nested = [image_inputs[:3], image_inputs[3:]]\n+            encoded_images_nested = image_processing(image_inputs_nested, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = (7, 5, 3, 16, 16)\n+            self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+\n+            # Image processor should return same pixel values, independently of ipnut format\n+            self.assertTrue((encoded_images_nested == encoded_images).all())"
        },
        {
            "sha": "4894e0c5ffd7590e6b9bd1f6b52f1295f2d3fba5",
            "filename": "tests/models/perception_lm/test_modeling_perception_lm.py",
            "status": "added",
            "additions": 490,
            "deletions": 0,
            "changes": 490,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_modeling_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,490 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch PerceptionLM model.\"\"\"\n+\n+import unittest\n+\n+from huggingface_hub import hf_hub_download\n+\n+from transformers import (\n+    AutoProcessor,\n+    PerceptionLMConfig,\n+    PerceptionLMForConditionalGeneration,\n+    PerceptionLMModel,\n+    is_torch_available,\n+)\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_bitsandbytes,\n+    require_read_token,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...generation.test_utils import GenerationTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+class PerceptionLMVisionText2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        image_token_id=0,\n+        video_token_id=2,\n+        seq_length=7,\n+        tie_word_embeddings=True,\n+        projector_pooling_ratio=1,\n+        text_config={\n+            \"model_type\": \"llama\",\n+            \"seq_length\": 7,\n+            \"is_training\": True,\n+            \"use_input_mask\": True,\n+            \"use_token_type_ids\": False,\n+            \"use_labels\": True,\n+            \"vocab_size\": 99,\n+            \"hidden_size\": 32,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"intermediate_size\": 37,\n+            \"hidden_act\": \"gelu\",\n+            \"hidden_dropout_prob\": 0.1,\n+            \"attention_probs_dropout_prob\": 0.1,\n+            \"max_position_embeddings\": 512,\n+            \"type_vocab_size\": 16,\n+            \"type_sequence_label_size\": 2,\n+            \"initializer_range\": 0.02,\n+            \"num_labels\": 3,\n+            \"num_choices\": 4,\n+            \"pad_token_id\": 1,\n+        },\n+        is_training=True,\n+        vision_config={\n+            \"architecture\": \"vit_pe_core_large_patch14_336\",\n+            \"model_args\": {\n+                \"embed_dim\": 64,\n+                \"img_size\": (14, 14),\n+                \"depth\": 2,\n+                \"global_pool\": \"\",\n+                \"use_post_transformer_norm\": False,\n+                \"init_values\": 0.1,\n+                \"ref_feat_shape\": (1, 1),\n+            },\n+        },\n+    ):\n+        self.parent = parent\n+        self.image_token_id = image_token_id\n+        self.video_token_id = video_token_id\n+        self.text_config = text_config\n+        self.vision_config = vision_config\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n+\n+        self.num_hidden_layers = text_config[\"num_hidden_layers\"]\n+        self.vocab_size = text_config[\"vocab_size\"]\n+        self.hidden_size = text_config[\"hidden_size\"]\n+        self.num_attention_heads = text_config[\"num_attention_heads\"]\n+        self.is_training = is_training\n+        self.tie_word_embeddings = tie_word_embeddings\n+\n+        self.batch_size = 3\n+        self.num_tiles = 1\n+        self.num_frames = 1\n+        self.num_channels = 3\n+        self.image_size = self.vision_config[\"model_args\"][\"img_size\"][0]\n+        self.num_image_tokens = (self.vision_config[\"model_args\"][\"img_size\"][0] // 14) ** 2\n+        self.num_video_tokens = (self.vision_config[\"model_args\"][\"img_size\"][0] // 14) ** 2\n+        self.seq_length = seq_length + self.num_image_tokens\n+        self.encoder_seq_length = self.seq_length\n+\n+    def get_config(self):\n+        return PerceptionLMConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            vision_use_cls_token=True,\n+            image_token_id=self.image_token_id,\n+            video_token_id=self.video_token_id,\n+            tie_word_embeddings=self.tie_word_embeddings,\n+        )\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_tiles,\n+                self.num_channels,\n+                self.vision_config[\"model_args\"][\"img_size\"][0],\n+                self.vision_config[\"model_args\"][\"img_size\"][1],\n+            ]\n+        )\n+        pixel_values_videos = floats_tensor(\n+            [\n+                self.batch_size,\n+                self.num_frames,\n+                self.num_channels,\n+                self.vision_config[\"model_args\"][\"img_size\"][0],\n+                self.vision_config[\"model_args\"][\"img_size\"][1],\n+            ]\n+        )\n+        config = self.get_config()\n+\n+        return config, pixel_values, pixel_values_videos\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_values_videos = self.prepare_config_and_inputs()\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], config.text_config.vocab_size - 2) + 2\n+        attention_mask = torch.ones(input_ids.shape, dtype=torch.long).to(torch_device)\n+        input_ids[input_ids == config.image_token_id] = self.pad_token_id\n+        input_ids[input_ids == config.video_token_id] = self.pad_token_id\n+        input_ids[:, : self.num_image_tokens] = config.image_token_id\n+        input_ids[:, self.num_image_tokens : self.num_video_tokens + self.num_image_tokens] = config.video_token_id\n+\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"pixel_values_videos\": pixel_values_videos,\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class PerceptionLMForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `PerceptionLMForConditionalGeneration`.\n+    \"\"\"\n+\n+    all_model_classes = (\n+        (\n+            PerceptionLMModel,\n+            PerceptionLMForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_pruning = False\n+    test_head_masking = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = PerceptionLMVisionText2TextModelTester(self)\n+        common_properties = [\n+            \"image_token_id\",\n+            \"video_token_id\",\n+        ]\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=PerceptionLMConfig,\n+            has_text_modality=False,\n+            common_properties=common_properties,\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    def test_inputs_embeds(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"pixel_values_videos\"]\n+\n+            wte = model.get_input_embeddings()\n+            inputs[\"inputs_embeds\"] = wte(input_ids)\n+\n+            with torch.no_grad():\n+                model(**inputs)\n+\n+    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n+    # while some other models require pixel_values to be present\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+            del inputs[\"pixel_values_videos\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n+    def test_mismatching_num_image_tokens(self):\n+        \"\"\"\n+        Tests that VLMs through an error with explicit message saying what is wrong\n+        when number of images doesn't match number of image tokens in the text.\n+        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        for model_class in self.all_model_classes:\n+            if model_class == PerceptionLMModel:\n+                continue\n+            model = model_class(config).to(torch_device)\n+            _ = model(**input_dict)  # successful forward with no modifications\n+\n+            # remove one image but leave the image token in text\n+            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            with self.assertRaises(ValueError):\n+                _ = model(**input_dict)\n+\n+            # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n+            input_ids = input_dict[\"input_ids\"][:1]\n+            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = torch.cat([input_ids, input_ids], dim=0)\n+\n+            # one image and two image tokens raise an error\n+            with self.assertRaises(ValueError):\n+                _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+            # two images and two image tokens don't raise an error\n+            pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n+            _ = model(input_ids=input_ids, pixel_values=pixel_values)\n+\n+    def test_training(self):\n+        self.all_model_classes = (PerceptionLMForConditionalGeneration,) if is_torch_available() else ()\n+        super().test_training()\n+\n+    def test_training_gradient_checkpointing(self):\n+        self.all_model_classes = (PerceptionLMForConditionalGeneration,) if is_torch_available() else ()\n+        super().test_training_gradient_checkpointing()\n+\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        self.all_model_classes = (PerceptionLMForConditionalGeneration,) if is_torch_available() else ()\n+        super().test_training_gradient_checkpointing_use_reentrant()\n+\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        self.all_model_classes = (PerceptionLMForConditionalGeneration,) if is_torch_available() else ()\n+        super().test_training_gradient_checkpointing_use_reentrant_false()\n+\n+    @unittest.skip(reason=\"Timm Eva (PE) weights cannot be fully constructed in _init_weights\")\n+    def test_can_init_all_missing_weights(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Timm Eva (PE) weights cannot be fully constructed in _init_weights\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"PE/TIMM's attention implementation is self configured and won't raise ValueError on global attention implementation.\"\n+    )\n+    def test_flash_attn_2_can_dispatch_composite_models(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\"ViT PE / TimmWrapperModel cannot be tested with meta device\")\n+    def test_can_be_initialized_on_meta(self):\n+        pass\n+\n+    @unittest.skip(\"ViT PE / TimmWrapperModel cannot be tested with meta device\")\n+    def test_can_load_with_meta_device_context_manager(self):\n+        pass\n+\n+    @unittest.skip(\"Specifying both inputs_embeds and pixel_values are not supported for PerceptionLM\")\n+    def test_generate_from_inputs_embeds_0_greedy(self):\n+        pass\n+\n+    @unittest.skip(\"Specifying both inputs_embeds and pixel_values are not supported for PerceptionLM\")\n+    def test_generate_from_inputs_embeds_1_beam_search(self):\n+        pass\n+\n+    @unittest.skip(\"Specifying both inputs_embeds and pixel_values are not supported for PerceptionLM\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    ## Skip flash attention releated tests below\n+    ## correct configuration:\n+    ## from_pretrained(model_id, attn_implementation={\"text_config\": \"flash_attention_2\", \"vision_config\": \"eager\"}\n+    @unittest.skip(\"Flash attn test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_eager_matches_fa2_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Flash attn test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @unittest.skip(\"Flash attn test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_flash_attn_2_from_config(self):\n+        pass\n+\n+    @unittest.skip(\"SDPA test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_eager_matches_sdpa_generate_with_dynamic_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Flash attn test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(\"SDPA test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_eager_matches_sdpa_generate(self):\n+        pass\n+\n+    @unittest.skip(\"Flash attn test is not configured correctly as we need to configure vision/timm model to 'eager'.\")\n+    def test_flash_attn_2_inference_equivalence(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"PerceptionLMForConditionalGeneration does not have language_model, vision_tower, multi_modal_projector.\"\n+    )\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        pass\n+\n+    @unittest.skip(\"Cannot set `output_attentions` for timm models.\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Cannot set `output_attentions` for timm models.\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(\"Cannot set `output_attentions` for timm models.\")\n+    def test_generate_compilation_all_outputs(self):\n+        pass\n+\n+\n+TEST_MODEL_PATH = \"facebook/Perception-LM-1B\"\n+\n+\n+@require_torch\n+@require_bitsandbytes\n+@slow\n+@require_read_token\n+class PerceptionLMForConditionalGenerationIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(TEST_MODEL_PATH)\n+        self.image_file = hf_hub_download(\n+            repo_id=\"shumingh/perception_lm_test_images\",\n+            filename=\"14496_0.PNG\",\n+            repo_type=\"dataset\",\n+        )\n+        self.video_file = hf_hub_download(\n+            repo_id=\"shumingh/perception_lm_test_videos\",\n+            filename=\"GUWR5TyiY-M_000012_000022.mp4\",\n+            repo_type=\"dataset\",\n+        )\n+        self.conversation1 = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\", \"url\": self.image_file},\n+                    {\"type\": \"text\", \"text\": \"Describe the bar plot in the image.\"},\n+                ],\n+            }\n+        ]\n+        self.conversation2 = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"url\": self.video_file,\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Can you describe the video in detail?\"},\n+                ],\n+            }\n+        ]\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def test_small_model_integration_test(self):\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(\n+            TEST_MODEL_PATH, load_in_4bit=True, cache_dir=\"./\"\n+        )\n+\n+        inputs = self.processor.apply_chat_template(\n+            [self.conversation1],\n+            num_frames=32,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            video_load_backend=\"decord\",\n+            padding=True,\n+            padding_side=\"left\",\n+        ).to(torch_device)\n+\n+        generate_ids = model.generate(**inputs, max_new_tokens=18)\n+        input_length = inputs[\"input_ids\"].shape[1]\n+        generate_ids_without_inputs = generate_ids[:, input_length:]\n+\n+        EXPECTED_DECODED_TEXT = \"The bar plot displays the values of four categories: step, horror, mood, and lumber\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(generate_ids_without_inputs[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_small_model_integration_test_batched(self):\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(TEST_MODEL_PATH, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(TEST_MODEL_PATH)\n+        inputs = processor.apply_chat_template(\n+            [self.conversation1, self.conversation2],\n+            num_frames=32,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+            video_load_backend=\"decord\",\n+            padding=True,\n+            padding_side=\"left\",\n+        ).to(torch_device)\n+\n+        generate_ids = model.generate(**inputs, max_new_tokens=18)\n+        input_length = inputs[\"input_ids\"].shape[1]\n+        generate_ids_without_inputs = generate_ids[:, input_length:]\n+\n+        EXPECTED_DECODED_TEXT = ['The bar plot displays the values of four categories: step, horror, mood, and lumber', 'The video shows a group of people in green shirts and white shorts performing a jump rope routine']  # fmt: skip\n+\n+        self.assertEqual(\n+            processor.batch_decode(generate_ids_without_inputs, skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )\n+\n+    def test_generation_no_images(self):\n+        # model_id = \"facebook/Perception-LM-1B\"\n+        model = PerceptionLMForConditionalGeneration.from_pretrained(TEST_MODEL_PATH, load_in_4bit=True)\n+        processor = AutoProcessor.from_pretrained(TEST_MODEL_PATH)\n+\n+        # Prepare inputs with no images\n+        inputs = processor(text=\"Hello, I am\", return_tensors=\"pt\").to(torch_device)\n+\n+        # Make sure that `generate` works\n+        _ = model.generate(**inputs, max_new_tokens=20)"
        },
        {
            "sha": "7ae377d14a74a7f4439edfe348bb41ed5507646f",
            "filename": "tests/models/perception_lm/test_processor_perception_lm.py",
            "status": "added",
            "additions": 146,
            "deletions": 0,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_processor_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,146 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import json\n+import shutil\n+import tempfile\n+import unittest\n+\n+from transformers import (\n+    AutoProcessor,\n+    AutoTokenizer,\n+    PerceptionLMProcessor,\n+)\n+from transformers.testing_utils import require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import PerceptionLMImageProcessorFast, PerceptionLMVideoProcessor\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+# TEST_MODEL_PATH = \"facebook/Perception-LM-1B\"\n+TEST_MODEL_PATH = \"shumingh/plm_1b_hf\"  # should be replaced by the above once checkpoints are merged\n+\n+\n+@require_vision\n+class PerceptionLMProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = PerceptionLMProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = PerceptionLMImageProcessorFast(\n+            tile_size=448, max_num_tiles=4, vision_input_type=\"thumb+tile\"\n+        )\n+        video_processor = PerceptionLMVideoProcessor()\n+        tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL_PATH)\n+        tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<|image|>\", \"<|video|>\"]})\n+        processor_kwargs = cls.prepare_processor_dict()\n+        processor = PerceptionLMProcessor(\n+            image_processor=image_processor, video_processor=video_processor, tokenizer=tokenizer, **processor_kwargs\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n+        cls.image_token_id = processor.image_token_id\n+        cls.video_token_id = processor.video_token_id\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n+    @staticmethod\n+    def prepare_processor_dict():\n+        return {\n+            \"chat_template\": CHAT_TEMPLATE,\n+            \"patch_size\": 14,\n+            \"pooling_ratio\": 2,\n+        }  # fmt: skip\n+\n+    def test_chat_template_is_saved(self):\n+        processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n+        processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n+        # chat templates aren't serialized to json in processors\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+\n+        # they have to be saved as separate file and loaded back from that file\n+        # so we check if the same template is loaded\n+        processor_dict = self.prepare_processor_dict()\n+        self.assertTrue(processor_loaded.chat_template == processor_dict.get(\"chat_template\", None))\n+\n+    def test_image_token_filling(self):\n+        processor = self.processor_class.from_pretrained(self.tmpdirname)\n+        # Important to check with non square image\n+        image = torch.randn((1, 3, 450, 500))\n+        #  5 tiles (thumbnail tile + 4 tiles)\n+        #  448/patch_size/pooling_ratio = 16 => 16*16 tokens per tile\n+        expected_image_tokens = 16 * 16 * 5\n+        image_token_index = processor.image_token_id\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = processor(\n+            text=[processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)\n+\n+\n+CHAT_TEMPLATE = (\n+    \"{{- bos_token }}\"\n+    \"{%- if messages[0]['role'] == 'system' -%}\"\n+    \"    {%- set system_message = messages[0]['content']|trim %}\\n\"\n+    \"    {%- set messages = messages[1:] %}\\n\"\n+    \"{%- else %}\"\n+    \"    {%- set system_message = 'You are a helpful language and vision assistant. You are able to understand the visual content that the user provides, and assist the user with a variety of tasks using natural language.' %}\"\n+    \"{%- endif %}\"\n+    \"{{- '<|start_header_id|>system<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{{- system_message }}\"\n+    \"{{- '<|eot_id|>' }}\"\n+    \"{%- for message in messages %}\"\n+    \"{{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'image') %}\"\n+    \"{{ '<|image|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'video') %}\"\n+    \"{{ '<|video|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- for content in message['content'] | selectattr('type', 'equalto', 'text') %}\"\n+    \"{{- content['text'] | trim }}\"\n+    \"{%- endfor %}\"\n+    \"{{'<|eot_id|>' }}\"\n+    \"{%- endfor %}\"\n+    \"{%- if add_generation_prompt %}\"\n+    \"{{- '<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n' }}\"\n+    \"{%- endif %}\"\n+)"
        },
        {
            "sha": "c54fbc847dd1970c6438a57f86b74a0a4683a14c",
            "filename": "tests/models/perception_lm/test_video_processing_perception_lm.py",
            "status": "added",
            "additions": 124,
            "deletions": 0,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_video_processing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29/tests%2Fmodels%2Fperception_lm%2Ftest_video_processing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperception_lm%2Ftest_video_processing_perception_lm.py?ref=bf607f6d3bd7bf95aaa1094c3a51c3476e4ddc29",
            "patch": "@@ -0,0 +1,124 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import unittest\n+\n+from transformers.image_utils import IMAGENET_STANDARD_MEAN, IMAGENET_STANDARD_STD\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torchvision_available, is_vision_available\n+\n+from ...test_video_processing_common import VideoProcessingTestMixin, prepare_video_inputs\n+\n+\n+if is_vision_available():\n+    if is_torchvision_available():\n+        from transformers import PerceptionLMVideoProcessor\n+\n+\n+class PerceptionLMVideoProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=5,\n+        num_frames=8,\n+        num_channels=3,\n+        min_resolution=30,\n+        max_resolution=80,\n+        do_resize=True,\n+        size=None,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_normalize=True,\n+        image_mean=IMAGENET_STANDARD_MEAN,\n+        image_std=IMAGENET_STANDARD_STD,\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"height\": 20, \"width\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_frames = num_frames\n+        self.num_channels = num_channels\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_video_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_video_shape(self, images):\n+        return self.num_frames, self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_video_inputs(self, equal_resolution=False, return_tensors=\"pil\"):\n+        videos = prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_frames=self.num_frames,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            return_tensors=return_tensors,\n+        )\n+        return videos\n+\n+\n+@require_torch\n+@require_vision\n+class PerceptionLMVideoProcessingTest(VideoProcessingTestMixin, unittest.TestCase):\n+    fast_video_processing_class = PerceptionLMVideoProcessor if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.video_processor_tester = PerceptionLMVideoProcessingTester(self)\n+\n+    @property\n+    def video_processor_dict(self):\n+        return self.video_processor_tester.prepare_video_processor_dict()\n+\n+    def test_video_processor_properties(self):\n+        video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n+        self.assertTrue(hasattr(video_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(video_processing, \"size\"))\n+        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(video_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(video_processing, \"image_std\"))\n+        self.assertTrue(hasattr(video_processing, \"do_convert_rgb\"))\n+\n+    def test_video_processor_from_dict_with_kwargs(self):\n+        video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict)\n+        self.assertEqual(video_processor.size, {\"height\": 20, \"width\": 20})\n+        self.assertEqual(video_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+        video_processor = self.fast_video_processing_class.from_dict(self.video_processor_dict, size=42, crop_size=84)\n+        self.assertEqual(video_processor.size, {\"height\": 42, \"width\": 42})\n+        self.assertEqual(video_processor.crop_size, {\"height\": 84, \"width\": 84})"
        }
    ],
    "stats": {
        "total": 3263,
        "additions": 3262,
        "deletions": 1
    }
}