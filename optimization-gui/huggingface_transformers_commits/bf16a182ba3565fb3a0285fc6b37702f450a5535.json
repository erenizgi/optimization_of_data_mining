{
    "author": "ydshieh",
    "message": "Remove `_supports_static_cache = True` for some model classes (#34975)\n\n* use mask_fill\r\n\r\n* remove comment\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "bf16a182ba3565fb3a0285fc6b37702f450a5535",
    "files": [
        {
            "sha": "3230952bf5c726abb7f2569e43e04cf514c900e0",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=bf16a182ba3565fb3a0285fc6b37702f450a5535",
            "patch": "@@ -675,6 +675,8 @@ def forward(\n         v1_chunked = [v1.squeeze(dim=0) for v1 in v1_chunked]\n         w2_chunked = [w2.squeeze(dim=0) for w2 in w2_chunked]\n         for expert_idx in range(0, self.moe_num_experts):\n+            # (This cause torch.compile to fail with `torch._dynamo.exc.Unsupported: dynamic shape operator: aten.nonzero.default`)\n+            # (set torch._dynamo.config.capture_dynamic_output_shape_ops = True may help but not tested)\n             topk_idx, token_idx = torch.where(expert_mask[expert_idx])\n             if token_idx.shape[0] == 0:\n                 continue\n@@ -831,7 +833,6 @@ class DbrxPreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n \n     def _init_weights(self, module: nn.Module):\n         std = self.config.initializer_range"
        },
        {
            "sha": "306457d572e84c6585dab94e1b0b020366a15c9c",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=bf16a182ba3565fb3a0285fc6b37702f450a5535",
            "patch": "@@ -330,6 +330,8 @@ def forward(self, hidden_states):\n         )  # [num_tokens, num_experts]\n         gates = zeros.scatter(1, top_k_indices, 1)  # [num_tokens, num_experts]\n         expert_size = gates.long().sum(0)  # [num_experts,]\n+        # (This cause torch.compile to fail with `torch._dynamo.exc.Unsupported: Backend compiler failed with a fake tensor exception at`)\n+        # (and `DataDependentOutputException`)\n         expert_size = expert_size.tolist()\n \n         # sort and group input tokens according to expert assignment\n@@ -841,7 +843,6 @@ class GraniteMoePreTrainedModel(PreTrainedModel):\n     _supports_sdpa = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n-    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -1155,8 +1156,6 @@ def _update_causal_mask(\n \n         if attention_mask is not None and attention_mask.dim() == 4:\n             # in this case we assume that the mask comes already in inverted form and requires no inversion or slicing\n-            if attention_mask.max() != 0:\n-                raise ValueError(\"Custom 4D attention mask should be passed in inverted form with max==0`\")\n             causal_mask = attention_mask\n         else:\n             causal_mask = torch.full("
        },
        {
            "sha": "3d0a956a7bf78d605fd6dd0ffeb96cff9b0a65e9",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=bf16a182ba3565fb3a0285fc6b37702f450a5535",
            "patch": "@@ -868,7 +868,7 @@ def forward(\n         )\n         hidden_states = nn.functional.dropout(hidden_states, p=self.config, training=self.training)\n         # Fill in zeros for cross_attention hidden_states of tokens attending to no images\n-        hidden_states[cross_attention_gate == 0] = hidden_states[cross_attention_gate == 0].fill_(0)\n+        hidden_states = hidden_states.masked_fill((cross_attention_gate == 0)[:, :, None], 0.0)\n         hidden_states = residual + self.act_cross_attn(self.alpha_cross_attn) * hidden_states\n \n         # Fully Connected\n@@ -917,7 +917,6 @@ class IdeficsPreTrainedModel(PreTrainedModel):\n     _no_split_modules = [\"IdeficsDecoderLayer\", \"IdeficsGatedCrossAttentionLayer\"]\n     _supports_sdpa = True\n     _supports_cache_class = True\n-    _supports_static_cache = True\n \n     def _init_weights(self, module):\n         # important: this ported version of Idefics isn't meant for training from scratch - only\n@@ -1155,7 +1154,7 @@ def forward(\n         elif position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        if (pixel_values, image_encoder_embeddings, perceiver_embeddings).count(None) != 2:\n+        if sum([x is None for x in [pixel_values, image_encoder_embeddings, perceiver_embeddings]]) != 2:\n             raise ValueError(\n                 \"Exactly 1 of pixel_values, image_encoder_embeddings or perceiver_embeddings has to be not-None.\"\n             )"
        },
        {
            "sha": "a5aa6e8a9537cf1b37df9ae518fcd6b985b3cf43",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bf16a182ba3565fb3a0285fc6b37702f450a5535/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=bf16a182ba3565fb3a0285fc6b37702f450a5535",
            "patch": "@@ -216,6 +216,8 @@ def forward(self, hidden_states):\n         )  # [num_tokens, num_experts]\n         gates = zeros.scatter(1, top_k_indices, 1)  # [num_tokens, num_experts]\n         expert_size = gates.long().sum(0)  # [num_experts,]\n+        # (This cause torch.compile to fail with `torch._dynamo.exc.Unsupported: Backend compiler failed with a fake tensor exception at`)\n+        # (and `DataDependentOutputException`)\n         expert_size = expert_size.tolist()\n \n         # sort and group input tokens according to expert assignment"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 8,
        "deletions": 7
    }
}