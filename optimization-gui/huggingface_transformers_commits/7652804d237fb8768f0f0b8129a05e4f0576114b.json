{
    "author": "SunMarc",
    "message": "Fix bnb regression due to empty state dict (#36663)\n\nfix",
    "sha": "7652804d237fb8768f0f0b8129a05e4f0576114b",
    "files": [
        {
            "sha": "8462fb84b1881f0c5ff7ad4c04414835c0b4de21",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7652804d237fb8768f0f0b8129a05e4f0576114b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7652804d237fb8768f0f0b8129a05e4f0576114b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=7652804d237fb8768f0f0b8129a05e4f0576114b",
            "patch": "@@ -833,7 +833,7 @@ def _load_state_dict_into_meta_model(\n     bin_state_dict = None\n     if shard_file.endswith(\".safetensors\"):\n         file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n-    else:\n+    elif shard_file.endswith(\".bin\"):\n         map_location = \"cpu\"\n         if (\n             device_map is not None\n@@ -848,6 +848,13 @@ def _load_state_dict_into_meta_model(\n \n     is_quantized = hf_quantizer is not None\n \n+    # get full state dict\n+    if is_quantized:\n+        if shard_file.endswith(\".safetensors\"):\n+            full_state_dict = load_state_dict(shard_file, map_location=\"cpu\")\n+        elif shard_file.endswith(\".bin\"):\n+            full_state_dict = bin_state_dict\n+\n     for serialized_param_name, empty_param in state_dict.items():\n         # serialized_param_name is the raw, serialized name\n         # fixed_param_name is the model's equivalent\n@@ -912,7 +919,7 @@ def _load_state_dict_into_meta_model(\n                         model,\n                         param,\n                         fixed_param_name,\n-                        state_dict,\n+                        full_state_dict,\n                         param_device=param_device,\n                         device_map=device_map,\n                     )\n@@ -928,7 +935,7 @@ def _load_state_dict_into_meta_model(\n                 )\n             else:\n                 hf_quantizer.create_quantized_param(\n-                    model, param, fixed_param_name, param_device, state_dict, unexpected_keys\n+                    model, param, fixed_param_name, param_device, full_state_dict, unexpected_keys\n                 )\n                 # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 10,
        "deletions": 3
    }
}