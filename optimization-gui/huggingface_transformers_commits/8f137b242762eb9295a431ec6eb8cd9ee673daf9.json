{
    "author": "bauwenst",
    "message": "Move `DataCollatorForMultipleChoice` from the docs to the package (#34763)\n\n* Add implementation for DataCollatorForMultipleChoice based on docs.\n\n* Add DataCollatorForMultipleChoice to import structure.\n\n* Remove custom DataCollatorForMultipleChoice implementations from example scripts.\n\n* Remove custom implementations of DataCollatorForMultipleChoice from docs in English, Spanish, Japanese and Korean.\n\n* Refactor torch version of DataCollatorForMultipleChoice to be more easily understandable.\n\n* Apply suggested changes and run make fixup.\n\n* fix copies, style and fixup\n\n* add missing documentation\n\n* nits\n\n* fix docstring\n\n* style\n\n* nits\n\n* isort\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "8f137b242762eb9295a431ec6eb8cd9ee673daf9",
    "files": [
        {
            "sha": "0da904f1131a9ecdc7adaead8bbbef0b87393678",
            "filename": "docs/source/en/main_classes/data_collator.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -71,3 +71,6 @@ Examples of use can be found in the [example scripts](../examples) or [example n\n \n [[autodoc]] data.data_collator.DataCollatorWithFlattening\n \n+# DataCollatorForMultipleChoice\n+\n+[[autodoc]] data.data_collator.DataCollatorForMultipleChoice"
        },
        {
            "sha": "0cac42a649859fb7c709c9b9dbaeb4f30d8f2b82",
            "filename": "docs/source/en/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 5,
            "deletions": 90,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmultiple_choice.md?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -109,99 +109,14 @@ The preprocessing function you want to create needs to:\n To apply the preprocessing function over the entire dataset, use ğŸ¤— Datasets [`~datasets.Dataset.map`] method. You can speed up the `map` function by setting `batched=True` to process multiple elements of the dataset at once:\n \n ```py\n-tokenized_swag = swag.map(preprocess_function, batched=True)\n+>>> tokenized_swag = swag.map(preprocess_function, batched=True)\n ```\n \n-ğŸ¤— Transformers doesn't have a data collator for multiple choice, so you'll need to adapt the [`DataCollatorWithPadding`] to create a batch of examples. It's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length.\n-\n-`DataCollatorForMultipleChoice` flattens all the model inputs, applies padding, and then unflattens the results:\n-\n-<frameworkcontent>\n-<pt>\n-```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import torch\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"pt\",\n-...         )\n-\n-...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-...         return batch\n-```\n-</pt>\n-<tf>\n+To create a batch of examples, it's more efficient to *dynamically pad* the sentences to the longest length in a batch during collation, instead of padding the whole dataset to the maximum length. [`DataCollatorForMultipleChoice`] flattens all the model inputs, applies padding, and then unflattens the results.\n ```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import tensorflow as tf\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"tf\",\n-...         )\n-\n-...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-...         return batch\n+>>> from transformers import DataCollatorForMultipleChoice\n+>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n ```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -271,7 +186,7 @@ At this point, only three steps remain:\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n ...     processing_class=tokenizer,\n-...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n+...     data_collator=collator,\n ...     compute_metrics=compute_metrics,\n ... )\n "
        },
        {
            "sha": "7f44479ad24d7e67d3c74e13a027821513c16676",
            "filename": "docs/source/es/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 5,
            "deletions": 90,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -91,99 +91,14 @@ Usa la funciÃ³n [`~datasets.Dataset.map`] de ğŸ¤— Datasets para aplicarle la fun\n tokenized_swag = swag.map(preprocess_function, batched=True)\n ```\n \n-ğŸ¤— Transformers no tiene un collator de datos para la tarea de selecciÃ³n mÃºltiple, asÃ­ que tendrÃ­as que crear uno. Puedes adaptar el [`DataCollatorWithPadding`] para crear un lote de ejemplos para selecciÃ³n mÃºltiple. Este tambiÃ©n\n-le *aÃ±adirÃ¡ relleno de manera dinÃ¡mica* a tu texto y a las etiquetas para que tengan la longitud del elemento mÃ¡s largo en su lote, de forma que tengan una longitud uniforme. Aunque es posible rellenar el texto en la funciÃ³n `tokenizer` haciendo\n+Para crear un lote de ejemplos para selecciÃ³n mÃºltiple, este tambiÃ©n le *aÃ±adirÃ¡ relleno de manera dinÃ¡mica* a tu texto y a las etiquetas para que tengan la longitud del elemento mÃ¡s largo en su lote, de forma que tengan una longitud uniforme. Aunque es posible rellenar el texto en la funciÃ³n `tokenizer` haciendo\n `padding=True`, el rellenado dinÃ¡mico es mÃ¡s eficiente.\n \n-El `DataCollatorForMultipleChoice` aplanarÃ¡ todas las entradas del modelo, les aplicarÃ¡ relleno y luego des-aplanarÃ¡ los resultados:\n-\n-<frameworkcontent>\n-<pt>\n+El [`DataCollatorForMultipleChoice`] aplanarÃ¡ todas las entradas del modelo, les aplicarÃ¡ relleno y luego des-aplanarÃ¡ los resultados.\n ```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import torch\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Collator de datos que le aÃ±adirÃ¡ relleno de forma automÃ¡tica a las entradas recibidas para\n-...     una tarea de selecciÃ³n mÃºltiple.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"pt\",\n-...         )\n-\n-...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-...         return batch\n+>>> from transformers import DataCollatorForMultipleChoice\n+>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n ```\n-</pt>\n-<tf>\n-```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import tensorflow as tf\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"tf\",\n-...         )\n-\n-...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-...         return batch\n-```\n-</tf>\n-</frameworkcontent>\n \n ## Entrenamiento\n \n@@ -226,7 +141,7 @@ En este punto, solo quedan tres pasos:\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n ...     processing_class=tokenizer,\n-...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n+...     data_collator=collator,\n ... )\n \n >>> trainer.train()"
        },
        {
            "sha": "ab4f2329f3b995974577afc1684ebe0249aa9a70",
            "filename": "docs/source/ja/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 4,
            "deletions": 89,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -113,96 +113,11 @@ pip install transformers datasets evaluate\n tokenized_swag = swag.map(preprocess_function, batched=True)\n ```\n \n-ğŸ¤— Transformers ã«ã¯å¤šè‚¢é¸æŠç”¨ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ãŒãªã„ãŸã‚ã€[`DataCollatâ€‹â€‹orWithPadding`] ã‚’èª¿æ•´ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’æœ€å¤§é•·ã¾ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹ã®ã§ã¯ãªãã€ç…§åˆä¸­ã«ãƒãƒƒãƒå†…ã®æœ€é•·ã®é•·ã•ã¾ã§æ–‡ã‚’ *å‹•çš„ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°* ã™ã‚‹æ–¹ãŒåŠ¹ç‡çš„ã§ã™ã€‚\n-\n-`DataCollatâ€‹â€‹orForMultipleChoice` ã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã‚’å¹³å¦åŒ–ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã€çµæœã‚’éå¹³å¦åŒ–ã—ã¾ã™ã€‚\n-\n-<frameworkcontent>\n-<pt>\n-```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import torch\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"pt\",\n-...         )\n-\n-...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-...         return batch\n-```\n-</pt>\n-<tf>\n+[`DataCollatorForMultipleChoice`] ã¯ã€ã™ã¹ã¦ã®ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã‚’å¹³å¦åŒ–ã—ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’é©ç”¨ã—ã¦ã€çµæœã‚’éå¹³å¦åŒ–ã—ã¾ã™ã€‚\n ```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import tensorflow as tf\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"tf\",\n-...         )\n-\n-...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-...         return batch\n+>>> from transformers import DataCollatorForMultipleChoice\n+>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n ```\n-</tf>\n-</frameworkcontent>\n \n ## Evaluate\n \n@@ -272,7 +187,7 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n ...     processing_class=tokenizer,\n-...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n+...     data_collator=collator,\n ...     compute_metrics=compute_metrics,\n ... )\n "
        },
        {
            "sha": "e0888f4a0b6de15fdd7afafea07194a91ce67424",
            "filename": "docs/source/ko/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 4,
            "deletions": 89,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -112,96 +112,11 @@ pip install transformers datasets evaluate\n tokenized_swag = swag.map(preprocess_function, batched=True)\n ```\n \n-ğŸ¤— Transformersì—ëŠ” ê°ê´€ì‹ìš© ë°ì´í„° ì½œë ˆì´í„°ê°€ ì—†ìœ¼ë¯€ë¡œ ì˜ˆì œ ë°°ì¹˜ë¥¼ ë§Œë“¤ë ¤ë©´ [`DataCollatorWithPadding`]ì„ ì¡°ì •í•´ì•¼ í•©ë‹ˆë‹¤. ë°ì´í„° ì •ë ¬ ì¤‘ì— ì „ì²´ ë°ì´í„° ì§‘í•©ì„ ìµœëŒ€ ê¸¸ì´ë¡œ íŒ¨ë”©í•˜ëŠ” ëŒ€ì‹  ë°°ì¹˜ ì¤‘ ê°€ì¥ ê¸´ ê¸¸ì´ë¡œ ë¬¸ì¥ì„ *ë™ì  íŒ¨ë”©*í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì…ë‹ˆë‹¤.\n-\n-`DataCollatorForMultipleChoice`ëŠ” ëª¨ë“  ëª¨ë¸ ì…ë ¥ì„ í‰íƒ„í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•˜ë©° ê·¸ ê²°ê³¼ë¥¼ ê²°ê³¼ë¥¼ ë‹¤ì°¨ì›í™”í•©ë‹ˆë‹¤:\n-\n-<frameworkcontent>\n-<pt>\n-```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import torch\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"pt\",\n-...         )\n-\n-...         batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-...         batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-...         return batch\n-```\n-</pt>\n-<tf>\n+[`DataCollatorForMultipleChoice`]ëŠ” ëª¨ë“  ëª¨ë¸ ì…ë ¥ì„ í‰íƒ„í™”í•˜ê³  íŒ¨ë”©ì„ ì ìš©í•˜ë©° ê·¸ ê²°ê³¼ë¥¼ ê²°ê³¼ë¥¼ ë‹¤ì°¨ì›í™”í•©ë‹ˆë‹¤:\n ```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import tensorflow as tf\n-\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"tf\",\n-...         )\n-\n-...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-...         return batch\n+>>> from transformers import DataCollatorForMultipleChoice\n+>>> collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n ```\n-</tf>\n-</frameworkcontent>\n \n ## í‰ê°€ í•˜ê¸°[[evaluate]]\n \n@@ -271,7 +186,7 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n ...     train_dataset=tokenized_swag[\"train\"],\n ...     eval_dataset=tokenized_swag[\"validation\"],\n ...     processing_class=tokenizer,\n-...     data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n+...     data_collator=collator,\n ...     compute_metrics=compute_metrics,\n ... )\n "
        },
        {
            "sha": "cc632480fbdd05484a23fd0073e3a0f0d089c17c",
            "filename": "examples/pytorch/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 6,
            "deletions": 62,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -23,27 +23,26 @@\n import sys\n from dataclasses import dataclass, field\n from itertools import chain\n-from typing import Optional, Union\n+from typing import Optional\n \n import datasets\n import numpy as np\n-import torch\n from datasets import load_dataset\n \n import transformers\n from transformers import (\n     AutoConfig,\n     AutoModelForMultipleChoice,\n     AutoTokenizer,\n+    DataCollatorForMultipleChoice,\n     HfArgumentParser,\n     Trainer,\n     TrainingArguments,\n     default_data_collator,\n     set_seed,\n )\n-from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n from transformers.trainer_utils import get_last_checkpoint\n-from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry\n+from transformers.utils import check_min_version, send_example_telemetry\n \n \n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n@@ -165,63 +164,6 @@ def __post_init__(self):\n             assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n \n \n-@dataclass\n-class DataCollatorForMultipleChoice:\n-    \"\"\"\n-    Data collator that will dynamically pad the inputs for multiple choice received.\n-\n-    Args:\n-        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n-            The tokenizer used for encoding the data.\n-        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n-            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n-            among:\n-\n-            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n-              if provided).\n-            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-              acceptable input length for the model if that argument is not provided.\n-            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-              lengths).\n-        max_length (`int`, *optional*):\n-            Maximum length of the returned list and optionally padding length (see above).\n-        pad_to_multiple_of (`int`, *optional*):\n-            If set will pad the sequence to a multiple of the provided value.\n-\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n-\n-    def __call__(self, features):\n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature.pop(label_name) for feature in features]\n-        batch_size = len(features)\n-        num_choices = len(features[0][\"input_ids\"])\n-        flattened_features = [\n-            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-        ]\n-        flattened_features = list(chain(*flattened_features))\n-\n-        batch = self.tokenizer.pad(\n-            flattened_features,\n-            padding=self.padding,\n-            max_length=self.max_length,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            return_tensors=\"pt\",\n-        )\n-\n-        # Un-flatten\n-        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-        # Add back labels\n-        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-        return batch\n-\n-\n def main():\n     # See all possible arguments in src/transformers/training_args.py\n     # or by passing the --help flag to this script.\n@@ -425,7 +367,9 @@ def preprocess_function(examples):\n     data_collator = (\n         default_data_collator\n         if data_args.pad_to_max_length\n-        else DataCollatorForMultipleChoice(tokenizer=tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None)\n+        else DataCollatorForMultipleChoice(\n+            tokenizer=tokenizer, pad_to_multiple_of=8 if training_args.fp16 else None, return_tensors=\"pt\"\n+        )\n     )\n \n     # Metric"
        },
        {
            "sha": "4119342163d6527b5d0cb835f21a9cd291486d04",
            "filename": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 62,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -24,10 +24,8 @@\n import math\n import os\n import random\n-from dataclasses import dataclass\n from itertools import chain\n from pathlib import Path\n-from typing import Optional, Union\n \n import datasets\n import evaluate\n@@ -47,12 +45,12 @@\n     AutoConfig,\n     AutoModelForMultipleChoice,\n     AutoTokenizer,\n-    PreTrainedTokenizerBase,\n+    DataCollatorForMultipleChoice,\n     SchedulerType,\n     default_data_collator,\n     get_scheduler,\n )\n-from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry\n+from transformers.utils import check_min_version, send_example_telemetry\n \n \n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n@@ -226,63 +224,6 @@ def parse_args():\n     return args\n \n \n-@dataclass\n-class DataCollatorForMultipleChoice:\n-    \"\"\"\n-    Data collator that will dynamically pad the inputs for multiple choice received.\n-\n-    Args:\n-        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n-            The tokenizer used for encoding the data.\n-        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n-            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n-            among:\n-\n-            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n-              if provided).\n-            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-              acceptable input length for the model if that argument is not provided.\n-            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-              lengths).\n-        max_length (`int`, *optional*):\n-            Maximum length of the returned list and optionally padding length (see above).\n-        pad_to_multiple_of (`int`, *optional*):\n-            If set will pad the sequence to a multiple of the provided value.\n-\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n-\n-    def __call__(self, features):\n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature.pop(label_name) for feature in features]\n-        batch_size = len(features)\n-        num_choices = len(features[0][\"input_ids\"])\n-        flattened_features = [\n-            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-        ]\n-        flattened_features = list(chain(*flattened_features))\n-\n-        batch = self.tokenizer.pad(\n-            flattened_features,\n-            padding=self.padding,\n-            max_length=self.max_length,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            return_tensors=\"pt\",\n-        )\n-\n-        # Un-flatten\n-        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n-        # Add back labels\n-        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n-        return batch\n-\n-\n def main():\n     args = parse_args()\n \n@@ -480,7 +421,9 @@ def preprocess_function(examples):\n             pad_to_multiple_of = 8\n         else:\n             pad_to_multiple_of = None\n-        data_collator = DataCollatorForMultipleChoice(tokenizer, pad_to_multiple_of=pad_to_multiple_of)\n+        data_collator = DataCollatorForMultipleChoice(\n+            tokenizer, pad_to_multiple_of=pad_to_multiple_of, return_tensors=\"pt\"\n+        )\n \n     train_dataloader = DataLoader(\n         train_dataset, shuffle=True, collate_fn=data_collator, batch_size=args.per_device_train_batch_size"
        },
        {
            "sha": "576a4b7631cbabd9c7b058df265beb80fa709925",
            "filename": "examples/research_projects/lxmert/demo.ipynb",
            "status": "modified",
            "additions": 6,
            "deletions": 9,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Flxmert%2Fdemo.ipynb?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -23,21 +23,18 @@\n     }\n    ],\n    \"source\": [\n-    \"from IPython.display import clear_output, Image, display\\n\",\n-    \"import PIL.Image\\n\",\n     \"import io\\n\",\n-    \"import json\\n\",\n-    \"import torch\\n\",\n+    \"\\n\",\n     \"import numpy as np\\n\",\n+    \"import PIL.Image\\n\",\n+    \"from IPython.display import Image, display\\n\",\n+    \"from modeling_frcnn import GeneralizedRCNN\\n\",\n     \"from processing_image import Preprocess\\n\",\n     \"from visualizing_image import SingleImageViz\\n\",\n-    \"from modeling_frcnn import GeneralizedRCNN\\n\",\n-    \"from utils import Config\\n\",\n+    \"\\n\",\n     \"import utils\\n\",\n     \"from transformers import LxmertForQuestionAnswering, LxmertTokenizer\\n\",\n-    \"import wget\\n\",\n-    \"import pickle\\n\",\n-    \"import os\\n\",\n+    \"from utils import Config\\n\",\n     \"\\n\",\n     \"\\n\",\n     \"# URL = \\\"https://raw.githubusercontent.com/airsplay/py-bottom-up-attention/master/demo/data/images/input.jpg\\\",\\n\","
        },
        {
            "sha": "e159549a105c256879d996cce1d4f4697ff0e1b4",
            "filename": "examples/research_projects/movement-pruning/Saving_PruneBERT.ipynb",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fmovement-pruning%2FSaving_PruneBERT.ipynb?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -31,19 +31,19 @@\n    \"source\": [\n     \"# Includes\\n\",\n     \"\\n\",\n-    \"import h5py\\n\",\n-    \"import os\\n\",\n     \"import json\\n\",\n+    \"import os\\n\",\n     \"from collections import OrderedDict\\n\",\n     \"\\n\",\n-    \"from scipy import sparse\\n\",\n+    \"import h5py\\n\",\n     \"import numpy as np\\n\",\n-    \"\\n\",\n     \"import torch\\n\",\n+    \"from scipy import sparse\\n\",\n     \"from torch import nn\\n\",\n     \"\\n\",\n     \"from transformers import *\\n\",\n     \"\\n\",\n+    \"\\n\",\n     \"os.chdir(\\\"../../\\\")\"\n    ]\n   },"
        },
        {
            "sha": "9f61beea8e249c7f9766eb87a99beb4be9e10745",
            "filename": "examples/research_projects/visual_bert/demo.ipynb",
            "status": "modified",
            "additions": 91,
            "deletions": 88,
            "changes": 179,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fresearch_projects%2Fvisual_bert%2Fdemo.ipynb?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9"
        },
        {
            "sha": "4c9a3ad789135b338ff88d4ed6c141eb3aed73b2",
            "filename": "examples/tensorflow/multiple-choice/run_swag.py",
            "status": "modified",
            "additions": 4,
            "deletions": 68,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -25,7 +25,7 @@\n from dataclasses import dataclass, field\n from itertools import chain\n from pathlib import Path\n-from typing import Optional, Union\n+from typing import Optional\n \n import datasets\n import tensorflow as tf\n@@ -37,6 +37,7 @@\n     TF2_WEIGHTS_NAME,\n     AutoConfig,\n     AutoTokenizer,\n+    DataCollatorForMultipleChoice,\n     DefaultDataCollator,\n     HfArgumentParser,\n     PushToHubCallback,\n@@ -45,8 +46,7 @@\n     create_optimizer,\n     set_seed,\n )\n-from transformers.tokenization_utils_base import PreTrainedTokenizerBase\n-from transformers.utils import PaddingStrategy, check_min_version, send_example_telemetry\n+from transformers.utils import check_min_version, send_example_telemetry\n \n \n # Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n@@ -55,69 +55,6 @@\n logger = logging.getLogger(__name__)\n \n \n-# region Helper classes and functions\n-\n-\n-@dataclass\n-class DataCollatorForMultipleChoice:\n-    \"\"\"\n-    Data collator that will dynamically pad the inputs for multiple choice received.\n-\n-    Args:\n-        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n-            The tokenizer used for encoding the data.\n-        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n-            Select a strategy to pad the returned sequences (according to the model's padding side and padding index)\n-            among:\n-\n-            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n-              if provided).\n-            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-              acceptable input length for the model if that argument is not provided.\n-            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-              lengths).\n-        max_length (`int`, *optional*):\n-            Maximum length of the returned list and optionally padding length (see above).\n-        pad_to_multiple_of (`int`, *optional*):\n-            If set will pad the sequence to a multiple of the provided value.\n-\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    padding: Union[bool, str, PaddingStrategy] = True\n-    max_length: Optional[int] = None\n-    pad_to_multiple_of: Optional[int] = None\n-\n-    def __call__(self, features):\n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature.pop(label_name) for feature in features]\n-        batch_size = len(features)\n-        num_choices = len(features[0][\"input_ids\"])\n-        flattened_features = [\n-            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-        ]\n-        flattened_features = list(chain(*flattened_features))\n-\n-        batch = self.tokenizer.pad(\n-            flattened_features,\n-            padding=self.padding,\n-            max_length=self.max_length,\n-            pad_to_multiple_of=self.pad_to_multiple_of,\n-            return_tensors=\"np\",\n-        )\n-\n-        # Un-flatten\n-        batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-        # Add back labels\n-        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-        return batch\n-\n-\n-# endregion\n-\n-\n # region Arguments\n @dataclass\n class ModelArguments:\n@@ -424,8 +361,7 @@ def preprocess_function(examples):\n     if data_args.pad_to_max_length:\n         data_collator = DefaultDataCollator(return_tensors=\"np\")\n     else:\n-        # custom class defined above, as HF has no data collator for multiple choice\n-        data_collator = DataCollatorForMultipleChoice(tokenizer)\n+        data_collator = DataCollatorForMultipleChoice(tokenizer, return_tensors=\"tf\")\n     # endregion\n \n     with training_args.strategy.scope():"
        },
        {
            "sha": "f4ebc44437f4580c5e0c8b0da22ec8b8ea04d239",
            "filename": "examples/training/distributed_training.py",
            "status": "added",
            "additions": 113,
            "deletions": 0,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Ftraining%2Fdistributed_training.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/examples%2Ftraining%2Fdistributed_training.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftraining%2Fdistributed_training.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -0,0 +1,113 @@\n+import argparse\n+import os\n+\n+import torch\n+import torch.distributed as dist\n+\n+\n+# Environment variables set by torch.distributed.launch\n+LOCAL_RANK = int(os.environ[\"LOCAL_RANK\"])\n+WORLD_SIZE = int(os.environ[\"WORLD_SIZE\"])\n+WORLD_RANK = int(os.environ[\"RANK\"])\n+\n+LOCAL_RANK = int(os.environ[\"OMPI_COMM_WORLD_LOCAL_RANK\"])\n+WORLD_SIZE = int(os.environ[\"OMPI_COMM_WORLD_SIZE\"])\n+WORLD_RANK = int(os.environ[\"OMPI_COMM_WORLD_RANK\"])\n+\n+\n+def run(backend):\n+    tensor = torch.zeros(1)\n+    # Need to put tensor on a GPU device for nccl backend\n+    if backend == \"nccl\":\n+        device = torch.device(\"cuda:{}\".format(LOCAL_RANK))\n+        tensor = tensor.to(device)\n+\n+    if WORLD_RANK == 0:\n+        for rank_recv in range(1, WORLD_SIZE):\n+            dist.send(tensor=tensor, dst=rank_recv)\n+            print(\"worker_{} sent data to Rank {}\\n\".format(0, rank_recv))\n+    else:\n+        dist.recv(tensor=tensor, src=0)\n+        print(\"worker_{} has received data from rank {}\\n\".format(WORLD_RANK, 0))\n+\n+\n+def init_processes(backend):\n+    dist.init_process_group(backend, rank=WORLD_RANK, world_size=WORLD_SIZE)\n+    run(backend)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--local_rank\", type=int, help=\"Local rank. Necessary for using the torch.distributed.launch utility.\"\n+    )\n+    parser.add_argument(\"--backend\", type=str, default=\"nccl\", choices=[\"nccl\", \"gloo\"])\n+    args = parser.parse_args()\n+\n+    init_processes(backend=args.backend)\n+\n+\"\"\"\"\n+python-m torch.distributed.launch \\\n+--nproc_per_node=2 --nnodes=2 --node_rank=0 \\\n+test_compile.py\n+\n+python3 -m torch.distributed.launch \\\n+--nproc_per_node=2 --nnodes=2 --node_rank=1 \\\n+--master_addr=104.171.200.62 --master_port=1234 \\\n+main.py \\\n+--backend=nccl --use_syn --batch_size=8192 --arch=resnet152\n+\n+\n+\n+mpirun -np 4 \\\n+-H 104.171.200.62:2,104.171.200.182:2 \\\n+-x MASTER_ADDR=104.171.200.62 \\\n+-x MASTER_PORT=1234 \\\n+-x PATH \\\n+-bind-to none -map-by slot \\\n+-mca pml ob1 -mca btl ^openib \\\n+python3 main.py\n+\"\"\"\n+\n+\n+\"\"\"\"\n+You need a host file with the name of hosts.\n+for example I have arthur@ip-26-0-162-46 and arthur@ip-26-0-162-239\n+\n+________\n+hostfile\n+ip-26-0-162-46 slots=8\n+ip-26-0-162-239 slots=8\n+________\n+\n+mpirun --hostfile hostfile -np 16 \\\n+    --bind-to none --map-by slot \\\n+    -x MASTER_ADDR=<master-node-ip> \\\n+    -x MASTER_PORT=29500 \\\n+    -x NCCL_DEBUG=INFO \\\n+    -x NCCL_SOCKET_IFNAME=^lo,docker0 \\\n+    -x CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\n+    python your_script.py --backend nccl\n+\n+\n+to get the master IP you need to do a few things:\n+hostname -I | awk '{print $1}'\n+\n+\n+Use `ping ip-26-0-162-46` to check if connected\n+\n+26.0.162.46\n+\n+mpirun --hostfile hostfile -np 16 \\\n+    --bind-to none --map-by slot \\\n+    -x MASTER_ADDR=26.0.162.46 \\\n+    -x MASTER_PORT=29500 \\\n+    -x NCCL_DEBUG=INFO \\\n+    -x NCCL_SOCKET_IFNAME=^lo,docker0 \\\n+    -x CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\n+    python your_script.py --backend nccl\n+\n+\n+mpirun --hostfile hostfile -np 2     -x NCCL_DEBUG=INFO     python -c \"import os;print(os.environ['OMPI_COMM_WORLD_LOCAL_RANK'])\" -b 8 -e 128M -f 2 -g 1\n+to test your setup\n+\"\"\""
        },
        {
            "sha": "bf54fb23db6d4507dc58d354911a4e82ec5246c5",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -101,6 +101,7 @@\n     \"data.data_collator\": [\n         \"DataCollator\",\n         \"DataCollatorForLanguageModeling\",\n+        \"DataCollatorForMultipleChoice\",\n         \"DataCollatorForPermutationLanguageModeling\",\n         \"DataCollatorForSeq2Seq\",\n         \"DataCollatorForSOP\",\n@@ -5187,6 +5188,7 @@\n     from .data.data_collator import (\n         DataCollator,\n         DataCollatorForLanguageModeling,\n+        DataCollatorForMultipleChoice,\n         DataCollatorForPermutationLanguageModeling,\n         DataCollatorForSeq2Seq,\n         DataCollatorForSOP,"
        },
        {
            "sha": "1aa9ecf32a52d10972cb4b2b76578a4318e62411",
            "filename": "src/transformers/agents/tools.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fagents%2Ftools.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fagents%2Ftools.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Ftools.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -609,7 +609,7 @@ def compile_jinja_template(template):\n         raise ImportError(\"template requires jinja2 to be installed.\")\n \n     if version.parse(jinja2.__version__) < version.parse(\"3.1.0\"):\n-        raise ImportError(\"template requires jinja2>=3.1.0 to be installed. Your version is \" f\"{jinja2.__version__}.\")\n+        raise ImportError(f\"template requires jinja2>=3.1.0 to be installed. Your version is {jinja2.__version__}.\")\n \n     def raise_exception(message):\n         raise TemplateError(message)"
        },
        {
            "sha": "188f8ea35ef4f1d7874d2c6e36d9daf70aef6abf",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -751,7 +751,7 @@ def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n             id2label = kwargs[\"id2label\"] if kwargs[\"id2label\"] is not None else []\n             if len(id2label) != num_labels:\n                 raise ValueError(\n-                    f\"You passed along `num_labels={num_labels }` with an incompatible id to label map: \"\n+                    f\"You passed along `num_labels={num_labels}` with an incompatible id to label map: \"\n                     f\"{kwargs['id2label']}. Since those arguments are inconsistent with each other, you should remove \"\n                     \"one of them.\"\n                 )"
        },
        {
            "sha": "1e06b7ca3901a8140ab7b92fd79b259aa61d3aa0",
            "filename": "src/transformers/data/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fdata%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fdata%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2F__init__.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -14,6 +14,7 @@\n \n from .data_collator import (\n     DataCollatorForLanguageModeling,\n+    DataCollatorForMultipleChoice,\n     DataCollatorForPermutationLanguageModeling,\n     DataCollatorForSeq2Seq,\n     DataCollatorForSOP,"
        },
        {
            "sha": "4af7d609f03dc6484fb85b8173185b8ecfd1fc99",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 97,
            "deletions": 6,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -532,12 +532,95 @@ def _numpy_collate_batch(examples, tokenizer, pad_to_multiple_of: Optional[int]\n     return result\n \n \n-def tolist(x):\n-    if isinstance(x, list):\n-        return x\n-    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n-        x = x.numpy()\n-    return x.tolist()\n+@dataclass\n+class DataCollatorForMultipleChoice(DataCollatorMixin):\n+    \"\"\"\n+    Data collator that dynamically pads a batch of nested examples for multiple choice, so that all choices\n+    of all examples have the same length.\n+\n+    Args:\n+        tokenizer ([`PreTrainedTokenizer`] or [`PreTrainedTokenizerFast`]):\n+            The tokenizer used for encoding the data.\n+        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `True`):\n+            Select a strategy to pad the returned sequences according to the model's padding side and padding index\n+            among:\n+\n+            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single sequence\n+              is provided).\n+            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+              acceptable input length for the model if that argument is not provided.\n+            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+              lengths).\n+        max_length (`int`, *optional*):\n+            Maximum length of the returned list and optionally padding length (see above).\n+        pad_to_multiple_of (`int`, *optional*):\n+            Pad the sequence to a multiple of the provided value.\n+\n+            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n+            7.5 (Volta).\n+        return_tensors (`str`, *optional*, defaults to `\"pt\"`):\n+            The type of Tensor to return. Allowable values are \"np\", \"pt\" and \"tf\".\n+    \"\"\"\n+\n+    tokenizer: PreTrainedTokenizerBase\n+    padding: Union[bool, str, PaddingStrategy] = True\n+    max_length: Optional[int] = None\n+    pad_to_multiple_of: Optional[int] = None\n+    return_tensors: str = \"pt\"\n+\n+    def torch_call(self, examples: List[Dict[str, Any]]):  # Refactored implementation from the docs.\n+        import torch\n+\n+        # Take labels out of the examples beforehand, because they aren't nested.\n+        label_name = \"label\" if \"label\" in examples[0].keys() else \"labels\"\n+        labels = [example.pop(label_name) for example in examples]\n+\n+        batch_size = len(examples)\n+        num_choices = len(examples[0][\"input_ids\"])\n+\n+        # Go from e.g. 2 examples of 2 choices [{input_ids: [[1], [2]]}, {input_ids: [[3], [4]]}]\n+        # to 4 examples [{input_ids: [1]}, {input_ids: [2]}] + [{input_ids: [3]}, {input_ids: [4]}]\n+        flat_examples = sum(\n+            ([{k: v[i] for k, v in example.items()} for i in range(num_choices)] for example in examples), start=[]\n+        )\n+\n+        # Pad all choices of all examples as if you're padding any other batch of examples.\n+        batch = self.tokenizer.pad(\n+            flat_examples,\n+            padding=self.padding,\n+            max_length=self.max_length,\n+            pad_to_multiple_of=self.pad_to_multiple_of,\n+            return_tensors=\"pt\",\n+        )\n+\n+        # Reshape from B*C x L into B x C x L, and add the labels back in.\n+        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n+        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n+        return batch\n+\n+    def tf_call(self, features):  # Implementation taken from the docs.\n+        import tensorflow as tf\n+\n+        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n+        labels = [feature.pop(label_name) for feature in features]\n+        batch_size = len(features)\n+        num_choices = len(features[0][\"input_ids\"])\n+        flattened_features = [\n+            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n+        ]\n+        flattened_features = sum(flattened_features, [])  # Sometimes written as list(chain(*flattened_features))\n+\n+        batch = self.tokenizer.pad(\n+            flattened_features,\n+            padding=self.padding,\n+            max_length=self.max_length,\n+            pad_to_multiple_of=self.pad_to_multiple_of,\n+            return_tensors=\"tf\",\n+        )\n+\n+        batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n+        batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n+        return batch\n \n \n @dataclass\n@@ -1268,6 +1351,14 @@ def numpy_mask_tokens(self, inputs: Any, mask_labels: Any) -> Tuple[Any, Any]:\n         return inputs, labels\n \n \n+def tolist(x):\n+    if isinstance(x, list):\n+        return x\n+    elif hasattr(x, \"numpy\"):  # Checks for TF tensors without needing the import\n+        x = x.numpy()\n+    return x.tolist()\n+\n+\n @dataclass\n class DataCollatorForSOP(DataCollatorForLanguageModeling):\n     \"\"\""
        },
        {
            "sha": "6f200bc355933bcb9ec119e08b560241fc1433c5",
            "filename": "src/transformers/integrations/tiktoken.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fintegrations%2Ftiktoken.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftiktoken.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -35,9 +35,7 @@ def convert_tiktoken_to_fast(encoding: Any, output_dir: str):\n \n         dump_tiktoken_bpe(encoding._mergeable_ranks, save_file_absolute)\n     except ImportError:\n-        raise ValueError(\n-            \"`tiktoken` is required to save a `tiktoken` file. Install it with \" \"`pip install tiktoken`.\"\n-        )\n+        raise ValueError(\"`tiktoken` is required to save a `tiktoken` file. Install it with `pip install tiktoken`.\")\n \n     tokenizer = TikTokenConverter(\n         vocab_file=save_file_absolute, pattern=encoding._pat_str, additional_special_tokens=encoding._special_tokens"
        },
        {
            "sha": "0f642c5e8e8aec83b63119b6581893a14a707c86",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -270,7 +270,7 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n         self.config = config\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n-                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n+                f\"The hidden size {(config.hidden_size,)} is not a multiple of the number of attention \"\n                 f\"heads {config.num_attention_heads}.\"\n             )\n "
        },
        {
            "sha": "1b6834a5179754a05b4de64d538603e83383e8ff",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -271,7 +271,7 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n         self.config = config\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n             raise ValueError(\n-                f\"The hidden size {config.hidden_size,} is not a multiple of the number of attention \"\n+                f\"The hidden size {(config.hidden_size,)} is not a multiple of the number of attention \"\n                 f\"heads {config.num_attention_heads}.\"\n             )\n "
        },
        {
            "sha": "7413f985285d845d964781cd3640e4bb59e31930",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -183,7 +183,7 @@ def _rope_scaling_validation(self):\n \n         if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n+                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n             )\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_factor = self.rope_scaling.get(\"factor\", None)"
        },
        {
            "sha": "3d71c1b96fd6d659519eb715536297aeccc1ad26",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -166,7 +166,7 @@ def _rope_scaling_validation(self):\n \n         if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n             raise ValueError(\n-                \"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, \" f\"got {self.rope_scaling}\"\n+                f\"`rope_scaling` must be a dictionary with two fields, `type` and `factor`, got {self.rope_scaling}\"\n             )\n         rope_scaling_type = self.rope_scaling.get(\"type\", None)\n         rope_scaling_factor = self.rope_scaling.get(\"factor\", None)"
        },
        {
            "sha": "c71619e4e8f96737c68b5d985387db44ea5fb02c",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -2195,7 +2195,7 @@ def pytest_terminal_summary_main(tr, id):\n             f.write(\"slowest durations\\n\")\n             for i, rep in enumerate(dlist):\n                 if rep.duration < durations_min:\n-                    f.write(f\"{len(dlist)-i} durations < {durations_min} secs were omitted\")\n+                    f.write(f\"{len(dlist) - i} durations < {durations_min} secs were omitted\")\n                     break\n                 f.write(f\"{rep.duration:02.2f}s {rep.when:<8} {rep.nodeid}\\n\")\n \n@@ -2580,7 +2580,7 @@ def run_test_in_subprocess(test_case, target_func, inputs=None, timeout=None):\n     process.join(timeout=timeout)\n \n     if results[\"error\"] is not None:\n-        test_case.fail(f'{results[\"error\"]}')\n+        test_case.fail(f\"{results['error']}\")\n \n \n def run_test_using_subprocess(func):"
        },
        {
            "sha": "2966c213b481dd39d465284163b778f8eed44c38",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -182,7 +182,7 @@ def create_and_check_for_fov(self, config, pixel_values, labels):\n         model_name = model.__class__.__name__\n         self.parent.assertTrue(\n             diff <= 1e-03,\n-            msg=(f\"Batched and Single row outputs are not equal in {model_name} for fov. \" f\"Difference={diff}.\"),\n+            msg=(f\"Batched and Single row outputs are not equal in {model_name} for fov. Difference={diff}.\"),\n         )\n \n     def prepare_config_and_inputs_for_common(self):"
        },
        {
            "sha": "5e910bc79bd40471321b6c664c510f9607df2472",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f137b242762eb9295a431ec6eb8cd9ee673daf9/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f137b242762eb9295a431ec6eb8cd9ee673daf9/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=8f137b242762eb9295a431ec6eb8cd9ee673daf9",
            "patch": "@@ -687,7 +687,7 @@ def convert_to_sharded_checkpoint(self, folder, save_safe=True, load_safe=True):\n         keys = list(state_dict.keys())\n \n         shard_files = [\n-            shard_name.replace(f\".{extension}\", f\"-{idx+1:05d}-of-{len(keys):05d}.{extension}\")\n+            shard_name.replace(f\".{extension}\", f\"-{idx + 1:05d}-of-{len(keys):05d}.{extension}\")\n             for idx in range(len(keys))\n         ]\n         index = {\"metadata\": {}, \"weight_map\": {key: shard_files[i] for i, key in enumerate(keys)}}"
        }
    ],
    "stats": {
        "total": 1031,
        "additions": 361,
        "deletions": 670
    }
}