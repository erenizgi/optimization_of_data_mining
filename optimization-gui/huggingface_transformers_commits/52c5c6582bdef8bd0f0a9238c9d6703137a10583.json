{
    "author": "LysandreJik",
    "message": "Correctly return finish reason length when finished (#42157)\n\n* Correctly return finish reason length when finished\n\n* Typos + fixup\n\n* Fix a few tests\n\n* Update src/transformers/cli/chat.py\n\nCo-authored-by: Lucain <lucainp@gmail.com>\n\n---------\n\nCo-authored-by: Lucain <lucainp@gmail.com>",
    "sha": "52c5c6582bdef8bd0f0a9238c9d6703137a10583",
    "files": [
        {
            "sha": "b3b6edd5e692d99e6ee711a83c9019e6d35e6bfa",
            "filename": "src/transformers/cli/chat.py",
            "status": "modified",
            "additions": 31,
            "deletions": 5,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/52c5c6582bdef8bd0f0a9238c9d6703137a10583/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52c5c6582bdef8bd0f0a9238c9d6703137a10583/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=52c5c6582bdef8bd0f0a9238c9d6703137a10583",
            "patch": "@@ -19,7 +19,7 @@\n import string\n import time\n from collections.abc import AsyncIterator\n-from typing import Annotated\n+from typing import Annotated, Any, Optional\n \n import click\n import typer\n@@ -103,12 +103,14 @@ def __init__(self, model_id: str, user_id: str):\n         self.model_id = model_id\n         self.user_id = user_id\n \n-    async def stream_output(self, stream: AsyncIterator[ChatCompletionStreamOutput]) -> tuple[str, int]:\n+    async def stream_output(self, stream: AsyncIterator[ChatCompletionStreamOutput]) -> tuple[str, str | Any | None]:\n         self._console.print(f\"[bold blue]<{self.model_id}>:\")\n         with Live(console=self._console, refresh_per_second=4) as live:\n             text = \"\"\n+            finish_reason: str | None = None\n             async for token in await stream:\n                 outputs = token.choices[0].delta.content\n+                finish_reason = getattr(token.choices[0], \"finish_reason\", finish_reason)\n \n                 if not outputs:\n                     continue\n@@ -147,7 +149,7 @@ async def stream_output(self, stream: AsyncIterator[ChatCompletionStreamOutput])\n \n         self._console.print()\n \n-        return text\n+        return text, finish_reason\n \n     def input(self) -> str:\n         \"\"\"Gets user input from the console.\"\"\"\n@@ -169,6 +171,18 @@ def print_color(self, text: str, color: str):\n         self._console.print(f\"[bold {color}]{text}\")\n         self._console.print()\n \n+    def confirm(self, message: str, default: bool = False) -> bool:\n+        \"\"\"Displays a yes/no prompt to the user, returning True for confirmation.\"\"\"\n+        default_hint = \"Y/n\" if default else \"y/N\"\n+        response = self._console.input(f\"[bold yellow]{message} ({default_hint}): \")\n+        self._console.print()\n+\n+        response = response.strip().lower()\n+        if not response:\n+            return default\n+\n+        return response in {\"y\", \"yes\"}\n+\n     def print_help(self, minimal: bool = False):\n         \"\"\"Prints the help message to the console.\"\"\"\n         self._console.print(Markdown(HELP_STRING_MINIMAL if minimal else HELP_STRING))\n@@ -362,9 +376,15 @@ async def _inner_run(self):\n         config = self.config\n \n         async with AsyncInferenceClient(base_url=self.base_url) as client:\n+            pending_user_input: Optional[str] = None\n             while True:\n                 try:\n-                    user_input = interface.input()\n+                    if pending_user_input is not None:\n+                        user_input = pending_user_input\n+                        pending_user_input = None\n+                        interface.print_user_message(user_input)\n+                    else:\n+                        user_input = interface.input()\n \n                     # User commands\n                     if user_input == \"!exit\":\n@@ -448,9 +468,15 @@ async def _inner_run(self):\n                         },\n                     )\n \n-                    model_output = await interface.stream_output(stream)\n+                    model_output, finish_reason = await interface.stream_output(stream)\n \n                     chat.append({\"role\": \"assistant\", \"content\": model_output})\n+\n+                    if finish_reason == \"length\":\n+                        interface.print_color(\"Generation stopped after reaching the token limit.\", \"yellow\")\n+                        if interface.confirm(\"Continue generating?\"):\n+                            pending_user_input = \"Please continue. Do not repeat text.â€\"\n+                            continue\n                 except KeyboardInterrupt:\n                     break\n "
        },
        {
            "sha": "216f86c033a16428a9747a63b99dc07d3c194bbb",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/52c5c6582bdef8bd0f0a9238c9d6703137a10583/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52c5c6582bdef8bd0f0a9238c9d6703137a10583/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=52c5c6582bdef8bd0f0a9238c9d6703137a10583",
            "patch": "@@ -835,11 +835,18 @@ def stream_chat_completion(request_id, decode_stream):\n                 # they come from the assistant.\n                 yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n \n+                n_tokens_generated = 0\n                 for result in self.running_continuous_batching_manager.request_id_iter(request_id):\n+                    n_tokens_generated += 1\n+\n                     if result.status == RequestStatus.FINISHED:\n+                        generated_all_tokens = n_tokens_generated >= generation_config.max_new_tokens\n+                        final_token_is_eos = result == tokenizer.eos_token\n+                        reason = \"length\" if (generated_all_tokens and not final_token_is_eos) else \"stop\"\n+\n                         yield self.build_chat_completion_chunk(\n                             request_id,\n-                            finish_reason=\"stop\",\n+                            finish_reason=reason,\n                             model=model_id_and_revision,\n                         )\n                         break\n@@ -1080,8 +1087,13 @@ def generate_with_cache(**kwargs):\n                 # they come from the assistant.\n                 yield self.build_chat_completion_chunk(request_id, role=\"assistant\", model=model_id_and_revision)\n \n+                result = \"\"\n+                n_tokens_generated = 0\n+\n                 for result in streamer:\n-                    # Temporary hack for GPTOS 3: don't emit the final \"<|return|>\"\n+                    n_tokens_generated += 1\n+\n+                    # Temporary hack for GPT-OSS 3: don't emit the final \"<|return|>\"\n                     if \"gptoss\" in model.config.architectures[0].lower():\n                         result = result.removesuffix(\"<|return|>\")\n                     results += result\n@@ -1170,7 +1182,11 @@ def generate_with_cache(**kwargs):\n                         yield self.build_chat_completion_chunk(\n                             _request_id, content=result, model=model_id_and_revision\n                         )\n-                yield self.build_chat_completion_chunk(_request_id, finish_reason=\"stop\", model=model_id_and_revision)\n+\n+                generated_all_tokens = n_tokens_generated >= generation_config.max_new_tokens\n+                final_token_is_eos = result == streamer.tokenizer.eos_token\n+                reason = \"length\" if (generated_all_tokens and not final_token_is_eos) else \"stop\"\n+                yield self.build_chat_completion_chunk(_request_id, finish_reason=reason, model=model_id_and_revision)\n \n                 thread.join()\n             except Exception as e:"
        },
        {
            "sha": "6b966e607c3aad5f850aee467b54ebe74336c578",
            "filename": "tests/cli/test_serve.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/52c5c6582bdef8bd0f0a9238c9d6703137a10583/tests%2Fcli%2Ftest_serve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52c5c6582bdef8bd0f0a9238c9d6703137a10583/tests%2Fcli%2Ftest_serve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_serve.py?ref=52c5c6582bdef8bd0f0a9238c9d6703137a10583",
            "patch": "@@ -291,10 +291,9 @@ def test_requests(self, test_name: str, request_flags: dict):\n         # NOTE: the output of our server is wrapped by `InferenceClient`, which sends fields even when they\n         # are empty.\n \n-        # Finish reason: the last payload should have a finish reason of \"stop\", all others should be empty\n-        # TODO: we may add other finish reasons in the future, and this may need more logic\n+        # Finish reason: the last payload should have a finish reason of \"length\" or \"stop\", all others should be empty\n         finish_reasons = [payload.choices[0].finish_reason for payload in all_payloads]\n-        self.assertEqual(finish_reasons[-1], \"stop\")\n+        self.assertTrue(finish_reasons[-1] in [\"length\", \"stop\"])\n         self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n \n         # Role: the first payload should have a role of \"assistant\", all others should be empty\n@@ -328,6 +327,18 @@ def test_generation_config_in_request(self):\n         # sets `do_sample=True`\n         self.assertEqual(output_text, '<think>\\nOkay, the user just asked, \"')\n \n+    def test_early_return_due_to_length(self):\n+        request = {\n+            \"model\": \"Qwen/Qwen3-0.6B\",\n+            \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n+            \"stream\": True,\n+            \"max_tokens\": 3,\n+        }\n+\n+        all_payloads = self.run_server(request)\n+        last_payload = all_payloads[-1]\n+        self.assertTrue(last_payload.choices[0][\"finish_reason\"] == \"length\")\n+\n     # TODO: one test for each request flag, to confirm it is working as expected\n     # TODO: speed-based test to confirm that KV cache is working across requests\n \n@@ -549,7 +560,7 @@ def test_tool_call(self):\n         # Finally, the last payload should contain a finish reason\n         finish_reasons = [payload.choices[0].finish_reason for payload in all_payloads]\n         # TODO: I think the finish reason for a tool call is different? double check this\n-        self.assertEqual(finish_reasons[-1], \"stop\")\n+        self.assertTrue(finish_reasons[-1] in [\"stop\", \"length\"])\n         self.assertTrue(all(reason is None for reason in finish_reasons[:-1]))\n \n "
        }
    ],
    "stats": {
        "total": 77,
        "additions": 65,
        "deletions": 12
    }
}