{
    "author": "Avasam",
    "message": "Merge type hints from `microsoft/python-type-stubs` (post dropping support for Python 3.8) (#38335)\n\n* Merge type hints from microsoft/python-type-stubs (post Python 3.8)\n\n* Remove mention of pylance\n\n* Resolved conflict\n\n* Merge type hints from microsoft/python-type-stubs (post Python 3.8)\n\n* Remove mention of pylance\n\n* Resolved conflict\n\n* Update src/transformers/models/auto/configuration_auto.py\n\nCo-authored-by: Avasam <samuel.06@hotmail.com>\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "2872e8bac5cd36e5a369532ebfed06494a27ff33",
    "files": [
        {
            "sha": "2dc3d0b0c0cd5968d4f70532510e8cef2d1c7ca0",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=2872e8bac5cd36e5a369532ebfed06494a27ff33",
            "patch": "@@ -17,8 +17,11 @@\n import copy\n import importlib\n import json\n+import os\n import warnings\n from collections import OrderedDict\n+from collections.abc import Iterator\n+from typing import Any, TypeVar, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n@@ -42,6 +45,9 @@\n \n logger = logging.get_logger(__name__)\n \n+_T = TypeVar(\"_T\")\n+# Tokenizers will depend on packages installed, too much variance and there are no common base or Protocol\n+_LazyAutoMappingValue = tuple[Union[type[Any], None], Union[type[Any], None]]\n \n CLASS_DOCSTRING = \"\"\"\n     This is a generic model class that will be instantiated as one of the model classes of the library when created\n@@ -408,7 +414,7 @@ class _BaseAutoModelClass:\n     # Base class for auto models.\n     _model_mapping = None\n \n-    def __init__(self, *args, **kwargs):\n+    def __init__(self, *args, **kwargs) -> None:\n         raise EnvironmentError(\n             f\"{self.__class__.__name__} is designed to be instantiated \"\n             f\"using the `{self.__class__.__name__}.from_pretrained(pretrained_model_name_or_path)` or \"\n@@ -456,7 +462,7 @@ def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedC\n         return config\n \n     @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], *model_args, **kwargs):\n         config = kwargs.pop(\"config\", None)\n         trust_remote_code = kwargs.get(\"trust_remote_code\", None)\n         kwargs[\"_from_auto\"] = True\n@@ -592,7 +598,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n         )\n \n     @classmethod\n-    def register(cls, config_class, model_class, exist_ok=False):\n+    def register(cls, config_class, model_class, exist_ok=False) -> None:\n         \"\"\"\n         Register a new model for this class.\n \n@@ -650,7 +656,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n         return super().from_pretrained(pretrained_model_name_or_path, *model_args, **kwargs)\n \n \n-def insert_head_doc(docstring, head_doc=\"\"):\n+def insert_head_doc(docstring, head_doc: str = \"\"):\n     if len(head_doc) > 0:\n         return docstring.replace(\n             \"one of the model classes of the library \",\n@@ -661,7 +667,7 @@ def insert_head_doc(docstring, head_doc=\"\"):\n     )\n \n \n-def auto_class_update(cls, checkpoint_for_example=\"google-bert/bert-base-cased\", head_doc=\"\"):\n+def auto_class_update(cls, checkpoint_for_example: str = \"google-bert/bert-base-cased\", head_doc: str = \"\"):\n     # Create a new class with the right name from the base class\n     model_mapping = cls._model_mapping\n     name = cls.__name__\n@@ -759,7 +765,7 @@ def add_generation_mixin_to_remote_model(model_class):\n     return model_class\n \n \n-class _LazyAutoMapping(OrderedDict):\n+class _LazyAutoMapping(OrderedDict[type[PretrainedConfig], _LazyAutoMappingValue]):\n     \"\"\"\n     \" A mapping config to object (model or tokenizer for instance) that will load keys and values when it is accessed.\n \n@@ -768,19 +774,19 @@ class _LazyAutoMapping(OrderedDict):\n         - model_mapping: The map model type to model (or tokenizer) class\n     \"\"\"\n \n-    def __init__(self, config_mapping, model_mapping):\n+    def __init__(self, config_mapping, model_mapping) -> None:\n         self._config_mapping = config_mapping\n         self._reverse_config_mapping = {v: k for k, v in config_mapping.items()}\n         self._model_mapping = model_mapping\n         self._model_mapping._model_mapping = self\n         self._extra_content = {}\n         self._modules = {}\n \n-    def __len__(self):\n+    def __len__(self) -> int:\n         common_keys = set(self._config_mapping.keys()).intersection(self._model_mapping.keys())\n         return len(common_keys) + len(self._extra_content)\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: type[PretrainedConfig]) -> _LazyAutoMappingValue:\n         if key in self._extra_content:\n             return self._extra_content[key]\n         model_type = self._reverse_config_mapping[key.__name__]\n@@ -802,32 +808,32 @@ def _load_attr_from_module(self, model_type, attr):\n             self._modules[module_name] = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n         return getattribute_from_module(self._modules[module_name], attr)\n \n-    def keys(self):\n+    def keys(self) -> list[type[PretrainedConfig]]:\n         mapping_keys = [\n             self._load_attr_from_module(key, name)\n             for key, name in self._config_mapping.items()\n             if key in self._model_mapping.keys()\n         ]\n         return mapping_keys + list(self._extra_content.keys())\n \n-    def get(self, key, default):\n+    def get(self, key: type[PretrainedConfig], default: _T) -> Union[_LazyAutoMappingValue, _T]:\n         try:\n             return self.__getitem__(key)\n         except KeyError:\n             return default\n \n-    def __bool__(self):\n+    def __bool__(self) -> bool:\n         return bool(self.keys())\n \n-    def values(self):\n+    def values(self) -> list[_LazyAutoMappingValue]:\n         mapping_values = [\n             self._load_attr_from_module(key, name)\n             for key, name in self._model_mapping.items()\n             if key in self._config_mapping.keys()\n         ]\n         return mapping_values + list(self._extra_content.values())\n \n-    def items(self):\n+    def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n         mapping_items = [\n             (\n                 self._load_attr_from_module(key, self._config_mapping[key]),\n@@ -838,18 +844,18 @@ def items(self):\n         ]\n         return mapping_items + list(self._extra_content.items())\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[type[PretrainedConfig]]:\n         return iter(self.keys())\n \n-    def __contains__(self, item):\n+    def __contains__(self, item: type) -> bool:\n         if item in self._extra_content:\n             return True\n         if not hasattr(item, \"__name__\") or item.__name__ not in self._reverse_config_mapping:\n             return False\n         model_type = self._reverse_config_mapping[item.__name__]\n         return model_type in self._model_mapping\n \n-    def register(self, key, value, exist_ok=False):\n+    def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, exist_ok=False) -> None:\n         \"\"\"\n         Register a new model in this mapping.\n         \"\"\""
        },
        {
            "sha": "22410963508d176caa4230ae78853b59868df98d",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 34,
            "deletions": 27,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=2872e8bac5cd36e5a369532ebfed06494a27ff33",
            "patch": "@@ -15,10 +15,12 @@\n \"\"\"Auto Config class.\"\"\"\n \n import importlib\n+import os\n import re\n import warnings\n from collections import OrderedDict\n-from typing import List, Union\n+from collections.abc import Callable, Iterator, KeysView, ValuesView\n+from typing import Any, TypeVar, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n@@ -28,7 +30,10 @@\n logger = logging.get_logger(__name__)\n \n \n-CONFIG_MAPPING_NAMES = OrderedDict(\n+_CallableT = TypeVar(\"_CallableT\", bound=Callable[..., Any])\n+\n+\n+CONFIG_MAPPING_NAMES = OrderedDict[str, str](\n     [\n         # Add configs here\n         (\"albert\", \"AlbertConfig\"),\n@@ -380,7 +385,7 @@\n )\n \n \n-MODEL_NAMES_MAPPING = OrderedDict(\n+MODEL_NAMES_MAPPING = OrderedDict[str, str](\n     [\n         # Add full (and cased) model names here\n         (\"albert\", \"ALBERT\"),\n@@ -795,7 +800,7 @@\n     \"xlm_prophetnet\",\n ]\n \n-SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict(\n+SPECIAL_MODEL_TYPE_TO_MODULE_NAME = OrderedDict[str, str](\n     [\n         (\"openai-gpt\", \"openai\"),\n         (\"data2vec-audio\", \"data2vec\"),\n@@ -827,7 +832,7 @@\n )\n \n \n-def model_type_to_module_name(key):\n+def model_type_to_module_name(key) -> str:\n     \"\"\"Converts a config key to the corresponding module.\"\"\"\n     # Special treatment\n     if key in SPECIAL_MODEL_TYPE_TO_MODULE_NAME:\n@@ -844,7 +849,7 @@ def model_type_to_module_name(key):\n     return key\n \n \n-def config_class_to_model_type(config):\n+def config_class_to_model_type(config) -> Union[str, None]:\n     \"\"\"Converts a config class name to the corresponding model type\"\"\"\n     for key, cls in CONFIG_MAPPING_NAMES.items():\n         if cls == config:\n@@ -856,17 +861,17 @@ def config_class_to_model_type(config):\n     return None\n \n \n-class _LazyConfigMapping(OrderedDict):\n+class _LazyConfigMapping(OrderedDict[str, type[PretrainedConfig]]):\n     \"\"\"\n     A dictionary that lazily load its values when they are requested.\n     \"\"\"\n \n-    def __init__(self, mapping):\n+    def __init__(self, mapping) -> None:\n         self._mapping = mapping\n         self._extra_content = {}\n         self._modules = {}\n \n-    def __getitem__(self, key):\n+    def __getitem__(self, key: str) -> type[PretrainedConfig]:\n         if key in self._extra_content:\n             return self._extra_content[key]\n         if key not in self._mapping:\n@@ -883,22 +888,22 @@ def __getitem__(self, key):\n         transformers_module = importlib.import_module(\"transformers\")\n         return getattr(transformers_module, value)\n \n-    def keys(self):\n+    def keys(self) -> list[str]:\n         return list(self._mapping.keys()) + list(self._extra_content.keys())\n \n-    def values(self):\n+    def values(self) -> list[type[PretrainedConfig]]:\n         return [self[k] for k in self._mapping.keys()] + list(self._extra_content.values())\n \n-    def items(self):\n+    def items(self) -> list[tuple[str, type[PretrainedConfig]]]:\n         return [(k, self[k]) for k in self._mapping.keys()] + list(self._extra_content.items())\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[str]:\n         return iter(list(self._mapping.keys()) + list(self._extra_content.keys()))\n \n-    def __contains__(self, item):\n+    def __contains__(self, item: object) -> bool:\n         return item in self._mapping or item in self._extra_content\n \n-    def register(self, key, value, exist_ok=False):\n+    def register(self, key: str, value: type[PretrainedConfig], exist_ok=False) -> None:\n         \"\"\"\n         Register a new configuration in this mapping.\n         \"\"\"\n@@ -910,7 +915,7 @@ def register(self, key, value, exist_ok=False):\n CONFIG_MAPPING = _LazyConfigMapping(CONFIG_MAPPING_NAMES)\n \n \n-class _LazyLoadAllMappings(OrderedDict):\n+class _LazyLoadAllMappings(OrderedDict[str, str]):\n     \"\"\"\n     A mapping that will load all pairs of key values at the first access (either by indexing, requestions keys, values,\n     etc.)\n@@ -940,28 +945,28 @@ def __getitem__(self, key):\n         self._initialize()\n         return self._data[key]\n \n-    def keys(self):\n+    def keys(self) -> KeysView[str]:\n         self._initialize()\n         return self._data.keys()\n \n-    def values(self):\n+    def values(self) -> ValuesView[str]:\n         self._initialize()\n         return self._data.values()\n \n-    def items(self):\n+    def items(self) -> KeysView[str]:\n         self._initialize()\n         return self._data.keys()\n \n-    def __iter__(self):\n+    def __iter__(self) -> Iterator[str]:\n         self._initialize()\n         return iter(self._data)\n \n-    def __contains__(self, item):\n+    def __contains__(self, item: object) -> bool:\n         self._initialize()\n         return item in self._data\n \n \n-def _get_class_name(model_class: Union[str, List[str]]):\n+def _get_class_name(model_class: Union[str, list[str]]):\n     if isinstance(model_class, (list, tuple)):\n         return \" or \".join([f\"[`{c}`]\" for c in model_class if c is not None])\n     return f\"[`{model_class}`]\"\n@@ -1000,7 +1005,9 @@ def _list_model_options(indent, config_to_class=None, use_model_types=True):\n     return \"\\n\".join(lines)\n \n \n-def replace_list_option_in_docstrings(config_to_class=None, use_model_types=True):\n+def replace_list_option_in_docstrings(\n+    config_to_class=None, use_model_types: bool = True\n+) -> Callable[[_CallableT], _CallableT]:\n     def docstring_decorator(fn):\n         docstrings = fn.__doc__\n         if docstrings is None:\n@@ -1035,14 +1042,14 @@ class AutoConfig:\n     This class cannot be instantiated directly using `__init__()` (throws an error).\n     \"\"\"\n \n-    def __init__(self):\n+    def __init__(self) -> None:\n         raise EnvironmentError(\n             \"AutoConfig is designed to be instantiated \"\n             \"using the `AutoConfig.from_pretrained(pretrained_model_name_or_path)` method.\"\n         )\n \n     @classmethod\n-    def for_model(cls, model_type: str, *args, **kwargs):\n+    def for_model(cls, model_type: str, *args, **kwargs) -> PretrainedConfig:\n         if model_type in CONFIG_MAPPING:\n             config_class = CONFIG_MAPPING[model_type]\n             return config_class(*args, **kwargs)\n@@ -1052,7 +1059,7 @@ def for_model(cls, model_type: str, *args, **kwargs):\n \n     @classmethod\n     @replace_list_option_in_docstrings()\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n+    def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], **kwargs):\n         r\"\"\"\n         Instantiate one of the configuration classes of the library from a pretrained model configuration.\n \n@@ -1199,7 +1206,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         )\n \n     @staticmethod\n-    def register(model_type, config, exist_ok=False):\n+    def register(model_type, config, exist_ok=False) -> None:\n         \"\"\"\n         Register a new configuration for this class.\n "
        },
        {
            "sha": "efb67aa3017d84568ae057847c1dac139d698f4a",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 613,
            "deletions": 617,
            "changes": 1230,
            "blob_url": "https://github.com/huggingface/transformers/blob/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2872e8bac5cd36e5a369532ebfed06494a27ff33/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=2872e8bac5cd36e5a369532ebfed06494a27ff33",
            "patch": "@@ -19,7 +19,7 @@\n import os\n import warnings\n from collections import OrderedDict\n-from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n+from typing import Any, Optional, Union\n \n from ...configuration_utils import PretrainedConfig\n from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n@@ -53,621 +53,617 @@\n \n logger = logging.get_logger(__name__)\n \n-if TYPE_CHECKING:\n-    # This significantly improves completion suggestion performance when\n-    # the transformers package is used with Microsoft's Pylance language server.\n-    TOKENIZER_MAPPING_NAMES: OrderedDict[str, Tuple[Optional[str], Optional[str]]] = OrderedDict()\n-else:\n-    TOKENIZER_MAPPING_NAMES = OrderedDict(\n-        [\n-            (\n-                \"albert\",\n-                (\n-                    \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                    \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"align\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"aria\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"aya_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bark\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bart\", (\"BartTokenizer\", \"BartTokenizerFast\")),\n-            (\n-                \"barthez\",\n-                (\n-                    \"BarthezTokenizer\" if is_sentencepiece_available() else None,\n-                    \"BarthezTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"bartpho\", (\"BartphoTokenizer\", None)),\n-            (\"bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bert-generation\", (\"BertGenerationTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"bert-japanese\", (\"BertJapaneseTokenizer\", None)),\n-            (\"bertweet\", (\"BertweetTokenizer\", None)),\n-            (\n-                \"big_bird\",\n-                (\n-                    \"BigBirdTokenizer\" if is_sentencepiece_available() else None,\n-                    \"BigBirdTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"bigbird_pegasus\", (\"PegasusTokenizer\", \"PegasusTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"biogpt\", (\"BioGptTokenizer\", None)),\n-            (\"bitnet\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"blenderbot\", (\"BlenderbotTokenizer\", \"BlenderbotTokenizerFast\")),\n-            (\"blenderbot-small\", (\"BlenderbotSmallTokenizer\", None)),\n-            (\"blip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"blip-2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bloom\", (None, \"BloomTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bridgetower\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"bros\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"byt5\", (\"ByT5Tokenizer\", None)),\n-            (\n-                \"camembert\",\n-                (\n-                    \"CamembertTokenizer\" if is_sentencepiece_available() else None,\n-                    \"CamembertTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"canine\", (\"CanineTokenizer\", None)),\n-            (\n-                \"chameleon\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"chinese_clip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"clap\",\n-                (\n-                    \"RobertaTokenizer\",\n-                    \"RobertaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"clip\",\n-                (\n-                    \"CLIPTokenizer\",\n-                    \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"clipseg\",\n-                (\n-                    \"CLIPTokenizer\",\n-                    \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"clvp\", (\"ClvpTokenizer\", None)),\n-            (\n-                \"code_llama\",\n-                (\n-                    \"CodeLlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"CodeLlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"codegen\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"colpali\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"cpm\",\n-                (\n-                    \"CpmTokenizer\" if is_sentencepiece_available() else None,\n-                    \"CpmTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"cpmant\", (\"CpmAntTokenizer\", None)),\n-            (\"ctrl\", (\"CTRLTokenizer\", None)),\n-            (\"data2vec-audio\", (\"Wav2Vec2CTCTokenizer\", None)),\n-            (\"data2vec-text\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"dbrx\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"deberta\", (\"DebertaTokenizer\", \"DebertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"deberta-v2\",\n-                (\n-                    \"DebertaV2Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"DebertaV2TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"deepseek_v3\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"diffllama\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"distilbert\", (\"DistilBertTokenizer\", \"DistilBertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"dpr\",\n-                (\n-                    \"DPRQuestionEncoderTokenizer\",\n-                    \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"electra\", (\"ElectraTokenizer\", \"ElectraTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"emu3\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"esm\", (\"EsmTokenizer\", None)),\n-            (\"falcon\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"falcon_mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"fastspeech2_conformer\",\n-                (\"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None, None),\n-            ),\n-            (\"flaubert\", (\"FlaubertTokenizer\", None)),\n-            (\"fnet\", (\"FNetTokenizer\", \"FNetTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"fsmt\", (\"FSMTTokenizer\", None)),\n-            (\"funnel\", (\"FunnelTokenizer\", \"FunnelTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"gemma\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"gemma2\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"gemma3\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"gemma3_text\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"git\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"glm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"glm4\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gpt-sw3\", (\"GPTSw3Tokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"gpt2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gpt_bigcode\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gpt_neo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gpt_neox\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gpt_neox_japanese\", (\"GPTNeoXJapaneseTokenizer\", None)),\n-            (\"gptj\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"gptsan-japanese\", (\"GPTSanJapaneseTokenizer\", None)),\n-            (\"grounding-dino\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"groupvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"helium\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"herbert\", (\"HerbertTokenizer\", \"HerbertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"hubert\", (\"Wav2Vec2CTCTokenizer\", None)),\n-            (\"ibert\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"idefics\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"idefics2\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"idefics3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"instructblip\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"instructblipvideo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"jamba\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"janus\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"jetmoe\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"jukebox\", (\"JukeboxTokenizer\", None)),\n-            (\n-                \"kosmos-2\",\n-                (\n-                    \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"layoutlm\", (\"LayoutLMTokenizer\", \"LayoutLMTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"layoutlmv2\", (\"LayoutLMv2Tokenizer\", \"LayoutLMv2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"layoutlmv3\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"layoutxlm\", (\"LayoutXLMTokenizer\", \"LayoutXLMTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"led\", (\"LEDTokenizer\", \"LEDTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"lilt\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"llama\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"llama4\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"llama4_text\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"llava_next\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"llava_next_video\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"llava_onevision\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"longformer\", (\"LongformerTokenizer\", \"LongformerTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"longt5\",\n-                (\n-                    \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"luke\", (\"LukeTokenizer\", None)),\n-            (\"lxmert\", (\"LxmertTokenizer\", \"LxmertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"m2m_100\", (\"M2M100Tokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mamba2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"marian\", (\"MarianTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\n-                \"mbart\",\n-                (\n-                    \"MBartTokenizer\" if is_sentencepiece_available() else None,\n-                    \"MBartTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"mbart50\",\n-                (\n-                    \"MBart50Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"MBart50TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"mega\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"megatron-bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mgp-str\", (\"MgpstrTokenizer\", None)),\n-            (\n-                \"mistral\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"mixtral\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"moonshine\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"moshi\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mpnet\", (\"MPNetTokenizer\", \"MPNetTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mpt\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mra\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"mt5\",\n-                (\n-                    \"MT5Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"MT5TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"musicgen\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"musicgen_melody\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"mvp\", (\"MvpTokenizer\", \"MvpTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"myt5\", (\"MyT5Tokenizer\", None)),\n-            (\"nemotron\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"nezha\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"nllb\",\n-                (\n-                    \"NllbTokenizer\" if is_sentencepiece_available() else None,\n-                    \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"nllb-moe\",\n-                (\n-                    \"NllbTokenizer\" if is_sentencepiece_available() else None,\n-                    \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"nystromformer\",\n-                (\n-                    \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                    \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"olmo2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"omdet-turbo\",\n-                (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n-            ),\n-            (\"oneformer\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"openai-gpt\",\n-                (\"OpenAIGPTTokenizer\", \"OpenAIGPTTokenizerFast\" if is_tokenizers_available() else None),\n-            ),\n-            (\"opt\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"owlv2\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"owlvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"paligemma\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"pegasus\",\n-                (\n-                    \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n-                    \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"pegasus_x\",\n-                (\n-                    \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n-                    \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"perceiver\",\n-                (\n-                    \"PerceiverTokenizer\",\n-                    None,\n-                ),\n-            ),\n-            (\n-                \"persimmon\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"phi\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"phi3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"phimoe\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"phobert\", (\"PhobertTokenizer\", None)),\n-            (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"pixtral\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n-            (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"qwen2\",\n-                (\n-                    \"Qwen2Tokenizer\",\n-                    \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"qwen2_5_omni\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"qwen2_5_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"qwen2_audio\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"qwen2_moe\",\n-                (\n-                    \"Qwen2Tokenizer\",\n-                    \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"qwen2_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"qwen3\",\n-                (\n-                    \"Qwen2Tokenizer\",\n-                    \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"qwen3_moe\",\n-                (\n-                    \"Qwen2Tokenizer\",\n-                    \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"rag\", (\"RagTokenizer\", None)),\n-            (\"realm\", (\"RealmTokenizer\", \"RealmTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"recurrent_gemma\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"reformer\",\n-                (\n-                    \"ReformerTokenizer\" if is_sentencepiece_available() else None,\n-                    \"ReformerTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"rembert\",\n-                (\n-                    \"RemBertTokenizer\" if is_sentencepiece_available() else None,\n-                    \"RemBertTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"retribert\", (\"RetriBertTokenizer\", \"RetriBertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"roberta\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"roberta-prelayernorm\",\n-                (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None),\n-            ),\n-            (\"roc_bert\", (\"RoCBertTokenizer\", None)),\n-            (\"roformer\", (\"RoFormerTokenizer\", \"RoFormerTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"rwkv\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"seamless_m4t\",\n-                (\n-                    \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n-                    \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"seamless_m4t_v2\",\n-                (\n-                    \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n-                    \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"shieldgemma2\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"siglip\", (\"SiglipTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\n-                \"siglip2\",\n-                (\n-                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"speech_to_text_2\", (\"Speech2Text2Tokenizer\", None)),\n-            (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\"splinter\", (\"SplinterTokenizer\", \"SplinterTokenizerFast\")),\n-            (\n-                \"squeezebert\",\n-                (\"SqueezeBertTokenizer\", \"SqueezeBertTokenizerFast\" if is_tokenizers_available() else None),\n-            ),\n-            (\"stablelm\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"starcoder2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"switch_transformers\",\n-                (\n-                    \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"t5\",\n-                (\n-                    \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"tapas\", (\"TapasTokenizer\", None)),\n-            (\"tapex\", (\"TapexTokenizer\", None)),\n-            (\"transfo-xl\", (\"TransfoXLTokenizer\", None)),\n-            (\"tvp\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"udop\",\n-                (\n-                    \"UdopTokenizer\" if is_sentencepiece_available() else None,\n-                    \"UdopTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"umt5\",\n-                (\n-                    \"T5Tokenizer\" if is_sentencepiece_available() else None,\n-                    \"T5TokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"video_llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"vilt\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"vipllava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"visual_bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"vits\", (\"VitsTokenizer\", None)),\n-            (\"wav2vec2\", (\"Wav2Vec2CTCTokenizer\", None)),\n-            (\"wav2vec2-bert\", (\"Wav2Vec2CTCTokenizer\", None)),\n-            (\"wav2vec2-conformer\", (\"Wav2Vec2CTCTokenizer\", None)),\n-            (\"wav2vec2_phoneme\", (\"Wav2Vec2PhonemeCTCTokenizer\", None)),\n-            (\"whisper\", (\"WhisperTokenizer\", \"WhisperTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\"xclip\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n-            (\n-                \"xglm\",\n-                (\n-                    \"XGLMTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XGLMTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\"xlm\", (\"XLMTokenizer\", None)),\n-            (\"xlm-prophetnet\", (\"XLMProphetNetTokenizer\" if is_sentencepiece_available() else None, None)),\n-            (\n-                \"xlm-roberta\",\n-                (\n-                    \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"xlm-roberta-xl\",\n-                (\n-                    \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"xlnet\",\n-                (\n-                    \"XLNetTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XLNetTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"xmod\",\n-                (\n-                    \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"yoso\",\n-                (\n-                    \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n-                    \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"zamba\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-            (\n-                \"zamba2\",\n-                (\n-                    \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n-                    \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n-                ),\n-            ),\n-        ]\n-    )\n+# Explicit rather than inferred generics to significantly improves completion suggestion performance for language servers.\n+TOKENIZER_MAPPING_NAMES = OrderedDict[str, tuple[Optional[str], Optional[str]]](\n+    [\n+        (\n+            \"albert\",\n+            (\n+                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n+                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"align\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"aria\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"aya_vision\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bark\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bart\", (\"BartTokenizer\", \"BartTokenizerFast\")),\n+        (\n+            \"barthez\",\n+            (\n+                \"BarthezTokenizer\" if is_sentencepiece_available() else None,\n+                \"BarthezTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"bartpho\", (\"BartphoTokenizer\", None)),\n+        (\"bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bert-generation\", (\"BertGenerationTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"bert-japanese\", (\"BertJapaneseTokenizer\", None)),\n+        (\"bertweet\", (\"BertweetTokenizer\", None)),\n+        (\n+            \"big_bird\",\n+            (\n+                \"BigBirdTokenizer\" if is_sentencepiece_available() else None,\n+                \"BigBirdTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"bigbird_pegasus\", (\"PegasusTokenizer\", \"PegasusTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"biogpt\", (\"BioGptTokenizer\", None)),\n+        (\"bitnet\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"blenderbot\", (\"BlenderbotTokenizer\", \"BlenderbotTokenizerFast\")),\n+        (\"blenderbot-small\", (\"BlenderbotSmallTokenizer\", None)),\n+        (\"blip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"blip-2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bloom\", (None, \"BloomTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bridgetower\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"bros\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"byt5\", (\"ByT5Tokenizer\", None)),\n+        (\n+            \"camembert\",\n+            (\n+                \"CamembertTokenizer\" if is_sentencepiece_available() else None,\n+                \"CamembertTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"canine\", (\"CanineTokenizer\", None)),\n+        (\n+            \"chameleon\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"chinese_clip\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"clap\",\n+            (\n+                \"RobertaTokenizer\",\n+                \"RobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"clip\",\n+            (\n+                \"CLIPTokenizer\",\n+                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"clipseg\",\n+            (\n+                \"CLIPTokenizer\",\n+                \"CLIPTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"clvp\", (\"ClvpTokenizer\", None)),\n+        (\n+            \"code_llama\",\n+            (\n+                \"CodeLlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"CodeLlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"codegen\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"cohere\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"cohere2\", (None, \"CohereTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"colpali\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"convbert\", (\"ConvBertTokenizer\", \"ConvBertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"cpm\",\n+            (\n+                \"CpmTokenizer\" if is_sentencepiece_available() else None,\n+                \"CpmTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"cpmant\", (\"CpmAntTokenizer\", None)),\n+        (\"ctrl\", (\"CTRLTokenizer\", None)),\n+        (\"data2vec-audio\", (\"Wav2Vec2CTCTokenizer\", None)),\n+        (\"data2vec-text\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"dbrx\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"deberta\", (\"DebertaTokenizer\", \"DebertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"deberta-v2\",\n+            (\n+                \"DebertaV2Tokenizer\" if is_sentencepiece_available() else None,\n+                \"DebertaV2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"deepseek_v3\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"diffllama\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"distilbert\", (\"DistilBertTokenizer\", \"DistilBertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"dpr\",\n+            (\n+                \"DPRQuestionEncoderTokenizer\",\n+                \"DPRQuestionEncoderTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"electra\", (\"ElectraTokenizer\", \"ElectraTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"emu3\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"esm\", (\"EsmTokenizer\", None)),\n+        (\"falcon\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"falcon_mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"fastspeech2_conformer\",\n+            (\"FastSpeech2ConformerTokenizer\" if is_g2p_en_available() else None, None),\n+        ),\n+        (\"flaubert\", (\"FlaubertTokenizer\", None)),\n+        (\"fnet\", (\"FNetTokenizer\", \"FNetTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"fsmt\", (\"FSMTTokenizer\", None)),\n+        (\"funnel\", (\"FunnelTokenizer\", \"FunnelTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"gemma\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"gemma2\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"gemma3\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"gemma3_text\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"git\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"glm\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"glm4\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gpt-sw3\", (\"GPTSw3Tokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"gpt2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gpt_bigcode\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gpt_neo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gpt_neox\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gpt_neox_japanese\", (\"GPTNeoXJapaneseTokenizer\", None)),\n+        (\"gptj\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"gptsan-japanese\", (\"GPTSanJapaneseTokenizer\", None)),\n+        (\"grounding-dino\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"groupvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"helium\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"herbert\", (\"HerbertTokenizer\", \"HerbertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"hubert\", (\"Wav2Vec2CTCTokenizer\", None)),\n+        (\"ibert\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"idefics\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"idefics2\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"idefics3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"instructblip\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"instructblipvideo\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"internvl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"jamba\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"janus\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"jetmoe\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"jukebox\", (\"JukeboxTokenizer\", None)),\n+        (\n+            \"kosmos-2\",\n+            (\n+                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n+                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"layoutlm\", (\"LayoutLMTokenizer\", \"LayoutLMTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"layoutlmv2\", (\"LayoutLMv2Tokenizer\", \"LayoutLMv2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"layoutlmv3\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"layoutxlm\", (\"LayoutXLMTokenizer\", \"LayoutXLMTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"led\", (\"LEDTokenizer\", \"LEDTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"lilt\", (\"LayoutLMv3Tokenizer\", \"LayoutLMv3TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"llama\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"llama4\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"llama4_text\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"llava_next\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"llava_next_video\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"llava_onevision\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"longformer\", (\"LongformerTokenizer\", \"LongformerTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"longt5\",\n+            (\n+                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n+                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"luke\", (\"LukeTokenizer\", None)),\n+        (\"lxmert\", (\"LxmertTokenizer\", \"LxmertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"m2m_100\", (\"M2M100Tokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"mamba\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mamba2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"marian\", (\"MarianTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\n+            \"mbart\",\n+            (\n+                \"MBartTokenizer\" if is_sentencepiece_available() else None,\n+                \"MBartTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"mbart50\",\n+            (\n+                \"MBart50Tokenizer\" if is_sentencepiece_available() else None,\n+                \"MBart50TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"mega\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"megatron-bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mgp-str\", (\"MgpstrTokenizer\", None)),\n+        (\n+            \"mistral\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"mixtral\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"mllama\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mluke\", (\"MLukeTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"mobilebert\", (\"MobileBertTokenizer\", \"MobileBertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"modernbert\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"moonshine\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"moshi\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mpnet\", (\"MPNetTokenizer\", \"MPNetTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mpt\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mra\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"mt5\",\n+            (\n+                \"MT5Tokenizer\" if is_sentencepiece_available() else None,\n+                \"MT5TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"musicgen\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"musicgen_melody\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"mvp\", (\"MvpTokenizer\", \"MvpTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"myt5\", (\"MyT5Tokenizer\", None)),\n+        (\"nemotron\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"nezha\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"nllb\",\n+            (\n+                \"NllbTokenizer\" if is_sentencepiece_available() else None,\n+                \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"nllb-moe\",\n+            (\n+                \"NllbTokenizer\" if is_sentencepiece_available() else None,\n+                \"NllbTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"nystromformer\",\n+            (\n+                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n+                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"olmo2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"omdet-turbo\",\n+            (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"oneformer\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"openai-gpt\",\n+            (\"OpenAIGPTTokenizer\", \"OpenAIGPTTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"opt\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"owlv2\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"owlvit\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"paligemma\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"pegasus\",\n+            (\n+                \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n+                \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"pegasus_x\",\n+            (\n+                \"PegasusTokenizer\" if is_sentencepiece_available() else None,\n+                \"PegasusTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"perceiver\",\n+            (\n+                \"PerceiverTokenizer\",\n+                None,\n+            ),\n+        ),\n+        (\n+            \"persimmon\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"phi\", (\"CodeGenTokenizer\", \"CodeGenTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"phi3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"phimoe\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"phobert\", (\"PhobertTokenizer\", None)),\n+        (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"pixtral\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n+        (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"qwen2\",\n+            (\n+                \"Qwen2Tokenizer\",\n+                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"qwen2_5_omni\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"qwen2_5_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"qwen2_audio\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"qwen2_moe\",\n+            (\n+                \"Qwen2Tokenizer\",\n+                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"qwen2_vl\", (\"Qwen2Tokenizer\", \"Qwen2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"qwen3\",\n+            (\n+                \"Qwen2Tokenizer\",\n+                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"qwen3_moe\",\n+            (\n+                \"Qwen2Tokenizer\",\n+                \"Qwen2TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"rag\", (\"RagTokenizer\", None)),\n+        (\"realm\", (\"RealmTokenizer\", \"RealmTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"recurrent_gemma\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"reformer\",\n+            (\n+                \"ReformerTokenizer\" if is_sentencepiece_available() else None,\n+                \"ReformerTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"rembert\",\n+            (\n+                \"RemBertTokenizer\" if is_sentencepiece_available() else None,\n+                \"RemBertTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"retribert\", (\"RetriBertTokenizer\", \"RetriBertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"roberta\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"roberta-prelayernorm\",\n+            (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"roc_bert\", (\"RoCBertTokenizer\", None)),\n+        (\"roformer\", (\"RoFormerTokenizer\", \"RoFormerTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"rwkv\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"seamless_m4t\",\n+            (\n+                \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n+                \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"seamless_m4t_v2\",\n+            (\n+                \"SeamlessM4TTokenizer\" if is_sentencepiece_available() else None,\n+                \"SeamlessM4TTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"shieldgemma2\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"siglip\", (\"SiglipTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\n+            \"siglip2\",\n+            (\n+                \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"speech_to_text_2\", (\"Speech2Text2Tokenizer\", None)),\n+        (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\"splinter\", (\"SplinterTokenizer\", \"SplinterTokenizerFast\")),\n+        (\n+            \"squeezebert\",\n+            (\"SqueezeBertTokenizer\", \"SqueezeBertTokenizerFast\" if is_tokenizers_available() else None),\n+        ),\n+        (\"stablelm\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"starcoder2\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"switch_transformers\",\n+            (\n+                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n+                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"t5\",\n+            (\n+                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n+                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"tapas\", (\"TapasTokenizer\", None)),\n+        (\"tapex\", (\"TapexTokenizer\", None)),\n+        (\"transfo-xl\", (\"TransfoXLTokenizer\", None)),\n+        (\"tvp\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"udop\",\n+            (\n+                \"UdopTokenizer\" if is_sentencepiece_available() else None,\n+                \"UdopTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"umt5\",\n+            (\n+                \"T5Tokenizer\" if is_sentencepiece_available() else None,\n+                \"T5TokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"video_llava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"vilt\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"vipllava\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"visual_bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"vits\", (\"VitsTokenizer\", None)),\n+        (\"wav2vec2\", (\"Wav2Vec2CTCTokenizer\", None)),\n+        (\"wav2vec2-bert\", (\"Wav2Vec2CTCTokenizer\", None)),\n+        (\"wav2vec2-conformer\", (\"Wav2Vec2CTCTokenizer\", None)),\n+        (\"wav2vec2_phoneme\", (\"Wav2Vec2PhonemeCTCTokenizer\", None)),\n+        (\"whisper\", (\"WhisperTokenizer\", \"WhisperTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"xclip\", (\"CLIPTokenizer\", \"CLIPTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"xglm\",\n+            (\n+                \"XGLMTokenizer\" if is_sentencepiece_available() else None,\n+                \"XGLMTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\"xlm\", (\"XLMTokenizer\", None)),\n+        (\"xlm-prophetnet\", (\"XLMProphetNetTokenizer\" if is_sentencepiece_available() else None, None)),\n+        (\n+            \"xlm-roberta\",\n+            (\n+                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n+                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"xlm-roberta-xl\",\n+            (\n+                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n+                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"xlnet\",\n+            (\n+                \"XLNetTokenizer\" if is_sentencepiece_available() else None,\n+                \"XLNetTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"xmod\",\n+            (\n+                \"XLMRobertaTokenizer\" if is_sentencepiece_available() else None,\n+                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"yoso\",\n+            (\n+                \"AlbertTokenizer\" if is_sentencepiece_available() else None,\n+                \"AlbertTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"zamba\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+        (\n+            \"zamba2\",\n+            (\n+                \"LlamaTokenizer\" if is_sentencepiece_available() else None,\n+                \"LlamaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n+    ]\n+)\n \n TOKENIZER_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, TOKENIZER_MAPPING_NAMES)\n \n CONFIG_TO_TYPE = {v: k for k, v in CONFIG_MAPPING_NAMES.items()}\n \n \n-def tokenizer_class_from_name(class_name: str):\n+def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     if class_name == \"PreTrainedTokenizerFast\":\n         return PreTrainedTokenizerFast\n \n@@ -696,17 +692,17 @@ def tokenizer_class_from_name(class_name: str):\n \n \n def get_tokenizer_config(\n-    pretrained_model_name_or_path: Union[str, os.PathLike],\n-    cache_dir: Optional[Union[str, os.PathLike]] = None,\n+    pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n+    cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n     force_download: bool = False,\n     resume_download: Optional[bool] = None,\n-    proxies: Optional[Dict[str, str]] = None,\n+    proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n     local_files_only: bool = False,\n     subfolder: str = \"\",\n     **kwargs,\n-):\n+) -> dict[str, Any]:\n     \"\"\"\n     Loads the tokenizer configuration from a pretrained model tokenizer configuration.\n \n@@ -728,7 +724,7 @@ def get_tokenizer_config(\n         resume_download:\n             Deprecated and ignored. All downloads are now resumed by default when possible.\n             Will be removed in v5 of Transformers.\n-        proxies (`Dict[str, str]`, *optional*):\n+        proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n         token (`str` or *bool*, *optional*):\n@@ -751,7 +747,7 @@ def get_tokenizer_config(\n     </Tip>\n \n     Returns:\n-        `Dict`: The configuration of the tokenizer.\n+        `dict`: The configuration of the tokenizer.\n \n     Examples:\n \n@@ -855,7 +851,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             resume_download:\n                 Deprecated and ignored. All downloads are now resumed by default when possible.\n                 Will be removed in v5 of Transformers.\n-            proxies (`Dict[str, str]`, *optional*):\n+            proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n             revision (`str`, *optional*, defaults to `\"main\"`):"
        }
    ],
    "stats": {
        "total": 1331,
        "additions": 670,
        "deletions": 661
    }
}