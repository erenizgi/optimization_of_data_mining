{
    "author": "zucchini-nlp",
    "message": "VideoLLaVA: add default values (#34916)\n\nadd default values",
    "sha": "73b4ab1085c1f21dc43dc30923407d95a6a8c316",
    "files": [
        {
            "sha": "3e1884271efe2b949d1c66da7eb1cef32d382998",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/73b4ab1085c1f21dc43dc30923407d95a6a8c316/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73b4ab1085c1f21dc43dc30923407d95a6a8c316/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=73b4ab1085c1f21dc43dc30923407d95a6a8c316",
            "patch": "@@ -40,9 +40,9 @@ class VideoLlavaProcessor(ProcessorMixin):\n             The image processor is a required input.\n         tokenizer ([`LlamaTokenizerFast`], *optional*):\n             The tokenizer is a required input.\n-        patch_size (`int`, *optional*):\n+        patch_size (`int`, *optional*, defaults to 14):\n             Patch size from the vision tower.\n-        vision_feature_select_strategy (`str`, *optional*):\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Shoudl be same as in model's config\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n@@ -51,7 +51,7 @@ class VideoLlavaProcessor(ProcessorMixin):\n             Special token used to denote video location.\n         chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n             in a chat into a tokenizable string.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+        num_additional_image_tokens (`int`, *optional*, defaults to 1):\n             Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n             extra tokens appended, no need to set this arg.\n     \"\"\"\n@@ -72,12 +72,12 @@ def __init__(\n         self,\n         image_processor=None,\n         tokenizer=None,\n-        patch_size=None,\n-        vision_feature_select_strategy=None,\n+        patch_size=14,\n+        vision_feature_select_strategy=\"default\",\n         image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n         video_token=\"<video>\",\n         chat_template=None,\n-        num_additional_image_tokens=0,\n+        num_additional_image_tokens=1,\n         **kwargs,\n     ):\n         self.patch_size = patch_size"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}