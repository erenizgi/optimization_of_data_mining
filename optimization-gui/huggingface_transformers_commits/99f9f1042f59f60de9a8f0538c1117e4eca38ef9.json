{
    "author": "jiqing-feng",
    "message": "Fix torchao usage (#37034)\n\n* fix load path\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix path\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* Fix torchao usage\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert useless change\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* revert fp8 test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix fp8 test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix fp8 test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix torch dtype\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "99f9f1042f59f60de9a8f0538c1117e4eca38ef9",
    "files": [
        {
            "sha": "3bf205e8e152aa66f4169cd573c809b9df215b21",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/99f9f1042f59f60de9a8f0538c1117e4eca38ef9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99f9f1042f59f60de9a8f0538c1117e4eca38ef9/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=99f9f1042f59f60de9a8f0538c1117e4eca38ef9",
            "patch": "@@ -20,7 +20,7 @@\n import importlib.metadata\n import json\n import os\n-from dataclasses import dataclass\n+from dataclasses import dataclass, is_dataclass\n from enum import Enum\n from inspect import Parameter, signature\n from typing import Any, Dict, List, Optional, Tuple, Union\n@@ -1627,6 +1627,7 @@ def get_apply_tensor_subclass(self):\n                 and is_torchao_available()\n                 and self.quant_type == \"int4_weight_only\"\n                 and version.parse(importlib.metadata.version(\"torchao\")) >= version.parse(\"0.8.0\")\n+                and quant_type_kwargs.get(\"layout\", None) is None\n             ):\n                 from torchao.dtypes import Int4CPULayout\n \n@@ -1643,7 +1644,17 @@ def to_dict(self):\n         if isinstance(self.quant_type, str):\n             # Handle layout serialization if present\n             if \"quant_type_kwargs\" in d and \"layout\" in d[\"quant_type_kwargs\"]:\n-                d[\"quant_type_kwargs\"][\"layout\"] = dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"])\n+                if is_dataclass(d[\"quant_type_kwargs\"][\"layout\"]):\n+                    d[\"quant_type_kwargs\"][\"layout\"] = [\n+                        d[\"quant_type_kwargs\"][\"layout\"].__class__.__name__,\n+                        dataclasses.asdict(d[\"quant_type_kwargs\"][\"layout\"]),\n+                    ]\n+                if isinstance(d[\"quant_type_kwargs\"][\"layout\"], list):\n+                    assert len(d[\"quant_type_kwargs\"][\"layout\"]) == 2, \"layout saves layout name and layour kwargs\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][0], str), \"layout name must be a string\"\n+                    assert isinstance(d[\"quant_type_kwargs\"][\"layout\"][1], dict), \"layout kwargs must be a dict\"\n+                else:\n+                    raise ValueError(\"layout must be a list\")\n         else:\n             # Handle AOBaseConfig serialization\n             from torchao.core.config import config_to_dict\n@@ -1661,6 +1672,9 @@ def from_dict(cls, config_dict, return_unused_kwargs=False, **kwargs):\n         assert ao_verison > version.parse(\"0.9.0\"), \"TorchAoConfig requires torchao > 0.9.0 for construction from dict\"\n         config_dict = config_dict.copy()\n         quant_type = config_dict.pop(\"quant_type\")\n+\n+        if isinstance(quant_type, str):\n+            return cls(quant_type=quant_type, **config_dict)\n         # Check if we only have one key which is \"default\"\n         # In the future we may update this\n         assert len(quant_type) == 1 and \"default\" in quant_type, ("
        },
        {
            "sha": "5f4a21e073a3cdfd07b7837371a3ca88d5710f7f",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 34,
            "deletions": 34,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/99f9f1042f59f60de9a8f0538c1117e4eca38ef9/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/99f9f1042f59f60de9a8f0538c1117e4eca38ef9/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=99f9f1042f59f60de9a8f0538c1117e4eca38ef9",
            "patch": "@@ -104,8 +104,8 @@ def test_json_serializable(self):\n         \"\"\"\n         quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=32, layout=TensorCoreTiledLayout())\n         d = quantization_config.to_dict()\n-        self.assertIsInstance(d[\"quant_type_kwargs\"][\"layout\"], dict)\n-        self.assertTrue(\"inner_k_tiles\" in d[\"quant_type_kwargs\"][\"layout\"])\n+        self.assertIsInstance(d[\"quant_type_kwargs\"][\"layout\"], list)\n+        self.assertTrue(\"inner_k_tiles\" in d[\"quant_type_kwargs\"][\"layout\"][1])\n         quantization_config.to_json_string(use_diff=False)\n \n \n@@ -159,7 +159,7 @@ def test_int4wo_quant_bfloat16_conversion(self):\n         # Note: we quantize the bfloat16 model on the fly to int4\n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            torch_dtype=None,\n+            torch_dtype=torch.bfloat16,\n             device_map=self.device,\n             quantization_config=quant_config,\n         )\n@@ -282,7 +282,7 @@ def test_autoquant(self):\n \n         quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            torch_dtype=torch.bfloat16,\n+            torch_dtype=\"auto\",\n             device_map=self.device,\n             quantization_config=quant_config,\n         )\n@@ -295,7 +295,7 @@ def test_autoquant(self):\n \n         check_autoquantized(self, quantized_model.model.layers[0].self_attn.v_proj)\n \n-        EXPECTED_OUTPUT = 'What are we having for dinner?\\n\\n10. \"Dinner is ready'\n+        EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJane: (sighs)\"\n         output = quantized_model.generate(\n             **input_ids, max_new_tokens=self.max_new_tokens, cache_implementation=\"static\"\n         )\n@@ -307,9 +307,7 @@ def test_autoquant(self):\n class TorchAoSerializationTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\"\n     max_new_tokens = 10\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n-    # TODO: investigate why we don't have the same output as the original model for this test\n-    SERIALIZED_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n- 1. What is the temperature outside\"\n     model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n     quant_scheme = \"int4_weight_only\"\n     quant_scheme_kwargs = (\n@@ -326,9 +324,10 @@ def setUpClass(cls):\n \n     def setUp(self):\n         self.quant_config = TorchAoConfig(self.quant_scheme, **self.quant_scheme_kwargs)\n+        torch_dtype = torch.bfloat16 if self.quant_scheme == \"int4_weight_only\" else \"auto\"\n         self.quantized_model = AutoModelForCausalLM.from_pretrained(\n             self.model_name,\n-            torch_dtype=torch.bfloat16,\n+            torch_dtype=torch_dtype,\n             device_map=self.device,\n             quantization_config=self.quant_config,\n         )\n@@ -342,50 +341,49 @@ def test_original_model_expected_output(self):\n         input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(self.device)\n         output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n \n-        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.ORIGINAL_EXPECTED_OUTPUT)\n+        self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n     def check_serialization_expected_output(self, device, expected_output):\n         \"\"\"\n         Test if we can serialize and load/infer the model again on the same device\n         \"\"\"\n+        torch_dtype = torch.bfloat16 if self.quant_scheme == \"int4_weight_only\" else \"auto\"\n         with tempfile.TemporaryDirectory() as tmpdirname:\n             self.quantized_model.save_pretrained(tmpdirname, safe_serialization=False)\n             loaded_quantized_model = AutoModelForCausalLM.from_pretrained(\n-                self.model_name, torch_dtype=torch.bfloat16, device_map=device\n+                tmpdirname, torch_dtype=torch_dtype, device_map=device\n             )\n             input_ids = self.tokenizer(self.input_text, return_tensors=\"pt\").to(device)\n \n             output = loaded_quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), expected_output)\n \n     def test_serialization_expected_output(self):\n-        self.check_serialization_expected_output(self.device, self.SERIALIZED_EXPECTED_OUTPUT)\n+        self.check_serialization_expected_output(self.device, self.EXPECTED_OUTPUT)\n \n \n class TorchAoSerializationW8A8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n     @require_torch_gpu\n     def test_serialization_expected_output_on_cuda(self):\n         \"\"\"\n         Test if we can serialize on device (cpu) and load/infer the model on cuda\n         \"\"\"\n-        self.check_serialization_expected_output(\"cuda\", self.SERIALIZED_EXPECTED_OUTPUT)\n+        self.check_serialization_expected_output(\"cuda\", self.EXPECTED_OUTPUT)\n \n \n class TorchAoSerializationW8CPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n \n     @require_torch_gpu\n     def test_serialization_expected_output_on_cuda(self):\n         \"\"\"\n         Test if we can serialize on device (cpu) and load/infer the model on cuda\n         \"\"\"\n-        self.check_serialization_expected_output(\"cuda\", self.SERIALIZED_EXPECTED_OUTPUT)\n+        self.check_serialization_expected_output(\"cuda\", self.EXPECTED_OUTPUT)\n \n \n @require_torch_gpu\n@@ -397,53 +395,55 @@ class TorchAoSerializationGPTTest(TorchAoSerializationTest):\n @require_torch_gpu\n class TorchAoSerializationW8A8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_dynamic_activation_int8_weight\", {}\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = \"cuda:0\"\n \n \n @require_torch_gpu\n class TorchAoSerializationW8GPUTest(TorchAoSerializationTest):\n     quant_scheme, quant_scheme_kwargs = \"int8_weight_only\", {}\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = \"cuda:0\"\n \n \n @require_torch_gpu\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n class TorchAoSerializationFP8GPUTest(TorchAoSerializationTest):\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = \"cuda:0\"\n \n-    def setUp(self):\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n         if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Float8WeightOnlyConfig\n \n-        self.quant_scheme = Float8WeightOnlyConfig()\n-        self.quant_scheme_kwargs = {}\n-        super().setUp()\n+        cls.quant_scheme = Float8WeightOnlyConfig()\n+        cls.quant_scheme_kwargs = {}\n+\n+        super().setUpClass()\n \n \n @require_torch_gpu\n @require_torchao_version_greater_or_equal(\"0.10.0\")\n class TorchAoSerializationA8W4Test(TorchAoSerializationTest):\n-    ORIGINAL_EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n-    SERIALIZED_EXPECTED_OUTPUT = ORIGINAL_EXPECTED_OUTPUT\n+    EXPECTED_OUTPUT = \"What are we having for dinner?\\n\\nJessica: (smiling)\"\n     device = \"cuda:0\"\n \n-    def setUp(self):\n+    # called only once for all test in this class\n+    @classmethod\n+    def setUpClass(cls):\n         if not torch.cuda.is_available() or torch.cuda.get_device_capability()[0] < 9:\n             raise unittest.SkipTest(\"CUDA compute capability 9.0 or higher required for FP8 tests\")\n \n         from torchao.quantization import Int8DynamicActivationInt4WeightConfig\n \n-        self.quant_scheme = Int8DynamicActivationInt4WeightConfig()\n-        self.quant_scheme_kwargs = {}\n-        super().setUp()\n+        cls.quant_scheme = Int8DynamicActivationInt4WeightConfig()\n+        cls.quant_scheme_kwargs = {}\n+\n+        super().setUpClass()\n \n \n if __name__ == \"__main__\":"
        }
    ],
    "stats": {
        "total": 86,
        "additions": 50,
        "deletions": 36
    }
}