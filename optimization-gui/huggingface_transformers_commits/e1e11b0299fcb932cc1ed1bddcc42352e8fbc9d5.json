{
    "author": "Cyrilvallez",
    "message": "Fix undeterministic order in modular dependencies (#39005)\n\n* sort correctly\n\n* Update modeling_minimax.py\n\n* Update modular_model_converter.py",
    "sha": "e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
    "files": [
        {
            "sha": "49d27f7789c2b953717fb11a84c14562e593c61a",
            "filename": "examples/modular-transformers/configuration_my_new_model.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fconfiguration_my_new_model.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -14,6 +14,7 @@ class MyNewModelConfig(PretrainedConfig):\n     This is the configuration class to store the configuration of a [`MyNewModelModel`]. It is used to instantiate an MyNewModel\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n     defaults will yield a similar configuration to that of the MyNewModel-7B.\n+    e.g. [meta-my_new_model/MyNewModel-2-7b-hf](https://huggingface.co/meta-my_new_model/MyNewModel-2-7b-hf)\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information."
        },
        {
            "sha": "5fc7d2f7c354da5aee49ca60d68312cbe09c9163",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 14,
            "deletions": 252,
            "changes": 266,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -4,37 +4,25 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_dummy.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_dummy import DummyConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -232,15 +220,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -311,27 +292,7 @@ def forward(\n         return outputs\n \n \n-DUMMY_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DummyConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Dummy Model outputting raw hidden-states without any specific head on top.\",\n-    DUMMY_START_DOCSTRING,\n-)\n+@auto_docstring\n class DummyPreTrainedModel(PreTrainedModel):\n     config_class = DummyConfig\n     base_model_prefix = \"model\"\n@@ -360,88 +321,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-DUMMY_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Dummy Model outputting raw hidden-states without any specific head on top.\",\n-    DUMMY_START_DOCSTRING,\n-)\n+@auto_docstring\n class DummyModel(DummyPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`DummyDecoderLayer`]\n-\n-    Args:\n-        config: DummyConfig\n-    \"\"\"\n-\n     def __init__(self, config: DummyConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -465,7 +346,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(DUMMY_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -513,8 +394,12 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n         hidden_states = inputs_embeds\n@@ -559,126 +444,3 @@ def forward(\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask"
        },
        {
            "sha": "40bd423067e4a5b4d11cfd17e90e647bd094b32c",
            "filename": "examples/modular-transformers/modeling_dummy_bert.py",
            "status": "modified",
            "additions": 22,
            "deletions": 137,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy_bert.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -14,24 +14,16 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    logging,\n-)\n+from ...utils import auto_docstring, get_torch_version, logging\n from .configuration_dummy_bert import DummyBertConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google-dummy_bert/dummy_bert-base-uncased\"\n-_CONFIG_FOR_DOC = \"DummyBertConfig\"\n-\n \n class DummyBertEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n@@ -432,7 +424,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class DummyBertLayer(nn.Module):\n+class DummyBertLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -557,27 +549,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:\n@@ -739,12 +719,8 @@ def load_tf_weights_in_dummy_bert(model, config, tf_checkpoint_path):\n     return model\n \n \n+@auto_docstring\n class DummyBertPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = DummyBertConfig\n     load_tf_weights = load_tf_weights_in_dummy_bert\n     base_model_prefix = \"dummy_bert\"\n@@ -770,79 +746,8 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-DUMMY_BERT_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`DummyBertConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-DUMMY_BERT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare DummyBert Model transformer outputting raw hidden-states without any specific head on top.\",\n-    DUMMY_BERT_START_DOCSTRING,\n-)\n-class DummyBertModel(DummyBertPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n     all you need](https://huggingface.co/papers/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n@@ -852,10 +757,15 @@ class DummyBertModel(DummyBertPreTrainedModel):\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n-\n+)\n+class DummyBertModel(DummyBertPreTrainedModel):\n     _no_split_modules = [\"DummyBertEmbeddings\", \"DummyBertLayer\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -884,12 +794,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(DUMMY_BERT_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -906,26 +811,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "393ca6f5a137581fc777713146a038ab0e599952",
            "filename": "examples/modular-transformers/modeling_from_uppercase_model.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_from_uppercase_model.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -10,6 +10,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import logging\n from .configuration_from_uppercase_model import FromUppercaseModelTextConfig, FromUppercaseModelVisionConfig\n@@ -138,7 +139,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return hidden_states\n \n \n-class FromUppercaseModelEncoderLayer(nn.Module):\n+class FromUppercaseModelEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Union[FromUppercaseModelVisionConfig, FromUppercaseModelTextConfig]):\n         super().__init__()\n         self.embed_dim = config.hidden_size"
        },
        {
            "sha": "3ddb9f80948f1d1849563e374d35bd2bdb6f9325",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 14,
            "deletions": 252,
            "changes": 266,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -4,37 +4,25 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_multimodal1.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_multimodal1 import Multimodal1TextConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -232,15 +220,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -311,27 +292,7 @@ def forward(\n         return outputs\n \n \n-MULTIMODAL1_TEXT_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`Multimodal1TextConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Multimodal1Text Model outputting raw hidden-states without any specific head on top.\",\n-    MULTIMODAL1_TEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Multimodal1TextPreTrainedModel(PreTrainedModel):\n     config_class = Multimodal1TextConfig\n     base_model_prefix = \"model\"\n@@ -360,88 +321,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-MULTIMODAL1_TEXT_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Multimodal1Text Model outputting raw hidden-states without any specific head on top.\",\n-    MULTIMODAL1_TEXT_START_DOCSTRING,\n-)\n+@auto_docstring\n class Multimodal1TextModel(Multimodal1TextPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`Multimodal1TextDecoderLayer`]\n-\n-    Args:\n-        config: Multimodal1TextConfig\n-    \"\"\"\n-\n     def __init__(self, config: Multimodal1TextConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -465,7 +346,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(MULTIMODAL1_TEXT_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -513,8 +394,12 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n         hidden_states = inputs_embeds\n@@ -559,126 +444,3 @@ def forward(\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n-\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask"
        },
        {
            "sha": "628bd013be8b02f8ef141a9e221f305a2d6e54eb",
            "filename": "examples/modular-transformers/modeling_multimodal2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 57,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal2.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -13,15 +13,10 @@\n from transformers.utils import add_start_docstrings\n \n from ...activations import ACT2FN\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import (\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    logging,\n-    replace_return_docstrings,\n-    torch_int,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n from .configuration_multimodal2 import Multimodal2Config, Multimodal2TextConfig, Multimodal2VisionConfig\n \n \n@@ -229,7 +224,7 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Multimodal2VisionEncoderLayer(nn.Module):\n+class Multimodal2VisionEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.embed_dim = config.hidden_size\n@@ -344,21 +339,12 @@ def forward(\n         for idx, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    causal_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -458,24 +444,6 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=Fals\n         return embeddings\n \n \n-MULTIMODAL2_VISION_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`Multimodal2ImageProcessor.__call__`] for details.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        interpolate_pos_encoding (`bool`, *optional*, defaults `False`):\n-            Whether to interpolate the pre-trained position encodings.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n class Multimodal2VisionTransformer(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -488,19 +456,14 @@ def __init__(self, config):\n         self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: Optional[bool] = False,\n     ) -> BaseModelOutputWithPooling:\n-        r\"\"\"\n-        Returns:\n-\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -530,17 +493,15 @@ def forward(\n         )\n \n \n+@auto_docstring\n class Multimodal2VisionPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = Multimodal2Config\n     base_model_prefix = \"multimodal2_vision\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n     _supports_flash_attn_2 = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -567,8 +528,7 @@ def get_input_embeddings(self) -> nn.Module:\n         return self.vision_model.embeddings.patch_embedding\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(MULTIMODAL2_VISION_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Multimodal2VisionConfig)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: Optional[torch.FloatTensor] = None,\n@@ -577,9 +537,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n     ) -> BaseModelOutputWithPooling:\n         r\"\"\"\n-        Returns:\n-\n-        Examples:\n+        Example:\n \n         ```python\n         >>> from PIL import Image"
        },
        {
            "sha": "ad27fc25448404e11477f73c70cdc76acaa0b439",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 259,
            "changes": 280,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -4,36 +4,24 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_my_new_model2.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, StaticCache\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-)\n+from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_my_new_model2 import MyNewModel2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -230,15 +218,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -309,27 +290,7 @@ def forward(\n         return outputs\n \n \n-MY_NEW_MODEL2_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`MyNewModel2Config`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare MyNewModel2 Model outputting raw hidden-states without any specific head on top.\",\n-    MY_NEW_MODEL2_START_DOCSTRING,\n-)\n+@auto_docstring\n class MyNewModel2PreTrainedModel(PreTrainedModel):\n     config_class = MyNewModel2Config\n     base_model_prefix = \"model\"\n@@ -358,88 +319,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-MY_NEW_MODEL2_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare MyNewModel2 Model outputting raw hidden-states without any specific head on top.\",\n-    MY_NEW_MODEL2_START_DOCSTRING,\n-)\n+@auto_docstring\n class MyNewModel2Model(MyNewModel2PreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`MyNewModel2DecoderLayer`]\n-\n-    Args:\n-        config: MyNewModel2Config\n-    \"\"\"\n-\n     def __init__(self, config: MyNewModel2Config):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -463,19 +344,19 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwarg for now\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -507,8 +388,12 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n         )\n \n         # embed positions\n@@ -540,6 +425,7 @@ def forward(\n                 use_cache=use_cache,\n                 cache_position=cache_position,\n                 position_embeddings=position_embeddings,\n+                **kwargs,\n             )\n \n             hidden_states = layer_outputs[0]\n@@ -560,132 +446,9 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The MyNewModel2 Model transformer with a sequence classification head on top (linear layer).\n \n     [`MyNewModel2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n@@ -696,8 +459,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n     no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n     padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n     each row of the batch).\n-    \"\"\",\n-    MY_NEW_MODEL2_START_DOCSTRING,\n+    \"\"\"\n )\n class MyNewModel2ForSequenceClassification(MyNewModel2PreTrainedModel):\n     def __init__(self, config):\n@@ -716,7 +478,7 @@ def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(MY_NEW_MODEL2_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "429adbe688805f7a653254fb2d333a7eea3091f9",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 45,
            "deletions": 56,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -22,68 +22,48 @@\n \n \n @dataclass\n-class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for NewTaskModel outputs, with hidden states and attentions.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+)\n+class NewTaskModelModelOutputWithPast(BaseModelOutputWithPast):\n+    r\"\"\"\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n-class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for NewTaskModel causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n+    \"\"\"\n+)\n+class NewTaskModelCausalLMOutputWithPast(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+        Language modeling loss (for next-token prediction).\n+    logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n+        Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+        Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+        `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+        Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+        `past_key_values` input) to speed up sequential decoding.\n+    image_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+        image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -157,6 +137,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    def set_decoder(self, decoder):\n+        self.language_model = decoder\n+\n+    def get_decoder(self):\n+        return self.language_model\n+\n     def _update_causal_mask(\n         self,\n         attention_mask,\n@@ -406,10 +392,13 @@ def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n     def set_decoder(self, decoder):\n-        self.model = decoder\n+        self.model.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.model\n+        return self.model.get_decoder()\n+\n+    def get_image_features(self, pixel_values):\n+        return self.model.get_image_features(pixel_values)\n \n     # Make modules available throught conditional class for BC\n     @property"
        },
        {
            "sha": "320b8eee15cb802aed08a298cdb50344d6ddf003",
            "filename": "examples/modular-transformers/modeling_roberta.py",
            "status": "modified",
            "additions": 22,
            "deletions": 137,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_roberta.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -14,24 +14,16 @@\n \n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa, _prepare_4d_causal_attention_mask_for_sdpa\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPastAndCrossAttentions, BaseModelOutputWithPoolingAndCrossAttentions\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward, find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import (\n-    add_code_sample_docstrings,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    get_torch_version,\n-    logging,\n-)\n+from ...utils import auto_docstring, get_torch_version, logging\n from .configuration_roberta import RobertaConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"google-roberta/roberta-base-uncased\"\n-_CONFIG_FOR_DOC = \"RobertaConfig\"\n-\n \n class RobertaEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n@@ -435,7 +427,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-class RobertaLayer(nn.Module):\n+class RobertaLayer(GradientCheckpointingLayer):\n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n@@ -560,27 +552,15 @@ def forward(\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n             past_key_value = past_key_values[i] if past_key_values is not None else None\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer_module.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = layer_module(\n-                    hidden_states,\n-                    attention_mask,\n-                    layer_head_mask,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    past_key_value,\n-                    output_attentions,\n-                )\n+            layer_outputs = layer_module(\n+                hidden_states,\n+                attention_mask,\n+                layer_head_mask,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask=encoder_attention_mask,\n+                past_key_value=past_key_value,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n             if use_cache:\n@@ -742,12 +722,8 @@ def load_tf_weights_in_roberta(model, config, tf_checkpoint_path):\n     return model\n \n \n+@auto_docstring\n class RobertaPreTrainedModel(PreTrainedModel):\n-    \"\"\"\n-    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n-    models.\n-    \"\"\"\n-\n     config_class = RobertaConfig\n     load_tf_weights = load_tf_weights_in_roberta\n     base_model_prefix = \"roberta\"\n@@ -773,79 +749,8 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n \n \n-ROBERTA_START_DOCSTRING = r\"\"\"\n-\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`RobertaConfig`]): Model configuration class with all the parameters of the model.\n-            Initializing with a config file does not load the weights associated with the model, only the\n-            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-ROBERTA_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `({0})`):\n-            Indices of input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-        token_type_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Segment token indices to indicate first and second portions of the inputs. Indices are selected in `[0,\n-            1]`:\n-\n-            - 0 corresponds to a *sentence A* token,\n-            - 1 corresponds to a *sentence B* token.\n-\n-            [What are token type IDs?](../glossary#token-type-ids)\n-        position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.max_position_embeddings - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        head_mask (`torch.FloatTensor` of shape `(num_heads,)` or `(num_layers, num_heads)`, *optional*):\n-            Mask to nullify selected heads of the self-attention modules. Mask values selected in `[0, 1]`:\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-\n-        inputs_embeds (`torch.FloatTensor` of shape `({0}, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Roberta Model transformer outputting raw hidden-states without any specific head on top.\",\n-    ROBERTA_START_DOCSTRING,\n-)\n-class RobertaModel(RobertaPreTrainedModel):\n-    \"\"\"\n-\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of\n     cross-attention is added between the self-attention layers, following the architecture described in [Attention is\n     all you need](https://huggingface.co/papers/1706.03762) by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,\n@@ -855,10 +760,15 @@ class RobertaModel(RobertaPreTrainedModel):\n     to `True`. To be used in a Seq2Seq model, the model needs to initialized with both `is_decoder` argument and\n     `add_cross_attention` set to `True`; an `encoder_hidden_states` is then expected as an input to the forward pass.\n     \"\"\"\n-\n+)\n+class RobertaModel(RobertaPreTrainedModel):\n     _no_split_modules = [\"RobertaEmbeddings\", \"RobertaLayer\"]\n \n     def __init__(self, config, add_pooling_layer=True):\n+        r\"\"\"\n+        add_pooling_layer (bool, *optional*, defaults to `True`):\n+            Whether to add a pooling layer\n+        \"\"\"\n         super().__init__(config)\n         self.config = config\n \n@@ -887,12 +797,7 @@ class PreTrainedModel\n         for layer, heads in heads_to_prune.items():\n             self.encoder.layer[layer].attention.prune_heads(heads)\n \n-    @add_start_docstrings_to_model_forward(ROBERTA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n-    @add_code_sample_docstrings(\n-        checkpoint=_CHECKPOINT_FOR_DOC,\n-        output_type=BaseModelOutputWithPoolingAndCrossAttentions,\n-        config_class=_CONFIG_FOR_DOC,\n-    )\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -909,26 +814,6 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPoolingAndCrossAttentions]:\n-        r\"\"\"\n-        encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n-            the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n-            Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n-            the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n-            Contains precomputed key and value hidden states of the attention blocks. Can be used to speed up decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        \"\"\"\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states"
        },
        {
            "sha": "a99174908d994999b08a79d06af8390b729bc4de",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 6,
            "deletions": 252,
            "changes": 258,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -12,33 +12,17 @@\n from transformers.modeling_outputs import CausalLMOutputWithPast\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-)\n+from ...utils import auto_docstring, can_return_tuple\n from .configuration_super import SuperConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class SuperRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -233,15 +217,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,\n@@ -312,27 +289,7 @@ def forward(\n         return outputs\n \n \n-SUPER_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`SuperConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Super Model outputting raw hidden-states without any specific head on top.\",\n-    SUPER_START_DOCSTRING,\n-)\n+@auto_docstring\n class SuperPreTrainedModel(PreTrainedModel):\n     config_class = SuperConfig\n     base_model_prefix = \"model\"\n@@ -361,88 +318,8 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n \n \n-SUPER_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length) or `BlockMask`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            If the model is configured to use flex_attention, it will attempt to convert the mask Tensor into a BlockMask,\n-            but you can also pass a `BlockMask` object directly here.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`.\n-\n-            [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`Cache`, *optional*):\n-            Pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used to speed up sequential decoding. This typically consists in the `past_key_values`\n-            returned by the model at a previous stage of decoding, when `use_cache=True` or `config.use_cache=True`.\n-\n-            It is a [`~cache_utils.Cache`] instance. For more details, see our [kv cache guide](https://huggingface.co/docs/transformers/en/kv_cache).\n-\n-            If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that don't\n-            have their past key value states given to this model) of shape `(batch_size, 1)` instead of all `input_ids`\n-            of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare Super Model outputting raw hidden-states without any specific head on top.\",\n-    SUPER_START_DOCSTRING,\n-)\n+@auto_docstring\n class SuperModel(SuperPreTrainedModel):\n-    \"\"\"\n-    Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`SuperDecoderLayer`]\n-\n-    Args:\n-        config: SuperConfig\n-    \"\"\"\n-\n     def __init__(self, config: SuperConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -466,7 +343,7 @@ def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n     @can_return_tuple\n-    @add_start_docstrings_to_model_forward(SUPER_INPUTS_DOCSTRING)\n+    @auto_docstring\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n@@ -494,126 +371,3 @@ def forward(\n         )\n         out.logits *= 2**4\n         return out\n-\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask"
        },
        {
            "sha": "ec49c0fbebce2a7b98aeaaf7bc7770366596608f",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -14,13 +14,9 @@\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n from .configuration_switch_function import SwitchFunctionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def rotate_half(x):\n     # Split and rotate. Note that this function is different from e.g. Llama.\n     x1 = x[..., ::2]\n@@ -145,15 +141,8 @@ def forward(\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n         attention_interface: Callable = eager_attention_forward\n-\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         attn_output, attn_weights = attention_interface(\n             self,"
        },
        {
            "sha": "910d568a1e7203fd9c0420330ddf7f96dcaabd63",
            "filename": "examples/modular-transformers/modeling_test_detr.py",
            "status": "modified",
            "additions": 72,
            "deletions": 184,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/examples%2Fmodular-transformers%2Fmodeling_test_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_test_detr.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -16,26 +16,18 @@\n from ...activations import ACT2FN\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import meshgrid\n-from ...utils import (\n-    ModelOutput,\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    is_timm_available,\n-    replace_return_docstrings,\n-    requires_backends,\n-)\n+from ...utils import ModelOutput, auto_docstring, is_timm_available, requires_backends\n from ...utils.backbone_utils import load_backbone\n from .configuration_test_detr import TestDetrConfig\n \n \n if is_timm_available():\n     from timm import create_model\n \n-_CONFIG_FOR_DOC = \"TestDetrConfig\"\n-\n \n @use_kernel_forward_from_hub(\"MultiScaleDeformableAttention\")\n class MultiScaleDeformableAttention(nn.Module):\n@@ -93,32 +85,24 @@ def forward(\n \n \n @dataclass\n-class TestDetrDecoderOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the TestDetrDecoder. This class adds two attributes to\n     BaseModelOutputWithCrossAttentions, namely:\n     - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n     - a stacked tensor of intermediate reference points.\n-\n-    Args:\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the model at the output of each layer\n-            plus the initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights after the attention softmax, used to compute the weighted average in\n-            the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-            used to compute the weighted average in the cross-attention heads.\n+    \"\"\"\n+)\n+class TestDetrDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n@@ -130,47 +114,27 @@ class TestDetrDecoderOutput(ModelOutput):\n \n \n @dataclass\n-class TestDetrModelOutput(ModelOutput):\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     Base class for outputs of the Deformable DETR encoder-decoder model.\n-\n-    Args:\n-        init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-            Initial reference points sent through the Transformer decoder.\n-        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-            Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-        intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-            Stacked intermediate hidden states (output of each layer of the decoder).\n-        intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-            Stacked intermediate reference points (reference points of each layer of the decoder).\n-        decoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, num_queries, hidden_size)`. Hidden-states of the decoder at the output of each layer\n-            plus the initial embedding outputs.\n-        decoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, num_queries,\n-            num_queries)`. Attentions weights of the decoder, after the attention softmax, used to compute the weighted\n-            average in the self-attention heads.\n-        cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the\n-            weighted average in the cross-attention heads.\n-        encoder_last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Sequence of hidden-states at the output of the last layer of the encoder of the model.\n-        encoder_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each layer) of\n-            shape `(batch_size, sequence_length, hidden_size)`. Hidden-states of the encoder at the output of each\n-            layer plus the initial embedding outputs.\n-        encoder_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_queries, num_heads, 4, 4)`.\n-            Attentions weights of the encoder, after the attention softmax, used to compute the weighted average in the\n-            self-attention heads.\n-        enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-            picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-            foreground and background).\n-        enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-            Logits of predicted bounding boxes coordinates in the first stage.\n+    \"\"\"\n+)\n+class TestDetrModelOutput(ModelOutput):\n+    r\"\"\"\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n     \"\"\"\n \n     init_reference_points: Optional[torch.FloatTensor] = None\n@@ -635,7 +599,7 @@ def forward(\n         return attn_output, attn_weights_reshaped\n \n \n-class TestDetrEncoderLayer(nn.Module):\n+class TestDetrEncoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TestDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -724,7 +688,7 @@ def forward(\n         return outputs\n \n \n-class TestDetrDecoderLayer(nn.Module):\n+class TestDetrDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: TestDetrConfig):\n         super().__init__()\n         self.embed_dim = config.d_model\n@@ -837,6 +801,7 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n class TestDetrPreTrainedModel(PreTrainedModel):\n     config_class = TestDetrConfig\n     base_model_prefix = \"model\"\n@@ -1001,29 +966,16 @@ def forward(\n         for i, encoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n                 encoder_states = encoder_states + (hidden_states,)\n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    encoder_layer.__call__,\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings,\n-                    reference_points,\n-                    spatial_shapes,\n-                    spatial_shapes_list,\n-                    level_start_index,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = encoder_layer(\n-                    hidden_states,\n-                    attention_mask,\n-                    position_embeddings=position_embeddings,\n-                    reference_points=reference_points,\n-                    spatial_shapes=spatial_shapes,\n-                    spatial_shapes_list=spatial_shapes_list,\n-                    level_start_index=level_start_index,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                position_embeddings=position_embeddings,\n+                reference_points=reference_points,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                level_start_index=level_start_index,\n+                output_attentions=output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1155,31 +1107,17 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n-                    hidden_states,\n-                    position_embeddings,\n-                    reference_points_input,\n-                    spatial_shapes,\n-                    spatial_shapes_list,\n-                    level_start_index,\n-                    encoder_hidden_states,\n-                    encoder_attention_mask,\n-                    output_attentions,\n-                )\n-            else:\n-                layer_outputs = decoder_layer(\n-                    hidden_states,\n-                    position_embeddings=position_embeddings,\n-                    encoder_hidden_states=encoder_hidden_states,\n-                    reference_points=reference_points_input,\n-                    spatial_shapes=spatial_shapes,\n-                    spatial_shapes_list=spatial_shapes_list,\n-                    level_start_index=level_start_index,\n-                    encoder_attention_mask=encoder_attention_mask,\n-                    output_attentions=output_attentions,\n-                )\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings,\n+                reference_points_input,\n+                spatial_shapes,\n+                spatial_shapes_list,\n+                level_start_index,\n+                encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                encoder_attention_mask,\n+                output_attentions,\n+            )\n \n             hidden_states = layer_outputs[0]\n \n@@ -1253,67 +1191,11 @@ def build_position_encoding(config):\n     return position_embedding\n \n \n-TEST_DETR_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`TestDetrConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-TEST_DETR_INPUTS_DOCSTRING = r\"\"\"\n-    Args:\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n-            Pixel values. Padding will be ignored by default should you provide it.\n-\n-            Pixel values can be obtained using [`AutoImageProcessor`]. See [`TestDetrImageProcessor.__call__`]\n-            for details.\n-\n-        pixel_mask (`torch.LongTensor` of shape `(batch_size, height, width)`, *optional*):\n-            Mask to avoid performing attention on padding pixel values. Mask values selected in `[0, 1]`:\n-\n-            - 1 for pixels that are real (i.e. **not masked**),\n-            - 0 for pixels that are padding (i.e. **masked**).\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n-            Not used by default. Can be used to mask object queries.\n-        encoder_outputs (`tuple(tuple(torch.FloatTensor)`, *optional*):\n-            Tuple consists of (`last_hidden_state`, *optional*: `hidden_states`, *optional*: `attentions`)\n-            `last_hidden_state` of shape `(batch_size, sequence_length, hidden_size)`, *optional*) is a sequence of\n-            hidden-states at the output of the last layer of the encoder. Used in the cross-attention of the decoder.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n-            can choose to directly pass a flattened representation of an image.\n-        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n-            embedded representation.\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"\"\"\n+@auto_docstring(\n+    custom_intro=\"\"\"\n     The bare Deformable DETR Model (consisting of a backbone and encoder-decoder Transformer) outputting raw\n     hidden-states without any specific head on top.\n-    \"\"\",\n-    TEST_DETR_START_DOCSTRING,\n+    \"\"\"\n )\n class TestDetrModel(TestDetrPreTrainedModel):\n     def __init__(self, config: TestDetrConfig):\n@@ -1486,8 +1368,7 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n         object_query = self.enc_output_norm(self.enc_output(object_query))\n         return object_query, output_proposals\n \n-    @add_start_docstrings_to_model_forward(TEST_DETR_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=TestDetrModelOutput, config_class=_CONFIG_FOR_DOC)\n+    @auto_docstring\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -1501,7 +1382,14 @@ def forward(\n         return_dict: Optional[bool] = None,\n     ) -> Union[tuple[torch.FloatTensor], TestDetrModelOutput]:\n         r\"\"\"\n-        Returns:\n+        decoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, num_queries)`, *optional*):\n+            Not used by default. Can be used to mask object queries.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing the flattened feature map (output of the backbone + projection layer), you\n+            can choose to directly pass a flattened representation of an image.\n+        decoder_inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+            Optionally, instead of initializing the queries with a tensor of zeros, you can choose to directly pass an\n+            embedded representation.\n \n         Examples:\n "
        },
        {
            "sha": "0709d31f558b410f292105d5345c6e57a8d20696",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -469,10 +469,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         # this will be used to easily index which expert is going to be sollicitated\n         expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n \n-        expert_hitted = (expert_mask.sum(dim=(-1, -2)) > 0).nonzero(as_tuple=True)[0].tolist()\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n         for expert_idx in expert_hitted:\n             expert_layer = self.experts[expert_idx]\n-            idx, top_x = torch.where(expert_mask[expert_idx])\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n             # Index the correct hidden states and compute the expert hidden state for\n             # the current expert. We need to make sure to multiply the output hidden\n             # states by `routing_weights` on the corresponding tokens (top-1 and top-2)"
        },
        {
            "sha": "7630dc2387f39870d6e0a0ee4861080a47d53d9e",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=e1e11b0299fcb932cc1ed1bddcc42352e8fbc9d5",
            "patch": "@@ -1439,7 +1439,7 @@ def compute_relative_order(self, missing_dependencies: set) -> dict[str, int]:\n \n         original_dependencies = []\n         other_files_dependencies = defaultdict(list)\n-        for dep in tuple(missing_dependencies):\n+        for dep in sorted(missing_dependencies):\n             if dep in self.added_objects_file_mapping:\n                 file = self.added_objects_file_mapping[dep]\n                 other_files_dependencies[file].append(dep)"
        }
    ],
    "stats": {
        "total": 1840,
        "additions": 238,
        "deletions": 1602
    }
}