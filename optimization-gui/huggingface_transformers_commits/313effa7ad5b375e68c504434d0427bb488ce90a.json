{
    "author": "lmarshall12",
    "message": "[onnx] use logical `or` for grounding dino mask (#40625)\n\n* change |= operator to use torch logical or for friendly export to different backends\n\n* change |= operator to use torch logical or for friendly export to different backends in grounding dino model\n\n---------\n\nCo-authored-by: Lewis Marshall <lewism@elderda.co.uk>",
    "sha": "313effa7ad5b375e68c504434d0427bb488ce90a",
    "files": [
        {
            "sha": "5d674caca6fa7dc52246addce3ff0f8d16a9edda",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/313effa7ad5b375e68c504434d0427bb488ce90a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313effa7ad5b375e68c504434d0427bb488ce90a/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=313effa7ad5b375e68c504434d0427bb488ce90a",
            "patch": "@@ -24,11 +24,7 @@\n from torch import Tensor, nn\n \n from ...activations import ACT2FN\n-from ...file_utils import (\n-    ModelOutput,\n-    is_timm_available,\n-    requires_backends,\n-)\n+from ...file_utils import ModelOutput, is_timm_available, requires_backends\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import meshgrid\n@@ -1878,7 +1874,7 @@ def generate_masks_with_special_tokens_and_transfer_map(input_ids: torch.LongTen\n     # special_tokens_mask: batch_size, num_token. 1 for special tokens. 0 for normal tokens\n     special_tokens_mask = torch.zeros((batch_size, num_token), device=input_ids.device).bool()\n     for special_token in SPECIAL_TOKENS:\n-        special_tokens_mask |= input_ids == special_token\n+        special_tokens_mask = torch.logical_or(special_tokens_mask, input_ids == special_token)\n \n     # idxs: each row is a list of indices of special tokens\n     idxs = torch.nonzero(special_tokens_mask)"
        },
        {
            "sha": "c3d498323de4dc3b60dcf8c3c8ac02f25d8c3ed1",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/313effa7ad5b375e68c504434d0427bb488ce90a/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/313effa7ad5b375e68c504434d0427bb488ce90a/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=313effa7ad5b375e68c504434d0427bb488ce90a",
            "patch": "@@ -1790,7 +1790,7 @@ def generate_masks_with_special_tokens_and_transfer_map(input_ids: torch.LongTen\n     # special_tokens_mask: batch_size, num_token. 1 for special tokens. 0 for normal tokens\n     special_tokens_mask = torch.zeros((batch_size, num_token), device=input_ids.device).bool()\n     for special_token in SPECIAL_TOKENS:\n-        special_tokens_mask |= input_ids == special_token\n+        special_tokens_mask = torch.logical_or(special_tokens_mask, input_ids == special_token)\n \n     # idxs: each row is a list of indices of special tokens\n     idxs = torch.nonzero(special_tokens_mask)"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 3,
        "deletions": 7
    }
}