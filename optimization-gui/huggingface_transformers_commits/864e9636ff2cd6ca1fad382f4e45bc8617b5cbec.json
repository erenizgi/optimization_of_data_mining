{
    "author": "faaany",
    "message": "[tests] fix `test_nemotron_8b_generation_sdpa` (#37665)\n\nadd max_new_tokens",
    "sha": "864e9636ff2cd6ca1fad382f4e45bc8617b5cbec",
    "files": [
        {
            "sha": "f7dcf273252c988d43807a40aa54cae2843654be",
            "filename": "tests/models/nemotron/test_modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/864e9636ff2cd6ca1fad382f4e45bc8617b5cbec/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/864e9636ff2cd6ca1fad382f4e45bc8617b5cbec/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnemotron%2Ftest_modeling_nemotron.py?ref=864e9636ff2cd6ca1fad382f4e45bc8617b5cbec",
            "patch": "@@ -195,7 +195,7 @@ def test_nemotron_8b_generation_sdpa(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(text, return_tensors=\"pt\").to(torch_device)\n \n-        output = model.generate(**inputs, do_sample=False)\n+        output = model.generate(**inputs, do_sample=False, max_new_tokens=10)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT, output_text)\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}