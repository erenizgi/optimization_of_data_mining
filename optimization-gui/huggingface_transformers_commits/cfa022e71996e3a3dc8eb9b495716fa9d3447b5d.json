{
    "author": "gante",
    "message": "[tests] gpt2 + `CausalLMModelTester` (#41003)\n\n* tmp commit\n\n* tmp commit\n\n* tmp commit\n\n* rm old GPT2ModelTester\n\n* nit bug\n\n* add facilities for encoder-decoder tests; add comments on ALL overwrites/extra fns\n\n* vision_encoder_decoder",
    "sha": "cfa022e71996e3a3dc8eb9b495716fa9d3447b5d",
    "files": [
        {
            "sha": "790e4d32cee036daf1397264390c6c25d4cb0b1b",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=cfa022e71996e3a3dc8eb9b495716fa9d3447b5d",
            "patch": "@@ -316,6 +316,27 @@ def test_token_classification_model(self):\n             (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.num_labels),\n         )\n \n+    def test_question_answering_model(self):\n+        if self.model_tester.question_answering_class is None:\n+            self.skipTest(\"Model does not support question answering\")\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        model = self.model_tester.question_answering_class(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask)\n+        self.assertEqual(\n+            result.start_logits.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length),\n+        )\n+        self.assertEqual(\n+            result.end_logits.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length),\n+        )\n+\n     @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n     def test_model_rope_scaling_from_config(self, scaling_type):\n         \"\"\""
        },
        {
            "sha": "737ccb7bb8f13ff1abde843e6191175a048011fc",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 252,
            "deletions": 591,
            "changes": 843,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=cfa022e71996e3a3dc8eb9b495716fa9d3447b5d",
            "patch": "@@ -12,12 +12,11 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import math\n import unittest\n \n import pytest\n \n-from transformers import DynamicCache, GPT2Config, is_torch_available\n+from transformers import GPT2Config, is_torch_available\n from transformers.testing_utils import (\n     Expectations,\n     cleanup,\n@@ -28,10 +27,8 @@\n     torch_device,\n )\n \n-from ...generation.test_utils import GenerationTesterMixin\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_modeling_common import floats_tensor, ids_tensor\n \n \n if is_torch_available():\n@@ -48,149 +45,92 @@\n     )\n \n \n-class GPT2ModelTester:\n+class GPT2ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = GPT2Config\n+        base_model_class = GPT2Model\n+        causal_lm_class = GPT2LMHeadModel\n+        sequence_classification_class = GPT2ForSequenceClassification\n+        token_classification_class = GPT2ForTokenClassification\n+        question_answering_class = GPT2ForQuestionAnswering\n+\n     def __init__(\n         self,\n         parent,\n-        batch_size=14,\n-        seq_length=7,\n-        is_training=True,\n         use_token_type_ids=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        use_mc_token_ids=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n         num_choices=4,\n-        scope=None,\n+        **kwargs,\n     ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.use_mc_token_ids = use_mc_token_ids\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n+        super().__init__(parent, use_token_type_ids=use_token_type_ids, **kwargs)\n         self.num_choices = num_choices\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def get_large_model_config(self):\n-        return GPT2Config.from_pretrained(\"openai-community/gpt2\")\n \n     def prepare_config_and_inputs(\n-        self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n+        self, extra_inputs=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n     ):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n+        # Overwritten: `GPT2DoubleHeadsModel` uses extra inputs\n+        (config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels) = (\n+            super().prepare_config_and_inputs()\n+        )\n \n-        mc_token_ids = None\n-        if self.use_mc_token_ids:\n+        if extra_inputs:\n             mc_token_ids = ids_tensor([self.batch_size, self.num_choices], self.seq_length)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n+            head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n+            config_and_inputs = (\n+                config,\n+                input_ids,\n+                input_mask,\n+                head_mask,\n+                token_type_ids,\n+                mc_token_ids,\n+                sequence_labels,\n+                token_labels,\n+                choice_labels,\n+            )\n+        else:\n+            config_and_inputs = (\n+                config,\n+                input_ids,\n+                token_type_ids,\n+                input_mask,\n+                sequence_labels,\n+                token_labels,\n+                choice_labels,\n+            )\n \n         config = self.get_config(\n-            gradient_checkpointing=gradient_checkpointing,\n             scale_attn_by_inverse_layer_idx=scale_attn_by_inverse_layer_idx,\n             reorder_and_upcast_attn=reorder_and_upcast_attn,\n         )\n \n-        head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n+        return config_and_inputs\n \n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n-    def get_config(\n-        self, gradient_checkpointing=False, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False\n-    ):\n-        return GPT2Config(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            n_inner=self.intermediate_size,\n-            activation_function=self.hidden_act,\n-            resid_pdrop=self.hidden_dropout_prob,\n-            attn_pdrop=self.attention_probs_dropout_prob,\n-            n_positions=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-            use_cache=True,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            gradient_checkpointing=gradient_checkpointing,\n-            scale_attn_by_inverse_layer_idx=scale_attn_by_inverse_layer_idx,\n-            reorder_and_upcast_attn=reorder_and_upcast_attn,\n-        )\n-\n-    def get_pipeline_config(self):\n-        config = self.get_config()\n-        config.vocab_size = 300\n+    def get_config(self, scale_attn_by_inverse_layer_idx=False, reorder_and_upcast_attn=False):\n+        # Overwritten: `GPT2Config` has extra flags and we want to test them\n+        config = super().get_config()\n+        config.scale_attn_by_inverse_layer_idx = scale_attn_by_inverse_layer_idx\n+        config.reorder_and_upcast_attn = reorder_and_upcast_attn\n         return config\n \n+    def prepare_config_and_inputs_for_common(self):\n+        # Overwritten: we want `token_type_ids` as part of the common inputs\n+        config_and_inputs = self.prepare_config_and_inputs(extra_inputs=True)\n+        config, input_ids, _, head_mask, token_type_ids, _, _, _, _ = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"head_mask\": head_mask}\n+        return config, inputs_dict\n+\n     def prepare_config_and_inputs_for_decoder(self):\n+        # Extra function: used in `encoder_decoder` tests\n         (\n             config,\n             input_ids,\n             input_mask,\n             head_mask,\n             token_type_ids,\n-            mc_token_ids,\n+            _,\n             sequence_labels,\n             token_labels,\n             choice_labels,\n-        ) = self.prepare_config_and_inputs()\n+        ) = self.prepare_config_and_inputs(extra_inputs=True)\n \n         encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n         encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n@@ -208,283 +148,9 @@ def prepare_config_and_inputs_for_decoder(self):\n             encoder_attention_mask,\n         )\n \n-    def create_and_check_gpt2_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = GPT2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, token_type_ids=token_type_ids, head_mask=head_mask)\n-        result = model(input_ids, token_type_ids=token_type_ids)\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(len(result.past_key_values), config.n_layer)\n-\n-    def create_and_check_gpt2_model_past(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = GPT2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(input_ids, token_type_ids=token_type_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids, token_type_ids=token_type_ids)\n-        outputs_no_past = model(input_ids, token_type_ids=token_type_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        output, past = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-        next_token_types = ids_tensor([self.batch_size, 1], self.type_vocab_size)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_token_type_ids = torch.cat([token_type_ids, next_token_types], dim=-1)\n-\n-        output_from_no_past = model(next_input_ids, token_type_ids=next_token_type_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, token_type_ids=next_token_types, past_key_values=past)[\n-            \"last_hidden_state\"\n-        ]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n-    def create_and_check_gpt2_model_attention_mask_past(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = GPT2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # create attention mask\n-        attn_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n-        half_seq_length = self.seq_length // 2\n-        attn_mask[:, half_seq_length:] = 0\n-\n-        # first forward pass\n-        output, past = model(input_ids, attention_mask=attn_mask).to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # change a random masked slice from input_ids\n-        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).item() + 1\n-        random_other_next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size).squeeze(-1)\n-        input_ids[:, -random_seq_idx_to_change] = random_other_next_tokens\n-\n-        # append to next input_ids and attn_mask\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        attn_mask = torch.cat(\n-            [attn_mask, torch.ones((attn_mask.shape[0], 1), dtype=torch.long, device=torch_device)],\n-            dim=1,\n-        )\n-\n-        # get two different outputs\n-        output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past, attention_mask=attn_mask)[\"last_hidden_state\"]\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx].detach()\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n-    def create_and_check_gpt2_model_past_large_inputs(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = GPT2Model(config=config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # first forward pass\n-        outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=input_mask, use_cache=True)\n-\n-        output, past = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_token_types = ids_tensor([self.batch_size, 3], self.type_vocab_size)\n-        next_mask = ids_tensor((self.batch_size, 3), vocab_size=2)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = torch.cat([input_ids, next_tokens], dim=-1)\n-        next_token_type_ids = torch.cat([token_type_ids, next_token_types], dim=-1)\n-        next_attention_mask = torch.cat([input_mask, next_mask], dim=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids, token_type_ids=next_token_type_ids, attention_mask=next_attention_mask\n-        )[\"last_hidden_state\"]\n-        output_from_past = model(\n-            next_tokens, token_type_ids=next_token_types, attention_mask=next_attention_mask, past_key_values=past\n-        )[\"last_hidden_state\"]\n-        self.parent.assertTrue(output_from_past.shape[1] == next_tokens.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = ids_tensor((1,), output_from_past.shape[-1]).item()\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx].detach()\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx].detach()\n-\n-        # test that outputs are equal for slice\n-        self.parent.assertTrue(torch.allclose(output_from_past_slice, output_from_no_past_slice, atol=1e-3))\n-\n-    def create_and_check_lm_head_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = GPT2LMHeadModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n-        self.parent.assertEqual(result.loss.shape, ())\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args, gradient_checkpointing=False\n-    ):\n-        model = GPT2LMHeadModel(config)\n-        model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n-\n-        result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n-        self.parent.assertEqual(result.loss.shape, ())\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        result.loss.backward()\n-\n-    def create_and_check_double_lm_head_model(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, *args\n-    ):\n-        model = GPT2DoubleHeadsModel(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        multiple_choice_inputs_ids = input_ids.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n-        multiple_choice_input_mask = input_mask.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n-        multiple_choice_token_type_ids = token_type_ids.unsqueeze(1).expand(-1, self.num_choices, -1).contiguous()\n-\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"mc_token_ids\": mc_token_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-            \"labels\": multiple_choice_inputs_ids,\n-        }\n-\n-        result = model(**inputs)\n-        self.parent.assertEqual(result.loss.shape, ())\n-        self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_choices, self.seq_length, self.vocab_size)\n-        )\n-        self.parent.assertEqual(result.mc_logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_gpt2_for_question_answering(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n-    ):\n-        config.num_labels = self.num_labels\n-        model = GPT2ForQuestionAnswering(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_gpt2_for_sequence_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n-    ):\n-        config.num_labels = self.num_labels\n-        model = GPT2ForSequenceClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, labels=sequence_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_gpt2_for_token_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n-    ):\n-        config.num_labels = self.num_labels\n-        model = GPT2ForTokenClassification(config)\n-        model.to(torch_device)\n-        model.eval()\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_gpt2_weight_initialization(self, config, *args):\n-        model = GPT2Model(config)\n-        model_std = model.config.initializer_range / math.sqrt(2 * model.config.n_layer)\n-        for key in model.state_dict():\n-            if \"c_proj\" in key and \"weight\" in key:\n-                self.parent.assertLessEqual(abs(torch.std(model.state_dict()[key]) - model_std), 0.001)\n-                self.parent.assertLessEqual(abs(torch.mean(model.state_dict()[key]) - 0.0), 0.01)\n-\n-    def create_and_check_cached_forward_with_and_without_attention_mask(self, config, input_ids, *args):\n-        # Relevant issue: https://github.com/huggingface/transformers/issues/31943\n-        model = GPT2Model(config)\n-        model.to(torch_device)\n-        model.eval()\n-\n-        # We want this for SDPA, eager works with a `None` attention mask\n-        assert model.config._attn_implementation == \"sdpa\", (\n-            \"This test assumes the model to have the SDPA implementation for its attention calculations.\"\n-        )\n-\n-        # Prepare cache and non_cache input, needs a full attention mask\n-        cached_len = input_ids.shape[-1] // 2\n-        input_mask = torch.ones(size=input_ids.size()).to(torch_device)\n-        cache_inputs = {\"input_ids\": input_ids[:, :cached_len], \"attention_mask\": input_mask[:, :cached_len]}\n-        non_cache_inputs = {\"input_ids\": input_ids[:, cached_len:], \"attention_mask\": input_mask}\n-\n-        # Cached forward once with the attention mask provided and the other time without it (which should assume full attention)\n-        cache_outputs = model(**cache_inputs)\n-        # Caches are mutable (unlike legacy tuples), so we need to copy them before using multiple times\n-        pkv_copy = DynamicCache(config=config)\n-        pkv_copy.update(\n-            cache_outputs.past_key_values.layers[0].keys, cache_outputs.past_key_values.layers[0].values, 0\n-        )\n-        pkv_copy.update(\n-            cache_outputs.past_key_values.layers[1].keys, cache_outputs.past_key_values.layers[1].values, 1\n-        )\n-        full_outputs_with_attention_mask = model(**non_cache_inputs, past_key_values=pkv_copy).last_hidden_state\n-        full_outputs_without_attention_mask = model(\n-            non_cache_inputs[\"input_ids\"], past_key_values=cache_outputs.past_key_values\n-        ).last_hidden_state\n-\n-        self.parent.assertTrue(\n-            torch.allclose(full_outputs_with_attention_mask, full_outputs_without_attention_mask, atol=1e-5)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"head_mask\": head_mask,\n-        }\n-\n-        return config, inputs_dict\n-\n \n @require_torch\n-class GPT2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class GPT2ModelTest(CausalLMModelTest, unittest.TestCase):\n     all_model_classes = (\n         (\n             GPT2Model,\n@@ -513,9 +179,10 @@ class GPT2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n     test_missing_keys = False\n     test_model_parallel = True\n+    model_tester_class = GPT2ModelTester\n \n-    # special case for DoubleHeads model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        # Overwritten: special case for DoubleHeads model\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n \n         if return_labels:\n@@ -537,220 +204,91 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n                 )\n         return inputs_dict\n \n-    def setUp(self):\n-        self.model_tester = GPT2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GPT2Config, n_embd=37)\n-\n-    def tearDown(self):\n-        super().tearDown()\n-        # clean-up as much as possible GPU memory occupied by PyTorch\n-        cleanup(torch_device)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_gpt2_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model(*config_and_inputs)\n-\n-    def test_gpt2_model_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_past(*config_and_inputs)\n-\n-    def test_gpt2_model_att_mask_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_attention_mask_past(*config_and_inputs)\n-\n-    def test_gpt2_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_gpt2_lm_head_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_lm_head_model(*config_and_inputs)\n-\n     def test_gpt2_double_lm_head_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_double_lm_head_model(*config_and_inputs)\n-\n-    def test_gpt2_question_answering_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_for_question_answering(*config_and_inputs)\n-\n-    def test_gpt2_sequence_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_for_sequence_classification(*config_and_inputs)\n-\n-    def test_gpt2_token_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_for_token_classification(*config_and_inputs)\n-\n-    def test_gpt2_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n-    def test_gpt2_scale_attn_by_inverse_layer_idx(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)\n-\n-    def test_gpt2_reorder_and_upcast_attn(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(reorder_and_upcast_attn=True)\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)\n-\n-    def test_gpt2_weight_initialization(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_weight_initialization(*config_and_inputs)\n-\n-    def test_cached_forward_with_and_without_attention_mask(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_cached_forward_with_and_without_attention_mask(*config_and_inputs)\n-\n-    @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant_false(self):\n-        pass\n-\n-    @slow\n-    def test_batch_generation(self):\n-        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n+        # extra test: model-specific class\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(extra_inputs=True)\n+        config, input_ids, input_mask, _, token_type_ids, mc_token_ids, _, _, _ = config_and_inputs\n+        model = GPT2DoubleHeadsModel(config)\n         model.to(torch_device)\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        tokenizer.padding_side = \"left\"\n-\n-        # Define PAD Token = EOS Token = 50256\n-        tokenizer.pad_token = tokenizer.eos_token\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        # use different length sentences to test batching\n-        sentences = [\n-            \"Hello, my dog is a little\",\n-            \"Today, I\",\n-        ]\n+        model.eval()\n \n-        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n-        input_ids = inputs[\"input_ids\"].to(torch_device)\n-        token_type_ids = torch.cat(\n-            [\n-                input_ids.new_full((input_ids.shape[0], input_ids.shape[1] - 1), 0),\n-                input_ids.new_full((input_ids.shape[0], 1), 500),\n-            ],\n-            dim=-1,\n+        multiple_choice_inputs_ids = input_ids.unsqueeze(1).expand(-1, self.model_tester.num_choices, -1).contiguous()\n+        multiple_choice_input_mask = input_mask.unsqueeze(1).expand(-1, self.model_tester.num_choices, -1).contiguous()\n+        multiple_choice_token_type_ids = (\n+            token_type_ids.unsqueeze(1).expand(-1, self.model_tester.num_choices, -1).contiguous()\n         )\n \n-        outputs = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n-            max_length=20,\n-        )\n+        inputs = {\n+            \"input_ids\": multiple_choice_inputs_ids,\n+            \"mc_token_ids\": mc_token_ids,\n+            \"attention_mask\": multiple_choice_input_mask,\n+            \"token_type_ids\": multiple_choice_token_type_ids,\n+            \"labels\": multiple_choice_inputs_ids,\n+        }\n \n-        outputs_tt = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n-            token_type_ids=token_type_ids,\n-            max_length=20,\n+        result = model(**inputs)\n+        self.assertEqual(result.loss.shape, ())\n+        self.assertEqual(\n+            result.logits.shape,\n+            (\n+                self.model_tester.batch_size,\n+                self.model_tester.num_choices,\n+                self.model_tester.seq_length,\n+                self.model_tester.vocab_size,\n+            ),\n         )\n+        self.assertEqual(result.mc_logits.shape, (self.model_tester.batch_size, self.model_tester.num_choices))\n \n-        inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n-\n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n-        inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n-\n-        batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-        batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n-        non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n-        padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n-\n-        expected_output_sentence = [\n-            \"Hello, my dog is a little bit of a mess. I'm not sure if he's going\",\n-            \"Today, I'm going to be doing a lot of research on this. I\",\n-        ]\n-        self.assertListEqual(expected_output_sentence, batch_out_sentence)\n-        self.assertTrue(batch_out_sentence_tt != batch_out_sentence)  # token_type_ids should change output\n-        self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])\n+    def test_gpt2_scale_attn_by_inverse_layer_idx(self):\n+        # extra test: model-specific flag\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n+        config, input_ids, token_type_ids, _, _, _, _ = config_and_inputs\n \n-    @slow\n-    def test_batch_generation_2heads(self):\n-        model = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")\n+        model = GPT2LMHeadModel(config)\n         model.to(torch_device)\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        tokenizer.padding_side = \"left\"\n-\n-        # This tokenizer has no pad token, so we have to set it in some way\n-        # Define PAD Token = EOS Token = 50256\n-        tokenizer.pad_token = tokenizer.eos_token\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        # use different length sentences to test batching\n-        sentences = [\n-            \"Hello, my dog is a little\",\n-            \"Today, I\",\n-        ]\n-\n-        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n-        input_ids = inputs[\"input_ids\"].to(torch_device)\n-        token_type_ids = torch.cat(\n-            [\n-                input_ids.new_full((input_ids.shape[0], input_ids.shape[1] - 1), 0),\n-                input_ids.new_full((input_ids.shape[0], 1), 500),\n-            ],\n-            dim=-1,\n+        result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n+        self.assertEqual(result.loss.shape, ())\n+        self.assertEqual(\n+            result.logits.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.vocab_size),\n         )\n+        result.loss.backward()\n \n-        outputs = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n-            max_length=20,\n-        )\n+    def test_gpt2_reorder_and_upcast_attn(self):\n+        # extra test: model-specific flag\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs(reorder_and_upcast_attn=True)\n+        config, input_ids, token_type_ids, _, _, _, _ = config_and_inputs\n \n-        outputs_tt = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n-            token_type_ids=token_type_ids,\n-            max_length=20,\n+        model = GPT2LMHeadModel(config)\n+        model.to(torch_device)\n+        result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n+        self.assertEqual(result.loss.shape, ())\n+        self.assertEqual(\n+            result.logits.shape,\n+            (self.model_tester.batch_size, self.model_tester.seq_length, self.model_tester.vocab_size),\n         )\n+        result.loss.backward()\n \n-        inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n-\n-        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n-        inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n-\n-        batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-        batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n-        non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n-        padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n+    def test_training_gradient_checkpointing(self):\n+        # overwritten: GPT2DoubleHeadsModel fails this test, non-standard class\n+        self.original_all_model_classes = self.all_model_classes\n+        self.all_model_classes = (cls for cls in self.all_model_classes if cls.__name__ != \"GPT2DoubleHeadsModel\")\n+        super().test_training_gradient_checkpointing()\n+        self.all_model_classes = self.original_all_model_classes\n \n-        expected_output_sentence = [\n-            \"Hello, my dog is a little bit of a mess. I'm not sure if he's going\",\n-            \"Today, I'm going to be doing a lot of research on this. I\",\n-        ]\n-        self.assertListEqual(expected_output_sentence, batch_out_sentence)\n-        self.assertTrue(batch_out_sentence_tt != batch_out_sentence)  # token_type_ids should change output\n-        self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        # overwritten: GPT2DoubleHeadsModel fails this test, non-standard class\n+        self.original_all_model_classes = self.all_model_classes\n+        self.all_model_classes = (cls for cls in self.all_model_classes if cls.__name__ != \"GPT2DoubleHeadsModel\")\n+        super().test_training_gradient_checkpointing_use_reentrant()\n+        self.all_model_classes = self.original_all_model_classes\n \n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"openai-community/gpt2\"\n-        model = GPT2Model.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        # overwritten: GPT2DoubleHeadsModel fails this test, non-standard class\n+        self.original_all_model_classes = self.all_model_classes\n+        self.all_model_classes = (cls for cls in self.all_model_classes if cls.__name__ != \"GPT2DoubleHeadsModel\")\n+        super().test_training_gradient_checkpointing_use_reentrant_false()\n+        self.all_model_classes = self.original_all_model_classes\n \n \n @require_torch\n@@ -915,3 +453,126 @@ def test_flash_attn_2_generate_padding_left(self):\n \n         self.assertListEqual(output_native, output_fa_2)\n         self.assertListEqual(output_native, expected_output)\n+\n+    @slow\n+    def test_batch_generation(self):\n+        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n+        model.to(torch_device)\n+        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n+\n+        tokenizer.padding_side = \"left\"\n+\n+        # Define PAD Token = EOS Token = 50256\n+        tokenizer.pad_token = tokenizer.eos_token\n+        model.config.pad_token_id = model.config.eos_token_id\n+\n+        # use different length sentences to test batching\n+        sentences = [\n+            \"Hello, my dog is a little\",\n+            \"Today, I\",\n+        ]\n+\n+        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n+        input_ids = inputs[\"input_ids\"].to(torch_device)\n+        token_type_ids = torch.cat(\n+            [\n+                input_ids.new_full((input_ids.shape[0], input_ids.shape[1] - 1), 0),\n+                input_ids.new_full((input_ids.shape[0], 1), 500),\n+            ],\n+            dim=-1,\n+        )\n+\n+        outputs = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_length=20,\n+        )\n+\n+        outputs_tt = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            token_type_ids=token_type_ids,\n+            max_length=20,\n+        )\n+\n+        inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n+\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n+        inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n+        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+\n+        batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+        batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n+        non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n+        padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n+\n+        expected_output_sentence = [\n+            \"Hello, my dog is a little bit of a mess. I'm not sure if he's going\",\n+            \"Today, I'm going to be doing a lot of research on this. I\",\n+        ]\n+        self.assertListEqual(expected_output_sentence, batch_out_sentence)\n+        self.assertTrue(batch_out_sentence_tt != batch_out_sentence)  # token_type_ids should change output\n+        self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])\n+\n+    @slow\n+    def test_batch_generation_2heads(self):\n+        model = GPT2DoubleHeadsModel.from_pretrained(\"openai-community/gpt2\")\n+        model.to(torch_device)\n+        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n+\n+        tokenizer.padding_side = \"left\"\n+\n+        # This tokenizer has no pad token, so we have to set it in some way\n+        # Define PAD Token = EOS Token = 50256\n+        tokenizer.pad_token = tokenizer.eos_token\n+        model.config.pad_token_id = model.config.eos_token_id\n+\n+        # use different length sentences to test batching\n+        sentences = [\n+            \"Hello, my dog is a little\",\n+            \"Today, I\",\n+        ]\n+\n+        inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True)\n+        input_ids = inputs[\"input_ids\"].to(torch_device)\n+        token_type_ids = torch.cat(\n+            [\n+                input_ids.new_full((input_ids.shape[0], input_ids.shape[1] - 1), 0),\n+                input_ids.new_full((input_ids.shape[0], 1), 500),\n+            ],\n+            dim=-1,\n+        )\n+\n+        outputs = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            max_length=20,\n+        )\n+\n+        outputs_tt = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=inputs[\"attention_mask\"].to(torch_device),\n+            token_type_ids=token_type_ids,\n+            max_length=20,\n+        )\n+\n+        inputs_non_padded = tokenizer(sentences[0], return_tensors=\"pt\").input_ids.to(torch_device)\n+        output_non_padded = model.generate(input_ids=inputs_non_padded, max_length=20)\n+\n+        num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n+        inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n+        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+\n+        batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+        batch_out_sentence_tt = tokenizer.batch_decode(outputs_tt, skip_special_tokens=True)\n+        non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n+        padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n+\n+        expected_output_sentence = [\n+            \"Hello, my dog is a little bit of a mess. I'm not sure if he's going\",\n+            \"Today, I'm going to be doing a lot of research on this. I\",\n+        ]\n+        self.assertListEqual(expected_output_sentence, batch_out_sentence)\n+        self.assertTrue(batch_out_sentence_tt != batch_out_sentence)  # token_type_ids should change output\n+        self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])"
        },
        {
            "sha": "8272b7e48fe4575e4293b53a2acd6b4c309e084d",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 8,
            "deletions": 24,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cfa022e71996e3a3dc8eb9b495716fa9d3447b5d/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=cfa022e71996e3a3dc8eb9b495716fa9d3447b5d",
            "patch": "@@ -906,19 +906,11 @@ def prepare_config_and_inputs(self):\n         model_tester_encoder = ViTModelTester(self, batch_size=13)\n         model_tester_decoder = GPT2ModelTester(self, batch_size=13, hidden_size=32, max_position_embeddings=512)\n         encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n-        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs()\n+        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs(extra_inputs=True)\n         config, pixel_values, labels = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            decoder_head_mask,\n-            decoder_token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = decoder_config_and_inputs\n+        decoder_config, decoder_input_ids, decoder_attention_mask, decoder_head_mask, _, _, _, _, _ = (\n+            decoder_config_and_inputs\n+        )\n \n         # make sure that cross attention layers are added\n         decoder_config.add_cross_attention = True\n@@ -1028,19 +1020,11 @@ def prepare_config_and_inputs(self):\n         model_tester_encoder = DonutSwinModelTester(self, batch_size=13)\n         model_tester_decoder = GPT2ModelTester(self, batch_size=13, hidden_size=32, max_position_embeddings=512)\n         encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n-        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs()\n+        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs(extra_inputs=True)\n         config, pixel_values, labels = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            decoder_head_mask,\n-            decoder_token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = decoder_config_and_inputs\n+        decoder_config, decoder_input_ids, decoder_attention_mask, decoder_head_mask, _, _, _, _, _ = (\n+            decoder_config_and_inputs\n+        )\n \n         # make sure that cross attention layers are added\n         decoder_config.add_cross_attention = True"
        }
    ],
    "stats": {
        "total": 896,
        "additions": 281,
        "deletions": 615
    }
}