{
    "author": "vasqu",
    "message": "[`Ernie 4.5`] Add ernie text models (#39228)\n\n* init\n\n* copied from remote\n\n* add proper structure and llama like structure\n\n* fixup\n\n* revert to state that works\n\n* get closer to llama\n\n* slow and steady\n\n* some removal\n\n* masks work\n\n* it is indeed the rope implementation, how dafuq does it mesh with the cache now hmm\n\n* nice\n\n* getting closer\n\n* closer to transformers style\n\n* let's simplify this, batching works now\n\n* simplified\n\n* working version with modular\n\n* it is indeed the rotation per weights, make it complete llama style\n\n* cleanup conversion, next to look at -> tokenizer\n\n* remove llama artefacts\n\n* fix modeling tests (common ones)\n\n* style\n\n* integration test + first look into tokenization (will need more work, focussing on modeling other models first)\n\n* style\n\n* working moe version, based on remote\n\n* lets keep it simple and go step by step - transformers annotations for modular and transformers style rope (complex view)\n\n* more cleanup\n\n* refactor namings and remove addition forXXX classes\n\n* our moe won't cut it it seems, correction bias seems to be missing in remote code version\n\n* tokenization change (remote)\n\n* our moe version works when adding normalization :D\n\n* cleanup moe\n\n* nits\n\n* cleanup modeling -> let's get to modular next\n\n* style\n\n* modular v1\n\n* minor things + attempt at conversion (which doesn't work)\n\n* no conversion follow glm, fixup modular and other nits\n\n* modular cleanup\n\n* fixes\n\n* tests, tests, tests + some moe dtype forcing\n\n* simplify modular, fix fatal fa2 bug, remaining tests\n\n* fix import issue?\n\n* some initial docs, fix bnb faulty behavior --> needs to fix some tests because of gate needing to be float\n\n* fix sdpa test, load on init dtype only\n\n* fixup post merge\n\n* style\n\n* fix doc links\n\n* tokenization cleanup beginnings\n\n* simplify tokenizer by a lot as its basically llama\n\n* tokenizer is full llama with different defaults + extra special tokens\n\n* sync og special tokens of ernie\n\n* fix decoding with numbers (also in remote done what a timing), begin of tok tests\n\n* align with remote and preserve special tokens, adjust tests to ernie legacy behavior, warning for questionable behavior (also in llama)\n\n* nits\n\n* docs\n\n* my daily post merge it is\n\n* check\n\n* tokenization update with explanations and conversion script\n\n* review on modular (til), revert some tokenizer things i did prior, remove mtp comment (low prio)\n\n* post merge fixes\n\n* fixup tokenization, llama fast is the way to go\n\n* more fixups\n\n* check\n\n* import fixes\n\n* correction bias following the paddle code\n\n* fix\n\n* fix TP plan, fix correction bias sharding during forward\n\n* style\n\n* whoops\n\n* fix tied weights\n\n* docs and last nit\n\n* license\n\n* flasky tests\n\n* move repo id, update when merged on the hub",
    "sha": "b4115a426eb284ccf4494d90aba3d264ccbd6f25",
    "files": [
        {
            "sha": "93d9886d85eaecbc193ee64677ca01dcb7c16c9e",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -441,6 +441,10 @@\n         title: Encoder Decoder Models\n       - local: model_doc/ernie\n         title: ERNIE\n+      - local: model_doc/ernie4_5\n+        title: Ernie4_5\n+      - local: model_doc/ernie4_5_moe\n+        title: Ernie4_5_MoE\n       - local: model_doc/ernie_m\n         title: ErnieM\n       - local: model_doc/esm"
        },
        {
            "sha": "b350b9d429ae07dff998eabd0c6e0e9c574bfd70",
            "filename": "docs/source/en/model_doc/ernie4_5.md",
            "status": "added",
            "additions": 99,
            "deletions": 0,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5.md?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,99 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Ernie 4.5\n+\n+## Overview\n+\n+The Ernie 4.5 model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n+This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n+model without mixture of experts (moe) with 0.3B parameters in total. It uses the standard [Llama](./llama.md) at its core.\n+\n+Other models from the family can be found at [Ernie 4.5 MoE](./ernie4_5_moe.md).\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>\n+</div>\n+\n+\n+## Usage Tips\n+\n+### Generate text\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"baidu/ERNIE-4.5-0.3B-PT\"\n+\n+# load the tokenizer and the model\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# prepare the model input\n+inputs = tokenizer(\"Hey, are you conscious? Can you talk to me?\", return_tensors=\"pt\")\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True\n+)\n+model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+# conduct text completion\n+generated_ids = model.generate(\n+    **model_inputs,\n+    max_new_tokens=32,\n+)\n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n+\n+# decode the generated ids\n+generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n+```\n+\n+This model was contributed by [Anton Vlasjuk](https://huggingface.co/AntonV).\n+The original code can be found [here](https://github.com/PaddlePaddle/ERNIE).\n+\n+\n+## Ernie4_5Config\n+\n+[[autodoc]] Ernie4_5Config\n+\n+## Ernie4_5Model\n+\n+[[autodoc]] Ernie4_5Model\n+    - forward\n+\n+## Ernie4_5ForCausalLM\n+\n+[[autodoc]] Ernie4_5ForCausalLM\n+    - forward"
        },
        {
            "sha": "9d8703e5929d5c61b989126192ed501d2913ec0c",
            "filename": "docs/source/en/model_doc/ernie4_5_moe.md",
            "status": "added",
            "additions": 183,
            "deletions": 0,
            "changes": 183,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_moe.md?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,183 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Ernie 4.5 MoE\n+\n+## Overview\n+\n+The Ernie 4.5 MoE model was released in the [Ernie 4.5 Model Family](https://ernie.baidu.com/blog/posts/ernie4.5/) release by baidu.\n+This family of models contains multiple different architectures and model sizes. This model in specific targets the base text\n+model with mixture of experts (moe) - one with 21B total, 3B active parameters and another one with 300B total, 47B active parameters.\n+It uses the standard [Llama](./llama.md) at its core combined with a specialized MoE based on [Mixtral](./mixtral.md) with additional shared\n+experts.\n+\n+Other models from the family can be found at [Ernie 4.5](./ernie4_5.md).\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://ernie.baidu.com/blog/posts/ernie4.5/overview.png\"/>\n+</div>\n+\n+\n+## Usage Tips\n+\n+### Generate text\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n+\n+# load the tokenizer and the model\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+)\n+\n+# prepare the model input\n+inputs = tokenizer(\"Hey, are you conscious? Can you talk to me?\", return_tensors=\"pt\")\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True\n+)\n+model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+# conduct text completion\n+generated_ids = model.generate(\n+    **model_inputs,\n+    max_new_tokens=32,\n+)\n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n+\n+# decode the generated ids\n+generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n+```\n+\n+### Distributed Generation with Tensor Parallelism\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n+\n+# load the tokenizer and the model\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    device_map=\"auto\",\n+    torch_dtype=torch.bfloat16,\n+    tp_plan=\"auto\",\n+)\n+\n+# prepare the model input\n+inputs = tokenizer(\"Hey, are you conscious? Can you talk to me?\", return_tensors=\"pt\")\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True\n+)\n+model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+# conduct text completion\n+generated_ids = model.generate(\n+    **model_inputs,\n+    max_new_tokens=32,\n+)\n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n+\n+# decode the generated ids\n+generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n+```\n+\n+### Quantization with Bitsandbytes\n+\n+```python\n+import torch\n+from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoTokenizer\n+\n+model_name = \"baidu/ERNIE-4.5-21B-A3B-PT\"\n+\n+# load the tokenizer and the model\n+tokenizer = AutoTokenizer.from_pretrained(model_name)\n+model = AutoModelForCausalLM.from_pretrained(\n+    model_name,\n+    device_map=\"auto\",\n+    quantization_config=BitsAndBytesConfig(load_in_4bit=True),\n+)\n+\n+# prepare the model input\n+inputs = tokenizer(\"Hey, are you conscious? Can you talk to me?\", return_tensors=\"pt\")\n+prompt = \"Hey, are you conscious? Can you talk to me?\"\n+messages = [\n+    {\"role\": \"user\", \"content\": prompt}\n+]\n+text = tokenizer.apply_chat_template(\n+    messages,\n+    tokenize=False,\n+    add_generation_prompt=True\n+)\n+model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+# conduct text completion\n+generated_ids = model.generate(\n+    **model_inputs,\n+    max_new_tokens=32,\n+)\n+output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n+\n+# decode the generated ids\n+generate_text = tokenizer.decode(output_ids, skip_special_tokens=True)\n+```\n+\n+This model was contributed by [Anton Vlasjuk](https://huggingface.co/AntonV).\n+The original code can be found [here](https://github.com/PaddlePaddle/ERNIE).\n+\n+\n+## Ernie4_5_MoEConfig\n+\n+[[autodoc]] Ernie4_5_MoEConfig\n+\n+## Ernie4_5_MoEModel\n+\n+[[autodoc]] Ernie4_5_MoEModel\n+    - forward\n+\n+## Ernie4_5_MoEForCausalLM\n+\n+[[autodoc]] Ernie4_5_MoEForCausalLM\n+    - forward\n+    - generate"
        },
        {
            "sha": "56e4145250a05f417366bfcad76d19082885206b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -3129,6 +3129,17 @@ def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n         else:\n             output_embeddings.weight = input_embeddings.weight\n \n+        # Passing hooks over to the embeddings if needed\n+        # (currently limited to tensor parallel hooks and flags only)\n+        if hasattr(input_embeddings, \"_is_hooked\") and getattr(input_embeddings, \"_hf_tp_plan\", None):\n+            output_embeddings._is_hooked = input_embeddings._is_hooked\n+            output_embeddings._hf_tp_plan = input_embeddings._hf_tp_plan\n+            output_embeddings._forward_hooks = input_embeddings._forward_hooks\n+            output_embeddings._forward_pre_hooks = input_embeddings._forward_pre_hooks\n+            output_embeddings.__repr__ = (\n+                lambda: f\"{output_embeddings.__repr__()}\\nTP Plan: {output_embeddings._hf_tp_plan}\"\n+            )\n+\n         if getattr(output_embeddings, \"bias\", None) is not None:\n             output_embeddings.bias.data = nn.functional.pad(\n                 output_embeddings.bias.data,"
        },
        {
            "sha": "b4642b9cafb2e20a0c404ab7755e7386b88204e5",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -128,6 +128,8 @@\n         (\"encoder-decoder\", \"EncoderDecoderConfig\"),\n         (\"eomt\", \"EomtConfig\"),\n         (\"ernie\", \"ErnieConfig\"),\n+        (\"ernie4_5\", \"Ernie4_5Config\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoEConfig\"),\n         (\"ernie_m\", \"ErnieMConfig\"),\n         (\"esm\", \"EsmConfig\"),\n         (\"falcon\", \"FalconConfig\"),\n@@ -520,6 +522,8 @@\n         (\"encoder-decoder\", \"Encoder decoder\"),\n         (\"eomt\", \"EoMT\"),\n         (\"ernie\", \"ERNIE\"),\n+        (\"ernie4_5\", \"Ernie4_5\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoE\"),\n         (\"ernie_m\", \"ErnieM\"),\n         (\"esm\", \"ESM\"),\n         (\"falcon\", \"Falcon\"),"
        },
        {
            "sha": "36aaec4d53c2a61ca6f4a45eb23a54129c55dc5f",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -119,6 +119,8 @@\n         (\"emu3\", \"Emu3Model\"),\n         (\"encodec\", \"EncodecModel\"),\n         (\"ernie\", \"ErnieModel\"),\n+        (\"ernie4_5\", \"Ernie4_5Model\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoEModel\"),\n         (\"ernie_m\", \"ErnieMModel\"),\n         (\"esm\", \"EsmModel\"),\n         (\"falcon\", \"FalconModel\"),\n@@ -594,6 +596,8 @@\n         (\"electra\", \"ElectraForCausalLM\"),\n         (\"emu3\", \"Emu3ForCausalLM\"),\n         (\"ernie\", \"ErnieForCausalLM\"),\n+        (\"ernie4_5\", \"Ernie4_5ForCausalLM\"),\n+        (\"ernie4_5_moe\", \"Ernie4_5_MoEForCausalLM\"),\n         (\"falcon\", \"FalconForCausalLM\"),\n         (\"falcon_h1\", \"FalconH1ForCausalLM\"),\n         (\"falcon_mamba\", \"FalconMambaForCausalLM\"),"
        },
        {
            "sha": "eda3d29ae777e7ecc6fa36d3607a0f27b9a5e2e0",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -212,6 +212,8 @@\n         (\"electra\", (\"ElectraTokenizer\", \"ElectraTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"emu3\", (\"GPT2Tokenizer\", \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"ernie4_5\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"ernie4_5_moe\", (None, \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"ernie_m\", (\"ErnieMTokenizer\" if is_sentencepiece_available() else None, None)),\n         (\"esm\", (\"EsmTokenizer\", None)),\n         (\"falcon\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "5d6e69432c9a8ac0471c3dd91660eaf912dc149c",
            "filename": "src/transformers/models/ernie4_5/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2F__init__.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_ernie4_5 import *\n+    from .modeling_ernie4_5 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e6e2795b5daa2bcd2a873997ed8890fa5703b115",
            "filename": "src/transformers/models/ernie4_5/configuration_ernie4_5.py",
            "status": "added",
            "additions": 202,
            "deletions": 0,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconfiguration_ernie4_5.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,202 @@\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Ernie 4.5 model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Ernie4_5Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Ernie4_5Model`]. It is used to instantiate an Ernie 4.5\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the Ernie 4.5 0.3B.\n+    e.g. [baidu/ERNIE-4.5-0.3B-PT](https://huggingface.co/baidu/ERNIE-4.5-0.3B-PT)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 103424):\n+            Vocabulary size of the Ernie 4.5 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Ernie4_5Model`]\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 18):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 2):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 500000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        use_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in any of the projections including mlp and attention for example.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The attention head dimension. If None, it will default to hidden_size // num_attention_heads\n+\n+    ```python\n+    >>> from transformers import Ernie4_5Model, Ernie4_5Config\n+\n+    >>> # Initializing a Ernie4_5 0.3B style configuration\n+    >>> configuration = Ernie4_5Config()\n+\n+    >>> # Initializing a model from the 0.3B style configuration\n+    >>> model = Ernie4_5Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ernie4_5\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `Ernie4_5Model`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=103424,\n+        hidden_size=1024,\n+        intermediate_size=3072,\n+        num_hidden_layers=18,\n+        num_attention_heads=16,\n+        num_key_value_heads=2,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=131072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-05,\n+        use_cache=True,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=500000.0,\n+        rope_scaling=None,\n+        use_bias=False,\n+        head_dim=128,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.use_bias = use_bias\n+        self.head_dim = head_dim if head_dim is not None else self.hidden_size // self.num_attention_heads\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, copy it it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Ernie4_5Config\"]"
        },
        {
            "sha": "25994bb1436f7d411d0644f6dfc0bc6d69d59c38",
            "filename": "src/transformers/models/ernie4_5/convert_ernie4_5_tokenizer.py",
            "status": "added",
            "additions": 72,
            "deletions": 0,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconvert_ernie4_5_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconvert_ernie4_5_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fconvert_ernie4_5_tokenizer.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,72 @@\n+# Copyright (c) 2025 HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+\n+from transformers import LlamaTokenizer, LlamaTokenizerFast\n+\n+\n+DEFAULT_CHAT_TEMPLATE = '{%- if not add_generation_prompt is defined -%}\\n    {%- set add_generation_prompt = true -%}\\n{%- endif -%}\\n{%- if not cls_token is defined -%}\\n    {%- set cls_token = \"<|begin_of_sentence|>\" -%}\\n{%- endif -%}\\n{%- if not sep_token is defined -%}\\n    {%- set sep_token = \"<|end_of_sentence|>\" -%}\\n{%- endif -%}\\n{{- cls_token -}}\\n{%- for message in messages -%}\\n    {%- if message[\"role\"] == \"user\" -%}\\n        {{- \"User: \" + message[\"content\"] + \"\\n\" -}}\\n    {%- elif message[\"role\"] == \"assistant\" -%}\\n        {{- \"Assistant: \" + message[\"content\"] + sep_token -}}\\n    {%- elif message[\"role\"] == \"system\" -%}\\n        {{- message[\"content\"] + \"\\n\" -}}\\n    {%- endif -%}\\n{%- endfor -%}\\n{%- if add_generation_prompt -%}\\n    {{- \"Assistant: \" -}}\\n{%- endif -%}'\n+DEFAULT_TEXT_ADD_TOKENS = [\n+    \"<mask:4>\",\n+    \"<mask:5>\",\n+    \"<mask:6>\",\n+    \"<mask:7>\",\n+]\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--repo_name\",\n+        help=\"Name of the repo where the tokenizer is located at.\",\n+        default=\"baidu/ERNIE-4.5-0.3B-Base-PT\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\",\n+        help=\"Whether or not to push the model to the hub at `output_dir` instead of saving it locally.\",\n+        action=\"store_true\",\n+        default=False,\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write the tokenizer\",\n+    )\n+    args = parser.parse_args()\n+\n+    hf_tok = LlamaTokenizer.from_pretrained(\n+        args.repo_name,\n+        pad_token=\"<unk>\",\n+        cls_token=\"<|begin_of_sentence|>\",\n+        sep_token=\"<|end_of_sentence|>\",\n+        mask_token=\"<mask:1>\",\n+        add_bos_token=False,\n+        add_prefix_space=False,\n+        chat_template=DEFAULT_CHAT_TEMPLATE,\n+        legacy=True,\n+    )\n+    hf_tok.model_max_length = 131072\n+    hf_tok.init_kwargs.pop(\"auto_map\", None)\n+    # special tokens which we need to map as additional special tokens instead\n+    hf_tok.init_kwargs.pop(\"header_start_token\", None)\n+    hf_tok.init_kwargs.pop(\"header_end_token\", None)\n+    hf_tok.init_kwargs.pop(\"sys_start_token\", None)\n+    hf_tok.init_kwargs.pop(\"sys_end_token\", None)\n+    for token in DEFAULT_TEXT_ADD_TOKENS:\n+        hf_tok.add_tokens([token], special_tokens=True)\n+\n+    # save slow model and convert on load time\n+    hf_tok.save_pretrained(\"/tmp/ernie4_5_tokenizer\")\n+    hf_tok_fast = LlamaTokenizerFast.from_pretrained(\"/tmp/ernie4_5_tokenizer\", from_slow=True)\n+    hf_tok_fast.save_pretrained(args.output_dir, push_to_hub=args.push_to_hub)"
        },
        {
            "sha": "aab89aa0a261357511b98946870b2d9e04d91663",
            "filename": "src/transformers/models/ernie4_5/modeling_ernie4_5.py",
            "status": "added",
            "additions": 503,
            "deletions": 0,
            "changes": 503,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodeling_ernie4_5.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,503 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/ernie4_5/modular_ernie4_5.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ernie4_5.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import check_model_inputs\n+from .configuration_ernie4_5 import Ernie4_5Config\n+\n+\n+class Ernie4_5RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Ernie4_5Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        # keeping it in full precision\n+        return cos, sin\n+\n+\n+class Ernie4_5MLP(nn.Module):\n+    def __init__(self, config: Ernie4_5Config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., 0::2]\n+    x2 = x[..., 1::2]\n+    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    # glm rope style (with full dim) and full precision\n+    original_dtype = q.dtype\n+\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Interleave them instead of usual shape\n+    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+\n+    q_embed = (q.float() * cos) + (rotate_half(q).float() * sin)\n+    k_embed = (k.float() * cos) + (rotate_half(k).float() * sin)\n+\n+    return q_embed.to(original_dtype), k_embed.to(original_dtype)\n+\n+\n+class Ernie4_5Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Ernie4_5Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+\n+        self.attention_dropout = 0.0\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.use_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Ernie4_5RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Ernie4_5RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Ernie4_5DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Ernie4_5Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Ernie4_5Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Ernie4_5MLP(config)\n+        self.input_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor]:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Ernie4_5PreTrainedModel(PreTrainedModel):\n+    config: Ernie4_5Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Ernie4_5DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _supports_static_cache = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Ernie4_5DecoderLayer,\n+        \"attentions\": Ernie4_5Attention,\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Ernie4_5RMSNorm):\n+            module.weight.data.fill_(1.0)\n+\n+\n+@auto_docstring\n+class Ernie4_5Model(Ernie4_5PreTrainedModel):\n+    def __init__(self, config: Ernie4_5Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Ernie4_5DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Ernie4_5RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Ernie4_5RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Ernie4_5ForCausalLM(Ernie4_5PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Ernie4_5Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Ernie4_5ForCausalLM\", \"Ernie4_5Model\", \"Ernie4_5PreTrainedModel\"]"
        },
        {
            "sha": "f76c7c6bdae73384f9c2a9dbb2975507e5b75518",
            "filename": "src/transformers/models/ernie4_5/modular_ernie4_5.py",
            "status": "added",
            "additions": 123,
            "deletions": 0,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5%2Fmodular_ernie4_5.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,123 @@\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Ernie 4.5 model\"\"\"\n+\n+import torch\n+from torch import nn\n+\n+from ...modeling_rope_utils import dynamic_rope_update\n+from ...utils import auto_docstring, can_return_tuple\n+from ..glm.modeling_glm import rotate_half\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaMLP,\n+    LlamaRotaryEmbedding,\n+)\n+from .configuration_ernie4_5 import Ernie4_5Config\n+\n+\n+class Ernie4_5RotaryEmbedding(LlamaRotaryEmbedding):\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        # keeping it in full precision\n+        return cos, sin\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    # glm rope style (with full dim) and full precision\n+    original_dtype = q.dtype\n+\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Interleave them instead of usual shape\n+    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+\n+    q_embed = (q.float() * cos) + (rotate_half(q).float() * sin)\n+    k_embed = (k.float() * cos) + (rotate_half(k).float() * sin)\n+\n+    return q_embed.to(original_dtype), k_embed.to(original_dtype)\n+\n+\n+class Ernie4_5MLP(LlamaMLP):\n+    def __init__(self, config: Ernie4_5Config):\n+        super().__init__()\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n+\n+\n+class Ernie4_5Attention(LlamaAttention):\n+    def __init__(self, config: Ernie4_5Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.attention_dropout = 0.0\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.use_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+\n+\n+class Ernie4_5ForCausalLM(LlamaForCausalLM):\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"Ernie4_5ForCausalLM\",\n+    \"Ernie4_5Model\",  # noqa: F822\n+    \"Ernie4_5PreTrainedModel\",  # noqa: F822\n+]"
        },
        {
            "sha": "eb30318fa25ec560d549b08421ea08f02b0dce6f",
            "filename": "src/transformers/models/ernie4_5_moe/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2F__init__.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_ernie4_5_moe import *\n+    from .modeling_ernie4_5_moe import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "cec4f4661a77a77fb5c0994159e3b7efae4d1089",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "added",
            "additions": 254,
            "deletions": 0,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,254 @@\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Ernie 4.5 MoE model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_rope_utils import rope_config_validation\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Ernie4_5_MoEConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Ernie4_5_MoEModel`]. It is used to instantiate a\n+    Ernie 4.5 MoE model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of [baidu/ERNIE-4.5-21B-A3B-PT](https://huggingface.co/baidu/ERNIE-4.5-21B-A3B-PT).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 103424):\n+            Vocabulary size of the Ernie 4.5 MoE model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Ernie4_5_MoEModel`]\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        hidden_size (`int`, *optional*, defaults to 2560):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 12288):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 28):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 20):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 4):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `32`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 131072):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 500000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        use_bias (`bool`, *optional*, defaults to `False`):\n+            Whether to use a bias in any of the projections including mlp and attention for example.\n+        moe_intermediate_size (`int`, *optional*, defaults to 1536):\n+            Intermediate size of the routed expert.\n+        moe_k (`int`, *optional*, defaults to 6):\n+            Number of selected experts.\n+        moe_num_experts (`int`, *optional*, defaults to 64):\n+            Number of routed experts.\n+        moe_num_shared_experts (`int`, *optional*, defaults to 2):\n+            The number of experts that are shared for all MoE forwards.\n+        moe_layer_start_index (`int`, *optional*, defaults to 1):\n+            The first index at which MoE layers start to appear.\n+        moe_layer_end_index (`int`, *optional*, defaults to -1):\n+            The last possible index for a MoE layer.\n+        moe_layer_interval (`int`, *optional*, defaults to 1):\n+            The intervals between MoE layers to appear.\n+        moe_norm_min (`float`, *optional*, defaults to 1e-12):\n+            Minimum division value during routing normalization.\n+        output_router_logits (`bool`, *optional*, defaults to `False`):\n+            Whether or not the router logits should be returned by the model. Enabling this will also\n+            allow the model to output the auxiliary loss, including load balancing loss and router z-loss.\n+        router_aux_loss_coef (`float`, *optional*, defaults to 0.001):\n+            The aux loss factor for the total loss.\n+\n+    ```python\n+    >>> from transformers import Ernie4_5_MoEModel, Ernie4_5_MoEConfig\n+\n+    >>> # Initializing a Ernie4_5_MoE style configuration\n+    >>> configuration = Ernie4_5_MoEConfig()\n+\n+    >>> # Initializing a model from the ERNIE-4.5-21B-A3B style configuration\n+    >>> model = Ernie4_5_MoEModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ernie4_5_moe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    attribute_map = {\"num_experts\": \"moe_num_experts\", \"num_experts_per_tok\": \"moe_k\"}\n+\n+    # Default tensor parallel plan for base model `Ernie4_5_MoE`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        # sequence parallel is pretty slow\n+        # \"norm.weight\": \"sequence_parallel\",\n+        # \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n+        # \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.*.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.*.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.*.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"local\",\n+        \"layers.*.mlp.gate_proj\": \"local_colwise\",\n+        \"layers.*.mlp.up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp\": \"gather\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=103424,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        hidden_size=2560,\n+        intermediate_size=12288,\n+        num_hidden_layers=28,\n+        num_attention_heads=20,\n+        num_key_value_heads=4,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=131072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        tie_word_embeddings=True,\n+        rope_theta=500000.0,\n+        rope_scaling=None,\n+        use_bias=False,\n+        moe_intermediate_size=1536,\n+        moe_k=6,\n+        moe_num_experts=64,\n+        moe_num_shared_experts=2,\n+        moe_layer_start_index=1,\n+        moe_layer_end_index=-1,\n+        moe_layer_interval=1,\n+        moe_norm_min=1e-12,\n+        output_router_logits=False,\n+        router_aux_loss_coef=0.001,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.use_bias = use_bias\n+\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n+        # MoE arguments\n+        self.moe_intermediate_size = moe_intermediate_size\n+        self.moe_k = moe_k\n+        self.moe_num_experts = moe_num_experts\n+        self.moe_num_shared_experts = moe_num_shared_experts\n+        self.moe_layer_start_index = moe_layer_start_index\n+        self.moe_layer_end_index = self.num_hidden_layers - 1 if moe_layer_end_index == -1 else moe_layer_end_index\n+        self.moe_layer_interval = moe_layer_interval\n+        self.moe_norm_min = moe_norm_min\n+        self.output_router_logits = output_router_logits\n+        self.router_aux_loss_coef = router_aux_loss_coef\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"Ernie4_5_MoEConfig\"]"
        },
        {
            "sha": "89b9b7bb17051960d53aca672b483eb6acc54f06",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "added",
            "additions": 779,
            "deletions": 0,
            "changes": 779,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,779 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ernie4_5_moe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.generic import OutputRecorder, check_model_inputs\n+from .configuration_ernie4_5_moe import Ernie4_5_MoEConfig\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Ernie4_5_MoERMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Ernie4_5_MoERMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class Ernie4_5_MoEMLP(nn.Module):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = intermediate_size if intermediate_size is not None else config.intermediate_size\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Ernie4_5_MoERotaryEmbedding(nn.Module):\n+    def __init__(self, config: Ernie4_5_MoEConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        # keeping it in full precision\n+        return cos, sin\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., 0::2]\n+    x2 = x[..., 1::2]\n+    return torch.stack((-x2, x1), dim=-1).flatten(-2)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    # glm rope style (with full dim) and full precision\n+    original_dtype = q.dtype\n+\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+\n+    # Interleave them instead of usual shape\n+    cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+    sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n+\n+    q_embed = (q.float() * cos) + (rotate_half(q).float() * sin)\n+    k_embed = (k.float() * cos) + (rotate_half(k).float() * sin)\n+\n+    return q_embed.to(original_dtype), k_embed.to(original_dtype)\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class Ernie4_5_MoEAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Ernie4_5_MoEConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+\n+        self.attention_dropout = 0.0\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.use_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Ernie4_5_MoEStatics(nn.Module):\n+    \"\"\"\n+    Stores MoE (Mixture of Experts) statistics\n+        - Bias for the gating\n+        - Additionally, usage per expert in the original codebase\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        num_experts_groups = 1\n+        num_experts = config.moe_num_experts\n+\n+        self.e_score_correction_bias = nn.Parameter(\n+            torch.zeros(num_experts_groups, num_experts, dtype=torch.float32),\n+            requires_grad=False,\n+        )\n+\n+    def forward(self, hidden_states):\n+        # NOTE: This is a workaround to enable TP with a module that only has parameters\n+        #\n+        # Otherwise, it stays as `DTensor` when called in the \"super\" forward\n+        #   1. All other tensors are local (`torch.Tensor`)\n+        #   2. Isolate does not work on `nn.Module` which only has parameters\n+        return hidden_states + self.e_score_correction_bias.squeeze()\n+\n+\n+class Ernie4_5_MoESparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n+\n+    Ernie 4.5 MoE's original formula is based on case (2) with\n+    (optional) shared experts and a corrections bias during gating.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n+\n+        # correction bias (yes it seems to be a typo with statics <> statistics)\n+        self.moe_statics = Ernie4_5_MoEStatics(config)\n+\n+        # gating\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.experts = nn.ModuleList(\n+            [Ernie4_5_MoEMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n+        )\n+        self.norm_min = config.moe_norm_min\n+\n+        # (optional) shared experts for all forwards\n+        self.shared_experts = None\n+        if config.moe_num_shared_experts > 0:\n+            self.shared_experts = Ernie4_5_MoEMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+\n+        # (Optional) shared experts\n+        if self.shared_experts is not None:\n+            shared_output = self.shared_experts(hidden_states)\n+\n+        device_type = (\n+            hidden_states.device.type\n+            if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n+            else \"cpu\"\n+        )\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            # router_logits: (batch * sequence_length, n_experts)\n+            router_logits = self.gate(hidden_states.float())\n+\n+            # NOTE: we are using the original code base at\n+            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n+            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            routing_weights = self.moe_statics(routing_weights)\n+            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            )\n+            routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hitted:\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+\n+        # Add (optional) shared experts to the result\n+        if self.shared_experts is not None:\n+            final_hidden_states = final_hidden_states + shared_output\n+\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n+\n+\n+class Ernie4_5_MoEDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config, layer_idx):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Ernie4_5_MoEAttention(config, layer_idx)\n+\n+        if (\n+            ((layer_idx + 1) % config.moe_layer_interval == 0)\n+            and layer_idx >= config.moe_layer_start_index\n+            and layer_idx <= config.moe_layer_end_index\n+        ):\n+            self.mlp = Ernie4_5_MoESparseMoeBlock(config)\n+        else:\n+            self.mlp = Ernie4_5_MoEMLP(config)\n+\n+        self.input_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.post_attention_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> torch.FloatTensor:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n+                `(batch, sequence_length)` where padding elements are indicated by 0.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_router_logits (`bool`, *optional*):\n+                Whether or not to return the logits of all the routers. They are useful for computing the router loss,\n+                and should not be returned during inference.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            position_embeddings (`tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            position_embeddings=position_embeddings,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        # For the MoE layers, we need to unpack\n+        if isinstance(hidden_states, tuple):\n+            hidden_states, _ = hidden_states\n+        hidden_states = residual + hidden_states\n+\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEPreTrainedModel(PreTrainedModel):\n+    config: Ernie4_5_MoEConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Ernie4_5_MoEDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_static_cache = False  # MoE models don't work with torch.compile (`torch.where(condition)` not supported)\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"router_logits\": OutputRecorder(Ernie4_5_MoESparseMoeBlock, index=1),\n+        \"hidden_states\": Ernie4_5_MoEDecoderLayer,\n+        \"attentions\": Ernie4_5_MoEAttention,\n+    }\n+    _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Ernie4_5_MoERMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Ernie4_5_MoEStatics):\n+            module.e_score_correction_bias.data.zero_()\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEModel(Ernie4_5_MoEPreTrainedModel):\n+    def __init__(self, config: Ernie4_5_MoEConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Ernie4_5_MoEDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Ernie4_5_MoERMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = Ernie4_5_MoERotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_tokens = value\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+def load_balancing_loss_func(\n+    gate_logits: Union[torch.Tensor, tuple[torch.Tensor], None],\n+    num_experts: Optional[int] = None,\n+    top_k=2,\n+    attention_mask: Optional[torch.Tensor] = None,\n+) -> Union[torch.Tensor, int]:\n+    r\"\"\"\n+    Computes auxiliary load balancing loss as in Switch Transformer - implemented in Pytorch.\n+\n+    See Switch Transformer (https://huggingface.co/papers/2101.03961) for more details. This function implements the loss\n+    function presented in equations (4) - (6) of the paper. It aims at penalizing cases where the routing between\n+    experts is too unbalanced.\n+\n+    Args:\n+        gate_logits:\n+            Logits from the `gate`, should be a tuple of model.config.num_hidden_layers tensors of\n+            shape [batch_size X sequence_length, num_experts].\n+        num_experts:\n+            Number of experts\n+        top_k:\n+            The number of experts to route per-token, can be also interpreted as the `top-k` routing\n+            parameter.\n+        attention_mask (`torch.Tensor`, *optional*):\n+            The attention_mask used in forward function\n+            shape [batch_size X sequence_length] if not None.\n+\n+    Returns:\n+        The auxiliary loss.\n+    \"\"\"\n+    if gate_logits is None or not isinstance(gate_logits, tuple):\n+        return 0\n+\n+    if isinstance(gate_logits, tuple):\n+        compute_device = gate_logits[0].device\n+        concatenated_gate_logits = torch.cat([layer_gate.to(compute_device) for layer_gate in gate_logits], dim=0)\n+\n+    routing_weights = torch.nn.functional.softmax(concatenated_gate_logits, dim=-1)\n+\n+    _, selected_experts = torch.topk(routing_weights, top_k, dim=-1)\n+\n+    expert_mask = torch.nn.functional.one_hot(selected_experts, num_experts)\n+\n+    if attention_mask is None:\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.mean(expert_mask.float(), dim=0)\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.mean(routing_weights, dim=0)\n+    else:\n+        batch_size, sequence_length = attention_mask.shape\n+        num_hidden_layers = concatenated_gate_logits.shape[0] // (batch_size * sequence_length)\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of expert_mask\n+        expert_attention_mask = (\n+            attention_mask[None, :, :, None, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, top_k, num_experts))\n+            .reshape(-1, top_k, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the percentage of tokens routed to each experts\n+        tokens_per_expert = torch.sum(expert_mask.float() * expert_attention_mask, dim=0) / torch.sum(\n+            expert_attention_mask, dim=0\n+        )\n+\n+        # Compute the mask that masks all padding tokens as 0 with the same shape of tokens_per_expert\n+        router_per_expert_attention_mask = (\n+            attention_mask[None, :, :, None]\n+            .expand((num_hidden_layers, batch_size, sequence_length, num_experts))\n+            .reshape(-1, num_experts)\n+            .to(compute_device)\n+        )\n+\n+        # Compute the average probability of routing to these experts\n+        router_prob_per_expert = torch.sum(routing_weights * router_per_expert_attention_mask, dim=0) / torch.sum(\n+            router_per_expert_attention_mask, dim=0\n+        )\n+\n+    overall_loss = torch.sum(tokens_per_expert * router_prob_per_expert.unsqueeze(0))\n+    return overall_loss * num_experts\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEForCausalLM(Ernie4_5_MoEPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Ernie4_5_MoEModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.use_bias)\n+\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.moe_num_experts\n+        self.num_experts_per_tok = config.moe_k\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_router_logits: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeCausalLMOutputWithPast:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+\n+        output_router_logits = (\n+            output_router_logits if output_router_logits is not None else self.config.output_router_logits\n+        )\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs: MoeModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_router_logits=output_router_logits,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.vocab_size, **kwargs)\n+\n+        aux_loss = None\n+        if output_router_logits:\n+            aux_loss = load_balancing_loss_func(\n+                outputs.router_logits,\n+                self.num_experts,\n+                self.num_experts_per_tok,\n+                attention_mask,\n+            )\n+            if labels is not None:\n+                loss += self.router_aux_loss_coef * aux_loss.to(loss.device)  # make sure to reside in the same device\n+\n+        return MoeCausalLMOutputWithPast(\n+            loss=loss,\n+            aux_loss=aux_loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            router_logits=outputs.router_logits,\n+        )\n+\n+\n+__all__ = [\"Ernie4_5_MoEForCausalLM\", \"Ernie4_5_MoEModel\", \"Ernie4_5_MoEPreTrainedModel\"]"
        },
        {
            "sha": "daf122929bc7c7e87b21ad310d2acc0604bc32e1",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "added",
            "additions": 333,
            "deletions": 0,
            "changes": 333,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,333 @@\n+# Copyright (c) 2025 Baidu, Inc. and HuggingFace Inc. team. All Rights Reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Ernie 4.5 MoE model.\"\"\"\n+\n+from typing import Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n+from ...modeling_outputs import MoeModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils.generic import check_model_inputs\n+from ..ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding, apply_rotary_pos_emb, rotate_half  # noqa: F401\n+from ..llama.modeling_llama import LlamaAttention, LlamaRMSNorm\n+from ..mixtral.modeling_mixtral import (\n+    MixtralForCausalLM,\n+    MixtralModel,\n+    MixtralPreTrainedModel,\n+)\n+from ..qwen3_moe.modeling_qwen3_moe import Qwen3MoeDecoderLayer, Qwen3MoeMLP\n+from .configuration_ernie4_5_moe import Ernie4_5_MoEConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Ernie4_5_MoERMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class Ernie4_5_MoEMLP(Qwen3MoeMLP):\n+    def __init__(self, config, intermediate_size=None):\n+        super().__init__(config, intermediate_size)\n+\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=config.use_bias)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=config.use_bias)\n+\n+\n+class Ernie4_5_MoERotaryEmbedding(Ernie4_5RotaryEmbedding):\n+    pass\n+\n+\n+class Ernie4_5_MoEAttention(LlamaAttention):\n+    def __init__(self, config: Ernie4_5_MoEConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+\n+        self.attention_dropout = 0.0\n+\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.use_bias)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.use_bias)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.use_bias)\n+\n+\n+class Ernie4_5_MoEStatics(nn.Module):\n+    \"\"\"\n+    Stores MoE (Mixture of Experts) statistics\n+        - Bias for the gating\n+        - Additionally, usage per expert in the original codebase\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        num_experts_groups = 1\n+        num_experts = config.moe_num_experts\n+\n+        self.e_score_correction_bias = nn.Parameter(\n+            torch.zeros(num_experts_groups, num_experts, dtype=torch.float32),\n+            requires_grad=False,\n+        )\n+\n+    def forward(self, hidden_states):\n+        # NOTE: This is a workaround to enable TP with a module that only has parameters\n+        #\n+        # Otherwise, it stays as `DTensor` when called in the \"super\" forward\n+        #   1. All other tensors are local (`torch.Tensor`)\n+        #   2. Isolate does not work on `nn.Module` which only has parameters\n+        return hidden_states + self.e_score_correction_bias.squeeze()\n+\n+\n+class Ernie4_5_MoESparseMoeBlock(nn.Module):\n+    \"\"\"\n+    This implementation is\n+    strictly equivalent to standard MoE with full capacity (no\n+    dropped tokens). It's faster since it formulates MoE operations\n+    in terms of block-sparse operations to accommodate imbalanced\n+    assignments of tokens to experts, whereas standard MoE either\n+    (1) drop tokens at the cost of reduced performance or (2) set\n+    capacity factor to number of experts and thus waste computation\n+    and memory on padding.\n+\n+    Ernie 4.5 MoE's original formula is based on case (2) with\n+    (optional) shared experts and a corrections bias during gating.\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.num_experts = config.moe_num_experts\n+        self.top_k = config.moe_k\n+\n+        # correction bias (yes it seems to be a typo with statics <> statistics)\n+        self.moe_statics = Ernie4_5_MoEStatics(config)\n+\n+        # gating\n+        self.gate = nn.Linear(config.hidden_size, config.moe_num_experts, bias=False, dtype=torch.float32)\n+        self.experts = nn.ModuleList(\n+            [Ernie4_5_MoEMLP(config, config.moe_intermediate_size) for _ in range(config.moe_num_experts)]\n+        )\n+        self.norm_min = config.moe_norm_min\n+\n+        # (optional) shared experts for all forwards\n+        self.shared_experts = None\n+        if config.moe_num_shared_experts > 0:\n+            self.shared_experts = Ernie4_5_MoEMLP(config, config.moe_intermediate_size * config.moe_num_shared_experts)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+    ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+\n+        # (Optional) shared experts\n+        if self.shared_experts is not None:\n+            shared_output = self.shared_experts(hidden_states)\n+\n+        device_type = (\n+            hidden_states.device.type\n+            if isinstance(hidden_states.device.type, str) and hidden_states.device.type != \"mps\"\n+            else \"cpu\"\n+        )\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            # router_logits: (batch * sequence_length, n_experts)\n+            router_logits = self.gate(hidden_states.float())\n+\n+            # NOTE: we are using the original code base at\n+            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n+            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n+            routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+            routing_weights = self.moe_statics(routing_weights)\n+            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            routing_weights = routing_weights / torch.clamp(\n+                routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n+            )\n+            routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        expert_hitted = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hitted:\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+\n+        # Add (optional) shared experts to the result\n+        if self.shared_experts is not None:\n+            final_hidden_states = final_hidden_states + shared_output\n+\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states, router_logits\n+\n+\n+class Ernie4_5_MoEDecoderLayer(Qwen3MoeDecoderLayer, nn.Module):\n+    def __init__(self, config, layer_idx):\n+        nn.Module().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = Ernie4_5_MoEAttention(config, layer_idx)\n+\n+        if (\n+            ((layer_idx + 1) % config.moe_layer_interval == 0)\n+            and layer_idx >= config.moe_layer_start_index\n+            and layer_idx <= config.moe_layer_end_index\n+        ):\n+            self.mlp = Ernie4_5_MoESparseMoeBlock(config)\n+        else:\n+            self.mlp = Ernie4_5_MoEMLP(config)\n+\n+        self.input_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+        self.post_attention_layernorm = Ernie4_5_MoERMSNorm(config.hidden_size, config.rms_norm_eps)\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEPreTrainedModel(MixtralPreTrainedModel):\n+    _keep_in_fp32_modules_strict = [\"gate\", \"moe_statics\"]\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+        elif isinstance(module, Ernie4_5_MoERMSNorm):\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, Ernie4_5_MoEStatics):\n+            module.e_score_correction_bias.data.zero_()\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEModel(MixtralModel):\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> MoeModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        return MoeModelOutputWithPast(  # only diff with Mistral is the output type, we need MoE\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Ernie4_5_MoEForCausalLM(MixtralForCausalLM, Ernie4_5_MoEPreTrainedModel):\n+    def __init__(self, config):\n+        Ernie4_5_MoEPreTrainedModel().__init__(config)\n+        self.model = Ernie4_5_MoEModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.use_bias)\n+\n+        self.router_aux_loss_coef = config.router_aux_loss_coef\n+        self.num_experts = config.moe_num_experts\n+        self.num_experts_per_tok = config.moe_k\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(self, **super_kwargs):\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+        \"\"\"\n+        super().forward(**super_kwargs)\n+\n+\n+__all__ = [\n+    \"Ernie4_5_MoEForCausalLM\",\n+    \"Ernie4_5_MoEModel\",\n+    \"Ernie4_5_MoEPreTrainedModel\",\n+]"
        },
        {
            "sha": "b6479524a94806f38e0bc7a97e5c945553004192",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -104,9 +104,11 @@ def __init__(\n         is_decoder=False,\n         scope=None,\n         expert_interval=1,\n+        moe_layer_start_index=0,\n         moe_intermediate_size=12,\n         shared_expert_intermediate_size=36,\n         shared_expert_gate=True,\n+        moe_num_shared_experts=2,\n         num_experts_per_tok=2,\n         num_experts=8,\n         mamba_n_groups=1,\n@@ -146,9 +148,11 @@ def __init__(\n         self.head_dim = self.hidden_size // self.num_attention_heads\n         self.is_decoder = is_decoder\n         self.expert_interval = expert_interval\n+        self.moe_layer_start_index = moe_layer_start_index\n         self.moe_intermediate_size = moe_intermediate_size\n         self.shared_expert_intermediate_size = shared_expert_intermediate_size\n         self.shared_expert_gate = shared_expert_gate\n+        self.moe_num_shared_experts = moe_num_shared_experts\n         self.num_experts_per_tok = num_experts_per_tok\n         self.num_experts = num_experts\n         self.mamba_n_groups = mamba_n_groups"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ernie4_5/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2F__init__.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25"
        },
        {
            "sha": "1c5bffa2c6d6e0544c1142bcfe0df131b306b26a",
            "filename": "tests/models/ernie4_5/test_modeling_ernie4_5.py",
            "status": "added",
            "additions": 122,
            "deletions": 0,
            "changes": 122,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5%2Ftest_modeling_ernie4_5.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,122 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Ernie4.5 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import is_torch_available\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    require_torch,\n+    require_torch_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoTokenizer,\n+        Ernie4_5Config,\n+        Ernie4_5ForCausalLM,\n+        Ernie4_5Model,\n+    )\n+    from transformers.models.ernie4_5.modeling_ernie4_5 import Ernie4_5RotaryEmbedding\n+\n+\n+class Ernie4_5ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Ernie4_5Config\n+        base_model_class = Ernie4_5Model\n+        causal_lm_class = Ernie4_5ForCausalLM\n+\n+\n+@require_torch\n+class Ernie4_5ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            Ernie4_5Model,\n+            Ernie4_5ForCausalLM,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Ernie4_5Model,\n+            \"text-generation\": Ernie4_5ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False  # Broken by attention refactor cc @Cyrilvallez\n+    model_tester_class = Ernie4_5ModelTester\n+    rotary_embedding_layer = Ernie4_5RotaryEmbedding  # Enables RoPE tests if set\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Ernie4_5ForCausalLM if is_torch_available() else None\n+\n+\n+@require_torch_accelerator\n+class Ernie4_5IntegrationTest(unittest.TestCase):\n+    def setup(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_ernie4_5_0p3B(self):\n+        \"\"\"\n+        An integration test for Ernie 4.5 0.3B.\n+        \"\"\"\n+        expected_texts = Expectations(\n+            {\n+                (\"cuda\", None): \"User: Hey, are you conscious? Can you talk to me?\\nAssistant: Hey! I'm here to help you with whatever you need. Are you feeling a bit overwhelmed or stressed? I'm here to listen and provide support.\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_TEXT = expected_texts.get_expectation()\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-0.3B-PT\", revision=\"refs/pr/3\")\n+        model = Ernie4_5ForCausalLM.from_pretrained(\n+            \"baidu/ERNIE-4.5-0.3B-PT\",\n+            revision=\"refs/pr/3\",\n+            device_map=\"auto\",\n+            torch_dtype=torch.bfloat16,\n+        )\n+\n+        prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        messages = [{\"role\": \"user\", \"content\": prompt}]\n+        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+        generated_ids = model.generate(\n+            model_inputs.input_ids,\n+            max_new_tokens=128,\n+            do_sample=False,\n+        )\n+\n+        generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip(\"\\n\")\n+        self.assertEqual(generated_text, EXPECTED_TEXT)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ernie4_5_moe/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2F__init__.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25"
        },
        {
            "sha": "63fb00745c62aa2183cd641f5ee7da23585aed5e",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -0,0 +1,199 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Ernie4.5 MoE model.\"\"\"\n+\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers import Ernie4_5_MoEConfig, is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    is_flaky,\n+    require_bitsandbytes,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_torch_large_accelerator,\n+    require_torch_multi_accelerator,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoTokenizer,\n+        Ernie4_5_MoEForCausalLM,\n+        Ernie4_5_MoEModel,\n+    )\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class Ernie4_5_MoEModelTester(CausalLMModelTester):\n+    config_class = Ernie4_5_MoEConfig\n+    if is_torch_available():\n+        base_model_class = Ernie4_5_MoEModel\n+        causal_lm_class = Ernie4_5_MoEForCausalLM\n+\n+\n+@require_torch\n+class Ernie4_5_MoEModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            Ernie4_5_MoEModel,\n+            Ernie4_5_MoEForCausalLM,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Ernie4_5_MoEModel,\n+            \"text-generation\": Ernie4_5_MoEForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    test_headmasking = False\n+    test_pruning = False\n+    test_all_params_have_gradient = False\n+    model_tester_class = Ernie4_5_MoEModelTester\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @is_flaky()\n+    @slow\n+    def test_flash_attn_2_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(reason=\"Model does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+                )\n+                model.to(torch_device)\n+\n+                dummy_input = inputs_dict[model_class.main_input_name]\n+                dummy_input = dummy_input.to(torch_device)\n+                outputs = model(dummy_input, output_hidden_states=True)\n+                outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+\n+                logits = outputs.hidden_states[-1]\n+                logits_fa = outputs_fa.hidden_states[-1]\n+\n+                # higher tolerance, not sure where it stems from\n+                assert torch.allclose(logits_fa, logits, atol=1e-2, rtol=1e-2)\n+\n+    # Ignore copy\n+    def test_load_balancing_loss(self):\n+        r\"\"\"\n+        Let's make sure we can actually compute the loss and do a backward on it.\n+        \"\"\"\n+        config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_labels = 3\n+        config.num_experts = 8\n+        config.expert_interval = 2\n+        config.output_router_logits = True\n+        input_ids = input_dict[\"input_ids\"]\n+        attention_mask = input_ids.ne(1).to(torch_device)\n+        model = Ernie4_5_MoEForCausalLM(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(input_ids, attention_mask=attention_mask)\n+        self.assertEqual(result.router_logits[0].shape, (91, config.num_experts))\n+        torch.testing.assert_close(result.aux_loss.cpu(), torch.tensor(2, dtype=torch.float32), rtol=1e-2, atol=1e-2)\n+\n+        # First, we make sure that adding padding tokens doesn't change the loss\n+        # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n+        pad_length = 1000\n+        # Add padding tokens (assume that pad_token_id=1) to input_ids\n+        padding_block = torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(torch_device)\n+        padded_input_ids = torch.cat((padding_block, input_ids), dim=1)  # this is to simulate padding to the left\n+        padded_attention_mask = padded_input_ids.ne(1).to(torch_device)\n+\n+        padded_result = model(padded_input_ids, attention_mask=padded_attention_mask)\n+        torch.testing.assert_close(result.aux_loss.cpu(), padded_result.aux_loss.cpu(), rtol=1e-4, atol=1e-4)\n+\n+        # We make sure that the loss of including padding tokens != the loss without padding tokens\n+        # if attention_mask=None --> we don't exclude padding tokens\n+        include_padding_result = model(padded_input_ids, attention_mask=None)\n+\n+        # This is to mimic torch.testing.assert_not_close\n+        self.assertNotAlmostEqual(include_padding_result.aux_loss.item(), result.aux_loss.item())\n+\n+\n+# Run on runners with larger accelerators (for example A10 instead of T4) with a lot of CPU RAM (e.g. g5-12xlarge)\n+@require_torch_multi_accelerator\n+@require_torch_large_accelerator\n+@require_torch\n+class Ernie4_5_MoEIntegrationTest(unittest.TestCase):\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.model = None\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Ernie4_5_MoEForCausalLM.from_pretrained(\n+                \"baidu/ERNIE-4.5-21B-A3B-PT\",\n+                revision=\"refs/pr/11\",\n+                device_map=\"auto\",\n+                load_in_4bit=True,\n+            )\n+\n+        return cls.model\n+\n+    @require_bitsandbytes\n+    @slow\n+    def test_model_21b_a3b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant: Yes, I am conscious and I can communicate with you. How can I assist you with any questions or information you need?\"  # fmt: skip\n+\n+        model = self.get_model()\n+        tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\", revision=\"refs/pr/11\")\n+        prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        messages = [{\"role\": \"user\", \"content\": prompt}]\n+        text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        model_inputs = tokenizer([text], add_special_tokens=False, return_tensors=\"pt\").to(model.device)\n+\n+        generated_ids = model.generate(\n+            model_inputs.input_ids,\n+            max_new_tokens=32,\n+            do_sample=False,\n+        )\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True).strip(\"\\n\")\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)"
        },
        {
            "sha": "5589c8cc0d61dbdaaf7ece96bae4ec15a461a570",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -258,10 +258,10 @@ def _test_eager_matches_sdpa_inference(\n                 model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"sdpa\")\n             except ValueError:\n                 model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs)\n-            model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n+            model_sdpa = model_sdpa.eval().to(torch_device)\n \n             model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n-            model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n+            model_eager = model_eager.eval().to(torch_device)\n \n         set_model_for_less_flaky_test(model_eager)\n         set_model_for_less_flaky_test(model_sdpa)"
        },
        {
            "sha": "08e3f26245a35a07969dba5b7da99ac26886a52a",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b4115a426eb284ccf4494d90aba3d264ccbd6f25/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b4115a426eb284ccf4494d90aba3d264ccbd6f25/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=b4115a426eb284ccf4494d90aba3d264ccbd6f25",
            "patch": "@@ -32,6 +32,8 @@\n CONFIG_MAPPING = transformers.models.auto.configuration_auto.CONFIG_MAPPING\n \n SPECIAL_CASES_TO_ALLOW = {\n+    \"Ernie4_5Config\": [\"tie_word_embeddings\"],\n+    \"Ernie4_5_MoEConfig\": [\"tie_word_embeddings\"],\n     \"Lfm2Config\": [\"full_attn_idxs\", \"tie_word_embeddings\"],\n     # used internally during generation to provide the custom logit processors with their necessary information\n     \"DiaConfig\": ["
        }
    ],
    "stats": {
        "total": 2958,
        "additions": 2956,
        "deletions": 2
    }
}