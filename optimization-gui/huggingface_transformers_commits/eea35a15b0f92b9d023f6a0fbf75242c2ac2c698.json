{
    "author": "ydshieh",
    "message": "Fix `mllama` (#38704)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "eea35a15b0f92b9d023f6a0fbf75242c2ac2c698",
    "files": [
        {
            "sha": "729122951e059943b2aedd5de4e6552830806296",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/eea35a15b0f92b9d023f6a0fbf75242c2ac2c698/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eea35a15b0f92b9d023f6a0fbf75242c2ac2c698/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=eea35a15b0f92b9d023f6a0fbf75242c2ac2c698",
            "patch": "@@ -534,7 +534,7 @@ def test_11b_model_integration_generate(self):\n                 {\n                     (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                     (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nA dock in the lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n-                    (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                    (\"cuda\", 8): 'If I had to write a haiku for this one, it would be:.\\\\nA dock in the lake.\\\\nA mountain in the distance.\\\\nA long exposure.',\n                 }\n             )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()\n@@ -618,9 +618,9 @@ def test_11b_model_integration_forward(self):\n         actual_logits = output.logits[0, -1, :5].cpu()\n         expected_logits_all = Expectations(\n             {\n-                (\"xpu\", 3): torch.tensor([9.1562, 8.9141, 5.0664, 1.6855, 3.2324]),\n-                (\"cuda\", 7): torch.tensor([9.0781, 8.8750, 5.0781, 1.6221, 3.2207]),\n-                (\"cuda\", 8): torch.tensor([9.0703, 8.8750, 5.0781, 1.6279, 3.2207]),\n+                (\"xpu\", 3): torch.tensor([9.1562, 8.9141, 5.0664, 1.6855, 3.2324], dtype=actual_logits.dtype),\n+                (\"cuda\", 7): torch.tensor([9.0781, 8.8750, 5.0781, 1.6221, 3.2207], dtype=actual_logits.dtype),\n+                (\"cuda\", 8): torch.tensor([9.0703, 8.8750, 5.0781, 1.6279, 3.2207], dtype=actual_logits.dtype),\n             }\n         )\n \n@@ -665,7 +665,7 @@ def test_11b_model_integration_batched_generate(self):\n                 {\n                     (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n                     (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n-                    (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                    (\"cuda\", 8): 'If I had to write a haiku for this one, it would be:.\\\\nA dock in the lake.\\\\nA mountain in the distance.\\\\nA long exposure.',\n                  }\n             )  # fmt: skip\n         expected_output = expected_outputs.get_expectation()"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}