{
    "author": "gante",
    "message": "[tests] make `test_from_pretrained_low_cpu_mem_usage_equal` less flaky (#36255)\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "e3d99ec2f58e0e2a4df6b2b41152fdfb3f92a52f",
    "files": [
        {
            "sha": "66210cae80434509459f6fa4262cbd15e3fbbdb2",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e3d99ec2f58e0e2a4df6b2b41152fdfb3f92a52f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e3d99ec2f58e0e2a4df6b2b41152fdfb3f92a52f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=e3d99ec2f58e0e2a4df6b2b41152fdfb3f92a52f",
            "patch": "@@ -993,6 +993,7 @@ def test_from_pretrained_low_cpu_mem_usage_functional(self):\n         for mname in mnames:\n             _ = BertModel.from_pretrained(mname, low_cpu_mem_usage=True)\n \n+    @slow\n     @require_usr_bin_time\n     @require_accelerate\n     @mark.accelerate_tests\n@@ -1001,30 +1002,29 @@ def test_from_pretrained_low_cpu_mem_usage_equal(self):\n         # Now though these should be around the same.\n         # TODO: Look for good bounds to check that their timings are near the same\n \n-        mname = \"hf-internal-testing/tiny-random-bert\"\n+        mname = \"HuggingFaceTB/SmolLM-135M\"\n \n         preamble = \"from transformers import AutoModel\"\n         one_liner_str = f'{preamble}; AutoModel.from_pretrained(\"{mname}\", low_cpu_mem_usage=False)'\n         # Save this output as `max_rss_normal` if testing memory results\n         max_rss_normal = self.python_one_liner_max_rss(one_liner_str)\n-        # print(f\"{max_rss_normal=}\")\n \n         one_liner_str = f'{preamble};  AutoModel.from_pretrained(\"{mname}\", low_cpu_mem_usage=True)'\n         # Save this output as `max_rss_low_mem` if testing memory results\n         max_rss_low_mem = self.python_one_liner_max_rss(one_liner_str)\n \n-        # Should be within 2MBs of each other (overhead)\n+        # Should be within 5MBs of each other (overhead)\n         self.assertAlmostEqual(\n             max_rss_normal / 1024 / 1024,\n             max_rss_low_mem / 1024 / 1024,\n-            delta=2,\n+            delta=5,\n             msg=\"using `low_cpu_mem_usage` should incur the same memory usage in both cases.\",\n         )\n \n         # if you want to compare things manually, let's first look at the size of the model in bytes\n-        # model = BertModel.from_pretrained(mname, low_cpu_mem_usage=False)\n+        # model = AutoModel.from_pretrained(mname, low_cpu_mem_usage=False)\n         # total_numel = sum(dict((p.data_ptr(), p.numel()) for p in model.parameters()).values())\n-        # total_bytes = total_numel * 4  # 420MB\n+        # total_bytes = total_numel * 4\n         # Now the diff_bytes should be very close to total_bytes, but the reports are inconsistent.\n         # The easiest way to test this is to switch the model and torch.load to do all the work on\n         # gpu - that way one can measure exactly the total and peak memory used. Perhaps once we add"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}