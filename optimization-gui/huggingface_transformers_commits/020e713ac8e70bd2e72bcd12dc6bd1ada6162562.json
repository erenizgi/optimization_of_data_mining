{
    "author": "BlackSamorez",
    "message": "[FPQuant] MXFP8 and MXFP4 backwards support (#41897)\n\n* FP-Quant backwards\n\n* fp-quant v0.3.0 docker\n\n* availability version bump\n\n* fp_quant==0.3.1\n\n* fp_quant v0.3.2",
    "sha": "020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
    "files": [
        {
            "sha": "3d00eaa938e0c2d335fa169b0ec2f815c41bce54",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
            "patch": "@@ -81,7 +81,7 @@ RUN python3 -m pip uninstall -y flash-attn\n RUN cd transformers && python3 setup.py develop\n \n # Add fp-quant for quantization testing\n-RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.2.0\"\n+RUN python3 -m pip install --no-cache-dir \"fp-quant>=0.3.2\"\n \n # Low usage or incompatible lib, will enable later on\n "
        },
        {
            "sha": "ccf9337961658ad3aef7e672b9b2baab66c96d93",
            "filename": "src/transformers/integrations/fp_quant.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Fintegrations%2Ffp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffp_quant.py?ref=020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
            "patch": "@@ -35,6 +35,10 @@ def adapt_fp_quant_config(config: FPQuantConfig):\n \n     if config.backward_dtype == \"bf16\":\n         backward_dtype = FPQuantDtype.BF16\n+    elif config.backward_dtype == \"mxfp8\":\n+        backward_dtype = FPQuantDtype.MXFP8\n+    elif config.backward_dtype == \"mxfp4\":\n+        backward_dtype = FPQuantDtype.MXFP4\n     else:\n         raise ValueError(f\"Unsupported backward dtype: {config.backward_dtype}\")\n "
        },
        {
            "sha": "3338d01bb2dcbdfc84e8db6096d2cb0f54088627",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
            "patch": "@@ -973,13 +973,13 @@ def is_quark_available() -> bool:\n @lru_cache\n def is_fp_quant_available():\n     is_available, fp_quant_version = _is_package_available(\"fp_quant\", return_version=True)\n-    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.2.0\")\n+    return is_available and version.parse(fp_quant_version) >= version.parse(\"0.3.2\")\n \n \n @lru_cache\n def is_qutlass_available():\n     is_available, qutlass_version = _is_package_available(\"qutlass\", return_version=True)\n-    return is_available and version.parse(qutlass_version) >= version.parse(\"0.1.0\")\n+    return is_available and version.parse(qutlass_version) >= version.parse(\"0.2.0\")\n \n \n @lru_cache"
        },
        {
            "sha": "8ce450a60b9b014c36a394ab6d187883364ca677",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
            "patch": "@@ -1601,8 +1601,12 @@ def post_init(self):\n         else:\n             raise ValueError(\"Only 'mxfp4' and 'nvfp4' are supported for forward_dtype for now.\")\n \n-        if self.backward_dtype != \"bf16\":\n-            raise ValueError(\"Only 'bf16' is supported for backward_dtype for now.\")\n+        if self.backward_dtype not in [\"bf16\", \"mxfp8\", \"mxfp4\"]:\n+            raise ValueError(\"Only 'bf16', 'mxfp8' and 'mxfp4' are supported for backward_dtype for now.\")\n+\n+        if self.backward_dtype != \"bf16\" and self.forward_dtype != \"mxfp4\":\n+            raise ValueError(\"Only 'mxfp4' forward is compatible with non-bf16 backwards for now.\")\n+\n         if self.transform_init not in [\"hadamard\", \"identity\", \"gsr\"]:\n             raise ValueError(\"Only 'hadamard', 'identity' and 'gsr' are supported for transform_init.\")\n "
        },
        {
            "sha": "6ae0e7f23debb6b4f8eafd6d8b3561787a0cd1ff",
            "filename": "tests/quantization/fp_quant_integration/test_fp_quant.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/020e713ac8e70bd2e72bcd12dc6bd1ada6162562/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffp_quant_integration%2Ftest_fp_quant.py?ref=020e713ac8e70bd2e72bcd12dc6bd1ada6162562",
            "patch": "@@ -163,6 +163,13 @@ def getQuantizationConfig(cls):\n         return FPQuantConfig(forward_dtype=\"mxfp4\", pseudoquantization=False)\n \n \n+@require_qutlass\n+class FPQuantNVFP4Test(FPQuantBaseTest):\n+    @classmethod\n+    def getQuantizationConfig(cls):\n+        return FPQuantConfig(forward_dtype=\"nvfp4\", pseudoquantization=False)\n+\n+\n @require_qutlass\n class FPQuantMXFP4GS128Test(FPQuantBaseTest):\n     @classmethod"
        }
    ],
    "stats": {
        "total": 25,
        "additions": 20,
        "deletions": 5
    }
}