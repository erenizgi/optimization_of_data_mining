{
    "author": "ArthurZucker",
    "message": "Add support for Pixtral (#33449)\n\n* initial commit\r\n\r\n* gloups\r\n\r\n* updates\r\n\r\n* work\r\n\r\n* weights match\r\n\r\n* nits\r\n\r\n* nits\r\n\r\n* updates to support the tokenizer :)\r\n\r\n* updates\r\n\r\n* Pixtral processor (#33454)\r\n\r\n* rough outline\r\n\r\n* Add in image break and end tokens\r\n\r\n* Fix\r\n\r\n* Udo some formatting changes\r\n\r\n* Set patch_size default\r\n\r\n* Fix\r\n\r\n* Fix token expansion\r\n\r\n* nit in conversion script\r\n\r\n* Fix image token list creation\r\n\r\n* done\r\n\r\n* add expected results\r\n\r\n* Process list of list of images (#33465)\r\n\r\n* updates\r\n\r\n* working image and processor\r\n\r\n* this is the expected format\r\n\r\n* some fixes\r\n\r\n* push current updated\r\n\r\n* working mult images!\r\n\r\n* add a small integration test\r\n\r\n* Uodate configuration docstring\r\n\r\n* Formatting\r\n\r\n* Config docstring fix\r\n\r\n* simplify model test\r\n\r\n* fixup modeling and etests\r\n\r\n* Return BatchMixFeature in image processor\r\n\r\n* fix some copies\r\n\r\n* update\r\n\r\n* nits\r\n\r\n* Update model docstring\r\n\r\n* Apply suggestions from code review\r\n\r\n* Fix up\r\n\r\n* updates\r\n\r\n* revert modeling changes\r\n\r\n* update\r\n\r\n* update\r\n\r\n* fix load safe\r\n\r\n* addd liscence\r\n\r\n* update\r\n\r\n* use pixel_values as required by the model\r\n\r\n* skip some tests and refactor\r\n\r\n* Add pixtral image processing tests (#33476)\r\n\r\n* Image processing tests\r\n\r\n* Add processing tests\r\n\r\n* woops\r\n\r\n* defaults reflect pixtral image processor\r\n\r\n* fixup post merge\r\n\r\n* images -> pixel values\r\n\r\n* oups sorry Mr docbuilder\r\n\r\n* isort\r\n\r\n* fix\r\n\r\n* fix processor tests\r\n\r\n* small fixes\r\n\r\n* nit\r\n\r\n* update\r\n\r\n* last nits\r\n\r\n* oups this was really breaking!\r\n\r\n* nits\r\n\r\n* is composition needs to be true\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
    "files": [
        {
            "sha": "235ea81a7f1ea6259bbe7549bfdce76872b38cfc",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -862,6 +862,8 @@\n         title: Perceiver\n       - local: model_doc/pix2struct\n         title: Pix2Struct\n+      - local: model_doc/pixtral\n+        title: Pixtral\n       - local: model_doc/sam\n         title: Segment Anything\n       - local: model_doc/siglip"
        },
        {
            "sha": "c18426de4c031cd875bdeceae5c6d30d91f2c201",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -253,6 +253,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                          [Phi3](model_doc/phi3)                          |       ✅        |         ❌         |      ❌      |\n |                       [PhoBERT](model_doc/phobert)                       |       ✅        |         ✅         |      ✅      |\n |                    [Pix2Struct](model_doc/pix2struct)                    |       ✅        |         ❌         |      ❌      |\n+|                       [Pixtral](model_doc/pixtral)                       |       ❌        |         ❌         |      ❌      |\n |                        [PLBart](model_doc/plbart)                        |       ✅        |         ❌         |      ❌      |\n |                    [PoolFormer](model_doc/poolformer)                    |       ✅        |         ❌         |      ❌      |\n |                     [Pop2Piano](model_doc/pop2piano)                     |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "8df2bf5af5f9ca12526368c7821247f112d75a69",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "added",
            "additions": 98,
            "deletions": 0,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,98 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Pixtral\n+\n+## Overview\n+\n+The Pixtral model was released by the Mistral AI team on [Vllm](https://github.com/vllm-project/vllm/pull/8377), where a version of the code can be found!\n+\n+\n+Tips:\n+\n+- Pixtral is a multimodal model, the main contribution is the 2d ROPE on the images, and support for arbitrary image size (the images are not padded together nor are they resized)\n+- This model follows the `Llava` familiy, meaning image embeddings are placed instead of the `[IMG]` token placeholders. \n+- The format for one or mulitple prompts is the following:\n+```\n+\"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n+```\n+Then, the processor will replace each `[IMG]` token with  a number of `[IMG]` token that depends on the height and the width of the image. Each *row* of the image is separated by a `[IMG_BREAK]` token, and each image is separated by a  `[IMG_END]` token.\n+\n+This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ)\n+\n+Here is an example of how to run it:\n+\n+```python \n+from transformers import LlavaForConditionalGeneration, AutoProcessor\n+from PIL import Image\n+\n+model_id = \"hf-internal-testing/pixtral-12b\"\n+model = LlavaForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")\n+processor = AutoProcessor.from_pretrained(model_id)\n+\n+IMG_URLS = [\n+    \"https://picsum.photos/id/237/400/300\",\n+    \"https://picsum.photos/id/231/200/300\",\n+    \"https://picsum.photos/id/27/500/500\",\n+    \"https://picsum.photos/id/17/150/600\",\n+]\n+PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n+\n+inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\n+generate_ids = model.generate(**inputs, max_new_tokens=500)\n+ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+EXPECTED_GENERATION = \"\"\"\n+Describe the images.\n+Sure, let's break down each image description:\n+\n+1. **Image 1:**\n+   - **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n+   - **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n+\n+2. **Image 2:**\n+   - **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n+   - **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n+\n+3. **Image 3:**\n+   - **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n+   - **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n+\n+4. **Image 4:**\n+   - **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n+   - **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\n+\n+Each image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n+\"\"\"\n+\n+```\n+## PixtralVisionConfig\n+\n+[[autodoc]] PixtralVisionConfig\n+\n+## PixtralModel\n+\n+[[autodoc]] PixtralModel\n+    - forward\n+\n+## PixtralImageProcessor\n+\n+[[autodoc]] PixtralImageProcessor\n+    - preprocess\n+\n+## PixtralProcessor\n+\n+[[autodoc]] PixtralProcessor"
        },
        {
            "sha": "36775d8454ab8cdfaa474d54afcb897b6272f2b6",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 12,
            "deletions": 1,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -649,6 +649,7 @@\n         \"Pix2StructTextConfig\",\n         \"Pix2StructVisionConfig\",\n     ],\n+    \"models.pixtral\": [\"PixtralProcessor\", \"PixtralVisionConfig\"],\n     \"models.plbart\": [\"PLBartConfig\"],\n     \"models.poolformer\": [\"PoolFormerConfig\"],\n     \"models.pop2piano\": [\"Pop2PianoConfig\"],\n@@ -1199,6 +1200,7 @@\n     _import_structure[\"models.owlvit\"].extend([\"OwlViTFeatureExtractor\", \"OwlViTImageProcessor\"])\n     _import_structure[\"models.perceiver\"].extend([\"PerceiverFeatureExtractor\", \"PerceiverImageProcessor\"])\n     _import_structure[\"models.pix2struct\"].extend([\"Pix2StructImageProcessor\"])\n+    _import_structure[\"models.pixtral\"].append(\"PixtralImageProcessor\")\n     _import_structure[\"models.poolformer\"].extend([\"PoolFormerFeatureExtractor\", \"PoolFormerImageProcessor\"])\n     _import_structure[\"models.pvt\"].extend([\"PvtImageProcessor\"])\n     _import_structure[\"models.qwen2_vl\"].extend([\"Qwen2VLImageProcessor\"])\n@@ -1359,7 +1361,6 @@\n             \"AlignVisionModel\",\n         ]\n     )\n-\n     _import_structure[\"models.altclip\"].extend(\n         [\n             \"AltCLIPModel\",\n@@ -2977,6 +2978,7 @@\n             \"Pix2StructVisionModel\",\n         ]\n     )\n+    _import_structure[\"models.pixtral\"].extend([\"PixtralModel\", \"PixtralPreTrainedModel\"])\n     _import_structure[\"models.plbart\"].extend(\n         [\n             \"PLBartForCausalLM\",\n@@ -5434,6 +5436,10 @@\n         Pix2StructTextConfig,\n         Pix2StructVisionConfig,\n     )\n+    from .models.pixtral import (\n+        PixtralProcessor,\n+        PixtralVisionConfig,\n+    )\n     from .models.plbart import PLBartConfig\n     from .models.poolformer import (\n         PoolFormerConfig,\n@@ -6009,6 +6015,7 @@\n         from .models.owlvit import OwlViTFeatureExtractor, OwlViTImageProcessor\n         from .models.perceiver import PerceiverFeatureExtractor, PerceiverImageProcessor\n         from .models.pix2struct import Pix2StructImageProcessor\n+        from .models.pixtral import PixtralImageProcessor\n         from .models.poolformer import (\n             PoolFormerFeatureExtractor,\n             PoolFormerImageProcessor,\n@@ -7448,6 +7455,10 @@\n             Pix2StructTextModel,\n             Pix2StructVisionModel,\n         )\n+        from .models.pixtral import (\n+            PixtralModel,\n+            PixtralPreTrainedModel,\n+        )\n         from .models.plbart import (\n             PLBartForCausalLM,\n             PLBartForConditionalGeneration,"
        },
        {
            "sha": "2022048cd4553ffb9a894b2a20f2ac2f5a472a6f",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -187,6 +187,7 @@\n     phi3,\n     phobert,\n     pix2struct,\n+    pixtral,\n     plbart,\n     poolformer,\n     pop2piano,"
        },
        {
            "sha": "2cd7d550d90b7a58bc06c3fa8b99b21e96f508eb",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -205,6 +205,7 @@\n         (\"phi\", \"PhiConfig\"),\n         (\"phi3\", \"Phi3Config\"),\n         (\"pix2struct\", \"Pix2StructConfig\"),\n+        (\"pixtral\", \"PixtralVisionConfig\"),\n         (\"plbart\", \"PLBartConfig\"),\n         (\"poolformer\", \"PoolFormerConfig\"),\n         (\"pop2piano\", \"Pop2PianoConfig\"),\n@@ -509,6 +510,7 @@\n         (\"phi3\", \"Phi3\"),\n         (\"phobert\", \"PhoBERT\"),\n         (\"pix2struct\", \"Pix2Struct\"),\n+        (\"pixtral\", \"Pixtral\"),\n         (\"plbart\", \"PLBart\"),\n         (\"poolformer\", \"PoolFormer\"),\n         (\"pop2piano\", \"Pop2Piano\"),"
        },
        {
            "sha": "95d9ddef8f797978f0b0e1c74cb57fe177ed376b",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -114,6 +114,7 @@\n             (\"owlvit\", (\"OwlViTImageProcessor\",)),\n             (\"perceiver\", (\"PerceiverImageProcessor\",)),\n             (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n+            (\"pixtral\", (\"PixtralImageProcessor\",)),\n             (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n             (\"pvt\", (\"PvtImageProcessor\",)),\n             (\"pvt_v2\", (\"PvtImageProcessor\",)),"
        },
        {
            "sha": "e0d15f1e236590e23a3daf8b7127fa3a44ca9fc2",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -193,6 +193,7 @@\n         (\"persimmon\", \"PersimmonModel\"),\n         (\"phi\", \"PhiModel\"),\n         (\"phi3\", \"Phi3Model\"),\n+        (\"pixtral\", \"PixtralModel\"),\n         (\"plbart\", \"PLBartModel\"),\n         (\"poolformer\", \"PoolFormerModel\"),\n         (\"prophetnet\", \"ProphetNetModel\"),"
        },
        {
            "sha": "82d325248eabfbf6f7a7a2ddc4b646ff9a1cb808",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -82,6 +82,7 @@\n         (\"owlvit\", \"OwlViTProcessor\"),\n         (\"paligemma\", \"PaliGemmaProcessor\"),\n         (\"pix2struct\", \"Pix2StructProcessor\"),\n+        (\"pixtral\", \"PixtralProcessor\"),\n         (\"pop2piano\", \"Pop2PianoProcessor\"),\n         (\"qwen2_audio\", \"Qwen2AudioProcessor\"),\n         (\"qwen2_vl\", \"Qwen2VLProcessor\"),"
        },
        {
            "sha": "e735579108d8573bc0cb261d01142c217cd8de4c",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -385,6 +385,7 @@\n             (\"phi3\", (\"LlamaTokenizer\", \"LlamaTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"phobert\", (\"PhobertTokenizer\", None)),\n             (\"pix2struct\", (\"T5Tokenizer\", \"T5TokenizerFast\" if is_tokenizers_available() else None)),\n+            (\"pixtral\", (None, \"PreTrainedTokenizerFast\" if is_tokenizers_available() else None)),\n             (\"plbart\", (\"PLBartTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\"prophetnet\", (\"ProphetNetTokenizer\", None)),\n             (\"qdqbert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),"
        },
        {
            "sha": "3a4cb09855f0ec34f1bb289c33e3709e7056bb92",
            "filename": "src/transformers/models/llava/configuration_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fconfiguration_llava.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -73,7 +73,7 @@ class LlavaConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"llava\"\n-    is_composition = False\n+    is_composition = True\n \n     def __init__(\n         self,"
        },
        {
            "sha": "e09ed8e60127ddaede275005b453cf368d8a9d10",
            "filename": "src/transformers/models/pixtral/__init__.py",
            "status": "added",
            "additions": 70,
            "deletions": 0,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,70 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import OptionalDependencyNotAvailable, _LazyModule, is_torch_available, is_vision_available\n+\n+\n+_import_structure = {\n+    \"configuration_pixtral\": [\"PixtralVisionConfig\"],\n+    \"processing_pixtral\": [\"PixtralProcessor\"],\n+}\n+\n+\n+try:\n+    if not is_torch_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"modeling_pixtral\"] = [\n+        \"PixtralModel\",\n+        \"PixtralPreTrainedModel\",\n+    ]\n+\n+try:\n+    if not is_vision_available():\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"image_processing_pixtral\"] = [\"PixtralImageProcessor\"]\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_pixtral import PixtralProcessor, PixtralVisionConfig\n+\n+    try:\n+        if not is_torch_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .modeling_pixtral import (\n+            PixtralModel,\n+            PixtralPreTrainedModel,\n+        )\n+\n+    try:\n+        if not is_vision_available():\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .image_processing_pixtral import PixtralImageProcessor\n+\n+else:\n+    import sys\n+\n+    sys.modules[__name__] = _LazyModule(__name__, globals()[\"__file__\"], _import_structure)"
        },
        {
            "sha": "dcc1e458ca78a3b8b31b52cc51d931a6ad7e89d2",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "added",
            "additions": 103,
            "deletions": 0,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,103 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Pixtral model configuration\"\"\"\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class PixtralVisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`PixtralModel`]. It is used to instantiate an\n+    Pixtral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Pixtral-9B.\n+\n+    e.g. [pixtral-hf/pixtral-9b](https://huggingface.co/pixtral-hf/pixtral-9b)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 1024):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 24):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 16):\n+            Number of attention heads in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of input channels in the input images.\n+        image_size (`int`, *optional*, defaults to 1024):\n+            Max dimension of the input images.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Size of the image patches.\n+        hidden_act (`str`, *optional*, defaults to `\"gelu\"`):\n+            Activation function used in the hidden layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            Dropout probability for the attention layers.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie the word embeddings with the input embeddings.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import PixtralModel, PixtralVisionConfig, CLIPVisionConfig, LlamaConfig\n+\n+    >>> # Initializing a Pixtral 12B style configuration\n+    >>> config = PixtralVisionConfig()\n+\n+    >>> # Initializing a model from the pixtral 12B style configuration\n+    >>> model = PixtralModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"pixtral\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=1024,\n+        intermediate_size=4096,\n+        num_hidden_layers=24,\n+        num_attention_heads=16,\n+        num_channels=3,\n+        image_size=1024,\n+        patch_size=16,\n+        hidden_act=\"gelu\",\n+        attention_dropout=0.0,\n+        rope_theta=10000.0,\n+        tie_word_embeddings=False,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.attention_dropout = attention_dropout\n+        self.hidden_act = hidden_act\n+        self.rope_theta = rope_theta\n+        self.tie_word_embeddings = tie_word_embeddings\n+        self.head_dim = hidden_size // num_attention_heads"
        },
        {
            "sha": "c4190082d994719ae7d625e3b4169630696d855d",
            "filename": "src/transformers/models/pixtral/convert_pixtral_weights_to_hf.py",
            "status": "added",
            "additions": 285,
            "deletions": 0,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconvert_pixtral_weights_to_hf.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,285 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc. team. All rights reserved.\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import argparse\n+\n+import regex as re\n+import torch\n+from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n+from safetensors.torch import load_file as safe_load_file\n+from tokenizers import Regex, Tokenizer, decoders, pre_tokenizers, processors\n+from tokenizers.models import BPE\n+\n+from transformers import (\n+    LlavaConfig,\n+    LlavaForConditionalGeneration,\n+    MistralConfig,\n+    PixtralImageProcessor,\n+    PixtralProcessor,\n+    PixtralVisionConfig,\n+    PreTrainedTokenizerFast,\n+)\n+from transformers.convert_slow_tokenizer import bytes_to_unicode\n+\n+\n+\"\"\"\n+# Here is how to get the original tokens!\n+model_name = \"mistralai/Pixtral-12B-2409\"\n+tok = MistralTokenizer.from_model(model_name)\n+\n+from mistral_common.protocol.instruct.request import ChatCompletionRequest, UserMessage, ImageChunk, TextChunk\n+\n+EXPECTED_TOKENS = tok.encode_chat_completion(\n+    ChatCompletionRequest(\n+        messages=[\n+            UserMessage(\n+                content=[\n+                    TextChunk(text=\"Describe the images\"),\n+                ] + [ImageChunk(image=img) for img in IMG_URLS]\n+            )\n+        ],\n+        model=\"pixtral\",\n+    )\n+)\n+assert tokenizer.decode(inputs[\"input_ids\"][0]) == EXPECTED_TOKENS\n+\"\"\"\n+\n+OLD_KEY_TO_NEW_KEY_MAPPING = {\n+    # Layer Normalization Weights\n+    r\"vision_encoder.transformer.layers.(\\d+).input_layernorm.weight\": r\"vision_tower.transformer.layers.\\1.attention_norm.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).ffn_norm.weight\": r\"vision_tower.transformer.layers.\\1.ffn_norm.weight\",\n+    # Self Attention Projections\n+    r\"vision_encoder.transformer.layers.(\\d+).attention.wq.weight\": r\"vision_tower.transformer.layers.\\1.attention.q_proj.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).attention.wk.weight\": r\"vision_tower.transformer.layers.\\1.attention.k_proj.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).attention.wv.weight\": r\"vision_tower.transformer.layers.\\1.attention.v_proj.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).attention.wo.weight\": r\"vision_tower.transformer.layers.\\1.attention.o_proj.weight\",\n+    # MLP Projections\n+    r\"vision_encoder.transformer.layers.(\\d+).feed_forward.w1.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.gate_proj.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).feed_forward.w2.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.down_proj.weight\",\n+    r\"vision_encoder.transformer.layers.(\\d+).feed_forward.w3.weight\": r\"vision_tower.transformer.layers.\\1.feed_forward.up_proj.weight\",\n+    # Additional mappings\n+    r\"vision_encoder\": r\"vision_tower\",\n+    r\"vision_language_adapter.w_in\": r\"multi_modal_projector.linear_1\",\n+    r\"vision_language_adapter.w_out\": r\"multi_modal_projector.linear_2\",\n+    r\"layers.(\\d+).attention.wq.weight\": r\"language_model.model.layers.\\1.self_attn.q_proj.weight\",\n+    r\"layers.(\\d+).attention.wk.weight\": r\"language_model.model.layers.\\1.self_attn.k_proj.weight\",\n+    r\"layers.(\\d+).attention.wv.weight\": r\"language_model.model.layers.\\1.self_attn.v_proj.weight\",\n+    r\"layers.(\\d+).attention.wo.weight\": r\"language_model.model.layers.\\1.self_attn.o_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.w1.weight\": r\"language_model.model.layers.\\1.mlp.gate_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.w2.weight\": r\"language_model.model.layers.\\1.mlp.down_proj.weight\",\n+    r\"layers.(\\d+).feed_forward.w3.weight\": r\"language_model.model.layers.\\1.mlp.up_proj.weight\",\n+    r\"layers.(\\d+).ffn_norm.weight\": r\"language_model.model.layers.\\1.post_attention_layernorm.weight\",\n+    r\"layers.(\\d+).attention_norm.weight\": r\"language_model.model.layers.\\1.input_layernorm.weight\",\n+    r\"tok_embeddings.weight\": r\"language_model.model.embed_tokens.weight\",\n+    r\"output.weight\": r\"language_model.lm_head.weight\",\n+    r\"norm.weight\": r\"language_model.model.norm.weight\",\n+}\n+\n+\n+class MistralConverter:\n+    \"\"\"\n+    A general tiktoken converter.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        vocab=None,\n+        pattern=r\"\"\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\",\n+        add_prefix_space=False,\n+        additional_special_tokens=None,\n+        *args,\n+        **kwargs,\n+    ):\n+        super().__init__(*args)\n+        self.vocab = vocab\n+        self.pattern = pattern\n+        self.add_prefix_space = add_prefix_space\n+        self.additional_special_tokens = additional_special_tokens\n+\n+    def extract_vocab_merges_from_model(self, vocab: str):\n+        bpe_ranks = vocab\n+        byte_encoder = bytes_to_unicode()\n+\n+        def token_bytes_to_string(b):\n+            return \"\".join([byte_encoder[ord(char)] for char in b.decode(\"latin-1\")])\n+\n+        merges = []\n+        vocab = {}\n+        for idx, (token, rank) in enumerate(bpe_ranks.items()):\n+            if token not in self.additional_special_tokens:\n+                vocab[token_bytes_to_string(token)] = idx\n+                if len(token) == 1:\n+                    continue\n+                local = []\n+                for index in range(1, len(token)):\n+                    piece_l, piece_r = token[:index], token[index:]\n+                    if piece_l in bpe_ranks and piece_r in bpe_ranks and (piece_l + piece_r) in bpe_ranks:\n+                        local.append((piece_l, piece_r, rank))\n+                local = sorted(local, key=lambda x: (bpe_ranks[x[0]], bpe_ranks[x[1]]), reverse=False)\n+                merges.extend(local)\n+            else:\n+                vocab[token] = idx\n+        merges = sorted(merges, key=lambda val: val[2], reverse=False)\n+        merges = [(token_bytes_to_string(val[0]), token_bytes_to_string(val[1])) for val in merges]\n+        return vocab, merges\n+\n+    def tokenizer(self):\n+        vocab_scores, merges = self.extract_vocab_merges_from_model(self.vocab)\n+        tokenizer = Tokenizer(BPE(vocab_scores, merges, fuse_unk=False))\n+        if hasattr(tokenizer.model, \"ignore_merges\"):\n+            tokenizer.model.ignore_merges = True\n+        return tokenizer\n+\n+    def converted(self) -> Tokenizer:\n+        tokenizer = self.tokenizer()\n+        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n+            [\n+                pre_tokenizers.Split(Regex(self.pattern), behavior=\"isolated\", invert=False),\n+                pre_tokenizers.ByteLevel(add_prefix_space=self.add_prefix_space, use_regex=False),\n+            ]\n+        )\n+        tokenizer.decoder = decoders.ByteLevel()\n+        tokenizer.add_special_tokens(self.additional_special_tokens)\n+\n+        tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)\n+\n+        return tokenizer\n+\n+\n+def convert_mistral_tokenizer():\n+    model_name = \"mistralai/Pixtral-12B-2409\"\n+\n+    tokenizer = MistralTokenizer.from_model(model_name)\n+\n+    vocab = tokenizer.instruct_tokenizer.tokenizer._tekken_token2id_nospecial\n+    all_special = [\n+        token.value if hasattr(token, \"value\") else token\n+        for token in tokenizer.instruct_tokenizer.tokenizer._all_special_tokens\n+    ]\n+    specials_tokens = {token: all_special.index(token) for token in all_special}\n+    specials_tokens.update(vocab)\n+    vocab = specials_tokens\n+\n+    tokenizer = PreTrainedTokenizerFast(\n+        tokenizer_object=MistralConverter(vocab=vocab, additional_special_tokens=all_special).converted(),\n+        bos_token=\"<s>\",\n+        unk_token=\"<unk>\",\n+        eos_token=\"</s>\",\n+    )\n+    tokenizer.model_input_names = [\"input_ids\", \"attention_mask\"]\n+\n+    return tokenizer\n+\n+\n+def permute_for_rope(value, n_heads, config):\n+    dim1 = value.shape[0]\n+    dim2 = config.hidden_size\n+    return value.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n+\n+\n+def convert_dictionnary(original_state_dict, vision_config, text_config):\n+    new_dict = {}\n+\n+    all_keys = \"\\n\" + \"\\n\".join(original_state_dict.keys())\n+    old_keys = all_keys\n+    for old, new in OLD_KEY_TO_NEW_KEY_MAPPING.items():\n+        all_keys = re.sub(r\"\\n\" + old, r\"\\n\" + new, all_keys)\n+\n+    OLD_TO_NEW = dict(zip(old_keys.split(\"\\n\"), all_keys.split(\"\\n\")))\n+\n+    for key, value in original_state_dict.items():\n+        new_key = OLD_TO_NEW[key]\n+        if \"vision_encoder\" in key:\n+            _config = vision_config\n+            num_attention_heads = _config.num_attention_heads\n+        else:\n+            _config = text_config\n+            if \"q_proj\" in new_key:\n+                num_attention_heads = _config.num_attention_heads\n+            if \"k_proj\" in new_key:\n+                num_attention_heads = _config.num_key_value_heads\n+            # convert the text model (basically mistral model)\n+\n+        if \"q_proj\" in new_key or \"k_proj\" in new_key:\n+            value = permute_for_rope(value, num_attention_heads, _config)\n+\n+        new_dict[new_key] = value\n+    return new_dict\n+\n+\n+def convert_mistral_model(input_dir, output_dir):\n+    text_config = MistralConfig(\n+        attention_dropout=0.0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        head_dim=128,\n+        hidden_act=\"silu\",\n+        hidden_size=5120,\n+        initializer_range=0.02,\n+        intermediate_size=14336,\n+        max_position_embeddings=1024000,\n+        model_type=\"mistral\",\n+        num_attention_heads=32,\n+        num_hidden_layers=40,\n+        num_key_value_heads=8,\n+        rms_norm_eps=1e-05,\n+        rope_theta=1000000000.0,\n+        sliding_window=None,\n+        tie_word_embeddings=False,\n+        vocab_size=131072,\n+    )\n+\n+    vision_config = PixtralVisionConfig()\n+    config = LlavaConfig(\n+        vision_config,\n+        text_config,\n+        vision_feature_layer=-1,\n+        image_token_index=10,\n+        vision_feature_select_strategy=\"full\",\n+        image_seq_length=1,\n+    )\n+    config.architectures = [\"LlavaForConditionalGeneration\"]\n+    config.save_pretrained(output_dir)\n+\n+    original_state_dict = safe_load_file(f\"{input_dir}/consolidated.safetensors\")\n+    new_dict = convert_dictionnary(original_state_dict, vision_config, text_config)\n+\n+    with torch.device(\"meta\"):\n+        model = LlavaForConditionalGeneration(config)\n+    model.load_state_dict(new_dict, strict=True, assign=True)\n+\n+    model.save_pretrained(output_dir)\n+\n+    tokenizer = convert_mistral_tokenizer()\n+    image_processor = PixtralImageProcessor()\n+    processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor, image_token=\"[IMG]\")\n+    processor.save_pretrained(output_dir)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+\n+    args = parser.parse_args()\n+    convert_mistral_model(args.input_dir, args.output_dir)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "c6d18420bec5752e0910575a7fd9f127a825333a",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral.py",
            "status": "added",
            "additions": 519,
            "deletions": 0,
            "changes": 519,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,519 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for Pixtral.\"\"\"\n+\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    is_valid_image,\n+    to_numpy_array,\n+    valid_images,\n+    validate_kwargs,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_torch_device, is_torch_dtype, is_torch_tensor, is_vision_available, logging\n+from ...utils.import_utils import requires_backends\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+class BatchMixFeature(BatchFeature):\n+    def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n+        \"\"\"\n+        Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only). This should support casting in\n+        different `dtypes` and sending the `BatchFeature` to a different `device`.\n+\n+        Args:\n+            args (`Tuple`):\n+                Will be passed to the `to(...)` function of the tensors.\n+            kwargs (`Dict`, *optional*):\n+                Will be passed to the `to(...)` function of the tensors.\n+\n+        Returns:\n+            [`BatchFeature`]: The same instance after modification.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        import torch  # noqa\n+\n+        new_data = {}\n+        device = kwargs.get(\"device\")\n+        # Check if the args are a device or a dtype\n+        if device is None and len(args) > 0:\n+            # device should be always the first argument\n+            arg = args[0]\n+            if is_torch_dtype(arg):\n+                # The first argument is a dtype\n+                pass\n+            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):\n+                device = arg\n+            else:\n+                # it's something else\n+                raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n+        # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n+        for k, v in self.items():\n+            # check if v is a floating point\n+            if isinstance(v, list):\n+                new_data[k] = [\n+                    element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n+                ]\n+            elif torch.is_floating_point(v):\n+                # cast and send to device\n+                new_data[k] = v.to(*args, **kwargs)\n+            elif device is not None:\n+                new_data[k] = v.to(device=device)\n+            else:\n+                new_data[k] = v\n+        self.data = new_data\n+        return self\n+\n+\n+# Copied from transformers.models.idefics2.image_processing_idefics2.make_list_of_images\n+def make_list_of_images(images: ImageInput) -> List[List[np.ndarray]]:\n+    \"\"\"\n+    Convert a single image or a list of images to a list of numpy arrays.\n+\n+    Args:\n+        images (`ImageInput`):\n+            A single image or a list of images.\n+\n+    Returns:\n+        A list of numpy arrays.\n+    \"\"\"\n+    # If it's a single image, convert it to a list of lists\n+    if is_valid_image(images):\n+        images = [[images]]\n+    # If it's a list of images, it's a single batch, so convert it to a list of lists\n+    elif isinstance(images, (list, tuple)) and len(images) > 0 and is_valid_image(images[0]):\n+        images = [images]\n+    # If it's a list of batches, it's already in the right format\n+    elif (\n+        isinstance(images, (list, tuple))\n+        and len(images) > 0\n+        and isinstance(images[0], (list, tuple))\n+        and is_valid_image(images[0][0])\n+    ):\n+        pass\n+    else:\n+        raise ValueError(\n+            \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n+        )\n+    return images\n+\n+\n+# Adapted from function in image_transforms.py to ensure any transparent pixels are converted to white.\n+def convert_to_rgb(image: ImageInput) -> ImageInput:\n+    \"\"\"\n+    Converts an image to RGB format. Only converts if the image is of type PIL.Image.Image, otherwise returns the image\n+    as is.\n+    Args:\n+        image (Image):\n+            The image to convert.\n+    \"\"\"\n+    requires_backends(convert_to_rgb, [\"vision\"])\n+\n+    if not isinstance(image, PIL.Image.Image):\n+        return image\n+\n+    if image.mode == \"RGB\":\n+        return image\n+\n+    # First we convert to RGBA to set background to white.\n+    image = image.convert(\"RGBA\")\n+\n+    # Create a new image with a white background.\n+    new_image = PIL.Image.new(\"RGBA\", image.size, \"WHITE\")\n+    new_image.paste(image, (0, 0), image)\n+    new_image = new_image.convert(\"RGB\")\n+    return new_image\n+\n+\n+def _num_image_tokens(image_size: Tuple[int, int], patch_size: Tuple[int, int]) -> int:\n+    \"\"\"\n+    Calculate the number of image tokens given the image size and patch size.\n+\n+    Args:\n+        image_size (`Tuple[int, int]`):\n+            The size of the image as `(height, width)`.\n+        patch_size (`Tuple[int, int]`):\n+            The patch size as `(height, width)`.\n+\n+    Returns:\n+        `int`: The number of image tokens.\n+    \"\"\"\n+    height, width = image_size\n+    patch_height, patch_width = patch_size if isinstance(patch_size, (tuple, list)) else (patch_size, patch_size)\n+    num_width_tokens = (width - 1) // patch_width + 1\n+    num_height_tokens = (height - 1) // patch_height + 1\n+    return num_height_tokens, num_width_tokens\n+\n+\n+def get_resize_output_image_size(\n+    input_image: np.ndarray,\n+    size: Union[int, Tuple[int, int], List[int], Tuple[int]],\n+    patch_size: Union[int, Tuple[int, int], List[int], Tuple[int]],\n+    input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+) -> tuple:\n+    \"\"\"\n+    Find the target (height, width) dimension of the output image after resizing given the input image and the desired\n+    size.\n+\n+    Args:\n+        input_image (`np.ndarray`):\n+            The image to resize.\n+        size (`int` or `Tuple[int, int]`):\n+            Max image size an input image can be. Must be a dictionary with the key \"longest_edge\".\n+        patch_size (`int` or `Tuple[int, int]`):\n+            The patch_size as `(height, width)` to use for resizing the image. If patch_size is an integer, `(patch_size, patch_size)`\n+            will be used\n+        input_data_format (`ChannelDimension`, *optional*):\n+            The channel dimension format of the input image. If unset, will use the inferred format from the input.\n+\n+    Returns:\n+        `tuple`: The target (height, width) dimension of the output image after resizing.\n+    \"\"\"\n+    max_height, max_width = size if isinstance(size, (tuple, list)) else (size, size)\n+    patch_height, patch_width = patch_size if isinstance(patch_size, (tuple, list)) else (patch_size, patch_size)\n+    height, width = get_image_size(input_image, input_data_format)\n+\n+    ratio = max(height / max_height, width / max_width)\n+\n+    if ratio > 1:\n+        # Orgiginal implementation uses `round` which utilises bankers rounding, which can lead to surprising results\n+        height = int(np.ceil(height / ratio))\n+        width = int(np.ceil(width / ratio))\n+\n+    num_height_tokens, num_width_tokens = _num_image_tokens((height, width), (patch_height, patch_width))\n+    return num_height_tokens * patch_height, num_width_tokens * patch_width\n+\n+\n+# Hack to get tensor conversion used in BatchFeature without batching the images\n+def _get_is_as_tensor_fns(tensor_type: Union[str, TensorType]) -> Tuple[Callable, Callable]:\n+    return BatchFeature()._get_is_as_tensor_fns(tensor_type)\n+\n+\n+def convert_to_tensor(array, tensor_type: Union[str, TensorType]) -> Any:\n+    is_tensor, as_tensor = _get_is_as_tensor_fns(tensor_type)\n+    if is_tensor(array):\n+        return array\n+    return as_tensor(array)\n+\n+\n+class PixtralImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a Pixtral image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"longest_edge\": 1024}`):\n+            Size of the maximum dimension of either the height or width dimension of the image. Used to control how\n+            images are resized. If either the height or width are greater than `size[\"longest_edge\"]` then both the height and width are rescaled by `height / ratio`, `width /ratio` where `ratio = max(height / longest_edge, width / longest_edge)`\n+        patch_size (`Dict[str, int]` *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n+            Size of the patches in the model, used to calculate the output image size. Can be overridden by `patch_size` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        patch_size: Dict[str, int] = None,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"longest_edge\": 1024}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 16, \"width\": 16}\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.patch_size = patch_size\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else [0.48145466, 0.4578275, 0.40821073]\n+        self.image_std = image_std if image_std is not None else [0.26862954, 0.26130258, 0.27577711]\n+        self.do_convert_rgb = do_convert_rgb\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"do_resize\",\n+            \"size\",\n+            \"patch_size\",\n+            \"resample\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_convert_rgb\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        patch_size: Dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"], with the longest edge\n+        resized to keep the input aspect ratio.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Dict containing the longest possible edge of the image.\n+            patch_size (`Dict[str, int]`):\n+                Patch size used to calculate the size of the output image.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n+                Resampling filter to use when resiizing the image.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the image. If not provided, it will be the same as the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+        \"\"\"\n+        if \"longest_edge\" in size:\n+            size = (size[\"longest_edge\"], size[\"longest_edge\"])\n+        elif \"height\" in size and \"width\" in size:\n+            size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\"size must contain either 'longest_edge' or 'height' and 'width'.\")\n+\n+        if \"height\" in patch_size and \"width\" in patch_size:\n+            patch_size = (patch_size[\"height\"], patch_size[\"width\"])\n+        else:\n+            raise ValueError(\"patch_size must contain either 'shortest_edge' or 'height' and 'width'.\")\n+\n+        output_size = get_resize_output_image_size(\n+            image,\n+            size=size,\n+            patch_size=patch_size,\n+            input_data_format=input_data_format,\n+        )\n+        return resize(\n+            image,\n+            size=output_size,\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        patch_size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Describes the maximum input dimensions to the model.\n+            patch_size (`Dict[str, int]`, *optional*, defaults to `self.patch_size`):\n+                Patch size in the model. Used to calculate the image after resizing.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        patch_size = get_size_dict(patch_size, default_to_square=True)\n+\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        images_list = make_list_of_images(images)\n+\n+        if not valid_images(images_list[0]):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images_list = [[convert_to_rgb(image) for image in images] for images in images_list]\n+\n+        # All transformations expect numpy arrays.\n+        images_list = [[to_numpy_array(image) for image in images] for images in images_list]\n+\n+        if is_scaled_image(images_list[0][0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images_list[0][0])\n+\n+        batch_images = []\n+        batch_image_sizes = []\n+        for sample_images in images_list:\n+            images = []\n+            image_sizes = []\n+            for image in sample_images:\n+                if do_resize:\n+                    image = self.resize(\n+                        image=image,\n+                        size=size,\n+                        patch_size=patch_size,\n+                        resample=resample,\n+                        input_data_format=input_data_format,\n+                    )\n+\n+                if do_rescale:\n+                    image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+                if do_normalize:\n+                    image = self.normalize(\n+                        image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                    )\n+\n+                images.append(image)\n+                image_sizes.append(get_image_size(image, input_data_format))\n+            batch_images.append(images)\n+            batch_image_sizes.append(image_sizes)\n+\n+        images_list = [\n+            [to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images]\n+            for images in batch_images\n+        ]\n+\n+        # Convert to tensor type outside of BatchFeature to avoid batching the images of different sizes\n+        images_list = [[convert_to_tensor(image, return_tensors) for image in images] for images in images_list]\n+        return BatchMixFeature(data={\"pixel_values\": images_list, \"image_sizes\": batch_image_sizes}, tensor_type=None)"
        },
        {
            "sha": "0e10c78b7852af7120e1ef6dfa82a2857f6f04fc",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "added",
            "additions": 517,
            "deletions": 0,
            "changes": 517,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,517 @@\n+# coding=utf-8\n+# Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch Pixtral model.\"\"\"\n+\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ... import PreTrainedModel\n+from ...activations import ACT2FN\n+from ...modeling_outputs import BaseModelOutput\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+)\n+from .configuration_pixtral import PixtralVisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def position_ids_in_meshgrid(patch_embeds_list, max_width):\n+    positions = []\n+    for patch in patch_embeds_list:\n+        height, width = patch.shape[-2:]\n+        mesh = torch.meshgrid(torch.arange(height), torch.arange(width), indexing=\"ij\")\n+        h_grid, v_grid = torch.stack(mesh, dim=-1).reshape(-1, 2).chunk(2, -1)\n+        ids = h_grid * max_width + v_grid\n+        positions.append(ids[:, 0])\n+    return torch.cat(positions)\n+\n+\n+class PixtralRotaryEmbedding(nn.Module):\n+    \"\"\"\n+    The key with pixtral embedding is just that you have a frequency for each pixel positions.\n+    If you have height x width pixels (or embedding pixels)\n+\n+    then the frequency used for ROPE is given by indexing the pre_computed frequency on the\n+    width and height.\n+\n+    What you output is of dimension batch, height * width, dim with dim the embed dim.\n+\n+    This simply means that for each image hidden states, you are going to add\n+    a corresponding positional embedding, based on it's index in the grid.\n+    \"\"\"\n+\n+    def __init__(self, config, device):\n+        super().__init__()\n+        self.rope_type = \"default\"\n+        self.dim = config.head_dim\n+        self.base = config.rope_theta\n+        max_patches_per_side = config.image_size // config.patch_size\n+        freqs = 1.0 / (self.base ** (torch.arange(0, self.dim, 2).float() / self.dim))\n+\n+        h = torch.arange(max_patches_per_side, device=freqs.device)\n+        w = torch.arange(max_patches_per_side, device=freqs.device)\n+\n+        freqs_h = torch.outer(h, freqs[::2]).float()\n+        freqs_w = torch.outer(w, freqs[1::2]).float()\n+        inv_freq = torch.cat(\n+            [\n+                freqs_h[:, None, :].repeat(1, max_patches_per_side, 1),\n+                freqs_w[None, :, :].repeat(max_patches_per_side, 1, 1),\n+            ],\n+            dim=-1,\n+        ).reshape(-1, self.dim // 2)  # we reshape to only index on the position indexes, not tuple of indexes\n+        # Different from paper, but it uses a different permutation in order to obtain the same calculation\n+\n+        # TODO maybe make it torch compatible later on. We can also just slice\n+        self.register_buffer(\"inv_freq\", torch.cat((inv_freq, inv_freq), dim=-1), persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        freqs = self.inv_freq[position_ids]\n+        # position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            emb = freqs\n+            cos = emb.cos()\n+            sin = emb.sin()\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.rotate_half\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class PixtralAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n+        self.o_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, patches, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, patches, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, unsqueeze_dim=0)\n+\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n+\n+        if attention_mask is not None:\n+            attn_weights = attn_weights + attention_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.reshape(batch_size, patches, -1)\n+\n+        attn_output = self.o_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+# Copied from transformers.models.mistral.modeling_mistral.MistralMLP with Mistral->Pixtral\n+class PixtralMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_state):\n+        return self.down_proj(self.act_fn(self.gate_proj(hidden_state)) * self.up_proj(hidden_state))\n+\n+\n+# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Pixtral\n+class PixtralRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        PixtralRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class PixtralAttentionLayer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.attention_norm = PixtralRMSNorm(config.hidden_size, eps=1e-5)\n+        self.feed_forward = PixtralMLP(config)\n+        self.attention = PixtralAttention(config)\n+        self.ffn_norm = PixtralRMSNorm(config.hidden_size, eps=1e-5)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.attention_norm(hidden_states)\n+        hidden_states, attn_weights = self.attention(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.ffn_norm(hidden_states)\n+        hidden_states = self.feed_forward(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+        return outputs\n+\n+\n+class PixtralTransformer(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = torch.nn.ModuleList()\n+        for _ in range(config.num_hidden_layers):\n+            self.layers.append(PixtralAttentionLayer(config))\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    position_embeddings,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    position_embeddings=position_embeddings,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=[hidden_states], attentions=all_attentions\n+        )\n+\n+\n+PIXTRAL_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`PixtralVisionConfig`] or [`PixtralVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+\n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    PIXTRAL_START_DOCSTRING,\n+)\n+class PixtralPreTrainedModel(PreTrainedModel):\n+    config_class = PixtralVisionConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"PixtralVisionAttention\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+\n+    def _init_weights(self, module):\n+        # important: this ported version of Pixtral isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/LLaVA/tree/main/pixtral should serve for that purpose\n+        std = (\n+            self.config.initializer_range\n+            if hasattr(self.config, \"initializer_range\")\n+            else self.config.text_config.initializer_range\n+        )\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+PIXTRAL_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values: list of N_img images of variable sizes,\n+                each of shape (C, H, W)\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+def generate_block_attention_mask(patch_embeds_list, tensor):\n+    dtype = tensor.dtype\n+    device = tensor.device\n+    seq_len = tensor.shape[1]\n+    d_min = torch.finfo(dtype).min\n+    causal_mask = torch.full((seq_len, seq_len), fill_value=d_min, dtype=dtype, device=device)\n+\n+    block_end_idx = torch.tensor(patch_embeds_list).cumsum(-1)\n+    block_start_idx = torch.tensor([0] + patch_embeds_list[:-1]).cumsum(-1)\n+    for start, end in zip(block_start_idx, block_end_idx):\n+        causal_mask[start:end, start:end] = 0\n+\n+    causal_mask = causal_mask[None, None, :, :].expand(tensor.shape[0], 1, -1, -1)\n+    return causal_mask\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The PIXTRAL model which consists of a vision backbone and a language model.\"\"\",\n+    PIXTRAL_START_DOCSTRING,\n+)\n+class PixtralModel(PixtralPreTrainedModel):\n+    base_model_prefix = \"vision_encoder\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.config = config\n+        self.patch_conv = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=config.hidden_size,\n+            kernel_size=config.patch_size,\n+            stride=config.patch_size,\n+            bias=False,\n+        )\n+        self.ln_pre = PixtralRMSNorm(config.hidden_size, eps=1e-5)\n+        self.transformer = PixtralTransformer(config)\n+        self.patch_positional_embedding = PixtralRotaryEmbedding(config, device=self.device)\n+\n+    @add_start_docstrings_to_model_forward(PIXTRAL_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: List[torch.Tensor],\n+        output_hidden_states: Optional[bool] = False,\n+        output_attentions: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        *args,\n+        **kwargs,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        \"\"\"\n+        Returns:\n+            pixel_values: tensor of token features for\n+                all tokens of all images of shape (N_toks, D)\n+        \"\"\"\n+        # pass images through initial convolution independently\n+        patch_embeds_list = [self.patch_conv(img.unsqueeze(0).to(self.dtype)) for img in pixel_values]\n+\n+        # flatten to a single sequence\n+        patch_embeds = torch.cat([p.flatten(2).permute(0, 2, 1) for p in patch_embeds_list], dim=1)\n+        patch_embeds = self.ln_pre(patch_embeds)\n+\n+        # positional embeddings\n+        position_ids = position_ids_in_meshgrid(\n+            patch_embeds_list, max_width=self.config.image_size // self.config.patch_size\n+        ).to(self.device)\n+\n+        position_embedding = self.patch_positional_embedding(patch_embeds, position_ids)\n+        attention_mask = generate_block_attention_mask(\n+            [p.shape[-2] * p.shape[-1] for p in patch_embeds_list], patch_embeds\n+        )\n+        return self.transformer(patch_embeds, attention_mask, position_embedding)"
        },
        {
            "sha": "9362703c8aa6da07c030b5b30fa5f1f3514da35d",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "added",
            "additions": 282,
            "deletions": 0,
            "changes": 282,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,282 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Processor class for Pixtral.\n+\"\"\"\n+\n+from typing import List, Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput, is_valid_image, load_image\n+from ...processing_utils import ProcessorMixin\n+from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n+from ...utils import TensorType, is_torch_device, is_torch_dtype, is_torch_tensor, logging, requires_backends\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+# Copied from transformers.models.idefics2.processing_idefics2.is_url\n+def is_url(val) -> bool:\n+    return isinstance(val, str) and val.startswith(\"http\")\n+\n+\n+# Copied from transformers.models.idefics2.processing_idefics2.is_image_or_image_url\n+def is_image_or_image_url(elem):\n+    return is_url(elem) or is_valid_image(elem)\n+\n+\n+# Copied from transformers.models.pixtral.image_processing_pixtral.BatchMixFeature\n+class BatchMixFeature(BatchFeature):\n+    def to(self, *args, **kwargs) -> \"BatchMixFeature\":\n+        \"\"\"\n+        Send all values to device by calling `v.to(*args, **kwargs)` (PyTorch only). This should support casting in\n+        different `dtypes` and sending the `BatchFeature` to a different `device`.\n+\n+        Args:\n+            args (`Tuple`):\n+                Will be passed to the `to(...)` function of the tensors.\n+            kwargs (`Dict`, *optional*):\n+                Will be passed to the `to(...)` function of the tensors.\n+\n+        Returns:\n+            [`BatchFeature`]: The same instance after modification.\n+        \"\"\"\n+        requires_backends(self, [\"torch\"])\n+        import torch  # noqa\n+\n+        new_data = {}\n+        device = kwargs.get(\"device\")\n+        # Check if the args are a device or a dtype\n+        if device is None and len(args) > 0:\n+            # device should be always the first argument\n+            arg = args[0]\n+            if is_torch_dtype(arg):\n+                # The first argument is a dtype\n+                pass\n+            elif isinstance(arg, str) or is_torch_device(arg) or isinstance(arg, int):\n+                device = arg\n+            else:\n+                # it's something else\n+                raise ValueError(f\"Attempting to cast a BatchFeature to type {str(arg)}. This is not supported.\")\n+        # We cast only floating point tensors to avoid issues with tokenizers casting `LongTensor` to `FloatTensor`\n+        for k, v in self.items():\n+            # check if v is a floating point\n+            if isinstance(v, list):\n+                new_data[k] = [\n+                    element.to(*args, **kwargs) for sample in v for element in sample if is_torch_tensor(element)\n+                ]\n+            elif torch.is_floating_point(v):\n+                # cast and send to device\n+                new_data[k] = v.to(*args, **kwargs)\n+            elif device is not None:\n+                new_data[k] = v.to(device=device)\n+            else:\n+                new_data[k] = v\n+        self.data = new_data\n+        return self\n+\n+\n+class PixtralProcessor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Pixtral processor which wraps a Pixtral image processor and a Pixtral tokenizer into a single processor.\n+\n+    [`PixtralProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`LlamaTokenizerFast`]. See the\n+    [`~PixtralProcessor.__call__`] and [`~PixtralProcessor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`PixtralImageProcessor`], *optional*):\n+            The image processor is a required input.\n+        tokenizer ([`LlamaTokenizerFast`], *optional*):\n+            The tokenizer is a required input.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Patch size from the vision tower.\n+        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n+            in a chat into a tokenizable string.\n+        image_token (`str`, *optional*, defaults to `\"[IMG]\"`):\n+            Special token used to denote image location.\n+        image_break_token (`str`, *optional*, defaults to `\"[IMG_BREAK]\"`):\n+            Special token used to denote the end of a line of pixels in an image.\n+        image_end_token (`str`, *optional*, defaults to `\"[IMG_END]\"`):\n+            Special token used to denote the end of an image input.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+    valid_kwargs = [\n+        \"chat_template\",\n+        \"patch_size\",\n+        \"image_token\",\n+        \"image_break_token\",\n+        \"image_end_token\",\n+    ]\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        patch_size: int = 16,\n+        chat_template=None,\n+        image_token=\"[IMG]\",  # set the default and let users change if they have peculiar special tokens in rare cases\n+        image_break_token=\"[IMG_BREAK]\",\n+        image_end_token=\"[IMG_END]\",\n+        **kwargs,\n+    ):\n+        self.patch_size = patch_size\n+        self.image_token = image_token\n+        self.image_break_token = image_break_token\n+        self.image_end_token = image_end_token\n+        super().__init__(image_processor, tokenizer, chat_template=chat_template)\n+\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        padding: Union[bool, str, PaddingStrategy] = False,\n+        truncation: Union[bool, str, TruncationStrategy] = None,\n+        max_length=None,\n+        return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n+    ) -> BatchMixFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` and `kwrags` arguments to\n+        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n+                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                index) among:\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            max_length (`int`, *optional*):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            truncation (`bool`, *optional*):\n+                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+            `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+        \"\"\"\n+        if images is not None:\n+            if is_image_or_image_url(images):\n+                images = [[images]]\n+            elif isinstance(images, list) and is_image_or_image_url(images[0]):\n+                images = [images]\n+            elif (\n+                not isinstance(images, list)\n+                and not isinstance(images[0], list)\n+                and not is_image_or_image_url(images[0][0])\n+            ):\n+                raise ValueError(\n+                    \"Invalid input images. Please provide a single image or a list of images or a list of list of images.\"\n+                )\n+            images = [[load_image(im) for im in sample] for sample in images]\n+            image_inputs = self.image_processor(images, patch_size=self.patch_size, return_tensors=return_tensors)\n+        else:\n+            image_inputs = {}\n+\n+        if isinstance(text, str):\n+            text = [text]\n+        elif not isinstance(text, list) and not isinstance(text[0], str):\n+            raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n+\n+        # try to expand inputs in processing if we have the necessary parts\n+        prompt_strings = text\n+        if image_inputs.get(\"pixel_values\") is not None:\n+            # Replace the image token with the expanded image token sequence\n+            images = image_inputs[\"pixel_values\"]\n+            image_sizes = image_inputs.pop(\"image_sizes\")\n+            prompt_strings = []\n+\n+            for sample_images, sample_image_sizes, sample in zip(images, image_sizes, text):\n+                replace_strings = []\n+                # First calculate the number of tokens needed for each image and put in a placeholder\n+                for image, image_size in zip(sample_images, sample_image_sizes):\n+                    height, width = image_size\n+                    num_height_tokens = height // self.patch_size\n+                    num_width_tokens = width // self.patch_size\n+                    replace_tokens = [\n+                        [self.image_token] * num_width_tokens + [self.image_break_token]\n+                    ] * num_height_tokens\n+                    # Flatten list\n+                    replace_tokens = [item for sublist in replace_tokens for item in sublist]\n+                    replace_tokens[-1] = self.image_end_token\n+                    replace_str = \"\".join(replace_tokens)\n+                    replace_strings.append(replace_str)\n+                    sample = sample.replace(self.image_token, \"<placeholder>\", 1)\n+\n+                while \"<placeholder>\" in sample:\n+                    replace_str = replace_strings.pop(0)\n+                    sample = sample.replace(\"<placeholder>\", replace_str, 1)\n+\n+                prompt_strings.append(sample)\n+\n+        text_inputs = self.tokenizer(\n+            prompt_strings,\n+            return_tensors=return_tensors,\n+            padding=padding,\n+            truncation=truncation,\n+            max_length=max_length,\n+        )\n+        return BatchMixFeature(data={**text_inputs, **image_inputs})\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    @property\n+    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))"
        },
        {
            "sha": "2db7b38b580375ae67bcff3c9ccea6607d606448",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -7067,6 +7067,20 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class PixtralModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class PixtralPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class PLBartForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "436378582e54ca596d6a0d23437e10612ba8c798",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -506,6 +506,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class PixtralImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class PoolFormerFeatureExtractor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "5c05480ffa6dbb4593f07d36439902b8effb2325",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 47,
            "deletions": 0,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -569,3 +569,50 @@ def test_expansion_in_processing(self):\n \n         # check that both inputs are handled correctly and generate the same output\n         self.assertListEqual(output_expanded[:, -20:].tolist(), output[:, -20:].tolist())\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_pixtral(self):\n+        model_id = \"hf-internal-testing/pixtral-12b\"\n+        model = LlavaForConditionalGeneration.from_pretrained(model_id)\n+        processor = AutoProcessor.from_pretrained(model_id)\n+\n+        IMG_URLS = [\n+            Image.open(requests.get(\"https://picsum.photos/id/237/400/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/231/200/300\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/27/500/500\", stream=True).raw),\n+            Image.open(requests.get(\"https://picsum.photos/id/17/150/600\", stream=True).raw),\n+        ]\n+        PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n+\n+        # image = Image.open(requests.get(url, stream=True).raw)\n+        inputs = processor(text=PROMPT, images=IMG_URLS, return_tensors=\"pt\").to(\"cuda\")\n+        generate_ids = model.generate(**inputs, max_new_tokens=500)\n+        ouptut = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+\n+        # fmt: off\n+        EXPECTED_GENERATION = \"\"\"\n+Describe the images.\n+Sure, let's break down each image description:\n+\n+1. **Image 1:**\n+   - **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n+   - **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n+\n+2. **Image 2:**\n+   - **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n+   - **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n+\n+3. **Image 3:**\n+   - **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n+   - **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n+\n+4. **Image 4:**\n+   - **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n+   - **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\n+\n+Each image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n+\"\"\"\n+        # fmt: on\n+        # check that both inputs are handled correctly and generate the same output\n+        self.assertListEqual(ouptut, EXPECTED_GENERATION)"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/pixtral/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2F__init__.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462"
        },
        {
            "sha": "3994201c065c45e27671316f36b5ae4910146764",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "added",
            "additions": 217,
            "deletions": 0,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,217 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import random\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import PixtralImageProcessor\n+\n+\n+class PixtralImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        max_num_images_per_sample=3,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        patch_size=None,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"longest_edge\": 24}\n+        patch_size = patch_size if patch_size is not None else {\"height\": 8, \"width\": 8}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.max_num_images_per_sample = max_num_images_per_sample\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.patch_size = patch_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"patch_size\": self.patch_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_image_shape(self, image):\n+        if isinstance(image, Image.Image):\n+            width, height = image.size\n+        elif isinstance(image, np.ndarray):\n+            height, width = image.shape[:2]\n+        elif isinstance(image, torch.Tensor):\n+            height, width = image.shape[-2:]\n+\n+        max_height = max_width = self.size.get(\"longest_edge\")\n+\n+        ratio = max(height / max_height, width / max_width)\n+        if ratio > 1:\n+            height = int(np.ceil(height / ratio))\n+            width = int(np.ceil(width / ratio))\n+\n+        patch_height, patch_width = self.patch_size[\"height\"], self.patch_size[\"width\"]\n+        num_height_tokens = (height - 1) // patch_height + 1\n+        num_width_tokens = (width - 1) // patch_width + 1\n+\n+        height = num_height_tokens * patch_height\n+        width = num_width_tokens * patch_width\n+\n+        return self.num_channels, height, width\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        # Use prepare_image_inputs to make a list of list of single images\n+\n+        images_list = []\n+        for _ in range(self.batch_size):\n+            images = []\n+            for _ in range(random.randint(1, self.max_num_images_per_sample)):\n+                img = prepare_image_inputs(\n+                    batch_size=1,\n+                    num_channels=self.num_channels,\n+                    min_resolution=self.min_resolution,\n+                    max_resolution=self.max_resolution,\n+                    equal_resolution=equal_resolution,\n+                    numpify=numpify,\n+                    torchify=torchify,\n+                )[0]\n+                images.append(img)\n+            images_list.append(images)\n+        return images_list\n+\n+\n+@require_torch\n+@require_vision\n+class PixtralImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = PixtralImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = PixtralImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"patch_size\"))\n+        self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+        self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_call_pil(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PIL images\n+        image_inputs_list = self.image_processor_tester.prepare_image_inputs()\n+        for image_inputs in image_inputs_list:\n+            for image in image_inputs:\n+                self.assertIsInstance(image, Image.Image)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n+        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+        # Test batched\n+        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+            for encoded_image, image in zip(encoded_images, images):\n+                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+\n+    def test_call_numpy(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random numpy tensors\n+        image_inputs_list = self.image_processor_tester.prepare_image_inputs(numpify=True)\n+        for image_inputs in image_inputs_list:\n+            for image in image_inputs:\n+                self.assertIsInstance(image, np.ndarray)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n+        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+        # Test batched\n+        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+            for encoded_image, image in zip(encoded_images, images):\n+                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+\n+    def test_call_pytorch(self):\n+        # Initialize image_processing\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        # create random PyTorch tensors\n+        image_inputs_list = self.image_processor_tester.prepare_image_inputs(torchify=True)\n+        for image_inputs in image_inputs_list:\n+            for image in image_inputs:\n+                self.assertIsInstance(image, torch.Tensor)\n+\n+        # Test not batched input\n+        encoded_images = image_processing(image_inputs_list[0][0], return_tensors=\"pt\").pixel_values\n+        expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_inputs_list[0][0])\n+        self.assertEqual(tuple(encoded_images[0][0].shape), expected_output_image_shape)\n+\n+        # Test batched\n+        batch_encoded_images = image_processing(image_inputs_list, return_tensors=\"pt\").pixel_values\n+        for encoded_images, images in zip(batch_encoded_images, image_inputs_list):\n+            for encoded_image, image in zip(encoded_images, images):\n+                expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image)\n+                self.assertEqual(tuple(encoded_image.shape), expected_output_image_shape)\n+\n+    @unittest.skip(reason=\"PixtralImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")  # FIXME Amy\n+    def test_call_numpy_4_channels(self):\n+        pass"
        },
        {
            "sha": "bd41fa1c9e62fb7d0440c6541074a4fd6eb8e912",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "added",
            "additions": 292,
            "deletions": 0,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,292 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Pixtral model.\"\"\"\n+\n+import gc\n+import unittest\n+\n+import requests\n+\n+from transformers import (\n+    AutoProcessor,\n+    PixtralModel,\n+    PixtralVisionConfig,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import (\n+    require_bitsandbytes,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+else:\n+    is_torch_greater_or_equal_than_2_0 = False\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+class PixtralModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        hidden_size=32,\n+        projection_dim=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+\n+        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return PixtralVisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = PixtralModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n+        image_size = (self.image_size, self.image_size)\n+        patch_size = (self.patch_size, self.patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def create_and_check_model_with_projection(self, config, pixel_values):\n+        model = PixtralModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n+        image_size = (self.image_size, self.image_size)\n+        patch_size = (self.patch_size, self.patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n+        self.parent.assertEqual(result.image_embeds.shape, (self.batch_size, self.projection_dim))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class PixtralModelModelTest(ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Model tester for `PixtralModel`.\n+    \"\"\"\n+\n+    all_model_classes = (PixtralModel,) if is_torch_available() else ()\n+    test_pruning = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = PixtralModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=PixtralVisionConfig, has_text_modality=False)\n+\n+    @unittest.skip(\"model does not support input embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(\"model does not support input embeds\")\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in Pixtral models\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Compile not yet supported because in Pixtral models\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_batching_equivalence(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_multi_gpu_data_parallel_forward(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_model_parallelism(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_model_outputs_equivalence(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_save_load(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_model_main_input_name(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_initialization(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_gradient_checkpointing_backward_compatibility(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Not supported yet\")\n+    def test_determinism(self):\n+        pass\n+\n+\n+@require_torch\n+class PixtralModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        self.processor = AutoProcessor.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n+\n+    def tearDown(self):\n+        gc.collect()\n+        torch.cuda.empty_cache()\n+\n+    @slow\n+    @require_bitsandbytes\n+    def test_small_model_integration_test(self):\n+        # Let' s make sure we test the preprocessing to replace what is used\n+        model = PixtralModel.from_pretrained(\"hf-internal-testing/pixtral-12b\", load_in_4bit=True)\n+\n+        prompt = \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n+        image_file = \"https://pixtral-vl.github.io/static/images/view.jpg\"\n+        raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        inputs = self.processor(prompt, raw_image, return_tensors=\"pt\")\n+\n+        EXPECTED_INPUT_IDS = torch.tensor([[1, 32000, 28705, 13, 11123, 28747, 1824, 460, 272, 1722,315, 1023, 347, 13831, 925, 684, 739, 315, 3251, 456,1633, 28804, 13, 4816, 8048, 12738, 28747]])  # fmt: skip\n+        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n+\n+        output = model.generate(**inputs, max_new_tokens=20)\n+        EXPECTED_DECODED_TEXT = \"\\nUSER: What are the things I should be cautious about when I visit this place?\\nASSISTANT: When visiting this place, there are a few things one should be cautious about. Firstly,\"  # fmt: skip\n+\n+        self.assertEqual(\n+            self.processor.decode(output[0], skip_special_tokens=True),\n+            EXPECTED_DECODED_TEXT,\n+        )"
        },
        {
            "sha": "b70cab1c074480cfcf498a61d6b2a61269a91d21",
            "filename": "tests/models/pixtral/test_processor_pixtral.py",
            "status": "added",
            "additions": 233,
            "deletions": 0,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8bd2b1e8c23234cd607ca8d63f53c1edfea27462/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processor_pixtral.py?ref=8bd2b1e8c23234cd607ca8d63f53c1edfea27462",
            "patch": "@@ -0,0 +1,233 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import unittest\n+\n+import requests\n+import torch\n+\n+from transformers.testing_utils import require_vision\n+from transformers.utils import is_vision_available\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import AutoTokenizer, PixtralImageProcessor, PixtralProcessor\n+\n+\n+@require_vision\n+class PixtralProcessorTest(unittest.TestCase):\n+    processor_class = PixtralProcessor\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        cls.url_0 = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        cls.image_0 = Image.open(requests.get(cls.url_0, stream=True).raw)\n+        cls.url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        cls.image_1 = Image.open(requests.get(cls.url_1, stream=True).raw)\n+        cls.url_2 = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/snowman.jpg\"\n+        cls.image_2 = Image.open(requests.get(cls.url_2, stream=True).raw)\n+\n+    def setUp(self):\n+        super().setUp()\n+\n+        # FIXME - just load the processor directly from the checkpoint\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n+        image_processor = PixtralImageProcessor()\n+        self.processor = PixtralProcessor(tokenizer=tokenizer, image_processor=image_processor)\n+\n+    @unittest.skip(\"No chat template was set for this model (yet)\")\n+    def test_chat_template(self):\n+        expected_prompt = \"USER: [IMG]\\nWhat is shown in this image? ASSISTANT:\"\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        formatted_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n+        self.assertEqual(expected_prompt, formatted_prompt)\n+\n+    @unittest.skip(\"No chat template was set for this model (yet)\")\n+    def test_image_token_filling(self):\n+        # Important to check with non square image\n+        image = torch.randint(0, 2, (3, 500, 316))\n+        expected_image_tokens = 1526\n+        image_token_index = 32000\n+\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        inputs = self.processor(\n+            text=[self.processor.apply_chat_template(messages)],\n+            images=[image],\n+            return_tensors=\"pt\",\n+        )\n+        image_tokens = (inputs[\"input_ids\"] == image_token_index).sum().item()\n+        self.assertEqual(expected_image_tokens, image_tokens)\n+\n+    def test_processor_with_single_image(self):\n+        prompt_string = \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\"\n+\n+        # Make small for checking image token expansion\n+        self.processor.image_processor.size = {\"longest_edge\": 30}\n+        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+\n+        # Test passing in an image\n+        inputs_image = self.processor(text=prompt_string, images=self.image_0, return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 1)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n+            [21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = self.processor(text=prompt_string, images=self.url_0, return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 1)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to \"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the content of the image? ASSISTANT:\"\n+            [21510,  1058,  1032,    10,    10,    12,    10,    10,    13,  1010, 7493,  1681,  1278,  4701,  1307,  1278,  3937,  1063,  1349,  4290, 16002, 41150,  1058]\n+        )\n+        # fmt: on\n+\n+    def test_processor_with_multiple_images_single_list(self):\n+        prompt_string = \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\"\n+\n+        # Make small for checking image token expansion\n+        self.processor.image_processor.size = {\"longest_edge\": 30}\n+        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+\n+        # Test passing in an image\n+        inputs_image = self.processor(text=prompt_string, images=[self.image_0, self.image_1], return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 1)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 2)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = self.processor(text=prompt_string, images=[self.url_0, self.url_1], return_tensors=\"pt\")\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 1)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 2)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+    def test_processor_with_multiple_images_multiple_lists(self):\n+        prompt_string = [\n+            \"USER: [IMG][IMG]\\nWhat's the difference between these two images? ASSISTANT:\",\n+            \"USER: [IMG]\\nWhat's the content of the image? ASSISTANT:\",\n+        ]\n+        self.processor.tokenizer.pad_token = \"</s>\"\n+        image_inputs = [[self.image_0, self.image_1], [self.image_2]]\n+\n+        # Make small for checking image token expansion\n+        self.processor.image_processor.size = {\"longest_edge\": 30}\n+        self.processor.image_processor.patch_size = {\"height\": 2, \"width\": 2}\n+\n+        # Test passing in an image\n+        inputs_image = self.processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_image)\n+        self.assertTrue(len(inputs_image[\"input_ids\"]) == 2)\n+        self.assertIsInstance(inputs_image[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"]) == 2)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_image[\"pixel_values\"][0]) == 2)\n+        self.assertIsInstance(inputs_image[\"pixel_values\"][0][0], torch.Tensor)\n+\n+        # fmt: off\n+        input_ids = inputs_image[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on\n+\n+        # Test passing in a url\n+        inputs_url = self.processor(text=prompt_string, images=image_inputs, return_tensors=\"pt\", padding=True)\n+        self.assertIn(\"input_ids\", inputs_url)\n+        self.assertTrue(len(inputs_url[\"input_ids\"]) == 2)\n+        self.assertIsInstance(inputs_url[\"input_ids\"], torch.Tensor)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"]) == 2)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0], list)\n+        self.assertTrue(len(inputs_url[\"pixel_values\"][0]) == 2)\n+        self.assertIsInstance(inputs_url[\"pixel_values\"][0][0], torch.Tensor)\n+\n+        # fmt: off\n+        input_ids = inputs_url[\"input_ids\"]\n+        self.assertEqual(\n+            input_ids[0].tolist(),\n+            # Equivalent to [\"USER: [IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END][IMG][IMG][IMG_BREAK][IMG][IMG][IMG_END]\\nWhat's the difference between these two images? ASSISTANT:\"]\n+            [21510, 1058, 1032, 10, 10, 12, 10, 10, 13, 10, 10, 12, 10, 10, 13, 1010, 7493, 1681, 1278, 6592, 2396, 2576, 2295, 8061, 1063, 1349, 4290, 16002, 41150, 1058]\n+        )\n+        # fmt: on"
        }
    ],
    "stats": {
        "total": 2709,
        "additions": 2707,
        "deletions": 2
    }
}