{
    "author": "UserChen666",
    "message": "Adapt some test case on npu (#42335)\n\n* Adapt some test case on npu\n\n* Adapt some test case on npu\n\n---------\n\nCo-authored-by: mamba-chen <chenhao388@huawei.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "b0831697be1dab7b88cccd95231ba3fbc50d90ee",
    "files": [
        {
            "sha": "e05d5dbbb72f0e5dc89052228663d712ca9bde05",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0831697be1dab7b88cccd95231ba3fbc50d90ee/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0831697be1dab7b88cccd95231ba3fbc50d90ee/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=b0831697be1dab7b88cccd95231ba3fbc50d90ee",
            "patch": "@@ -222,10 +222,12 @@\n     IS_ROCM_SYSTEM = torch.version.hip is not None\n     IS_CUDA_SYSTEM = torch.version.cuda is not None\n     IS_XPU_SYSTEM = getattr(torch.version, \"xpu\", None) is not None\n+    IS_NPU_SYSTEM = getattr(torch, \"npu\", None) is not None\n else:\n     IS_ROCM_SYSTEM = False\n     IS_CUDA_SYSTEM = False\n     IS_XPU_SYSTEM = False\n+    IS_NPU_SYSTEM = False\n \n logger = transformers_logging.get_logger(__name__)\n \n@@ -3174,6 +3176,8 @@ def get_device_properties() -> DeviceProperties:\n         gen_mask = 0x000000FF00000000\n         gen = (arch & gen_mask) >> 32\n         return (\"xpu\", gen, None)\n+    elif IS_NPU_SYSTEM:\n+        return (\"npu\", None, None)\n     else:\n         return (torch_device, None, None)\n "
        },
        {
            "sha": "114a700919d2109217912b4ba6319d80826c6088",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0831697be1dab7b88cccd95231ba3fbc50d90ee/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0831697be1dab7b88cccd95231ba3fbc50d90ee/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=b0831697be1dab7b88cccd95231ba3fbc50d90ee",
            "patch": "@@ -168,6 +168,7 @@ def test_speculative_generation(self):\n                 (\"xpu\", 3): \"My favourite condiment is 100% beef and comes in a 12 oz. jar. It is sold in\",\n                 (\"cuda\", 7): \"My favourite condiment is 100% natural. It's a little spicy and a little sweet, but it's the\",\n                 (\"cuda\", 8): \"My favourite condiment is 100% beef, 100% beef, 100% beef.\",\n+                (\"npu\", None): \"My favourite condiment is 100% chicken and beef. I love it because it's so good and I love it\",\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n@@ -214,6 +215,7 @@ def test_export_static_cache(self):\n                 (\"xpu\", None): [\"My favourite condiment is 100% plain, unflavoured, and unadulterated. It is\"],\n                 (\"rocm\", (9, 5)): [\"My favourite condiment is 100% plain, unflavoured, and unadulterated.\"],\n                 (\"cuda\", None): cuda_expectation,\n+                (\"npu\", None): [\"My favourite condiment is 100% plain, unsalted, unsweetened, and unflavored. It is\"],\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT_COMPLETION = expected_text_completions.get_expectation()"
        },
        {
            "sha": "d8deefc7806714df9b59adc0d0bccae8d7fd7f34",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b0831697be1dab7b88cccd95231ba3fbc50d90ee/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b0831697be1dab7b88cccd95231ba3fbc50d90ee/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b0831697be1dab7b88cccd95231ba3fbc50d90ee",
            "patch": "@@ -448,7 +448,7 @@ def _can_output_attn(model):\n             if torch_device in [\"cpu\", \"cuda\"]:\n                 atol = atols[torch_device, enable_kernels, dtype]\n                 rtol = rtols[torch_device, enable_kernels, dtype]\n-            elif torch_device == \"hpu\":\n+            elif torch_device in [\"hpu\", \"npu\"]:\n                 atol = atols[\"cuda\", enable_kernels, dtype]\n                 rtol = rtols[\"cuda\", enable_kernels, dtype]\n             elif torch_device == \"xpu\":"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 7,
        "deletions": 1
    }
}