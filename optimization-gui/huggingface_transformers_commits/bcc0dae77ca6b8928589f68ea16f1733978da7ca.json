{
    "author": "yao-matrix",
    "message": "enable flex attention ut cases on XPU (#40989)\n\n* enable flex attention ut cases on XPU\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "bcc0dae77ca6b8928589f68ea16f1733978da7ca",
    "files": [
        {
            "sha": "8fdd2bfa9c21cc8c6345b560fba1ab6719e52e07",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=bcc0dae77ca6b8928589f68ea16f1733978da7ca",
            "patch": "@@ -259,8 +259,8 @@ def train_and_return_metrics(optim: str) -> tuple[int, float]:\n             f\" gpu_total_mem_bnb={gpu_total_mem_bnb}MB\",\n         )\n \n-        self.assertEqual(\n-            loss_orig, loss_bnb, f\"loss should be the same, but got loss_orig={loss_orig}, loss_bnb={loss_bnb}\"\n+        self.assertAlmostEqual(\n+            loss_orig, loss_bnb, 5, f\"loss should be the same, but got loss_orig={loss_orig}, loss_bnb={loss_bnb}\"\n         )\n \n     def run_trainer("
        },
        {
            "sha": "46a5cdd7bdd0571c3316432a616d197fdab9ac42",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=bcc0dae77ca6b8928589f68ea16f1733978da7ca",
            "patch": "@@ -25,7 +25,6 @@\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_torch_large_accelerator,\n     slow,\n     torch_device,\n@@ -449,7 +448,7 @@ def test_eager_matches_sdpa_generate(self):\n                     msg=f\"\\n{tokenizer.batch_decode(res_eager)} \\nvs\\n{tokenizer.batch_decode(res_sdpa)}\",\n                 )\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_flex_attention_with_grads(self):\n         \"\"\"\n         Overwriting as the namings/functionality on the attention part are different; for now it's more of a unique model."
        },
        {
            "sha": "c102c2c273ca7751be3244223eb36608115d4f5c",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=bcc0dae77ca6b8928589f68ea16f1733978da7ca",
            "patch": "@@ -24,7 +24,6 @@\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     torch_device,\n )\n \n@@ -1231,7 +1230,7 @@ def test_custom_4d_attention_mask(self):\n \n     # Based on tests.test_modeling_common.ModelTesterMixin.test_flex_attention_with_grads\n     # Update hidden size for encoder and decoder\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n             # TODO: raushan, fix for composite models after making VLMs support new attn API\n@@ -1516,7 +1515,7 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n \n     # Based on tests.test_modeling_common.ModelTesterMixin.test_flex_attention_with_grads\n     # Update hidden size for encoder\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n             # TODO: raushan, fix for composite models after making VLMs support new attn API"
        },
        {
            "sha": "8bad77e71b226274ab2c2b438062545758ab27c1",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=bcc0dae77ca6b8928589f68ea16f1733978da7ca",
            "patch": "@@ -26,6 +26,7 @@\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n+    require_torch_accelerator,\n     require_torch_gpu,\n     slow,\n     torch_device,\n@@ -534,7 +535,7 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_new_cache_format(self, num_beams, do_sample):\n         pass\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_flex_attention_with_grads(self):\n         \"\"\"\n         Overwriting as the base hidden size is big enough for compile."
        },
        {
            "sha": "ee51f183f19c59a08a8ae40f8904ecc35fd27875",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bcc0dae77ca6b8928589f68ea16f1733978da7ca/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=bcc0dae77ca6b8928589f68ea16f1733978da7ca",
            "patch": "@@ -4207,7 +4207,7 @@ def update_config_headdim(config, requested_dim):\n \n         return config\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 8,
        "deletions": 9
    }
}