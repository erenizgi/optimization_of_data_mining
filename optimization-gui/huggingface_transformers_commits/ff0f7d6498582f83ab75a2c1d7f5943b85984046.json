{
    "author": "remi-or",
    "message": "More data in benchmarking (#41848)\n\n* Reduce scope of cross-generate\n\n* Rm generate_sall configs\n\n* Workflow benchmarks more\n\n* Prevent crash when FA is not installed",
    "sha": "ff0f7d6498582f83ab75a2c1d7f5943b85984046",
    "files": [
        {
            "sha": "9b3f73a568c28d4606593f71b0b50c416980277e",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff0f7d6498582f83ab75a2c1d7f5943b85984046/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff0f7d6498582f83ab75a2c1d7f5943b85984046/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=ff0f7d6498582f83ab75a2c1d7f5943b85984046",
            "patch": "@@ -52,7 +52,7 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --cross-generate --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n           PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r"
        },
        {
            "sha": "7e66837c2465644219fa55d3d56bb264171a7a14",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff0f7d6498582f83ab75a2c1d7f5943b85984046/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff0f7d6498582f83ab75a2c1d7f5943b85984046/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=ff0f7d6498582f83ab75a2c1d7f5943b85984046",
            "patch": "@@ -3,6 +3,8 @@\n import logging\n from typing import Any\n \n+from transformers.utils.import_utils import is_flash_attn_2_available\n+\n \n KERNELIZATION_AVAILABLE = False\n try:\n@@ -18,6 +20,16 @@\n class BenchmarkConfig:\n     \"\"\"Configuration for a single benchmark scenario.\"\"\"\n \n+    all_attn_implementations = [\n+        (\"flash_attention_2\", None),\n+        (\"eager\", None),\n+        (\"sdpa\", \"math\"),\n+        (\"sdpa\", \"flash_attention\"),\n+        (\"flex_attention\", None),\n+    ]\n+\n+    all_compiled_modes = [None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"]\n+\n     def __init__(\n         self,\n         warmup_iterations: int = 5,\n@@ -59,6 +71,13 @@ def __init__(\n     def check_validity(self, skip_validity_check: bool = False) -> None:\n         if skip_validity_check:\n             return\n+        # Check FA is installed\n+        if self.attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n+            logger.warning(\n+                \"Flash attention does not support compile mode. Defaulting to SDPA w/ flash attention backend.\"\n+            )\n+            self.attn_implementation = \"sdpa\"\n+            self.sdpa_backend = \"flash_attention\"\n         # Flash attention does not support compile mode, so we turn it off # FIXME: it would be better to support it\n         is_fa = self.attn_implementation == \"flash_attention_2\"\n         is_fa |= self.attn_implementation == \"sdpa\" and self.sdpa_backend == \"flash_attention\"\n@@ -163,34 +182,6 @@ def cross_generate_configs(\n     return configs\n \n \n-def generate_all_configs(\n-    warmup_iterations: int = 5,\n-    measurement_iterations: int = 20,\n-    batch_size: int = 1,\n-    sequence_length: int = 128,\n-    num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = True,\n-) -> list[BenchmarkConfig]:\n-    all_attn_implementations = [\n-        (\"flash_attention_2\", None),\n-        (\"eager\", None),\n-        (\"sdpa\", \"math\"),\n-        (\"sdpa\", \"flash_attention\"),\n-        (\"flex_attention\", None),\n-    ]\n-    return cross_generate_configs(\n-        attn_impl_and_sdpa_backend=all_attn_implementations,\n-        compiled_mode=[None, \"default\", \"reduce-overhead\", \"max-autotune\", \"max-autotune-no-cudagraphs\"],\n-        kernelized=[False, KERNELIZATION_AVAILABLE],\n-        warmup_iterations=warmup_iterations,\n-        measurement_iterations=measurement_iterations,\n-        batch_size=batch_size,\n-        sequence_length=sequence_length,\n-        num_tokens_to_generate=num_tokens_to_generate,\n-        gpu_monitoring=gpu_monitoring,\n-    )\n-\n-\n def generate_main_configs(\n     warmup_iterations: int = 5,\n     measurement_iterations: int = 20,"
        },
        {
            "sha": "3b01af6017c45a5c75ee2fb283bc07f9621d789a",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ff0f7d6498582f83ab75a2c1d7f5943b85984046/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ff0f7d6498582f83ab75a2c1d7f5943b85984046/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=ff0f7d6498582f83ab75a2c1d7f5943b85984046",
            "patch": "@@ -23,7 +23,12 @@\n import sys\n import uuid\n \n-from framework.benchmark_config import BenchmarkConfig, generate_all_configs, generate_main_configs\n+from framework.benchmark_config import (\n+    KERNELIZATION_AVAILABLE,\n+    BenchmarkConfig,\n+    cross_generate_configs,\n+    generate_main_configs,\n+)\n from framework.benchmark_runner import BenchmarkRunner\n \n \n@@ -82,7 +87,10 @@\n     # If there is only one (batch_size, sequence_length, num_tokens_to_generate), we benchmark across configs\n     elif len(args.batch_size) * len(args.sequence_length) * len(args.num_tokens_to_generate) == 1:\n         if args.cross_generate:\n-            benchmark_configs = generate_all_configs(\n+            benchmark_configs = cross_generate_configs(\n+                attn_impl_and_sdpa_backend=BenchmarkConfig.all_attn_implementations,\n+                compiled_mode=[None, \"default\"],  # usually there is not much to gain by compiling with other modes\n+                kernelized=[False, KERNELIZATION_AVAILABLE],\n                 warmup_iterations=args.warmup,\n                 measurement_iterations=args.iterations,\n                 batch_size=args.batch_size[0],"
        }
    ],
    "stats": {
        "total": 61,
        "additions": 30,
        "deletions": 31
    }
}