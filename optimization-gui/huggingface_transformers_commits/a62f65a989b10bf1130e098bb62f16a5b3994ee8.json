{
    "author": "llbdyiu66",
    "message": "fix moe routing_weights (#39581)\n\n* fix moe routing_weights\n\n* fix ernie4_5_moe routing_weights\n\n* fix integration test\n\n---------\n\nCo-authored-by: llbdyiu66 <llbdyiu66@users.noreply.github.com>\nCo-authored-by: Vasqu <antonprogamer@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "a62f65a989b10bf1130e098bb62f16a5b3994ee8",
    "files": [
        {
            "sha": "14e598bff9e43c91f6887d43041c5dee864a5836",
            "filename": "src/transformers/models/ernie4_5_moe/modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a62f65a989b10bf1130e098bb62f16a5b3994ee8/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a62f65a989b10bf1130e098bb62f16a5b3994ee8/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodeling_ernie4_5_moe.py?ref=a62f65a989b10bf1130e098bb62f16a5b3994ee8",
            "patch": "@@ -339,12 +339,9 @@ def forward(\n             # router_logits: (batch * sequence_length, n_experts)\n             router_logits = self.gate(hidden_states.float())\n \n-            # NOTE: we are using the original code base at\n-            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n-            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_weights = self.moe_statics(routing_weights)\n-            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )"
        },
        {
            "sha": "3c4e068d37e6a6cfd3ce09959f02f0c2d2505391",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a62f65a989b10bf1130e098bb62f16a5b3994ee8/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a62f65a989b10bf1130e098bb62f16a5b3994ee8/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=a62f65a989b10bf1130e098bb62f16a5b3994ee8",
            "patch": "@@ -150,12 +150,9 @@ def forward(\n             # router_logits: (batch * sequence_length, n_experts)\n             router_logits = self.gate(hidden_states.float())\n \n-            # NOTE: we are using the original code base at\n-            # https://github.com/PaddlePaddle/Paddle/blob/9b40438ce0f6d76b4f08a7837dd1e28b26cf8ee6/python/paddle/incubate/nn/functional/moe_gate_dispatch.py#L109-L116\n-            # this might differ from the remote version regarding the bias (see `Ernie4_5_MoEStatics`)\n             routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n-            routing_weights = self.moe_statics(routing_weights)\n-            routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+            _, selected_experts = torch.topk(self.moe_statics(routing_weights), self.top_k, dim=-1)\n+            routing_weights = torch.gather(routing_weights, dim=-1, index=selected_experts)\n             routing_weights = routing_weights / torch.clamp(\n                 routing_weights.sum(dim=-1, keepdim=True), min=self.norm_min\n             )"
        },
        {
            "sha": "b8a813015512fd6894266dcc23f78e4daf13b258",
            "filename": "tests/models/ernie4_5_moe/test_modeling_ernie4_5_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a62f65a989b10bf1130e098bb62f16a5b3994ee8/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a62f65a989b10bf1130e098bb62f16a5b3994ee8/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fernie4_5_moe%2Ftest_modeling_ernie4_5_moe.py?ref=a62f65a989b10bf1130e098bb62f16a5b3994ee8",
            "patch": "@@ -181,7 +181,7 @@ def get_model(cls):\n     @require_bitsandbytes\n     @slow\n     def test_model_21b_a3b_generation(self):\n-        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant: Yes, I am conscious and I can communicate with you. How can I assist you with any questions or information you need?\"  # fmt: skip\n+        EXPECTED_TEXT_COMPLETION = \"User: Hey, are you conscious? Can you talk to me?\\nAssistant:  I don't have consciousness in the way humans do. I'm a text-based AI created to process and generate responses based on patterns in data.\"  # fmt: skip\n \n         model = self.get_model()\n         tokenizer = AutoTokenizer.from_pretrained(\"baidu/ERNIE-4.5-21B-A3B-PT\", revision=\"refs/pr/11\")"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 5,
        "deletions": 11
    }
}