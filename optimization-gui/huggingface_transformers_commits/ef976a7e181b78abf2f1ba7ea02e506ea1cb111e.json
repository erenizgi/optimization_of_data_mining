{
    "author": "muellerzr",
    "message": "Update trainer for easier handling of accumulate, compile fixes, and proper reporting (#34511)\n\n* Update trainer for easier handling of accumulate + proper reporting\r\n\r\n* test\r\n\r\n* Fixup tests\r\n\r\n* Full fix\r\n\r\n* Fix style\r\n\r\n* rm comment\r\n\r\n* Fix tests\r\n\r\n* Minimize test + remove py 311 check\r\n\r\n* Unused import\r\n\r\n* Forward contrib credits from discussions\r\n\r\n* Fix reported metrics\r\n\r\n* Refactor, good as it's going to get\r\n\r\n* rm pad tok id check\r\n\r\n* object detection and audio are being annoying\r\n\r\n* Fin\r\n\r\n* Fin x2\r\n\r\n---------\r\n\r\nCo-authored-by: Gyanateet Dutta <Ryukijano@users.noreply.github.com>",
    "sha": "ef976a7e181b78abf2f1ba7ea02e506ea1cb111e",
    "files": [
        {
            "sha": "2ef4c3615c9fa2af91ca9f41016f9df80c230d28",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ef976a7e181b78abf2f1ba7ea02e506ea1cb111e",
            "patch": "@@ -28,7 +28,7 @@\n import warnings\n from contextlib import contextmanager\n from dataclasses import dataclass\n-from functools import lru_cache, partial, wraps\n+from functools import partial, wraps\n from threading import Thread\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n from zipfile import is_zipfile\n@@ -5014,7 +5014,6 @@ def _is_quantized_training_enabled(self):\n         return self.hf_quantizer.is_trainable\n \n     @property\n-    @lru_cache\n     def loss_function(self):\n         if getattr(self.config, \"loss_type\", None) is not None:\n             loss_type = self.config.loss_type"
        },
        {
            "sha": "d41b7181be6334a111b165269bbdfd53af28daaa",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 39,
            "deletions": 34,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=ef976a7e181b78abf2f1ba7ea02e506ea1cb111e",
            "patch": "@@ -233,7 +233,6 @@\n     from accelerate.utils import (\n         DistributedDataParallelKwargs,\n         DistributedType,\n-        GradientAccumulationPlugin,\n         load_fsdp_model,\n         load_fsdp_optimizer,\n         save_fsdp_model,\n@@ -601,8 +600,10 @@ def __init__(\n             if not _is_peft_model(unwrapped_model)\n             else unwrapped_model.get_base_model().forward\n         )\n-\n-        self.model_accepts_loss_kwargs = \"loss_kwargs\" in inspect.signature(model_forward).parameters\n+        forward_params = inspect.signature(model_forward).parameters\n+        self.model_accepts_loss_kwargs = (\n+            \"loss_kwargs\" in forward_params and forward_params[\"loss_kwargs\"].kind == inspect.Parameter.VAR_KEYWORD\n+        )\n \n         self.neftune_noise_alpha = args.neftune_noise_alpha\n \n@@ -2444,7 +2445,7 @@ def _inner_training_loop(\n                 update_step += 1\n                 num_batches = args.gradient_accumulation_steps if update_step != (total_updates - 1) else remainder\n                 batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\n-                for inputs in batch_samples:\n+                for i, inputs in enumerate(batch_samples):\n                     step += 1\n                     do_sync_step = (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == steps_in_epoch\n                     # Since we perform prefetching, we need to manually set sync_gradients\n@@ -2484,7 +2485,13 @@ def _inner_training_loop(\n                     if step % args.gradient_accumulation_steps == 0:\n                         self.control = self.callback_handler.on_step_begin(args, self.state, self.control)\n \n-                    with self.accelerator.accumulate(model):\n+                    # We explicitly want to avoid relying on `accelerator.accumulate` for generation training\n+                    context = (\n+                        functools.partial(self.accelerator.no_sync, model=model)\n+                        if i == len(batch_samples) - 1\n+                        else contextlib.nullcontext\n+                    )\n+                    with context():\n                         tr_loss_step = self.training_step(model, inputs, num_items_in_batch)\n \n                     if (\n@@ -3636,15 +3643,11 @@ def training_step(\n             with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                 scaled_loss.backward()\n         else:\n-            if num_items_in_batch is not None:\n-                if self.compute_loss_func or self.model_accepts_loss_kwargs:\n-                    loss *= self.args.gradient_accumulation_steps\n-                # Average tokens across devices is orthogonal to gradient accumulation\n-                if self.args.average_tokens_across_devices:\n-                    loss *= self.args.world_size\n             self.accelerator.backward(loss, **kwargs)\n-\n-        return loss.detach() / self.args.gradient_accumulation_steps\n+            # Finally we need to normalize the loss for reporting\n+            if num_items_in_batch is None:\n+                return loss.detach() / self.args.gradient_accumulation_steps\n+            return loss.detach()\n \n     def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n         \"\"\"\n@@ -3656,9 +3659,6 @@ def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=N\n             labels = inputs.pop(\"labels\")\n         else:\n             labels = None\n-        if self.args.average_tokens_across_devices and num_items_in_batch is not None:\n-            num_items_in_batch_tensor = torch.tensor(num_items_in_batch, device=self.args.device)\n-            num_items_in_batch = int(self.accelerator.gather(num_items_in_batch_tensor).sum().cpu())\n         if self.model_accepts_loss_kwargs:\n             loss_kwargs = {}\n             if num_items_in_batch is not None:\n@@ -3692,6 +3692,9 @@ def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=N\n             # We don't use .loss here since the model may return tuples instead of ModelOutput.\n             loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n \n+        if self.args.average_tokens_across_devices and self.model_accepts_loss_kwargs:\n+            loss *= self.accelerator.num_processes\n+\n         return (loss, outputs) if return_outputs else loss\n \n     def is_local_process_zero(self) -> bool:\n@@ -4946,24 +4949,21 @@ def _add_sm_patterns_to_gitignore(self) -> None:\n             self.repo.git_push()\n \n     def create_accelerator_and_postprocess(self):\n+        # We explicitly don't rely on the `Accelerator` to do gradient accumulation\n         grad_acc_kwargs = {}\n         if is_accelerate_available(\"0.28.0\") and self.args.accelerator_config.gradient_accumulation_kwargs is not None:\n             grad_acc_kwargs = self.args.accelerator_config.gradient_accumulation_kwargs\n \n         # check if num_steps is attempted to be passed in gradient_accumulation_kwargs\n-        if \"num_steps\" in grad_acc_kwargs and self.args.gradient_accumulation_steps > 1:\n-            # raise because we do not know which setting is intended.\n-            raise ValueError(\n-                \"The `AcceleratorConfig`'s `num_steps` is set but `gradient_accumulation_steps` is greater than 1 in the passed `TrainingArguments`\"\n-                \"If using the passed `AcceleratorConfig` is desired, do not set the `TrainingArguments` `gradient_accumulation_steps`.\"\n-            )\n-        elif \"num_steps\" not in grad_acc_kwargs:\n-            # take the gradient_accumulation_steps setting from TrainingArguments.\n-            grad_acc_kwargs[\"num_steps\"] = self.args.gradient_accumulation_steps\n-\n-        grad_acc_kwargs[\"sync_with_dataloader\"] = False\n-\n-        gradient_accumulation_plugin = GradientAccumulationPlugin(**grad_acc_kwargs)\n+        if \"num_steps\" in grad_acc_kwargs:\n+            if self.args.gradient_accumulation_steps > 1:\n+                # raise because we do not know which setting is intended.\n+                raise ValueError(\n+                    \"The `AcceleratorConfig`'s `num_steps` is set but `gradient_accumulation_steps` is greater than 1 in the passed `TrainingArguments`\"\n+                    \"If using the passed `AcceleratorConfig` is desired, do not set the `TrainingArguments` `gradient_accumulation_steps`.\"\n+                )\n+            else:\n+                self.args.gradient_accumulation_steps = grad_acc_kwargs[\"num_steps\"]\n \n         accelerator_config = self.args.accelerator_config.to_dict()\n \n@@ -4994,7 +4994,6 @@ def create_accelerator_and_postprocess(self):\n \n         args = {\n             \"deepspeed_plugin\": self.args.deepspeed_plugin,\n-            \"gradient_accumulation_plugin\": gradient_accumulation_plugin,\n         }\n         if is_accelerate_available(\"0.28.0\"):\n             args[\"dataloader_config\"] = dataloader_config\n@@ -5090,12 +5089,18 @@ def get_batch_samples(self, epoch_iterator, num_batches):\n                 batch_samples += [next(epoch_iterator)]\n             except StopIteration:\n                 break\n+\n+        # Keep default behavior the same\n+        if not self.model_accepts_loss_kwargs:\n+            return batch_samples, None\n+\n         if len(batch_samples) > 0 and \"labels\" in batch_samples[0]:\n             # For now we don't support object detection\n             try:\n-                num_items_in_batch = sum(\n-                    [data_batch[\"labels\"][..., 1:].ne(-100).sum().item() for data_batch in batch_samples]\n-                )\n-            except TypeError:\n+                num_items_in_batch = sum([(batch[\"labels\"].ne(-100)).sum() for batch in batch_samples])\n+            except (TypeError, AttributeError):\n                 pass\n+\n+        if self.args.average_tokens_across_devices:\n+            num_items_in_batch = self.accelerator.gather(num_items_in_batch).sum().item()\n         return batch_samples, num_items_in_batch"
        },
        {
            "sha": "5658372fa71308c813093ac3fc9389f595d9d993",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 31,
            "deletions": 12,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef976a7e181b78abf2f1ba7ea02e506ea1cb111e/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=ef976a7e181b78abf2f1ba7ea02e506ea1cb111e",
            "patch": "@@ -272,6 +272,19 @@ def __getitem__(self, i):\n         return {\"input_ids\": self.x, \"labels\": self.x}\n \n \n+class SequenceClassificationDataset:\n+    def __init__(self, length=64, vocab_size=100, num_labels=5):\n+        self.length = length\n+        self.sequences = [torch.randint(0, vocab_size, (64,)).tolist() for _ in range(length)]\n+        self.labels = torch.randint(0, num_labels, (length,)).tolist()\n+\n+    def __len__(self):\n+        return self.length\n+\n+    def __getitem__(self, i):\n+        return {\"input_ids\": self.sequences[i], \"label\": self.labels[i]}\n+\n+\n class DynamicShapesDataset:\n     def __init__(self, length=64, seed=42, batch_size=8):\n         self.length = length\n@@ -1144,6 +1157,23 @@ def test_number_of_steps_in_training_with_ipex(self):\n             train_output = trainer.train()\n             self.assertEqual(train_output.global_step, 10)\n \n+    def test_torch_compile_loss_func_compatibility(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            args = TrainingArguments(\n+                tmp_dir,\n+                per_device_train_batch_size=2,\n+                torch_compile=True,\n+                max_steps=1,  # compile happens on the first step\n+            )\n+            trainer = Trainer(model=tiny_llama, args=args, train_dataset=train_dataset)  # noqa\n+            trainer.train()\n+\n     @require_peft\n     @require_bitsandbytes\n     def test_bnb_compile(self):\n@@ -3676,9 +3706,6 @@ def test_accelerator_config_from_dict(self):\n             self.assertEqual(trainer.accelerator.even_batches, False)\n             self.assertEqual(trainer.accelerator.use_seedable_sampler, True)\n \n-            if GRAD_ACCUM_KWARGS_VERSION_AVAILABLE:\n-                self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"sync_each_batch\"], True)\n-\n     def test_accelerator_config_from_yaml(self):\n         # Checks that accelerator kwargs can be passed through\n         # and the accelerator is initialized respectively\n@@ -3691,8 +3718,6 @@ def test_accelerator_config_from_yaml(self):\n                     \"even_batches\": False,\n                     \"use_seedable_sampler\": False,\n                 }\n-                if GRAD_ACCUM_KWARGS_VERSION_AVAILABLE:\n-                    accelerator_config[\"gradient_accumulation_kwargs\"] = {\"sync_each_batch\": True}\n                 json.dump(accelerator_config, f)\n             config = RegressionModelConfig(a=1.5, b=2.5)\n             model = RegressionPreTrainedModel(config)\n@@ -3706,9 +3731,6 @@ def test_accelerator_config_from_yaml(self):\n             self.assertEqual(trainer.accelerator.even_batches, False)\n             self.assertEqual(trainer.accelerator.use_seedable_sampler, False)\n \n-            if GRAD_ACCUM_KWARGS_VERSION_AVAILABLE:\n-                self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"sync_each_batch\"], True)\n-\n     def test_accelerator_config_from_dataclass(self):\n         # Checks that accelerator kwargs can be passed through\n         # and the accelerator is initialized respectively\n@@ -3754,10 +3776,7 @@ def test_accelerate_config_from_dataclass_grad_accum(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             args = RegressionTrainingArguments(output_dir=tmp_dir, accelerator_config=accelerator_config)\n             trainer = Trainer(model=model, args=args, eval_dataset=eval_dataset)\n-            self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"num_steps\"], 10)\n-            self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"adjust_scheduler\"], False)\n-            self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"sync_with_dataloader\"], False)\n-            self.assertEqual(trainer.accelerator.gradient_state.plugin_kwargs[\"sync_each_batch\"], True)\n+            self.assertEqual(trainer.args.gradient_accumulation_steps, 10)\n \n     def test_accelerator_config_from_partial(self):\n         # Checks that accelerator kwargs can be passed through"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 71,
        "deletions": 48
    }
}