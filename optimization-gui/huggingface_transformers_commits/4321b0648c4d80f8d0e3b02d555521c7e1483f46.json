{
    "author": "gante",
    "message": "[core] remove `GenerationMixin` inheritance by default in `PreTrainedModel` (#37173)",
    "sha": "4321b0648c4d80f8d0e3b02d555521c7e1483f46",
    "files": [
        {
            "sha": "09cbc9c446b563b19c8ef5656b15774dfda8df32",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -1430,27 +1430,6 @@ def compute_transition_scores(\n \n         return transition_scores\n \n-    def _validate_model_class(self):\n-        \"\"\"\n-        Confirms that the model class is compatible with generation. If not, raises an exception that points to the\n-        right class to use.\n-        \"\"\"\n-        # TODO(joao): remove this function in v4.50, i.e. when we remove the inheritance of `GenerationMixin` from\n-        # `PreTrainedModel`. With that inheritance removed, all model classes inheriting from `GenerationMixin` can\n-        # safely call `GenerationMixin.generate`\n-        if not self.can_generate():\n-            terminations_with_generation_support = [\n-                \"ForCausalLM\",\n-                \"ForConditionalGeneration\",\n-                \"ForSpeechSeq2Seq\",\n-                \"ForVision2Seq\",\n-            ]\n-            raise TypeError(\n-                f\"The current model class ({self.__class__.__name__}) is not compatible with `.generate()`, as \"\n-                \"it doesn't have a language model head. Classes that support generation often end in one of these \"\n-                f\"names: {terminations_with_generation_support}.\"\n-            )\n-\n     def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n         if assistant_model is None:\n             return\n@@ -2213,7 +2192,6 @@ def generate(\n         \"\"\"\n \n         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n-        self._validate_model_class()\n         tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n         assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n "
        },
        {
            "sha": "b8a7831176fe36039dc40dfcec7af2d3bb40cf98",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -55,7 +55,7 @@\n from .activations import get_activation\n from .configuration_utils import PretrainedConfig\n from .dynamic_module_utils import custom_object_save\n-from .generation import CompileConfig, GenerationConfig, GenerationMixin\n+from .generation import CompileConfig, GenerationConfig\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n from .integrations.accelerate import find_tied_parameters, init_empty_weights\n from .integrations.deepspeed import _load_state_dict_into_zero3_model, is_deepspeed_available\n@@ -1704,8 +1704,7 @@ def floating_point_ops(\n         return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n \n \n-# TODO (joao): remove `GenerationMixin` inheritance in v4.50\n-class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMixin, PeftAdapterMixin):\n+class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n     r\"\"\"\n     Base class for all models.\n \n@@ -2157,12 +2156,12 @@ def can_generate(cls) -> bool:\n                 continue\n             if \"PreTrainedModel\" not in str(base) and base.can_generate():\n                 return True\n-        # BC: Detects whether `prepare_inputs_for_generation` has been overwritten in the model. Prior to v4.45, this\n+        # Detects whether `prepare_inputs_for_generation` has been overwritten in the model. Prior to v4.45, this\n         # was how we detected whether a model could generate.\n-        if \"GenerationMixin\" not in str(cls.prepare_inputs_for_generation):\n-            logger.warning_once(\n+        if hasattr(cls, \"prepare_inputs_for_generation\"):  # implicit: doesn't inherit `GenerationMixin`\n+            logger.warning(\n                 f\"{cls.__name__} has generative capabilities, as `prepare_inputs_for_generation` is explicitly \"\n-                \"overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, \"\n+                \"defined. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, \"\n                 \"`PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability \"\n                 \"to call `generate` and other related functions.\"\n                 \"\\n  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the \"\n@@ -2172,7 +2171,6 @@ def can_generate(cls) -> bool:\n                 \"\\n  - If you are not the owner of the model architecture class, please contact the model code owner \"\n                 \"to update it.\"\n             )\n-            return True\n         # Otherwise, can't generate\n         return False\n "
        },
        {
            "sha": "9c5690b496756cc80fbb49b06865b60ddae22e45",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -730,8 +730,12 @@ def add_generation_mixin_to_remote_model(model_class):\n \n     # 3. Prior to v4.45, we could detect whether a model was `generate`-compatible if it had its own `generate` and/or\n     # `prepare_inputs_for_generation` method.\n-    has_custom_generate = \"GenerationMixin\" not in str(getattr(model_class, \"generate\"))\n-    has_custom_prepare_inputs = \"GenerationMixin\" not in str(getattr(model_class, \"prepare_inputs_for_generation\"))\n+    has_custom_generate = hasattr(model_class, \"generate\") and \"GenerationMixin\" not in str(\n+        getattr(model_class, \"generate\")\n+    )\n+    has_custom_prepare_inputs = hasattr(model_class, \"prepare_inputs_for_generation\") and \"GenerationMixin\" not in str(\n+        getattr(model_class, \"prepare_inputs_for_generation\")\n+    )\n     if has_custom_generate or has_custom_prepare_inputs:\n         model_class_with_generation_mixin = type(\n             model_class.__name__, (model_class, GenerationMixin), {**model_class.__dict__}"
        },
        {
            "sha": "3a311f4a734c6ef2ab60eb63a8d135b013caa480",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -1512,8 +1512,8 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n     @classmethod\n     def can_generate(cls) -> bool:\n         \"\"\"\n-        Legacy correction: BertForMaskedLM can't call `generate()` from GenerationMixin.\n-        Remove after v4.50, when we stop making `PreTrainedModel` inherit from `GenerationMixin`.\n+        Legacy correction: BertForMaskedLM can't call `generate()` from `GenerationMixin`, even though it has a\n+        `prepare_inputs_for_generation` method.\n         \"\"\"\n         return False\n "
        },
        {
            "sha": "221dd57485880f6210930376dc536028f3463452",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -1328,8 +1328,8 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n     @classmethod\n     def can_generate(cls) -> bool:\n         \"\"\"\n-        Legacy correction: ErnieForMaskedLM can't call `generate()` from GenerationMixin.\n-        Remove after v4.50, when we stop making `PreTrainedModel` inherit from `GenerationMixin`.\n+        Legacy correction: ErnieForMaskedLM can't call `generate()` from `GenerationMixin`, even though it has a\n+        `prepare_inputs_for_generation` method.\n         \"\"\"\n         return False\n "
        },
        {
            "sha": "822347950a1e1fa8d58f092fe9101c2555bcda89",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...configuration_utils import PretrainedConfig\n-from ...generation import GenerationConfig, LogitsProcessorList, StoppingCriteriaList\n+from ...generation import GenerationConfig, GenerationMixin, LogitsProcessorList, StoppingCriteriaList\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -1122,7 +1122,7 @@ def _cat_and_pad(tensors, pad_token_id):\n     \"\"\",\n     RAG_START_DOCSTRING,\n )\n-class RagTokenForGeneration(RagPreTrainedModel):\n+class RagTokenForGeneration(RagPreTrainedModel, GenerationMixin):\n     def __init__(\n         self,\n         config: Optional[PretrainedConfig] = None,"
        },
        {
            "sha": "ef3250a71257d4e2e0ef8c26bc810b65ace46466",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -999,6 +999,14 @@ def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_\n \n         return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n \n+    @classmethod\n+    def can_generate(cls) -> bool:\n+        \"\"\"\n+        Legacy correction: RemBertForMaskedLM can't call `generate()` from `GenerationMixin`, even though it has a\n+        `prepare_inputs_for_generation` method.\n+        \"\"\"\n+        return False\n+\n \n @add_start_docstrings(\n     \"\"\"RemBERT Model with a `language modeling` head on top for CLM fine-tuning.\"\"\", REMBERT_START_DOCSTRING"
        },
        {
            "sha": "a46ab875d425a07fac7efb6992db1d29eec952b3",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 39,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, L1Loss\n \n from ...activations import ACT2FN\n+from ...generation import GenerationMixin\n from ...integrations.deepspeed import is_deepspeed_zero3_enabled\n from ...integrations.fsdp import is_fsdp_managed_module\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_causal_attention_mask\n@@ -2242,7 +2243,7 @@ def forward(\n     \"\"\"SpeechT5 Model with a speech encoder and a text decoder.\"\"\",\n     SPEECHT5_START_DOCSTRING,\n )\n-class SpeechT5ForSpeechToText(SpeechT5PreTrainedModel):\n+class SpeechT5ForSpeechToText(SpeechT5PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"text_decoder_postnet.lm_head.weight\"]\n \n     def __init__(self, config: SpeechT5Config):\n@@ -2413,44 +2414,6 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        decoder_input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        head_mask=None,\n-        decoder_head_mask=None,\n-        cross_attn_head_mask=None,\n-        use_cache=None,\n-        encoder_outputs=None,\n-        **kwargs,\n-    ):\n-        # Note that this model doesn't inherit from the generation mixin, has unique generate function\n-\n-        # cut decoder_input_ids if past is used\n-        if past_key_values is not None:\n-            past_length = past_key_values[0][0].shape[2]\n-\n-            # Some generation methods already pass only the last input ID\n-            if decoder_input_ids.shape[1] > past_length:\n-                remove_prefix_length = past_length\n-            else:\n-                # Default to old behavior: keep only final ID\n-                remove_prefix_length = decoder_input_ids.shape[1] - 1\n-\n-            decoder_input_ids = decoder_input_ids[:, remove_prefix_length:]\n-\n-        return {\n-            \"encoder_outputs\": encoder_outputs,\n-            \"past_key_values\": past_key_values,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"head_mask\": head_mask,\n-            \"decoder_head_mask\": decoder_head_mask,\n-            \"cross_attn_head_mask\": cross_attn_head_mask,\n-            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n-        }\n-\n     @staticmethod\n     def _reorder_cache(past_key_values, beam_idx):\n         reordered_past = ()"
        },
        {
            "sha": "3655a188d839487ca2a527946940bc233fbc2d22",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 23,
            "deletions": 3,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -31,6 +31,7 @@\n from transformers.trainer_utils import set_seed\n from transformers.utils import cached_property\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n@@ -314,6 +315,15 @@ def get_config(self):\n             vocab_size=self.vocab_size,\n         )\n \n+    def get_subsampled_output_lengths(self, input_lengths):\n+        \"\"\"\n+        Computes the output length of the convolutional layers\n+        \"\"\"\n+        for stride in self.conv_stride:\n+            input_lengths = (input_lengths // stride) - 1\n+\n+        return input_lengths\n+\n     def create_and_check_model_forward(self, config, inputs_dict):\n         model = SpeechT5ForSpeechToText(config=config).to(torch_device).eval()\n \n@@ -359,10 +369,8 @@ def create_and_check_decoder_model_past_large_inputs(self, config, inputs_dict):\n \n \n @require_torch\n-class SpeechT5ForSpeechToTextTest(ModelTesterMixin, unittest.TestCase):\n+class SpeechT5ForSpeechToTextTest(ModelTesterMixin, unittest.TestCase, GenerationTesterMixin):\n     all_model_classes = (SpeechT5ForSpeechToText,) if is_torch_available() else ()\n-    # Doesn't run generation tests. TODO eustache/joao: shape checks probably need an update\n-    all_generative_model_classes = ()\n     is_encoder_decoder = True\n     test_pruning = False\n     test_headmasking = False\n@@ -727,6 +735,18 @@ def _mock_init_weights(self, module):\n         if hasattr(module, \"masked_spec_embed\") and module.masked_spec_embed is not None:\n             module.masked_spec_embed.data.fill_(3)\n \n+    @unittest.skip(reason=\"Temporarily broken\")  # TODO (joao, eustache): have a look at this test\n+    def test_generate_with_head_masking(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Temporarily broken\")  # TODO (joao, eustache): have a look at this test\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Very flaky\")  # TODO (joao, eustache): have a look at this test\n+    def test_generate_continue_from_past_key_values(self):\n+        pass\n+\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "b75d1187086e454eb5a5acbf657f2af4899f1294",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/4321b0648c4d80f8d0e3b02d555521c7e1483f46/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4321b0648c4d80f8d0e3b02d555521c7e1483f46/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=4321b0648c4d80f8d0e3b02d555521c7e1483f46",
            "patch": "@@ -1720,16 +1720,16 @@ class DummyBertWithParent(DummyBertWithMixin):\n         self.assertTrue(\"\" == cl.out)\n         self.assertTrue(can_generate)\n \n-        # 4 - BC: models with a custom `prepare_inputs_for_generation` can generate (it was assumed they inherited\n-        # `GenerationMixin`)\n+        # 4 - Legacy: models with a custom `prepare_inputs_for_generation` can generate (it was assumed\n+        # they inherited `GenerationMixin`). Deprecated in v4.45 and removed in v4.51.\n         class DummyBertWithPrepareInputs(BertModel):\n             def prepare_inputs_for_generation(self):\n                 pass\n \n         with CaptureLogger(logger) as cl:\n             can_generate = DummyBertWithPrepareInputs.can_generate()\n         self.assertTrue(\"it doesn't directly inherit from `GenerationMixin`\" in cl.out)\n-        self.assertTrue(can_generate)\n+        self.assertFalse(can_generate)\n \n     def test_save_and_load_config_with_custom_generation(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 137,
        "additions": 54,
        "deletions": 83
    }
}