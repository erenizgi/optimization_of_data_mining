{
    "author": "justinchuby",
    "message": "Remove unnecessary slicing in sdpa_attention_forward (#41900)\n\nRemove redundant slicing in sdpa_attention_forward\n\nThe slicing in sdpa_attention_forward was there only because some masks were not constructed correctly (I was told). When the dimension is dynamic, the slice op also prevents torch.export from correctly reasoning about its size.\n\nSigned-off-by: Justin Chu <justinchuby@users.noreply.github.com>",
    "sha": "f40ef032145c36387e26b7acd22f9a6a1811aee1",
    "files": [
        {
            "sha": "e7b7aadba3c72038b3373af745bc67ff1ff9fab3",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f40ef032145c36387e26b7acd22f9a6a1811aee1/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f40ef032145c36387e26b7acd22f9a6a1811aee1/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=f40ef032145c36387e26b7acd22f9a6a1811aee1",
            "patch": "@@ -63,9 +63,6 @@ def sdpa_attention_forward(\n         else:\n             sdpa_kwargs = {\"enable_gqa\": True}\n \n-    if attention_mask is not None and attention_mask.ndim == 4:\n-        attention_mask = attention_mask[:, :, :, : key.shape[-2]]\n-\n     # Instead of relying on the value set in the module directly, we use the is_causal passed in kwargs if it is presented\n     is_causal = is_causal if is_causal is not None else getattr(module, \"is_causal\", True)\n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 0,
        "deletions": 3
    }
}