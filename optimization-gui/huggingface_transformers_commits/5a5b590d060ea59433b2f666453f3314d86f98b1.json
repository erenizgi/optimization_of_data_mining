{
    "author": "zucchini-nlp",
    "message": "BLIP: fix input expansion logic (#34225)\n\nfix",
    "sha": "5a5b590d060ea59433b2f666453f3314d86f98b1",
    "files": [
        {
            "sha": "fa6a99f71a46166f9ac91364ba3cb40e2b759745",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=5a5b590d060ea59433b2f666453f3314d86f98b1",
            "patch": "@@ -137,7 +137,9 @@ def __call__(\n             # because BLIP expects image tokens to be at the beginning even before BOS token\n             if self.num_query_tokens is not None:\n                 image_tokens = self.image_token.content * self.num_query_tokens\n-                image_token_encoding = self.tokenizer([image_tokens], add_special_tokens=False, return_tensors=None)\n+                image_token_encoding = self.tokenizer(\n+                    [image_tokens] * len(text), add_special_tokens=False, return_tensors=None\n+                )\n                 for k in _text_encoding:\n                     text_encoding[k] = [\n                         img_encoding + txt_encoding"
        },
        {
            "sha": "05ff9871f4d731248fe4c5a4562faced3bca19b9",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=5a5b590d060ea59433b2f666453f3314d86f98b1",
            "patch": "@@ -131,7 +131,9 @@ def __call__(\n             if self.num_query_tokens is not None and images is not None:\n                 text_encoding = {}\n                 image_tokens = self.image_token.content * self.num_query_tokens\n-                image_token_encoding = self.tokenizer([image_tokens], add_special_tokens=False, return_tensors=None)\n+                image_token_encoding = self.tokenizer(\n+                    [image_tokens] * len(text), add_special_tokens=False, return_tensors=None\n+                )\n                 for k in _text_encoding:\n                     text_encoding[k] = [\n                         img_encoding + txt_encoding"
        },
        {
            "sha": "3e96d279a42f8dc4dbde76c2059862d1d49d1269",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a5b590d060ea59433b2f666453f3314d86f98b1/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=5a5b590d060ea59433b2f666453f3314d86f98b1",
            "patch": "@@ -131,7 +131,9 @@ def __call__(\n                 video_tokens = (\n                     self.video_token.content * self.num_query_tokens * 4\n                 )  # InstrucBLIP works with 4 frames only\n-                video_token_encoding = self.tokenizer([video_tokens], add_special_tokens=False, return_tensors=None)\n+                video_token_encoding = self.tokenizer(\n+                    [video_tokens] * len(text), add_special_tokens=False, return_tensors=None\n+                )\n                 for k in _text_encoding:\n                     text_encoding[k] = [\n                         img_encoding + txt_encoding"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}