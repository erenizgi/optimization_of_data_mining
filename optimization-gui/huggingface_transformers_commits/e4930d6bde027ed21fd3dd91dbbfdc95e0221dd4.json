{
    "author": "cyyever",
    "message": "ðŸš¨ [V5] Remove deprecated `resume_download` (#41122)\n\nRemove deprecated `resume_download`\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
    "files": [
        {
            "sha": "6957246f304ae6f32ef43d9c1ee8d66dfe8ccd9a",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -549,9 +549,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the configuration files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -674,7 +671,6 @@ def _get_config_dict(\n     ) -> tuple[dict[str, Any], dict[str, Any]]:\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n@@ -718,7 +714,6 @@ def _get_config_dict(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,"
        },
        {
            "sha": "677aa41712dd22099949979005da866fea9108c9",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -318,7 +318,6 @@ def get_cached_module_file(\n     module_file: str,\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -348,9 +347,6 @@ def get_cached_module_file(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -409,7 +405,6 @@ def get_cached_module_file(\n             cache_dir=cache_dir,\n             force_download=force_download,\n             proxies=proxies,\n-            resume_download=resume_download,\n             local_files_only=local_files_only,\n             token=token,\n             revision=revision,\n@@ -469,7 +464,6 @@ def get_cached_module_file(\n                     f\"{Path(module_file).parent / module_needed}.py\",\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n-                    resume_download=resume_download,\n                     proxies=proxies,\n                     token=token,\n                     revision=revision,\n@@ -496,7 +490,6 @@ def get_class_from_dynamic_module(\n     pretrained_model_name_or_path: Union[str, os.PathLike],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -539,9 +532,6 @@ def get_class_from_dynamic_module(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -606,7 +596,6 @@ def get_class_from_dynamic_module(\n         module_file + \".py\",\n         cache_dir=cache_dir,\n         force_download=force_download,\n-        resume_download=resume_download,\n         proxies=proxies,\n         token=token,\n         revision=code_revision,"
        },
        {
            "sha": "f1da1f5520755a8750741f8e76098cfd7e06d426",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -275,9 +275,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the feature extractor files and override the cached versions\n                 if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -433,7 +430,6 @@ def get_feature_extractor_dict(\n         \"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         subfolder = kwargs.pop(\"subfolder\", None)\n         token = kwargs.pop(\"token\", None)\n@@ -487,7 +483,6 @@ def get_feature_extractor_dict(\n                             cache_dir=cache_dir,\n                             force_download=force_download,\n                             proxies=proxies,\n-                            resume_download=resume_download,\n                             local_files_only=local_files_only,\n                             subfolder=subfolder,\n                             token=token,"
        },
        {
            "sha": "be7bee4d3ac4f7e5d7ee3e081731f834cc287062",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -796,9 +796,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the configuration files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -862,7 +859,6 @@ def from_pretrained(\n         ```\"\"\"\n         config_file_name = config_file_name if config_file_name is not None else GENERATION_CONFIG_NAME\n \n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         subfolder = kwargs.pop(\"subfolder\", \"\")\n@@ -906,7 +902,6 @@ def from_pretrained(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,"
        },
        {
            "sha": "8124a0af6f65e0d1fb62ff754f128c1344dffee7",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -119,9 +119,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the image processor files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -285,7 +282,6 @@ def get_image_processor_dict(\n         \"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -340,7 +336,6 @@ def get_image_processor_dict(\n                             cache_dir=cache_dir,\n                             force_download=force_download,\n                             proxies=proxies,\n-                            resume_download=resume_download,\n                             local_files_only=local_files_only,\n                             token=token,\n                             user_agent=user_agent,"
        },
        {
            "sha": "3f8c14aef5e5dc4a788c527d31f248b4f68ebc00",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -4503,7 +4503,6 @@ def from_pretrained(\n             tp_plan = \"auto\"\n \n         # Not used anymore -- remove them from the kwargs\n-        _ = kwargs.pop(\"resume_download\", None)\n         _ = kwargs.pop(\"mirror\", None)\n         _ = kwargs.pop(\"_fast_init\", None)\n         _ = kwargs.pop(\"low_cpu_mem_usage\", None)"
        },
        {
            "sha": "f91a4c5981a1636d43d0a59f2536c6b63ee2a6f9",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -126,9 +126,6 @@\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                 cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n@@ -260,7 +257,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             \"force_download\",\n             \"local_files_only\",\n             \"proxies\",\n-            \"resume_download\",\n             \"revision\",\n             \"subfolder\",\n             \"use_auth_token\","
        },
        {
            "sha": "c641645d5656e233302cc4113e8f7dc45fdb8d4f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -1255,9 +1255,6 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download the model weights and configuration files and override the\n                 cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request."
        },
        {
            "sha": "0d3dab2e8fd862bc133944e28a8ed4c0991a012d",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -107,7 +107,6 @@ def get_feature_extractor_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -132,9 +131,6 @@ def get_feature_extractor_config(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -187,7 +183,6 @@ def get_feature_extractor_config(\n         FEATURE_EXTRACTOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n-        resume_download=resume_download,\n         proxies=proxies,\n         token=token,\n         revision=revision,\n@@ -249,9 +244,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the feature extractor files and override the cached versions\n                 if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request."
        },
        {
            "sha": "e07aa3ee7d22b9ffece59c861b4eebb82eadd356",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -244,7 +244,6 @@ def get_image_processor_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -269,9 +268,6 @@ def get_image_processor_config(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -324,7 +320,6 @@ def get_image_processor_config(\n         IMAGE_PROCESSOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n-        resume_download=resume_download,\n         proxies=proxies,\n         token=token,\n         revision=revision,\n@@ -394,9 +389,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the image processor files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request."
        },
        {
            "sha": "46254c26484da82ca2c2bbf53482207aaae3fb68",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -224,9 +224,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the feature extractor files and override the cached versions\n                 if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request."
        },
        {
            "sha": "2de3cda52f96131c347eb0b1590a6fbd475c1f01",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -822,7 +822,6 @@ def get_tokenizer_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike[str]],\n     cache_dir: Optional[Union[str, os.PathLike[str]]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -848,9 +847,6 @@ def get_tokenizer_config(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -907,7 +903,6 @@ def get_tokenizer_config(\n         TOKENIZER_CONFIG_FILE,\n         cache_dir=cache_dir,\n         force_download=force_download,\n-        resume_download=resume_download,\n         proxies=proxies,\n         token=token,\n         revision=revision,\n@@ -975,9 +970,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download the model weights and configuration files and override the\n                 cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request."
        },
        {
            "sha": "9671c6195fa8256f99a563830109ad032cc25b1a",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -106,7 +106,6 @@ def get_video_processor_config(\n     pretrained_model_name_or_path: Union[str, os.PathLike],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -131,9 +130,6 @@ def get_video_processor_config(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -186,7 +182,6 @@ def get_video_processor_config(\n         VIDEO_PROCESSOR_NAME,\n         cache_dir=cache_dir,\n         force_download=force_download,\n-        resume_download=resume_download,\n         proxies=proxies,\n         token=token,\n         revision=revision,\n@@ -246,9 +241,6 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the video processor files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request."
        },
        {
            "sha": "0602986483a6a51def23b9a14d1c1c3b6e2df1f3",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -94,7 +94,6 @@ def from_pretrained(\n                 cache_dir=kwargs.pop(\"cache_dir\", None),\n                 force_download=kwargs.pop(\"force_download\", False),\n                 proxies=kwargs.pop(\"proxies\", None),\n-                resume_download=kwargs.pop(\"resume_download\", None),\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n                 token=kwargs.pop(\"use_auth_token\", None),\n                 revision=kwargs.pop(\"revision\", None),\n@@ -195,7 +194,6 @@ def _load_voice_preset(self, voice_preset: Optional[str] = None, **kwargs):\n                 cache_dir=kwargs.pop(\"cache_dir\", None),\n                 force_download=kwargs.pop(\"force_download\", False),\n                 proxies=kwargs.pop(\"proxies\", None),\n-                resume_download=kwargs.pop(\"resume_download\", None),\n                 local_files_only=kwargs.pop(\"local_files_only\", False),\n                 token=kwargs.pop(\"use_auth_token\", None),\n                 revision=kwargs.pop(\"revision\", None),"
        },
        {
            "sha": "c8b99164730e0238379292b4ba93049f30246ee6",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -3768,7 +3768,6 @@ def from_pretrained(\n             cache_dir=kwargs.pop(\"cache_dir\", None),\n             force_download=kwargs.pop(\"force_download\", False),\n             proxies=kwargs.pop(\"proxies\", None),\n-            resume_download=kwargs.pop(\"resume_download\", None),\n             local_files_only=kwargs.pop(\"local_files_only\", False),\n             token=kwargs.pop(\"use_auth_token\", None),\n             revision=kwargs.pop(\"revision\", None),"
        },
        {
            "sha": "ee73836f8abe29a0a331ce75f9bbc0eb0aa64456",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -4068,7 +4068,6 @@ def from_pretrained(\n             cache_dir=kwargs.pop(\"cache_dir\", None),\n             force_download=kwargs.pop(\"force_download\", False),\n             proxies=kwargs.pop(\"proxies\", None),\n-            resume_download=kwargs.pop(\"resume_download\", None),\n             local_files_only=kwargs.pop(\"local_files_only\", False),\n             token=kwargs.pop(\"use_auth_token\", None),\n             revision=kwargs.pop(\"revision\", None),"
        },
        {
            "sha": "c3fcfd3b8fb5641038d3187c9863b0876a3c5d9b",
            "filename": "src/transformers/models/wav2vec2/modeling_wav2vec2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_wav2vec2.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -1148,9 +1148,6 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n                 cached versions if they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n@@ -1204,7 +1201,6 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n \n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n         token = kwargs.pop(\"token\", None)\n@@ -1235,7 +1231,6 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                     model_path_or_id,\n                     filename=filepath,\n                     force_download=force_download,\n-                    resume_download=resume_download,\n                     proxies=proxies,\n                     local_files_only=local_files_only,\n                     token=token,\n@@ -1270,7 +1265,6 @@ def load_adapter(self, target_lang: str, force_load=True, **kwargs):\n                     model_path_or_id,\n                     filename=filepath,\n                     force_download=force_download,\n-                    resume_download=resume_download,\n                     proxies=proxies,\n                     local_files_only=local_files_only,\n                     token=token,"
        },
        {
            "sha": "952bc65ce70612bacd02fc4efdc5ad16daaadf82",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -908,7 +908,6 @@ def get_processor_dict(\n \n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n         local_files_only = kwargs.pop(\"local_files_only\", False)\n@@ -976,7 +975,6 @@ def get_processor_dict(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,\n@@ -993,7 +991,6 @@ def get_processor_dict(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,\n@@ -1008,7 +1005,6 @@ def get_processor_dict(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,\n@@ -1024,7 +1020,6 @@ def get_processor_dict(\n                         cache_dir=cache_dir,\n                         force_download=force_download,\n                         proxies=proxies,\n-                        resume_download=resume_download,\n                         local_files_only=local_files_only,\n                         token=token,\n                         user_agent=user_agent,\n@@ -1041,7 +1036,6 @@ def get_processor_dict(\n                     cache_dir=cache_dir,\n                     force_download=force_download,\n                     proxies=proxies,\n-                    resume_download=resume_download,\n                     local_files_only=local_files_only,\n                     token=token,\n                     user_agent=user_agent,"
        },
        {
            "sha": "ac72c6617d7ba5cc15f407d6bbfa3f44fed8d038",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -1826,9 +1826,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force the (re-)download the vocabulary files and override the cached versions if they\n                 exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n@@ -1883,7 +1880,6 @@ def from_pretrained(\n         # Otherwise use tokenizer.add_special_tokens({'unk_token': '<unk>'}) instead)\n         assert tokenizer.unk_token == \"<unk>\"\n         ```\"\"\"\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n         subfolder = kwargs.pop(\"subfolder\", None)\n@@ -1957,7 +1953,6 @@ def from_pretrained(\n                             TOKENIZER_CONFIG_FILE,\n                             cache_dir=cache_dir,\n                             force_download=force_download,\n-                            resume_download=resume_download,\n                             proxies=proxies,\n                             token=token,\n                             revision=revision,\n@@ -2025,7 +2020,6 @@ def from_pretrained(\n                         cache_dir=cache_dir,\n                         force_download=force_download,\n                         proxies=proxies,\n-                        resume_download=resume_download,\n                         local_files_only=local_files_only,\n                         token=token,\n                         user_agent=user_agent,"
        },
        {
            "sha": "8ebd369c30099aa1323d5ae1ed4e371e32852cd9",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -277,9 +277,6 @@ def cached_file(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -324,7 +321,6 @@ def cached_files(\n     filenames: list[str],\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -354,9 +350,6 @@ def cached_files(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -481,7 +474,6 @@ def cached_files(\n                 user_agent=user_agent,\n                 force_download=force_download,\n                 proxies=proxies,\n-                resume_download=resume_download,\n                 token=token,\n                 local_files_only=local_files_only,\n             )\n@@ -495,7 +487,6 @@ def cached_files(\n                 user_agent=user_agent,\n                 force_download=force_download,\n                 proxies=proxies,\n-                resume_download=resume_download,\n                 token=token,\n                 local_files_only=local_files_only,\n             )\n@@ -1023,7 +1014,6 @@ def get_checkpoint_shard_files(\n     cache_dir=None,\n     force_download=False,\n     proxies=None,\n-    resume_download=None,\n     local_files_only=False,\n     token=None,\n     user_agent=None,\n@@ -1077,7 +1067,6 @@ def get_checkpoint_shard_files(\n         cache_dir=cache_dir,\n         force_download=force_download,\n         proxies=proxies,\n-        resume_download=resume_download,\n         local_files_only=local_files_only,\n         token=token,\n         user_agent=user_agent,"
        },
        {
            "sha": "d4339057823adc5857f4d6cfbedb24dc8bba569f",
            "filename": "src/transformers/utils/peft_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Futils%2Fpeft_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fpeft_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -30,7 +30,6 @@ def find_adapter_config_file(\n     model_id: str,\n     cache_dir: Optional[Union[str, os.PathLike]] = None,\n     force_download: bool = False,\n-    resume_download: Optional[bool] = None,\n     proxies: Optional[dict[str, str]] = None,\n     token: Optional[Union[bool, str]] = None,\n     revision: Optional[str] = None,\n@@ -51,9 +50,6 @@ def find_adapter_config_file(\n         force_download (`bool`, *optional*, defaults to `False`):\n             Whether or not to force to (re-)download the configuration files and override the cached versions if they\n             exist.\n-        resume_download:\n-            Deprecated and ignored. All downloads are now resumed by default when possible.\n-            Will be removed in v5 of Transformers.\n         proxies (`dict[str, str]`, *optional*):\n             A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n             'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -90,7 +86,6 @@ def find_adapter_config_file(\n             ADAPTER_CONFIG_NAME,\n             cache_dir=cache_dir,\n             force_download=force_download,\n-            resume_download=resume_download,\n             proxies=proxies,\n             token=token,\n             revision=revision,"
        },
        {
            "sha": "117c3097228893cbd89d1a997663008ee284be7b",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=e4930d6bde027ed21fd3dd91dbbfdc95e0221dd4",
            "patch": "@@ -466,9 +466,6 @@ def from_pretrained(\n             force_download (`bool`, *optional*, defaults to `False`):\n                 Whether or not to force to (re-)download the video processor files and override the cached versions if\n                 they exist.\n-            resume_download:\n-                Deprecated and ignored. All downloads are now resumed by default when possible.\n-                Will be removed in v5 of Transformers.\n             proxies (`dict[str, str]`, *optional*):\n                 A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n                 'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n@@ -630,7 +627,6 @@ def get_video_processor_dict(\n         \"\"\"\n         cache_dir = kwargs.pop(\"cache_dir\", None)\n         force_download = kwargs.pop(\"force_download\", False)\n-        resume_download = kwargs.pop(\"resume_download\", None)\n         proxies = kwargs.pop(\"proxies\", None)\n         token = kwargs.pop(\"token\", None)\n         use_auth_token = kwargs.pop(\"use_auth_token\", None)\n@@ -683,7 +679,6 @@ def get_video_processor_dict(\n                             cache_dir=cache_dir,\n                             force_download=force_download,\n                             proxies=proxies,\n-                            resume_download=resume_download,\n                             local_files_only=local_files_only,\n                             token=token,\n                             user_agent=user_agent,"
        }
    ],
    "stats": {
        "total": 117,
        "additions": 0,
        "deletions": 117
    }
}