{
    "author": "MekkCyber",
    "message": "Fixes hqq by following a new path for bias parameter in pre_quantized models (#37530)\n\n* fix\n\n* add test",
    "sha": "7752e7487c3ae675ea79f54cbb056f8e90446d59",
    "files": [
        {
            "sha": "38d8a15cbfcf2c6a2ca031b0cc088b00f1727998",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7752e7487c3ae675ea79f54cbb056f8e90446d59/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7752e7487c3ae675ea79f54cbb056f8e90446d59/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=7752e7487c3ae675ea79f54cbb056f8e90446d59",
            "patch": "@@ -170,16 +170,13 @@ def check_quantized_param(\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         if self.pre_quantized:\n-            return (\n-                (isinstance(module, torch.nn.Linear) or isinstance(module, HQQLinear))\n-                and tensor_name != \"weight\"\n-                and tensor_name != \"bias\"\n-            )\n+            return (isinstance(module, torch.nn.Linear) or isinstance(module, HQQLinear)) and tensor_name != \"weight\"\n         else:\n-            # we need a special path for bias since hqq overwrote load_state_dict for this layer\n             return (\n                 isinstance(module, torch.nn.Linear)\n                 and tensor_name == \"weight\"\n+                # bias doesn't need to be quantized, we use this as a workaround to avoid loading bias into HQQLinear assuming it was loaded\n+                # in the state_dict directly with the weight because hqq overwrote load_state_dict for this layer\n                 or (isinstance(module, HQQLinear) and tensor_name == \"bias\")\n             )\n "
        },
        {
            "sha": "a686bbd7de7e9a0167074aa0503e3f20dad07851",
            "filename": "tests/quantization/hqq/test_hqq.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/7752e7487c3ae675ea79f54cbb056f8e90446d59/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7752e7487c3ae675ea79f54cbb056f8e90446d59/tests%2Fquantization%2Fhqq%2Ftest_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhqq%2Ftest_hqq.py?ref=7752e7487c3ae675ea79f54cbb056f8e90446d59",
            "patch": "@@ -165,6 +165,39 @@ def test_fp16_quantized_model(self):\n         check_hqqlayer(self, hqq_runner.model.model.decoder.layers[0].self_attn.v_proj)\n         check_forward(self, hqq_runner.model)\n \n+    def test_save_and_load_quantized_model(self):\n+        \"\"\"\n+        Test saving and loading a quantized model with bias\n+        \"\"\"\n+        import tempfile\n+\n+        quant_config = HqqConfig(nbits=8, group_size=64)\n+\n+        hqq_runner = HQQLLMRunner(\n+            model_id=\"facebook/opt-125m\", quant_config=quant_config, compute_dtype=torch.float16, device=torch_device\n+        )\n+\n+        input_tensor = torch.zeros((1, 8), dtype=torch.int32, device=torch_device)\n+\n+        # Get reference logits\n+        with torch.no_grad():\n+            logits_ref = hqq_runner.model.forward(input_tensor).logits\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            hqq_runner.model.save_pretrained(tmpdirname)\n+\n+            del hqq_runner.model\n+            torch.cuda.empty_cache()\n+\n+            model_loaded = AutoModelForCausalLM.from_pretrained(\n+                tmpdirname, torch_dtype=torch.float16, device_map=torch_device\n+            )\n+\n+            with torch.no_grad():\n+                logits_loaded = model_loaded.forward(input_tensor).logits\n+\n+            self.assertEqual((logits_loaded - logits_ref).abs().mean().item(), 0)\n+\n \n @slow\n @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 36,
        "deletions": 6
    }
}