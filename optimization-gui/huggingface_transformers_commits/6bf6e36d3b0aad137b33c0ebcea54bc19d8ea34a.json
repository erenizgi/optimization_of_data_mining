{
    "author": "ydshieh",
    "message": "[testing] update `test_longcat_generation_cpu` (#41368)\n\n* fix\n\n* Update tests/models/longcat_flash/test_modeling_longcat_flash.py\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "6bf6e36d3b0aad137b33c0ebcea54bc19d8ea34a",
    "files": [
        {
            "sha": "2b757093127362350bcad2059975904b94fe7966",
            "filename": "tests/models/longcat_flash/test_modeling_longcat_flash.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6bf6e36d3b0aad137b33c0ebcea54bc19d8ea34a/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6bf6e36d3b0aad137b33c0ebcea54bc19d8ea34a/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongcat_flash%2Ftest_modeling_longcat_flash.py?ref=6bf6e36d3b0aad137b33c0ebcea54bc19d8ea34a",
            "patch": "@@ -435,16 +435,16 @@ def test_shortcat_generation(self):\n     @require_large_cpu_ram\n     def test_longcat_generation_cpu(self):\n         # takes absolutely forever and a lot RAM, but allows to test the output in the CI\n-        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"cpu\", dtype=torch.bfloat16)\n+        model = LongcatFlashForCausalLM.from_pretrained(self.model_id, device_map=\"auto\", dtype=torch.bfloat16)\n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n \n         chat = [{\"role\": \"user\", \"content\": \"Paris is...\"}]\n         inputs = tokenizer.apply_chat_template(chat, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\")\n \n         with torch.no_grad():\n-            outputs = model.generate(inputs, max_new_tokens=10, do_sample=False)\n+            outputs = model.generate(inputs, max_new_tokens=3, do_sample=False)\n \n         response = tokenizer.batch_decode(outputs, skip_special_tokens=False)[0]\n-        expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is... a city of timeless charm, where\"\n+        expected_output = \"[Round 0] USER:Paris is... ASSISTANT:Paris is...\"\n \n         self.assertEqual(response, expected_output)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}