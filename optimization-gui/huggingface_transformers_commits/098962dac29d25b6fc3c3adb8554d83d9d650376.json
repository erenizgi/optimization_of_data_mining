{
    "author": "zucchini-nlp",
    "message": "BLIP: fix generation after hub update (#34876)\n\n* fix blip generation\r\n\r\n* dont remove it yet\r\n\r\n* Update src/transformers/models/blip_2/modeling_blip_2.py\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\r\n\r\n* address comments\r\n\r\n* modular\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "098962dac29d25b6fc3c3adb8554d83d9d650376",
    "files": [
        {
            "sha": "16b26ade7a62f96d0577bee42dda82a53c6e6fc9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -421,7 +421,12 @@ def prepare_inputs_for_generation(\n             model_input = kwargs.get(model_input_name)\n             if model_input is not None:\n                 if past_key_values is not None:\n-                    model_input = model_input[:, -input_ids.shape[1] :]\n+                    current_input_length = (\n+                        model_inputs[\"inputs_embeds\"].shape[1]\n+                        if model_inputs[\"inputs_embeds\"] is not None\n+                        else model_inputs[input_ids_key].shape[1]\n+                    )\n+                    model_input = model_input[:, -current_input_length:]\n                     model_input = model_input.clone(memory_format=torch.contiguous_format)\n                 model_inputs[model_input_name] = model_input\n "
        },
        {
            "sha": "2e32912421dc5bb93fbb2249d553fd7dfe20f3b7",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -2307,12 +2307,14 @@ def generate(\n         language_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n+\n         if input_ids is None:\n-            input_ids = (\n-                torch.LongTensor([[self.config.text_config.bos_token_id]])\n-                .repeat(batch_size, 1)\n-                .to(image_embeds.device)\n-            )\n+            start_tokens = [self.config.text_config.bos_token_id]\n+            if getattr(self.config, \"image_token_index\", None) is not None:\n+                start_tokens += [self.config.image_token_index] * self.config.num_query_tokens\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = input_ids.repeat(batch_size, 1)\n+\n         inputs_embeds = self.get_input_embeddings()(input_ids)\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)"
        },
        {
            "sha": "a63393ab1ddcc0791f4d597edfd26907dc395e84",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -1591,11 +1591,12 @@ def generate(\n         )\n \n         if input_ids is None:\n-            input_ids = (\n-                torch.LongTensor([[self.config.text_config.bos_token_id]])\n-                .repeat(batch_size, 1)\n-                .to(image_embeds.device)\n-            )\n+            start_tokens = [self.config.text_config.bos_token_id]\n+            if getattr(self.config, \"image_token_index\", None) is not None:\n+                start_tokens += [self.config.image_token_index] * self.config.num_query_tokens\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = input_ids.repeat(batch_size, 1)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n "
        },
        {
            "sha": "e922d1e3f26228778ed18ee2c00cb1bcf379ad01",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -1626,11 +1626,12 @@ def generate(\n         )\n \n         if input_ids is None:\n-            input_ids = (\n-                torch.LongTensor([[self.config.text_config.bos_token_id]])\n-                .repeat(batch_size, 1)\n-                .to(image_embeds.device)\n-            )\n+            start_tokens = [self.config.text_config.bos_token_id]\n+            if getattr(self.config, \"video_token_index\", None) is not None:\n+                start_tokens += [self.config.video_token_index] * self.config.num_query_tokens * 4\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = input_ids.repeat(batch_size, 1)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n "
        },
        {
            "sha": "126d81b6d3dcce95aa988f5bf013acf2f47a8e36",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -439,11 +439,12 @@ def generate(\n         )\n \n         if input_ids is None:\n-            input_ids = (\n-                torch.LongTensor([[self.config.text_config.bos_token_id]])\n-                .repeat(batch_size, 1)\n-                .to(image_embeds.device)\n-            )\n+            start_tokens = [self.config.text_config.bos_token_id]\n+            if getattr(self.config, \"video_token_index\", None) is not None:\n+                start_tokens += [self.config.video_token_index] * self.config.num_query_tokens * 4\n+            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+            input_ids = input_ids.repeat(batch_size, 1)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n "
        },
        {
            "sha": "a1ea708efd665bd35677d851b9c75142c0efd1c7",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -1994,8 +1994,8 @@ def test_inference_opt(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        print(predictions[0].tolist(), generated_text)\n-        self.assertEqual(predictions[0].tolist(), [2, 102, 693, 2828, 15, 5, 4105, 19, 10, 2335, 50118])\n+        expected_ids = [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 102, 693, 2828, 15, 5, 4105, 19, 10, 2335, 50118]  # fmt: skip\n+        self.assertEqual(predictions[0].tolist(), expected_ids)\n         self.assertEqual(\"a woman sitting on the beach with a dog\", generated_text)\n \n         # image and context\n@@ -2007,10 +2007,8 @@ def test_inference_opt(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(\n-            predictions[0].tolist(),\n-            [2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n-        )\n+        expected_ids = [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118]  # fmt: skip\n+        self.assertEqual(predictions[0].tolist(), expected_ids)\n         self.assertEqual(generated_text, \"Question: which city is this? Answer: it's not a city, it's a beach\")\n \n     def test_inference_interpolate_pos_encoding(self):\n@@ -2026,7 +2024,8 @@ def test_inference_interpolate_pos_encoding(self):\n         predictions = model.generate(**inputs, interpolate_pos_encoding=True)\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n-        self.assertEqual(predictions[0].tolist(), [2, 102, 693, 8, 2335, 15, 5, 4105, 50118])\n+        expected_ids = [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 102, 693, 8, 2335, 15, 5, 4105, 50118]  # fmt: skip\n+        self.assertEqual(predictions[0].tolist(), expected_ids)\n         self.assertEqual(generated_text, \"a woman and dog on the beach\")\n \n     def test_inference_opt_batched_beam_search(self):\n@@ -2042,8 +2041,9 @@ def test_inference_opt_batched_beam_search(self):\n         predictions = model.generate(**inputs, num_beams=2)\n \n         # Test output (in this case, slightly different from greedy search)\n-        self.assertEqual(predictions[0].tolist(), [2, 102, 693, 2828, 15, 5, 4105, 19, 69, 2335, 50118])\n-        self.assertEqual(predictions[1].tolist(), [2, 102, 693, 2828, 15, 5, 4105, 19, 69, 2335, 50118])\n+        expected_ids = [50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 50265, 2, 102, 693, 2828, 15, 5, 4105, 19, 69, 2335, 50118]  # fmt: skip\n+        self.assertEqual(predictions[0].tolist(), expected_ids)\n+        self.assertEqual(predictions[1].tolist(), expected_ids)\n \n     def test_inference_t5(self):\n         processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n@@ -2070,10 +2070,7 @@ def test_inference_t5(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n-        self.assertEqual(\n-            predictions[0].tolist(),\n-            [0, 3, 7, 152, 67, 839, 1],\n-        )\n+        self.assertEqual(predictions[0].tolist(), [0, 3, 7, 152, 67, 839, 1])\n         self.assertEqual(generated_text, \"san diego\")\n \n     def test_inference_t5_batched_beam_search(self):"
        },
        {
            "sha": "baacc12caa073d3dcee9360269e7ed489a0583d8",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/098962dac29d25b6fc3c3adb8554d83d9d650376/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/098962dac29d25b6fc3c3adb8554d83d9d650376/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=098962dac29d25b6fc3c3adb8554d83d9d650376",
            "patch": "@@ -945,7 +945,7 @@ def test_expansion_in_processing(self):\n         # Add args to the config to trigger new logic when inputs are expanded in processing file\n         processor.num_query_tokens = model.config.num_query_tokens\n         processor.tokenizer.add_special_tokens({\"additional_special_tokens\": [\"<image>\"]})\n-        model.config.image_token_index = len(processor.tokenizer) - 1\n+        model.config.image_token_index = len(processor.tokenizer) - 2\n         model.resize_token_embeddings(processor.tokenizer.vocab_size, pad_to_multiple_of=64)\n \n         # Generate again with new inputs"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 42,
        "deletions": 35
    }
}