{
    "author": "Cyrilvallez",
    "message": "Generic task-specific base classes (#39584)\n\n* first shot\n\n* Update modeling_layers.py\n\n* fix mro order\n\n* finalize llama\n\n* all modular and copied from from llama\n\n* fix",
    "sha": "019b74977d7469af3a86d3f5bbbb02ffe62af94d",
    "files": [
        {
            "sha": "16e7a7e05b5daea7d6c70b168b2ded80be77ff55",
            "filename": "src/transformers/modeling_layers.py",
            "status": "modified",
            "additions": 208,
            "deletions": 2,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodeling_layers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodeling_layers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_layers.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -11,12 +11,23 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n+from abc import ABC\n from functools import partial\n+from typing import Optional\n \n+import torch\n import torch.nn as nn\n \n-from transformers.utils import logging\n+from .cache_utils import Cache\n+from .modeling_outputs import (\n+    BaseModelOutputWithPast,\n+    QuestionAnsweringModelOutput,\n+    SequenceClassifierOutputWithPast,\n+    TokenClassifierOutput,\n+)\n+from .models.auto import AutoModel\n+from .processing_utils import Unpack\n+from .utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -81,3 +92,198 @@ def __call__(self, *args, **kwargs):\n \n             return self._gradient_checkpointing_func(partial(super().__call__, **kwargs), *args)\n         return super().__call__(*args, **kwargs)\n+\n+\n+@auto_docstring\n+class GenericForSequenceClassification(ABC):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        # Similar to `self.model = AutoModel.from_config(config)` but allows to change the base model name if needed in the child class\n+        setattr(self, self.base_model_prefix, AutoModel.from_config(config))\n+        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> SequenceClassifierOutputWithPast:\n+        transformer_outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        hidden_states = transformer_outputs.last_hidden_state\n+        logits = self.score(hidden_states)\n+\n+        if input_ids is not None:\n+            batch_size = input_ids.shape[0]\n+        else:\n+            batch_size = inputs_embeds.shape[0]\n+\n+        if self.config.pad_token_id is None and batch_size != 1:\n+            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n+        if self.config.pad_token_id is None:\n+            last_non_pad_token = -1\n+        elif input_ids is not None:\n+            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n+            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n+            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n+            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n+        else:\n+            last_non_pad_token = -1\n+            logger.warning_once(\n+                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n+                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n+            )\n+\n+        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n+\n+        return SequenceClassifierOutputWithPast(\n+            loss=loss,\n+            logits=pooled_logits,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class GenericForQuestionAnswering(ABC):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        # Similar to `self.model = AutoModel.from_config(config)` but allows to change the base model name if needed in the child class\n+        setattr(self, self.base_model_prefix, AutoModel.from_config(config))\n+        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return getattr(self, self.base_model_prefix).embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        getattr(self, self.base_model_prefix).embed_tokens = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        start_positions: Optional[torch.LongTensor] = None,\n+        end_positions: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> QuestionAnsweringModelOutput:\n+        outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            **kwargs,\n+        )\n+\n+        sequence_output = outputs.last_hidden_state\n+\n+        logits = self.qa_outputs(sequence_output)\n+        start_logits, end_logits = logits.split(1, dim=-1)\n+        start_logits = start_logits.squeeze(-1).contiguous()\n+        end_logits = end_logits.squeeze(-1).contiguous()\n+\n+        loss = None\n+        if start_positions is not None and end_positions is not None:\n+            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n+\n+        return QuestionAnsweringModelOutput(\n+            loss=loss,\n+            start_logits=start_logits,\n+            end_logits=end_logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+@auto_docstring\n+class GenericForTokenClassification(ABC):\n+    base_model_prefix = \"model\"\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        # Similar to `self.model = AutoModel.from_config(config)` but allows to change the base model name if needed in the child class\n+        setattr(self, self.base_model_prefix, AutoModel.from_config(config))\n+        if getattr(config, \"classifier_dropout\", None) is not None:\n+            classifier_dropout = config.classifier_dropout\n+        elif getattr(config, \"hidden_dropout\", None) is not None:\n+            classifier_dropout = config.hidden_dropout\n+        else:\n+            classifier_dropout = 0.1\n+        self.dropout = nn.Dropout(classifier_dropout)\n+        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs,\n+    ) -> TokenClassifierOutput:\n+        outputs: BaseModelOutputWithPast = getattr(self, self.base_model_prefix)(\n+            input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            **kwargs,\n+        )\n+        sequence_output = outputs.last_hidden_state\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.score(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits, labels, self.config)\n+\n+        return TokenClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "de6a10277180f4e372a30588ab35197936010ca1",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 13,
            "deletions": 209,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -24,21 +24,20 @@\n import torch\n from torch import nn\n \n-from transformers.utils import auto_docstring, logging\n+from transformers.utils import auto_docstring\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -47,9 +46,6 @@\n from .configuration_arcee import ArceeConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class ArceeMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -441,11 +437,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -492,205 +483,18 @@ def forward(\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n-class ArceeForSequenceClassification(ArceePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = ArceeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class ArceeForSequenceClassification(GenericForSequenceClassification, ArceePreTrainedModel):\n+    pass\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n-class ArceeForQuestionAnswering(ArceePreTrainedModel):\n-    base_model_prefix = \"transformer\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = ArceeModel(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class ArceeForQuestionAnswering(GenericForQuestionAnswering, ArceePreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n \n @auto_docstring(checkpoint=\"arcee-ai/AFM-4.5B\")\n-class ArceeForTokenClassification(ArceePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = ArceeModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class ArceeForTokenClassification(GenericForTokenClassification, ArceePreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "f3d6df532a63faf35c2893cee5ab547dafabbf09",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -816,11 +816,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "e2cbf27af962189df6d709f40812590a5dd71d03",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 101,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -31,19 +31,16 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_deepseek_v2 import DeepseekV2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class DeepseekV2MoEGate(nn.Module):\n     def __init__(self, config: DeepseekV2Config):\n         super().__init__()\n@@ -588,11 +585,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -638,96 +630,8 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The DeepseekV2 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`DeepseekV2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class DeepseekV2ForSequenceClassification(DeepseekV2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = DeepseekV2Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class DeepseekV2ForSequenceClassification(GenericForSequenceClassification, DeepseekV2PreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "98145e74ebebb255da17743db275b7ba4dabb267",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -629,11 +629,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "4660bfded73735acda96daede202318eace16e64",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 216,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -33,14 +33,13 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n@@ -745,219 +744,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The DiffLlama Model transformer with a sequence classification head on top (linear layer).\n+class DiffLlamaForSequenceClassification(GenericForSequenceClassification, DiffLlamaPreTrainedModel):\n+    pass\n \n-    [`DiffLlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n \n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class DiffLlamaForSequenceClassification(DiffLlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = DiffLlamaModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+class DiffLlamaForQuestionAnswering(GenericForQuestionAnswering, DiffLlamaPreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class DiffLlamaForQuestionAnswering(DiffLlamaPreTrainedModel):\n-    base_model_prefix = \"transformer\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = DiffLlamaModel(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class DiffLlamaForTokenClassification(DiffLlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = DiffLlamaModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class DiffLlamaForTokenClassification(GenericForTokenClassification, DiffLlamaPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "58a18655b216c4c29026ad28f7c2d937faea70fa",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 5,
            "deletions": 101,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -34,17 +34,12 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...integrations.flex_attention import compile_friendly_flex_attention\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    MoeCausalLMOutputWithPast,\n-    MoeModelOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-)\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_doge import DogeConfig\n \n@@ -53,9 +48,6 @@\n     from torch.nn.attention.flex_attention import BlockMask\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class DogeRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -814,96 +806,8 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Doge Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`DogeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class DogeForSequenceClassification(DogePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = DogeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class DogeForSequenceClassification(GenericForSequenceClassification, DogePreTrainedModel):\n+    pass\n \n \n __all__ = [\"DogeForCausalLM\", \"DogeModel\", \"DogePreTrainedModel\", \"DogeForSequenceClassification\"]"
        },
        {
            "sha": "e4b2e5429c7c39e763b0373609843c9f3debaf1f",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 161,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -28,24 +28,20 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_gemma import GemmaConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class GemmaRMSNorm(nn.Module):\n     def __init__(self, dim: int, eps: float = 1e-6):\n         super().__init__()\n@@ -492,159 +488,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Gemma Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`GemmaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class GemmaForSequenceClassification(GemmaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = GemmaModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+class GemmaForSequenceClassification(GenericForSequenceClassification, GemmaPreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class GemmaForTokenClassification(GemmaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = GemmaModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class GemmaForTokenClassification(GenericForTokenClassification, GemmaPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "459ae67fd449807322321149b654384809313212",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 157,
            "changes": 166,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -29,13 +29,12 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -583,159 +582,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Gemma2 Model transformer with a sequence classification head on top (linear layer).\n+class Gemma2ForSequenceClassification(GenericForSequenceClassification, Gemma2PreTrainedModel):\n+    pass\n \n-    [`Gemma2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n \n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Gemma2ForSequenceClassification(Gemma2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Gemma2Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Gemma2ForTokenClassification(Gemma2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Gemma2Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Gemma2ForTokenClassification(GenericForTokenClassification, Gemma2PreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "38d7c8cd0947519212e52a89274d38534633f962",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -565,17 +565,11 @@ def forward(\n \n \n class Gemma2ForSequenceClassification(GemmaForSequenceClassification):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = Gemma2Model(config)\n-        self.post_init()\n+    pass\n \n \n class Gemma2ForTokenClassification(GemmaForTokenClassification):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = Gemma2Model(config)\n-        self.post_init()\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "3b8b2d86c977233c83b4a10ecb579281a894710d",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 166,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -29,24 +29,20 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_glm import GlmConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class GlmMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -455,11 +451,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -505,159 +496,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Glm Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`GlmForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class GlmForSequenceClassification(GlmPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = GlmModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+class GlmForSequenceClassification(GenericForSequenceClassification, GlmPreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class GlmForTokenClassification(GlmPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = GlmModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class GlmForTokenClassification(GenericForTokenClassification, GlmPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "8888055c371028124ec4a086b28c5ecb0f965487",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 10,
            "deletions": 161,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -30,24 +30,20 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_glm4 import Glm4Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Glm4MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -509,159 +505,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Glm4 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Glm4ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Glm4ForSequenceClassification(Glm4PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Glm4Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Glm4ForTokenClassification(Glm4PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Glm4Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+class Glm4ForSequenceClassification(GenericForSequenceClassification, Glm4PreTrainedModel):\n+    pass\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Glm4ForTokenClassification(GenericForTokenClassification, Glm4PreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "93706258dc57dc076a54e2541b7d1fbe65971609",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -568,11 +568,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "58a9402bb6fa9d2b69424b554e4e79d07ea07306",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -509,11 +509,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "91c35ccf6c8f4a7c49f5e51cd727928625557c9d",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 11,
            "deletions": 162,
            "changes": 173,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -29,24 +29,20 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_helium import HeliumConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class HeliumRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n         super().__init__()\n@@ -409,7 +405,7 @@ class HeliumForCausalLM(HeliumPreTrainedModel, GenerationMixin):\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n-    def __init__(self, config: HeliumConfig):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.model = HeliumModel(config)\n         self.vocab_size = config.vocab_size\n@@ -485,159 +481,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Helium Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`HeliumForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class HeliumForSequenceClassification(HeliumPreTrainedModel):\n-    def __init__(self, config: HeliumConfig):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = HeliumModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+class HeliumForSequenceClassification(GenericForSequenceClassification, HeliumPreTrainedModel):\n+    pass\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class HeliumForTokenClassification(HeliumPreTrainedModel):\n-    def __init__(self, config: HeliumConfig):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = HeliumModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class HeliumForTokenClassification(GenericForTokenClassification, HeliumPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "0549c229546d32889a6fea5659a22246f66e493f",
            "filename": "src/transformers/models/helium/modular_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodular_helium.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -130,24 +130,15 @@ def __init__(self, config: HeliumConfig):\n \n \n class HeliumForCausalLM(GemmaForCausalLM):\n-    def __init__(self, config: HeliumConfig):\n-        super().__init__(config)\n-        self.model = HeliumModel(config)\n-        self.post_init()\n+    pass\n \n \n class HeliumForSequenceClassification(GemmaForSequenceClassification):\n-    def __init__(self, config: HeliumConfig):\n-        super().__init__(config)\n-        self.model = HeliumModel(config)\n-        self.post_init()\n+    pass\n \n \n class HeliumForTokenClassification(GemmaForTokenClassification):\n-    def __init__(self, config: HeliumConfig):\n-        super().__init__(config)\n-        self.model = HeliumModel(config)\n-        self.post_init()\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "44e1336907426ab22723a67d99c738149716df55",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 93,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -32,8 +32,11 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n@@ -1456,97 +1459,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Jamba Model with a sequence classification head on top (linear layer).\n-\n-    [`JambaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.mixtral.modeling_mixtral.MixtralForSequenceClassification with Mixtral->Jamba, MIXTRAL->JAMBA, BaseModelOutputWithPast->MoeModelOutputWithPast\n-class JambaForSequenceClassification(JambaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = JambaModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class JambaForSequenceClassification(GenericForSequenceClassification, JambaPreTrainedModel): ...\n \n \n __all__ = [\"JambaForCausalLM\", \"JambaForSequenceClassification\", \"JambaModel\", \"JambaPreTrainedModel\"]"
        },
        {
            "sha": "9eee14cb4adae3d3ef49b77b9bf867ac0714513a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 95,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -27,13 +27,14 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_jetmoe import JetMoeConfig\n \n \n@@ -1209,97 +1210,7 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The JetMoe Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`JetMoeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->JetMoe, LLAMA->JETMOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n-class JetMoeForSequenceClassification(JetMoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = JetMoeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class JetMoeForSequenceClassification(GenericForSequenceClassification, JetMoePreTrainedModel): ...\n \n \n __all__ = [\"JetMoeForCausalLM\", \"JetMoeModel\", \"JetMoePreTrainedModel\", \"JetMoeForSequenceClassification\"]"
        },
        {
            "sha": "548a62aa851437f156df412bc4445cd8f2312b61",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -708,11 +708,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "758f0ab61a84f38fa9ab48a62f829af3245f74b0",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 219,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -27,13 +27,15 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -439,11 +441,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -489,220 +486,14 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The LLaMa Model transformer with a sequence classification head on top (linear layer).\n+class LlamaForSequenceClassification(GenericForSequenceClassification, LlamaPreTrainedModel): ...\n \n-    [`LlamaForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n \n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class LlamaForSequenceClassification(LlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = LlamaModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+class LlamaForQuestionAnswering(GenericForQuestionAnswering, LlamaPreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class LlamaForQuestionAnswering(LlamaPreTrainedModel):\n-    base_model_prefix = \"transformer\"\n-\n-    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Llama\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = LlamaModel(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class LlamaForTokenClassification(LlamaPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = LlamaModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class LlamaForTokenClassification(GenericForTokenClassification, LlamaPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "4e6ce12c22b7aa45d0f930d00618b22135e05bdb",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 13,
            "deletions": 221,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -34,26 +34,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    MoeCausalLMOutputWithPast,\n-    MoeModelOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import OutputRecorder\n from .configuration_minimax import MiniMaxConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class MiniMaxRMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -917,219 +912,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The MiniMax Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`MiniMaxForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class MiniMaxForSequenceClassification(MiniMaxPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MiniMaxModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class MiniMaxForSequenceClassification(GenericForSequenceClassification, MiniMaxPreTrainedModel):\n+    pass\n \n \n-@auto_docstring\n-class MiniMaxForTokenClassification(MiniMaxPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MiniMaxModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+class MiniMaxForTokenClassification(GenericForTokenClassification, MiniMaxPreTrainedModel):\n+    pass\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class MiniMaxForQuestionAnswering(MiniMaxPreTrainedModel):\n-    base_model_prefix = \"model\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-        self.model = MiniMaxModel(config)  # diff with Llama: transformer->model\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class MiniMaxForQuestionAnswering(GenericForQuestionAnswering, MiniMaxPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "4981cdce5b0fde29e674a7ef7b8817280463ad89",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 12,
            "deletions": 225,
            "changes": 237,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -17,24 +17,20 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from .configuration_mistral import MistralConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MistralMLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -419,11 +415,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -469,219 +460,15 @@ def forward(\n         )\n \n \n-@auto_docstring\n-class MistralForTokenClassification(MistralPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MistralModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Mistral Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`MistralForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class MistralForSequenceClassification(MistralPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MistralModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n+class MistralForTokenClassification(GenericForTokenClassification, MistralPreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class MistralForSequenceClassification(GenericForSequenceClassification, MistralPreTrainedModel):\n+    pass\n \n \n-@auto_docstring\n-class MistralForQuestionAnswering(MistralPreTrainedModel):\n-    base_model_prefix = \"model\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-        self.model = MistralModel(config)  # diff with Llama: transformer->model\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class MistralForQuestionAnswering(GenericForQuestionAnswering, MistralPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "9ee1d51eec147e8da82605c41a220f0bc175d9d0",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 55,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -1,4 +1,4 @@\n-from typing import Callable, Optional, Union\n+from typing import Callable, Optional\n \n import torch\n from torch import nn\n@@ -8,15 +8,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import BaseModelOutputWithPast, QuestionAnsweringModelOutput\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+)\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n     LlamaForCausalLM,\n-    LlamaForQuestionAnswering,\n     LlamaForSequenceClassification,\n     LlamaForTokenClassification,\n     LlamaMLP,\n@@ -182,58 +184,7 @@ class MistralForSequenceClassification(LlamaForSequenceClassification):\n     pass\n \n \n-class MistralForQuestionAnswering(LlamaForQuestionAnswering):\n-    base_model_prefix = \"model\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = MistralModel(config)  # diff with Llama: transformer->model\n-        del self.transformer\n-\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class MistralForQuestionAnswering(GenericForQuestionAnswering, MistralPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "1b364e0c697dc5e88348a8d8ec25e05fa059e5e5",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 13,
            "deletions": 221,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -38,26 +38,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    MoeCausalLMOutputWithPast,\n-    MoeModelOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import OutputRecorder\n from .configuration_mixtral import MixtralConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class MixtralBlockSparseTop2MLP(nn.Module):\n     def __init__(self, config: MixtralConfig):\n         super().__init__()\n@@ -673,219 +668,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Mixtral Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`MixtralForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class MixtralForSequenceClassification(MixtralPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MixtralModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class MixtralForSequenceClassification(GenericForSequenceClassification, MixtralPreTrainedModel):\n+    pass\n \n \n-@auto_docstring\n-class MixtralForTokenClassification(MixtralPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = MixtralModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+class MixtralForTokenClassification(GenericForTokenClassification, MixtralPreTrainedModel):\n+    pass\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class MixtralForQuestionAnswering(MixtralPreTrainedModel):\n-    base_model_prefix = \"model\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-        self.model = MixtralModel(config)  # diff with Llama: transformer->model\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class MixtralForQuestionAnswering(GenericForQuestionAnswering, MixtralPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "c0885df8731b9417b9e596e3fe6221442d1c6c17",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 9,
            "deletions": 218,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -28,19 +28,19 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_nemotron import NemotronConfig\n \n \n@@ -944,223 +944,14 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Nemotron Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`NemotronForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n-class NemotronForSequenceClassification(NemotronPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = NemotronModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+class NemotronForSequenceClassification(GenericForSequenceClassification, NemotronPreTrainedModel): ...\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n \n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-# Copied from transformers.models.llama.modeling_llama.LlamaForQuestionAnswering with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n-class NemotronForQuestionAnswering(NemotronPreTrainedModel):\n+class NemotronForQuestionAnswering(GenericForQuestionAnswering, NemotronPreTrainedModel):\n     base_model_prefix = \"transformer\"\n \n-    # Copied from transformers.models.bloom.modeling_bloom.BloomForQuestionAnswering.__init__ with Bloom->Nemotron\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = NemotronModel(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n-class NemotronForTokenClassification(NemotronPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = NemotronModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class NemotronForTokenClassification(GenericForTokenClassification, NemotronPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "52b97bc64e6b231702a1a5e74352a299145f927b",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -418,11 +418,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "3ba2114dc453b0b4a96b9b26f566bf0111f5bf00",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -423,11 +423,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python"
        },
        {
            "sha": "92affb08eb6a0ba58de1742883a1c21731dd0b0d",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 157,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -30,18 +30,19 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_persimmon import PersimmonConfig\n \n \n@@ -760,161 +761,10 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Persimmon transformer with a sequence classification head on top (linear layer).\n-\n-    [`PersimmonForSequenceClassification`] uses the last token in order to do the classification, as other causal\n-    models (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->PERSIMMON,Llama->Persimmon\n-class PersimmonForSequenceClassification(PersimmonPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = PersimmonModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Persimmon, LLAMA->PERSIMMON\n-class PersimmonForTokenClassification(PersimmonPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = PersimmonModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n+class PersimmonForSequenceClassification(GenericForSequenceClassification, PersimmonPreTrainedModel): ...\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class PersimmonForTokenClassification(GenericForTokenClassification, PersimmonPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "5ee828dbf56ddbe488153dfb5f2946d75266fe1f",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 162,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -13,13 +13,12 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -462,11 +461,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -512,159 +506,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Phi Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`PhiForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class PhiForSequenceClassification(PhiPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = PhiModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class PhiForTokenClassification(PhiPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = PhiModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n+class PhiForSequenceClassification(GenericForSequenceClassification, PhiPreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n \n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class PhiForTokenClassification(GenericForTokenClassification, PhiPreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "67852e8fc9ba771d901a19172fcd00375b46dfdc",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 166,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -33,23 +33,19 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from .configuration_phi3 import Phi3Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Phi3MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -451,11 +447,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -540,159 +531,12 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Phi3 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Phi3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Phi3ForSequenceClassification(Phi3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Phi3Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Phi3ForTokenClassification(Phi3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Phi3Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n+class Phi3ForSequenceClassification(GenericForSequenceClassification, Phi3PreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n \n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Phi3ForTokenClassification(GenericForTokenClassification, Phi3PreTrainedModel):\n+    pass\n \n \n __all__ = ["
        },
        {
            "sha": "886c9a7d84eb87e211f708ff8814590d965934e8",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 93,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -27,13 +27,14 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_causal_attention_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast, SequenceClassifierOutputWithPast\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_phimoe import PhimoeConfig\n \n \n@@ -1350,95 +1351,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Phimoe Model transformer with a sequence classification head on top (linear layer).\n-    [`PhimoeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Phimoe, LLAMA->PHIMOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n-class PhimoeForSequenceClassification(PhimoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = PhimoeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n+class PhimoeForSequenceClassification(GenericForSequenceClassification, PhimoePreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "bbe8fe54cdcfd9883af639911bd6d0ae8207feed",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 225,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -15,25 +15,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_qwen2 import Qwen2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Qwen2MLP(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -435,11 +431,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -485,219 +476,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Qwen2 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Qwen2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Qwen2ForSequenceClassification(Qwen2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen2Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen2ForTokenClassification(Qwen2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen2Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen2ForQuestionAnswering(Qwen2PreTrainedModel):\n-    base_model_prefix = \"transformer\"\n+class Qwen2ForSequenceClassification(GenericForSequenceClassification, Qwen2PreTrainedModel):\n+    pass\n \n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = Qwen2Model(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n \n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n+class Qwen2ForTokenClassification(GenericForTokenClassification, Qwen2PreTrainedModel):\n+    pass\n \n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Qwen2ForQuestionAnswering(GenericForQuestionAnswering, Qwen2PreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n \n __all__ = ["
        },
        {
            "sha": "171ee4e657eee0eab1c7116ab071c905cb598654",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 211,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -32,19 +32,19 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n from ...modeling_outputs import (\n     MoeCausalLMOutputWithPast,\n     MoeModelOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_qwen2_moe import Qwen2MoeConfig\n \n \n@@ -1159,215 +1159,13 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Qwen2MoE Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Qwen2MoeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n+class Qwen2MoeForSequenceClassification(GenericForSequenceClassification, Qwen2MoePreTrainedModel): ...\n \n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n-class Qwen2MoeForSequenceClassification(Qwen2MoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen2MoeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n+class Qwen2MoeForTokenClassification(GenericForTokenClassification, Qwen2MoePreTrainedModel): ...\n \n-        transformer_outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n \n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->Qwen2Moe, LLAMA->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n-class Qwen2MoeForTokenClassification(Qwen2MoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen2MoeModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen2MoeForQuestionAnswering(Qwen2MoePreTrainedModel):\n-    base_model_prefix = \"model\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-        self.model = Qwen2MoeModel(config)  # diff with Llama: transformer->model\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, list[torch.FloatTensor]]] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: MoeModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Qwen2MoeForQuestionAnswering(GenericForQuestionAnswering, Qwen2MoePreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "aafaebf7cfb4d6ba785fa336a7c1d91014beac7b",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 220,
            "changes": 233,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -30,25 +30,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_qwen3 import Qwen3Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @use_kernel_forward_from_hub(\"RMSNorm\")\n class Qwen3RMSNorm(nn.Module):\n     def __init__(self, hidden_size, eps=1e-6):\n@@ -511,219 +507,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Qwen3 Model transformer with a sequence classification head on top (linear layer).\n+class Qwen3ForSequenceClassification(GenericForSequenceClassification, Qwen3PreTrainedModel):\n+    pass\n \n-    [`Qwen3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n \n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Qwen3ForSequenceClassification(Qwen3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen3Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n+class Qwen3ForTokenClassification(GenericForTokenClassification, Qwen3PreTrainedModel):\n+    pass\n \n-        # Initialize weights and apply final processing\n-        self.post_init()\n \n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen3ForTokenClassification(Qwen3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen3Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen3ForQuestionAnswering(Qwen3PreTrainedModel):\n-    base_model_prefix = \"transformer\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = Qwen3Model(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n-\n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n-\n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n-\n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Qwen3ForQuestionAnswering(GenericForQuestionAnswering, Qwen3PreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n \n __all__ = ["
        },
        {
            "sha": "230ac9a68c41d32f8f7a4d59535f8ab939e498c5",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 221,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -31,26 +31,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    MoeCausalLMOutputWithPast,\n-    MoeModelOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -696,219 +691,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Qwen3Moe Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Qwen3MoeForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Qwen3MoeForSequenceClassification(Qwen3MoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen3MoeModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen3MoeForTokenClassification(Qwen3MoePreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Qwen3MoeModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Qwen3MoeForQuestionAnswering(Qwen3MoePreTrainedModel):\n-    base_model_prefix = \"transformer\"\n-\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = Qwen3MoeModel(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n+class Qwen3MoeForSequenceClassification(GenericForSequenceClassification, Qwen3MoePreTrainedModel):\n+    pass\n \n-        sequence_output = outputs.last_hidden_state\n \n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n+class Qwen3MoeForTokenClassification(GenericForTokenClassification, Qwen3MoePreTrainedModel):\n+    pass\n \n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Qwen3MoeForQuestionAnswering(GenericForQuestionAnswering, Qwen3MoePreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n \n __all__ = ["
        },
        {
            "sha": "2af412933b87e72a9f9a5bcbf124329832b3a2d3",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 225,
            "changes": 238,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -30,25 +30,21 @@\n from ...integrations import use_kernel_forward_from_hub\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    QuestionAnsweringModelOutput,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ...utils.generic import check_model_inputs\n from .configuration_smollm3 import SmolLM3Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n@@ -465,11 +461,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -515,219 +506,16 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The SmolLM3 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`SmolLM3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class SmolLM3ForSequenceClassification(SmolLM3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = SmolLM3Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class SmolLM3ForTokenClassification(SmolLM3PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = SmolLM3Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n-\n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class SmolLM3ForQuestionAnswering(SmolLM3PreTrainedModel):\n-    base_model_prefix = \"transformer\"\n+class SmolLM3ForSequenceClassification(GenericForSequenceClassification, SmolLM3PreTrainedModel):\n+    pass\n \n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.transformer = SmolLM3Model(config)\n-        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def get_input_embeddings(self):\n-        return self.transformer.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.transformer.embed_tokens = value\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        start_positions: Optional[torch.LongTensor] = None,\n-        end_positions: Optional[torch.LongTensor] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> QuestionAnsweringModelOutput:\n-        outputs: BaseModelOutputWithPast = self.transformer(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            **kwargs,\n-        )\n-\n-        sequence_output = outputs.last_hidden_state\n \n-        logits = self.qa_outputs(sequence_output)\n-        start_logits, end_logits = logits.split(1, dim=-1)\n-        start_logits = start_logits.squeeze(-1).contiguous()\n-        end_logits = end_logits.squeeze(-1).contiguous()\n+class SmolLM3ForTokenClassification(GenericForTokenClassification, SmolLM3PreTrainedModel):\n+    pass\n \n-        loss = None\n-        if start_positions is not None and end_positions is not None:\n-            loss = self.loss_function(start_logits, end_logits, start_positions, end_positions, **kwargs)\n \n-        return QuestionAnsweringModelOutput(\n-            loss=loss,\n-            start_logits=start_logits,\n-            end_logits=end_logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class SmolLM3ForQuestionAnswering(GenericForQuestionAnswering, SmolLM3PreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n \n \n __all__ = ["
        },
        {
            "sha": "c728d9cdbc52be47376c141e38abffcb580bc5d9",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 158,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -31,18 +31,18 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import flash_attn_supports_top_left_mask, is_flash_attn_available\n-from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n )\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n-from ...utils.generic import TransformersKwargs\n from .configuration_stablelm import StableLmConfig\n \n \n@@ -988,161 +988,10 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The StableLm transformer with a sequence classification head on top (linear layer).\n-\n-    [`StableLmForSequenceClassification`] uses the last token in order to do the classification, as other causal\n-    models (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with LLAMA->STABLELM,Llama->StableLm\n-class StableLmForSequenceClassification(StableLmPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = StableLmModel(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-# Copied from transformers.models.llama.modeling_llama.LlamaForTokenClassification with Llama->StableLm, LLAMA->STABLELM\n-class StableLmForTokenClassification(StableLmPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = StableLmModel(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n+class StableLmForSequenceClassification(GenericForSequenceClassification, StableLmPreTrainedModel): ...\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n \n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class StableLmForTokenClassification(GenericForTokenClassification, StableLmPreTrainedModel): ...\n \n \n __all__ = ["
        },
        {
            "sha": "6f1114b31ad23ead60867e773d359346b1e16f70",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 166,
            "changes": 176,
            "blob_url": "https://github.com/huggingface/transformers/blob/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/019b74977d7469af3a86d3f5bbbb02ffe62af94d/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=019b74977d7469af3a86d3f5bbbb02ffe62af94d",
            "patch": "@@ -36,23 +36,19 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_layers import GradientCheckpointingLayer\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-    SequenceClassifierOutputWithPast,\n-    TokenClassifierOutput,\n+from ...modeling_layers import (\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n )\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from .configuration_starcoder2 import Starcoder2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n class Starcoder2MLP(nn.Module):\n     def __init__(self, config: Starcoder2Config):\n         super().__init__()\n@@ -428,11 +424,6 @@ def forward(\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> CausalLMOutputWithPast:\n         r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n         Example:\n \n         ```python\n@@ -478,159 +469,12 @@ def forward(\n         )\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    The Starcoder2 Model transformer with a sequence classification head on top (linear layer).\n-\n-    [`Starcoder2ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n-    (e.g. GPT-2) do.\n-\n-    Since it does classification on the last token, it requires to know the position of the last token. If a\n-    `pad_token_id` is defined in the configuration, it finds the last token that is not a padding token in each row. If\n-    no `pad_token_id` is defined, it simply takes the last value in each row of the batch. Since it cannot guess the\n-    padding tokens when `inputs_embeds` are passed instead of `input_ids`, it does the same (take the last value in\n-    each row of the batch).\n-    \"\"\"\n-)\n-class Starcoder2ForSequenceClassification(Starcoder2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Starcoder2Model(config)\n-        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs: Unpack[TransformersKwargs],\n-    ) -> SequenceClassifierOutputWithPast:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        transformer_outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        hidden_states = transformer_outputs.last_hidden_state\n-        logits = self.score(hidden_states)\n-\n-        if input_ids is not None:\n-            batch_size = input_ids.shape[0]\n-        else:\n-            batch_size = inputs_embeds.shape[0]\n-\n-        if self.config.pad_token_id is None and batch_size != 1:\n-            raise ValueError(\"Cannot handle batch sizes > 1 if no padding token is defined.\")\n-        if self.config.pad_token_id is None:\n-            last_non_pad_token = -1\n-        elif input_ids is not None:\n-            # To handle both left- and right- padding, we take the rightmost token that is not equal to pad_token_id\n-            non_pad_mask = (input_ids != self.config.pad_token_id).to(logits.device, torch.int32)\n-            token_indices = torch.arange(input_ids.shape[-1], device=logits.device, dtype=torch.int32)\n-            last_non_pad_token = (token_indices * non_pad_mask).argmax(-1)\n-        else:\n-            last_non_pad_token = -1\n-            logger.warning_once(\n-                f\"{self.__class__.__name__} will not detect padding tokens in `inputs_embeds`. Results may be \"\n-                \"unexpected if using padding tokens in conjunction with `inputs_embeds.`\"\n-            )\n-\n-        pooled_logits = logits[torch.arange(batch_size, device=logits.device), last_non_pad_token]\n-\n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n-\n-        return SequenceClassifierOutputWithPast(\n-            loss=loss,\n-            logits=pooled_logits,\n-            past_key_values=transformer_outputs.past_key_values,\n-            hidden_states=transformer_outputs.hidden_states,\n-            attentions=transformer_outputs.attentions,\n-        )\n-\n-\n-@auto_docstring\n-class Starcoder2ForTokenClassification(Starcoder2PreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.num_labels = config.num_labels\n-        self.model = Starcoder2Model(config)\n-        if getattr(config, \"classifier_dropout\", None) is not None:\n-            classifier_dropout = config.classifier_dropout\n-        elif getattr(config, \"hidden_dropout\", None) is not None:\n-            classifier_dropout = config.hidden_dropout\n-        else:\n-            classifier_dropout = 0.1\n-        self.dropout = nn.Dropout(classifier_dropout)\n-        self.score = nn.Linear(config.hidden_size, config.num_labels)\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @can_return_tuple\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Cache] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n-        use_cache: Optional[bool] = None,\n-        **kwargs,\n-    ) -> TokenClassifierOutput:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n-            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-\n-        outputs: BaseModelOutputWithPast = self.model(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            position_ids=position_ids,\n-            past_key_values=past_key_values,\n-            inputs_embeds=inputs_embeds,\n-            use_cache=use_cache,\n-            **kwargs,\n-        )\n-        sequence_output = outputs.last_hidden_state\n-        sequence_output = self.dropout(sequence_output)\n-        logits = self.score(sequence_output)\n+class Starcoder2ForSequenceClassification(GenericForSequenceClassification, Starcoder2PreTrainedModel):\n+    pass\n \n-        loss = None\n-        if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config)\n \n-        return TokenClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            hidden_states=outputs.hidden_states,\n-            attentions=outputs.attentions,\n-        )\n+class Starcoder2ForTokenClassification(GenericForTokenClassification, Starcoder2PreTrainedModel):\n+    pass\n \n \n __all__ = ["
        }
    ],
    "stats": {
        "total": 5325,
        "additions": 483,
        "deletions": 4842
    }
}