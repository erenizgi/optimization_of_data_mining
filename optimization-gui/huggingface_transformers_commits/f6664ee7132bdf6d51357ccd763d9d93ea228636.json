{
    "author": "uminaty",
    "message": "Add ALL_ATTENTION_FUNCTIONS compatibility for Pixtral model (#37960)\n\n* Add ALL_ATTENTION_FUNCTIONS compatibility for Pixtral model\n\n* Fix invalid operand type\n\n* Allow image_sizes to be optional in forward pass to fit tests\n\nDisallow using sdpa and output_attentions\n\n* Disallow using sdpa with output_attentions\n\n* Delete useless comments, use eager attention from smolvlm, use pattern from mistral\n\n* add _supports_attention_backend\n\n* use kwargs instead of position_ids\n\n---------\n\nCo-authored-by: aurelien.lac <aurelien.lac@lighton.ai>",
    "sha": "f6664ee7132bdf6d51357ccd763d9d93ea228636",
    "files": [
        {
            "sha": "57cb4f6591cdfe01fcd9f6358f905cd082595d0f",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 79,
            "deletions": 17,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6664ee7132bdf6d51357ccd763d9d93ea228636/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6664ee7132bdf6d51357ccd763d9d93ea228636/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=f6664ee7132bdf6d51357ccd763d9d93ea228636",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch Pixtral model.\"\"\"\n \n+from collections.abc import Callable\n from typing import Optional, Tuple, Union\n \n import torch\n@@ -22,13 +23,12 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_rope_utils import dynamic_rope_update\n-from ...utils import (\n-    add_start_docstrings,\n-    add_start_docstrings_to_model_forward,\n-    logging,\n-)\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_pixtral import PixtralVisionConfig\n \n \n@@ -132,8 +132,34 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n+# Copied from transformers.models.smolvlm.modeling_smolvlm.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class PixtralAttention(nn.Module):\n-    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+    \"\"\"\n+    Multi-headed attention compatible with ALL_ATTENTION_FUNCTIONS.\n+    \"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n@@ -142,6 +168,8 @@ def __init__(self, config):\n         self.num_heads = config.num_attention_heads\n         self.head_dim = self.embed_dim // self.num_heads\n \n+        self.is_causal = False\n+\n         self.scale = self.head_dim**-0.5\n         self.dropout = config.attention_dropout\n \n@@ -156,6 +184,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n         \"\"\"Input shape: Batch x Time x Channel\"\"\"\n \n@@ -172,17 +201,34 @@ def forward(\n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, unsqueeze_dim=0)\n \n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n-\n-        if attention_mask is not None:\n-            attn_weights = attn_weights + attention_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        # Since we use packing, if Flash-Attn 2 is selected we rely on position_ids\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            kwargs[\"position_ids\"] = kwargs[\"position_ids\"].to(hidden_states.device, non_blocking=True)\n+            attention_mask = None\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            is_causal=self.is_causal,\n+            output_attentions=output_attentions,\n+            **kwargs,\n+        )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n         attn_output = attn_output.reshape(batch_size, patches, -1)\n \n         attn_output = self.o_proj(attn_output)\n@@ -242,6 +288,7 @@ def forward(\n         attention_mask: torch.Tensor,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,\n         output_attentions: Optional[bool] = None,\n+        **kwargs,\n     ) -> Tuple[torch.FloatTensor]:\n         \"\"\"\n         Args:\n@@ -261,6 +308,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_embeddings=position_embeddings,\n             output_attentions=output_attentions,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n \n@@ -293,6 +341,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, BaseModelOutput]:\n         r\"\"\"\n         Args:\n@@ -334,13 +383,15 @@ def forward(\n                     attention_mask,\n                     position_embeddings,\n                     output_attentions,\n+                    **kwargs,\n                 )\n             else:\n                 layer_outputs = encoder_layer(\n                     hidden_states,\n                     attention_mask,\n                     position_embeddings=position_embeddings,\n                     output_attentions=output_attentions,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n@@ -381,6 +432,10 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PixtralAttentionLayer\"]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n@@ -457,7 +512,7 @@ def get_input_embeddings(self):\n     def forward(\n         self,\n         pixel_values: torch.Tensor,\n-        image_sizes: torch.Tensor,\n+        image_sizes: Optional[torch.Tensor] = None,\n         output_hidden_states: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -469,6 +524,10 @@ def forward(\n             pixel_values: tensor of token features for\n                 all tokens of all images of shape (N_toks, D)\n         \"\"\"\n+        if image_sizes is None:\n+            batch_size, _, height, width = pixel_values.shape\n+            image_sizes = [(height, width)] * batch_size\n+\n         # pass images through initial convolution independently\n         patch_embeds = self.patch_conv(pixel_values)\n         patch_embeds_list = [\n@@ -484,6 +543,8 @@ def forward(\n         position_ids = position_ids_in_meshgrid(\n             patch_embeds_list, max_width=self.config.image_size // self.config.patch_size\n         )\n+        kwargs[\"position_ids\"] = position_ids\n+\n         position_embeddings = self.patch_positional_embedding(patch_embeds, position_ids)\n \n         attention_mask = generate_block_attention_mask(\n@@ -497,6 +558,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             output_attentions=output_attentions,\n             return_dict=return_dict,\n+            **kwargs,\n         )\n \n         return out"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 79,
        "deletions": 17
    }
}