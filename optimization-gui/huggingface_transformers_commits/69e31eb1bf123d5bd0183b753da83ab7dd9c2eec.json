{
    "author": "jiqing-feng",
    "message": "change bnb tests (#34713)\n\n* fix training tests\n\n* fix xpu check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* rm pdb\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix 4bit logits check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix 4bit logits check\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* add xpu check on int8 training\n\n* fix training tests\n\n* add llama test on bnb\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* only cpu and xpu disable autocast training\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix format\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Titus <9048635+Titus-von-Koeller@users.noreply.github.com>",
    "sha": "69e31eb1bf123d5bd0183b753da83ab7dd9c2eec",
    "files": [
        {
            "sha": "c4287362b6bc1c0b1fb00b8b8efd8513b767bbd5",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 21,
            "deletions": 1,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e31eb1bf123d5bd0183b753da83ab7dd9c2eec/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e31eb1bf123d5bd0183b753da83ab7dd9c2eec/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=69e31eb1bf123d5bd0183b753da83ab7dd9c2eec",
            "patch": "@@ -53,6 +53,8 @@ def get_some_linear_layer(model):\n         except AttributeError:\n             # for AutoModelforCausalLM\n             return model.model.decoder.layers[0].fc1\n+    elif model.config.model_type == \"llama\":\n+        return model.model.layers[0].mlp.gate_proj\n     else:\n         return model.transformer.h[0].mlp.dense_4h_to_h\n \n@@ -106,6 +108,7 @@ class Base4bitTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Hello my name is John and I am a professional photographer. I\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John.\\nI am a friend of your father.\\n\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is John Doe, I am a student at the University\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is John and I am 25 years old.\")\n     MAX_NEW_TOKENS = 10\n \n     def setUp(self):\n@@ -555,6 +558,8 @@ def test_training(self):\n \n         if torch.cuda.is_available():\n             self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n+        elif torch.xpu.is_available():\n+            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n         else:\n             self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n \n@@ -588,11 +593,18 @@ def test_training(self):\n \n \n @apply_skip_if_not_implemented\n+@unittest.skipIf(torch_device == \"xpu\", reason=\"XPU has precision issue on gpt model, will test it once fixed\")\n class Bnb4BitGPT2Test(Bnb4BitTest):\n     model_name = \"openai-community/gpt2-xl\"\n     EXPECTED_RELATIVE_DIFFERENCE = 3.3191854854152187\n \n \n+@apply_skip_if_not_implemented\n+class Bnb4BitLlamaTest(Bnb4BitTest):\n+    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+    EXPECTED_RELATIVE_DIFFERENCE = 2.9461410686392764\n+\n+\n @require_bitsandbytes\n @require_accelerate\n @require_torch\n@@ -672,7 +684,7 @@ def test_serialization(self, quant_type=\"nf4\", double_quant=True, safe_serializa\n         encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         out_0 = model_0(**encoded_input)\n         out_1 = model_1(**encoded_input)\n-        self.assertTrue(torch.equal(out_0[\"logits\"], out_1[\"logits\"]))\n+        self.assertTrue(torch.allclose(out_0[\"logits\"], out_1[\"logits\"], atol=0.05))\n \n         # comparing generate() outputs\n         encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n@@ -734,6 +746,14 @@ class GPTSerializationTest(BaseSerializationTest):\n     model_name = \"openai-community/gpt2-xl\"\n \n \n+class LlamaSerializationTest(BaseSerializationTest):\n+    \"\"\"\n+    default BaseSerializationTest config tested with Llama family model\n+    \"\"\"\n+\n+    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+\n+\n @require_bitsandbytes\n @require_accelerate\n @require_torch_gpu_if_bnb_not_multi_backend_enabled"
        },
        {
            "sha": "26e8cb2fc731ec7d67ea827ff909cbec2d0a586f",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 46,
            "deletions": 9,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/69e31eb1bf123d5bd0183b753da83ab7dd9c2eec/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69e31eb1bf123d5bd0183b753da83ab7dd9c2eec/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=69e31eb1bf123d5bd0183b753da83ab7dd9c2eec",
            "patch": "@@ -48,6 +48,8 @@\n def get_some_linear_layer(model):\n     if model.config.model_type == \"gpt2\":\n         return model.transformer.h[0].mlp.c_fc\n+    elif model.config.model_type == \"llama\":\n+        return model.model.layers[0].mlp.gate_proj\n     return model.transformer.h[0].mlp.dense_4h_to_h\n \n \n@@ -65,12 +67,12 @@ def get_some_linear_layer(model):\n     class LoRALayer(nn.Module):\n         \"\"\"Wraps a linear layer with LoRA-like adapter - Used for testing purposes only\"\"\"\n \n-        def __init__(self, module: nn.Module, rank: int):\n+        def __init__(self, module: nn.Module, rank: int, dtype: torch.dtype):\n             super().__init__()\n             self.module = module\n             self.adapter = nn.Sequential(\n-                nn.Linear(module.in_features, rank, bias=False),\n-                nn.Linear(rank, module.out_features, bias=False),\n+                nn.Linear(module.in_features, rank, bias=False, dtype=dtype),\n+                nn.Linear(rank, module.out_features, bias=False, dtype=dtype),\n             )\n             small_std = (2.0 / (5 * min(module.in_features, module.out_features))) ** 0.5\n             nn.init.normal_(self.adapter[0].weight, std=small_std)\n@@ -858,29 +860,36 @@ def test_training(self):\n \n         if torch.cuda.is_available():\n             self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n+        elif torch.xpu.is_available():\n+            self.assertEqual(set(model.hf_device_map.values()), {f\"xpu:{torch.xpu.current_device()}\"})\n         else:\n             self.assertTrue(all(param.device.type == \"cpu\" for param in model.parameters()))\n \n         for param in model.parameters():\n             param.requires_grad = False  # freeze the model - train adapters later\n-            if param.ndim == 1:\n-                # cast the small parameters (e.g. layernorm) to fp32 for stability\n+            # cast all non INT8 parameters to fp32\n+            if param.dtype in (torch.float16, torch.bfloat16) and param.__class__.__name__ != \"Params4bit\":\n                 param.data = param.data.to(torch.float32)\n \n         # Step 2: add adapters\n         for _, module in model.named_modules():\n             if isinstance(module, OPTAttention):\n-                module.q_proj = LoRALayer(module.q_proj, rank=16)\n-                module.k_proj = LoRALayer(module.k_proj, rank=16)\n-                module.v_proj = LoRALayer(module.v_proj, rank=16)\n+                module.q_proj = LoRALayer(module.q_proj, rank=16, dtype=model.dtype)\n+                module.k_proj = LoRALayer(module.k_proj, rank=16, dtype=model.dtype)\n+                module.v_proj = LoRALayer(module.v_proj, rank=16, dtype=model.dtype)\n \n         # Step 3: dummy batch\n         batch = self.tokenizer(\"Test batch \", return_tensors=\"pt\").to(torch_device)\n \n         # Step 4: Check if the gradient is not None\n-        with torch.autocast(torch_device):\n+        if torch_device in {\"xpu\", \"cpu\"}:\n+            # XPU and CPU finetune do not support autocast for now.\n             out = model.forward(**batch)\n             out.logits.norm().backward()\n+        else:\n+            with torch.autocast(torch_device):\n+                out = model.forward(**batch)\n+                out.logits.norm().backward()\n \n         for module in model.modules():\n             if isinstance(module, LoRALayer):\n@@ -891,6 +900,7 @@ def test_training(self):\n \n \n @apply_skip_if_not_implemented\n+@unittest.skipIf(torch_device == \"xpu\", reason=\"XPU has precision issue on gpt model, will test it once fixed\")\n class MixedInt8GPT2Test(MixedInt8Test):\n     model_name = \"openai-community/gpt2-xl\"\n     EXPECTED_RELATIVE_DIFFERENCE = 1.8720077507258357\n@@ -922,3 +932,30 @@ def test_int8_from_pretrained(self):\n         output_sequences = model.generate(input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10)\n \n         self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)\n+\n+\n+class MixedInt8LlamaTest(MixedInt8Test):\n+    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n+    EXPECTED_RELATIVE_DIFFERENCE = 1.7869331026479096\n+    EXPECTED_OUTPUTS = set()\n+    EXPECTED_OUTPUTS.add(\"Hello my name is John Smith and I am a software engineer. I\")\n+\n+    def test_int8_from_pretrained(self):\n+        r\"\"\"\n+        Test whether loading a 8bit model from the Hub works as expected\n+        \"\"\"\n+        from bitsandbytes.nn import Int8Params\n+\n+        model_id = \"Jiqing/TinyLlama-1.1B-Chat-v1.0-bnb-8bit\"\n+\n+        model = AutoModelForCausalLM.from_pretrained(model_id)\n+\n+        linear = get_some_linear_layer(model)\n+        self.assertTrue(linear.weight.__class__ == Int8Params)\n+        self.assertTrue(hasattr(linear.weight, \"SCB\"))\n+\n+        # generate\n+        encoded_input = self.tokenizer(self.input_text, return_tensors=\"pt\")\n+        output_sequences = model.generate(input_ids=encoded_input[\"input_ids\"].to(torch_device), max_new_tokens=10)\n+\n+        self.assertIn(self.tokenizer.decode(output_sequences[0], skip_special_tokens=True), self.EXPECTED_OUTPUTS)"
        }
    ],
    "stats": {
        "total": 77,
        "additions": 67,
        "deletions": 10
    }
}