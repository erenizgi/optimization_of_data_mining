{
    "author": "cyyever",
    "message": "Fix pylint warnings (#39477)\n\n* Fix pylint warnings\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix variable names\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "822c5e45b2353e10cd432251215aa61eb982ac05",
    "files": [
        {
            "sha": "cbb751ad75bec9fb50a799abbabb74272d2475c0",
            "filename": "src/transformers/commands/add_new_model_like.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fadd_new_model_like.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1079,10 +1079,10 @@ def add_model_to_auto_classes(\n         new_model_patterns (`ModelPatterns`): The patterns for the new model.\n         model_classes (`dict[str, list[str]]`): A dictionary framework to list of model classes implemented.\n     \"\"\"\n-    for filename in AUTO_CLASSES_PATTERNS:\n+    for filename, patterns in AUTO_CLASSES_PATTERNS.items():\n         # Extend patterns with all model classes if necessary\n         new_patterns = []\n-        for pattern in AUTO_CLASSES_PATTERNS[filename]:\n+        for pattern in patterns:\n             if re.search(\"any_([a-z]*)_class\", pattern) is not None:\n                 framework = re.search(\"any_([a-z]*)_class\", pattern).groups()[0]\n                 if framework in model_classes:"
        },
        {
            "sha": "eddcfc0f9c349314e1dc70e51ddb64bec91d5b51",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -146,14 +146,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "f736e37d4e67bc6b85d4d40a110602efab0d91dc",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -428,8 +428,7 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n         if isinstance(value, str) and architecture in value:\n             value = value.replace(architecture, updated_architecture)\n \n-        for parameter in GGUF_TO_TRANSFORMERS_MAPPING:\n-            parameter_renames = GGUF_TO_TRANSFORMERS_MAPPING[parameter]\n+        for parameter, parameter_renames in GGUF_TO_TRANSFORMERS_MAPPING.items():\n             if prefix in parameter_renames and config_key in parameter_renames[prefix]:\n                 renamed_config_key = parameter_renames[prefix][config_key]\n                 if renamed_config_key == -1:"
        },
        {
            "sha": "dc6926d43f9b9e1b65238f22e068a434a80a6a38",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1572,8 +1572,8 @@ def _find_mismatched_keys(\n         # Fix the key names\n         new_state_dict = {keys_to_rename_mapping[k]: v for k, v in state_dict.items() if k in keys_to_rename_mapping}\n \n-        for key in new_state_dict.keys():\n-            if key in model_state_dict and new_state_dict[key].shape != model_state_dict[key].shape:\n+        for key, tensor in new_state_dict.items():\n+            if key in model_state_dict and tensor.shape != model_state_dict[key].shape:\n                 # This skips size mismatches for 4-bit weights. Two 4-bit values share an 8-bit container, causing size differences.\n                 # Without matching with module type or parameter type it seems like a practical way to detect valid 4bit weights.\n                 if not (\n@@ -1582,7 +1582,7 @@ def _find_mismatched_keys(\n                     and new_state_dict[key].numel() * 2 == model_state_dict[key].numel()\n                 ):\n                     mismatched_keys.append(key)\n-                    mismatched_shapes.append((new_state_dict[key].shape, model_state_dict[key].shape))\n+                    mismatched_shapes.append((tensor.shape, model_state_dict[key].shape))\n \n     return mismatched_keys, mismatched_shapes\n "
        },
        {
            "sha": "af2c4f3e8d734b8b9d228652afd56058b107e04c",
            "filename": "src/transformers/models/bark/convert_suno_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fconvert_suno_to_hf.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -130,12 +130,12 @@ def _load_model(ckpt_path, device, use_small=False, model_type=\"text\"):\n     state_dict = checkpoint[\"model\"]\n     # fixup checkpoint\n     unwanted_prefix = \"_orig_mod.\"\n-    for k, v in list(state_dict.items()):\n+    for k in state_dict:\n         if k.startswith(unwanted_prefix):\n             # replace part of the key with corresponding layer name in HF implementation\n             new_k = k[len(unwanted_prefix) :]\n-            for old_layer_name in new_layer_name_dict:\n-                new_k = new_k.replace(old_layer_name, new_layer_name_dict[old_layer_name])\n+            for old_layer_name, new_layer_name in new_layer_name_dict.items():\n+                new_k = new_k.replace(old_layer_name, new_layer_name)\n \n             state_dict[new_k] = state_dict.pop(k)\n "
        },
        {
            "sha": "23cda58bfe723d7674a51e6c3f478f207464f639",
            "filename": "src/transformers/models/bert/tokenization_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Ftokenization_bert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -392,14 +392,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "cacacd87574a28527c5dad2c2c5391c3babc9b8b",
            "filename": "src/transformers/models/bert_japanese/tokenization_bert_japanese.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_japanese%2Ftokenization_bert_japanese.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -796,14 +796,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "625d26dc69605ac5432fab21a5f0da449cbf08e3",
            "filename": "src/transformers/models/clip/tokenization_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Ftokenization_clip.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -225,14 +225,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "b354b0eeae3d7f899311fa5322e9b2df6ad642ba",
            "filename": "src/transformers/models/convbert/tokenization_convbert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvbert%2Ftokenization_convbert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -396,14 +396,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "f65389d1d18a35a73d3db5273773fb627dde16ef",
            "filename": "src/transformers/models/cvt/convert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcvt%2Fconvert_cvt_original_pytorch_checkpoint_to_pytorch.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -290,7 +290,7 @@ def convert_cvt_checkpoint(cvt_model, image_size, cvt_file_name, pytorch_dump_fo\n     id2label = id2label\n     label2id = {v: k for k, v in id2label.items()}\n \n-    config = config = CvtConfig(num_labels=num_labels, id2label=id2label, label2id=label2id)\n+    config = CvtConfig(num_labels=num_labels, id2label=id2label, label2id=label2id)\n \n     # For depth size 13 (13 = 1+2+10)\n     if cvt_model.rsplit(\"/\", 1)[-1][4:6] == \"13\":"
        },
        {
            "sha": "af7cdc1a5bac3afea8ed0651ae2892adcc03cbc3",
            "filename": "src/transformers/models/deprecated/realm/tokenization_realm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Frealm%2Ftokenization_realm.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -448,14 +448,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "288b46e267bccb358fa86e74ac2b98dcb99e8904",
            "filename": "src/transformers/models/deprecated/retribert/tokenization_retribert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fretribert%2Ftokenization_retribert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -389,14 +389,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "4e44468ab1d574666053b0ba772f2f0e098d5263",
            "filename": "src/transformers/models/distilbert/tokenization_distilbert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Ftokenization_distilbert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -405,14 +405,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "d8971dd6f40374e1d8a6e8ec479cd9da79b64da3",
            "filename": "src/transformers/models/electra/tokenization_electra.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Ftokenization_electra.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -395,14 +395,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "e0d0c3c5c5793c37a93a0deee09e2857cbeac9f7",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -258,7 +258,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n     # Convert and save processor\n     tokenizer_tiktoken = AutoTokenizer.from_pretrained(llm_model_id, trust_remote_code=True)\n     convert_tiktoken(tokenizer_tiktoken, output_dir)\n-    extra_special_tokens = extra_special_tokens = {\n+    extra_special_tokens = {\n         \"image_token\": \"<image>\",\n         \"boi_token\": \"<|image start|>\",\n         \"eoi_token\": \"<|image end|>\","
        },
        {
            "sha": "8e9aaaf3405f066d90731a3e84fd41cb3b352876",
            "filename": "src/transformers/models/falcon_h1/configuration_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fconfiguration_falcon_h1.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -254,7 +254,6 @@ def __init__(\n         if ssm_multipliers is not None:\n             self.ssm_multipliers = ssm_multipliers\n         else:\n-            #\n             self.ssm_multipliers = [1.0, 1.0, 1.0, 1.0, 1.0]\n \n         if ssm_in_multiplier is not None:"
        },
        {
            "sha": "e5d44e5e59064315ca330b6d9d7d0ffd04c59b12",
            "filename": "src/transformers/models/funnel/tokenization_funnel.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffunnel%2Ftokenization_funnel.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -455,14 +455,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "c1c6bacc87fc68bf5a4d998236e878fdf602a8f0",
            "filename": "src/transformers/models/herbert/tokenization_herbert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fherbert%2Ftokenization_herbert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -248,14 +248,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "911fc4d72dec54fec17edc5e676ecde5ca3ed86f",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -367,11 +367,11 @@ def forward(\n \n             if layer_past is not None and is_updated:\n                 # reuse k,v, cross_attentions, and compute only q\n-                query = query = self.q_attn(hidden_states)\n+                query = self.q_attn(hidden_states)\n                 key = curr_past_key_value.key_cache[self.layer_idx]\n                 value = curr_past_key_value.value_cache[self.layer_idx]\n             else:\n-                query = query = self.q_attn(hidden_states)\n+                query = self.q_attn(hidden_states)\n                 key, value = self.c_attn(current_states).split(self.split_size, dim=2)\n                 key = key.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)\n                 value = value.view(bsz, -1, self.num_heads, self.head_dim).transpose(1, 2)"
        },
        {
            "sha": "f6738693843be0fe9af32cfc4fe96f3e6fbceb59",
            "filename": "src/transformers/models/layoutlm/modeling_tf_layoutlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Fmodeling_tf_layoutlm.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -157,7 +157,7 @@ def call(\n             position_ids = tf.expand_dims(tf.range(start=0, limit=input_shape[-1]), axis=0)\n \n         if bbox is None:\n-            bbox = bbox = tf.fill(input_shape + [4], value=0)\n+            bbox = tf.fill(input_shape + [4], value=0)\n         try:\n             left_position_embeddings = tf.gather(self.x_position_embeddings, bbox[:, :, 0])\n             upper_position_embeddings = tf.gather(self.y_position_embeddings, bbox[:, :, 1])"
        },
        {
            "sha": "4caccd691d0e5fcb64637f72d5c2860f6f096e9e",
            "filename": "src/transformers/models/layoutlm/tokenization_layoutlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlm%2Ftokenization_layoutlm.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -396,14 +396,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "7d82b5cf41041b0024134c0f1a6294c0cace824c",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1458,14 +1458,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "dd1d7e205ea5588bec3f66307d5ae6c8412ee4a6",
            "filename": "src/transformers/models/lxmert/tokenization_lxmert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flxmert%2Ftokenization_lxmert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -395,14 +395,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "88628400ca49079a0689ecf5978c1256fd82234c",
            "filename": "src/transformers/models/mobilebert/tokenization_mobilebert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilebert%2Ftokenization_mobilebert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -397,14 +397,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "bf035cf8e4bd490023890a0d690bba32a123124b",
            "filename": "src/transformers/models/mpnet/tokenization_mpnet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpnet%2Ftokenization_mpnet.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -450,14 +450,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "8a9184cc395cab5beabfea30ef11eab928aca89c",
            "filename": "src/transformers/models/openai/tokenization_openai.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Ftokenization_openai.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -178,14 +178,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "49ce578d00b8cb70e95f22931cafde0d7aaba8c6",
            "filename": "src/transformers/models/owlvit/convert_owlvit_original_flax_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -314,7 +314,7 @@ def convert_clip_backbone(flax_params, torch_config):\n     # Copy flax CLIP backbone params to PyTorch params\n     for name, param in new_torch_params.items():\n         if name in torch_clip_params.keys():\n-            new_param = torch.from_numpy(new_torch_params[name])\n+            new_param = torch.from_numpy(param)\n             torch_clip_params[name].copy_(new_param)\n         else:\n             attn_params[name] = param"
        },
        {
            "sha": "32125b15123c4da9103fd2b5bdf068df191586ff",
            "filename": "src/transformers/models/paligemma/convert_paligemma_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma_weights_to_hf.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -291,9 +291,6 @@ def convert_paligemma_checkpoint(\n     processor.save_pretrained(pytorch_dump_folder_path)\n \n \n-#\n-\n-\n if __name__ == \"__main__\":\n     parser = argparse.ArgumentParser()\n     parser.add_argument("
        },
        {
            "sha": "f19d864f46ad61772d1260fa52568129859e12d3",
            "filename": "src/transformers/models/pix2struct/convert_pix2struct_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -93,11 +93,11 @@ def rename_and_convert_flax_params(flax_dict):\n \n     converted_torch_dict = {}\n     # convert converted_dict into torch format\n-    for key in converted_dict.keys():\n+    for key, value in converted_dict.items():\n         if (\"embed_tokens\" not in key) and (\"embedder\" not in key):\n-            converted_torch_dict[key] = torch.from_numpy(converted_dict[key].T)\n+            converted_torch_dict[key] = torch.from_numpy(value.T)\n         else:\n-            converted_torch_dict[key] = torch.from_numpy(converted_dict[key])\n+            converted_torch_dict[key] = torch.from_numpy(value)\n \n     return converted_torch_dict\n "
        },
        {
            "sha": "24401835c7fc62d916802c90aa7d9fede3e9db42",
            "filename": "src/transformers/models/prophetnet/tokenization_prophetnet.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Ftokenization_prophetnet.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -174,14 +174,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "afe4e2992003e7743ec393eb617d357a37e5f5af",
            "filename": "src/transformers/models/roc_bert/tokenization_roc_bert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Ftokenization_roc_bert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1005,14 +1005,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "cf10a1b528a99ee77c28a42b1bb14ed38a545024",
            "filename": "src/transformers/models/roformer/tokenization_roformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Ftokenization_roformer.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -186,14 +186,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "f2ccafe12d5a9cbdcf777e520401ec0ff863a0b7",
            "filename": "src/transformers/models/splinter/tokenization_splinter.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsplinter%2Ftokenization_splinter.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -421,14 +421,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "4834f66816672aaa6784a4b3464ac7d090f88ead",
            "filename": "src/transformers/models/squeezebert/tokenization_squeezebert.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsqueezebert%2Ftokenization_squeezebert.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -396,14 +396,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "8d19daf1c1f18ab96d5890b75fe718ea98a457df",
            "filename": "src/transformers/models/switch_transformers/convert_big_switch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -89,9 +89,9 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n         else:\n             all_layers[curr_real_layer_name] = {split_layer[-1]: content}\n \n-    for key in all_layers.keys():\n+    for key, layer in all_layers.items():\n         # open tensorstore file\n-        raw_weights = ts.open(unflatten_dict(all_layers[key])).result().read().result()\n+        raw_weights = ts.open(unflatten_dict(layer)).result().read().result()\n         raw_weights = torch.tensor(raw_weights)\n         weight_size = raw_weights.numel() * raw_weights.element_size()\n "
        },
        {
            "sha": "4d91a1add94411d3b2a854327176b19e148ab4da",
            "filename": "src/transformers/models/tapas/tokenization_tapas.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Ftokenization_tapas.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -2123,14 +2123,14 @@ def _is_chinese_char(self, cp):\n         # like the all of the other languages.\n         if (\n             (cp >= 0x4E00 and cp <= 0x9FFF)\n-            or (cp >= 0x3400 and cp <= 0x4DBF)  #\n-            or (cp >= 0x20000 and cp <= 0x2A6DF)  #\n-            or (cp >= 0x2A700 and cp <= 0x2B73F)  #\n-            or (cp >= 0x2B740 and cp <= 0x2B81F)  #\n-            or (cp >= 0x2B820 and cp <= 0x2CEAF)  #\n+            or (cp >= 0x3400 and cp <= 0x4DBF)\n+            or (cp >= 0x20000 and cp <= 0x2A6DF)\n+            or (cp >= 0x2A700 and cp <= 0x2B73F)\n+            or (cp >= 0x2B740 and cp <= 0x2B81F)\n+            or (cp >= 0x2B820 and cp <= 0x2CEAF)\n             or (cp >= 0xF900 and cp <= 0xFAFF)\n-            or (cp >= 0x2F800 and cp <= 0x2FA1F)  #\n-        ):  #\n+            or (cp >= 0x2F800 and cp <= 0x2FA1F)\n+        ):\n             return True\n \n         return False"
        },
        {
            "sha": "1ba7235029e60563599429899e83947ec98bcb6d",
            "filename": "src/transformers/models/umt5/convert_umt5_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fconvert_umt5_checkpoint_to_pytorch.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -50,7 +50,7 @@ def t5x_relpos_bias_lookup(params, i, prefix):\n \n def t5x_attention_lookup(params, i, prefix, layer_name=\"attention\"):\n     \"\"\"Returns the KOQV parameters of (self-)attention. Does not transpose.\"\"\"\n-    k_tmp = k_tmp = np.ascontiguousarray(params[f\"{prefix}/{prefix}/{layer_name}/key/kernel\"][:, i, :, :])\n+    k_tmp = np.ascontiguousarray(params[f\"{prefix}/{prefix}/{layer_name}/key/kernel\"][:, i, :, :])\n     k = k_tmp.reshape(k_tmp.shape[0], k_tmp.shape[1] * k_tmp.shape[2])\n     o_tmp = np.ascontiguousarray(params[f\"{prefix}/{prefix}/{layer_name}/out/kernel\"][:, i, :, :])\n     o = o_tmp.reshape(o_tmp.shape[0] * o_tmp.shape[1], o_tmp.shape[2])"
        },
        {
            "sha": "a40f2ad7fd8cf29f7e70d9a3971136211a500785",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1184,7 +1184,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         used_keys = set()\n \n         # get defaults from set model processor kwargs if they exist\n-        for modality in default_kwargs:\n+        for modality in default_kwargs:  # noqa: PLC0206\n             default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n             # update defaults with arguments from tokenizer init\n             for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n@@ -1202,7 +1202,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n \n         # update modality kwargs with passed kwargs\n         non_modality_kwargs = set(kwargs) - set(output_kwargs)\n-        for modality in output_kwargs:\n+        for modality, output_kwarg in output_kwargs.items():\n             for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n                 # check if we received a structured kwarg dict or not to handle it correctly\n                 if modality in kwargs:\n@@ -1220,7 +1220,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                 else:\n                     kwarg_value = \"__empty__\"\n                 if not isinstance(kwarg_value, str) or kwarg_value != \"__empty__\":\n-                    output_kwargs[modality][modality_key] = kwarg_value\n+                    output_kwarg[modality_key] = kwarg_value\n                     used_keys.add(modality_key)\n \n         # Determine if kwargs is a flat dictionary or contains nested dictionaries\n@@ -1234,18 +1234,18 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                             used_keys.add(subkey)\n         else:\n             # kwargs is a flat dictionary\n-            for key in kwargs:\n+            for key, kwarg in kwargs.items():\n                 if key not in used_keys:\n                     if key in ModelProcessorKwargs.__annotations__[\"common_kwargs\"].__annotations__.keys():\n-                        output_kwargs[\"common_kwargs\"][key] = kwargs[key]\n+                        output_kwargs[\"common_kwargs\"][key] = kwarg\n                     elif key not in possible_modality_keywords:\n                         logger.warning_once(\n                             f\"Keyword argument `{key}` is not a valid argument for this processor and will be ignored.\"\n                         )\n \n         # all modality-specific kwargs are updated with common kwargs\n-        for modality in output_kwargs:\n-            output_kwargs[modality].update(output_kwargs[\"common_kwargs\"])\n+        for kwarg in output_kwargs.values():\n+            kwarg.update(output_kwargs[\"common_kwargs\"])\n         return output_kwargs\n \n     @classmethod"
        },
        {
            "sha": "4c6a64705b77d448a7b1c347ef682084b742e8c7",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -253,8 +253,8 @@ def weight(_self: HQQLinear):\n             return\n \n         # Step 1: populate module with weight/bias from module state dict\n-        for key in module_state_dict:\n-            setattr(module, key, torch.nn.Parameter(module_state_dict[key]))\n+        for key, tensor in module_state_dict.items():\n+            setattr(module, key, torch.nn.Parameter(tensor))\n \n         # Step 2: Replace module with either HQQLinear or move it to device. We do this via setattr on the parent as doing on it on the module\n         # directly doesn't work."
        },
        {
            "sha": "d9a4041f7f413f56af5b8a2a0d5da7dec81b5be0",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -118,7 +118,7 @@ def generate_attention_matrix_from_mask(\n         colored_word = f\"{YELLOW}{word_repr}{RESET}\" if img_token in word else word_repr\n         row_display = \" \".join(\n             f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n-            if img_token in words[j] and mask[i, j] and img_token in words[i]\n+            if img_token in words[j] and mask[i, j] and img_token in word\n             else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n             if i == j\n             else BLACK_SQUARE\n@@ -130,9 +130,7 @@ def generate_attention_matrix_from_mask(\n         if sliding_window is not None:\n             sliding_window_row = \" \".join(\n                 f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n-                if img_token in words[j]\n-                and img_token in words[i]\n-                and token_type_buckets[0, i] == token_type_buckets[0, j]\n+                if img_token in words[j] and img_token in word and token_type_buckets[0, i] == token_type_buckets[0, j]\n                 else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n                 if i == j\n                 else BLACK_SQUARE"
        },
        {
            "sha": "3e438a13b35048d318f0944731fd0e825f832d65",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -247,7 +247,7 @@ def test_attention_outputs(self):\n                     continue\n \n                 self.assertListEqual(\n-                    list(self_attentions[i].shape[-2:]),\n+                    list(self_attn.shape[-2:]),\n                     [\n                         self.model_tester.num_output_groups[i],\n                         self.model_tester.num_output_groups[i - 1] if i > 0 else seq_len,"
        },
        {
            "sha": "34617218082e1ffffe7c2a2e0b6777541aebe0e9",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -1375,9 +1375,7 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                 self.assertEqual(output_pt[\"assistant_masks\"].shape, output_pt[\"input_ids\"].shape)\n \n                 for i, conv in enumerate(conversations):\n-                    chat_string = tokenizer_r.apply_chat_template(\n-                        conversations[i], tokenize=False, chat_template=dummy_template\n-                    )\n+                    chat_string = tokenizer_r.apply_chat_template(conv, tokenize=False, chat_template=dummy_template)\n                     assistant_start = output.char_to_token(i, chat_string.index(assistant_prefix_suffix[i][0][0]))\n                     assistant_end = output.char_to_token(\n                         i,"
        },
        {
            "sha": "b15c1d6c4f0406f5ed426034811673cdc8822f21",
            "filename": "utils/check_bad_commit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fcheck_bad_commit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fcheck_bad_commit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_bad_commit.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -97,6 +97,7 @@ def find_bad_commit(target_test, start_commit, end_commit):\n \n     result = subprocess.run(\n         [\"bash\", \"run_git_bisect.sh\"],\n+        check=False,\n         capture_output=True,\n         text=True,\n     )"
        },
        {
            "sha": "0c651d1a1248e3ee73000850838fd8fb32d375c4",
            "filename": "utils/check_self_hosted_runner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fcheck_self_hosted_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fcheck_self_hosted_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_self_hosted_runner.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -15,7 +15,7 @@ def get_runner_status(target_runners, token):\n         \"https://api.github.com/repos/huggingface/transformers/actions/runners\",\n     ]\n \n-    output = subprocess.run(cmd, shell=True, stdout=subprocess.PIPE)\n+    output = subprocess.run(cmd, check=False, shell=True, stdout=subprocess.PIPE)\n     o = output.stdout.decode(\"utf-8\")\n     status = json.loads(o)\n "
        },
        {
            "sha": "8aae0025188d541f1d50211c5a7829319125bea2",
            "filename": "utils/patch_helper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fpatch_helper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/822c5e45b2353e10cd432251215aa61eb982ac05/utils%2Fpatch_helper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fpatch_helper.py?ref=822c5e45b2353e10cd432251215aa61eb982ac05",
            "patch": "@@ -85,7 +85,7 @@ def get_prs_by_label(label):\n         \"--limit\",\n         \"100\",\n     ]\n-    result = subprocess.run(cmd, capture_output=True, text=True)\n+    result = subprocess.run(cmd, check=False, capture_output=True, text=True)\n     result.check_returncode()\n     prs = json.loads(result.stdout)\n     for pr in prs:\n@@ -97,7 +97,9 @@ def get_prs_by_label(label):\n \n def get_commit_timestamp(commit_sha):\n     \"\"\"Get UNIX timestamp of a commit using git.\"\"\"\n-    result = subprocess.run([\"git\", \"show\", \"-s\", \"--format=%ct\", commit_sha], capture_output=True, text=True)\n+    result = subprocess.run(\n+        [\"git\", \"show\", \"-s\", \"--format=%ct\", commit_sha], check=False, capture_output=True, text=True\n+    )\n     result.check_returncode()\n     return int(result.stdout.strip())\n \n@@ -115,6 +117,7 @@ def commit_in_history(commit_sha, base_branch=\"HEAD\"):\n     \"\"\"Return True if commit is already part of base_branch history.\"\"\"\n     result = subprocess.run(\n         [\"git\", \"merge-base\", \"--is-ancestor\", commit_sha, base_branch],\n+        check=False,\n         stdout=subprocess.DEVNULL,\n         stderr=subprocess.DEVNULL,\n     )"
        }
    ],
    "stats": {
        "total": 409,
        "additions": 202,
        "deletions": 207
    }
}