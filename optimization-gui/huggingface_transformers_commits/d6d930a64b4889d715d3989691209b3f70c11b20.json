{
    "author": "kylesayrs",
    "message": "[Modeling] Load FP8 safetensors such as DeepSeek (#36828)\n\nsupport loading fp8\n\nSigned-off-by: Kyle Sayers <kylesayrs@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "d6d930a64b4889d715d3989691209b3f70c11b20",
    "files": [
        {
            "sha": "abf8dec55cc060078824c1451511e8f54b7f7223",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/d6d930a64b4889d715d3989691209b3f70c11b20/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d6d930a64b4889d715d3989691209b3f70c11b20/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d6d930a64b4889d715d3989691209b3f70c11b20",
            "patch": "@@ -532,6 +532,9 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"I64\": torch.int64,\n }\n \n+if is_torch_greater_or_equal(\"2.1.0\"):\n+    str_to_torch_dtype[\"F8_E4M3\"] = torch.float8_e4m3fn\n+\n if is_torch_greater_or_equal(\"2.3.0\"):\n     str_to_torch_dtype[\"U16\"] = torch.uint16\n     str_to_torch_dtype[\"U32\"] = torch.uint32\n@@ -562,7 +565,11 @@ def load_state_dict(\n                 )\n             state_dict = {}\n             for k in f.keys():\n-                dtype = str_to_torch_dtype[f.get_slice(k).get_dtype()]\n+                k_dtype = f.get_slice(k).get_dtype()\n+                if k_dtype in str_to_torch_dtype:\n+                    dtype = str_to_torch_dtype[k_dtype]\n+                else:\n+                    raise ValueError(f\"Cannot load safetensors of unknown dtype {k_dtype}\")\n                 if map_location == \"meta\":\n                     state_dict[k] = torch.empty(size=f.get_slice(k).get_shape(), dtype=dtype, device=\"meta\")\n                 else:"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}