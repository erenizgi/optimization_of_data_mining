{
    "author": "jiqing-feng",
    "message": "Enables CPU AWQ model with IPEX version. (#33460)\n\n* enable cpu awq ipex linear\r\n\r\n* add doc for cpu awq with ipex kernel\r\n\r\n* add tests for cpu awq\r\n\r\n* fix code style\r\n\r\n* fix doc and tests\r\n\r\n* Update docs/source/en/quantization/awq.md\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Update tests/quantization/autoawq/test_awq.py\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* fix comments\r\n\r\n* fix log\r\n\r\n* fix log\r\n\r\n* fix style\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "b916efcb3c615132d6278ccc46c48cf7e5ea7792",
    "files": [
        {
            "sha": "ca26844edd0294e9f5172687e622f3239b0c1ed7",
            "filename": "docs/source/en/quantization/awq.md",
            "status": "modified",
            "additions": 41,
            "deletions": 0,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/docs%2Fsource%2Fen%2Fquantization%2Fawq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fawq.md?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -230,3 +230,44 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n Note this feature is supported on AMD GPUs.\n \n </Tip>\n+\n+\n+## CPU support\n+\n+Recent versions of `autoawq` supports CPU with ipex op optimizations. To get started, first install the latest version of `autoawq` by running:\n+\n+```bash\n+pip install intel-extension-for-pytorch\n+pip install git+https://github.com/casper-hansen/AutoAWQ.git\n+```\n+\n+Get started by passing an `AwqConfig()` with `version=\"ipex\"`.\n+\n+```python\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, AwqConfig\n+\n+quantization_config = AwqConfig(version=\"ipex\")\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\",\n+    quantization_config=quantization_config,\n+    device_map=\"cpu\",\n+)\n+\n+input_ids = torch.randint(0, 100, (1, 128), dtype=torch.long, device=\"cpu\")\n+output = model(input_ids)\n+print(output.logits)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\")\n+input_ids = tokenizer.encode(\"How to make a cake\", return_tensors=\"pt\")\n+pad_token_id = tokenizer.eos_token_id\n+output = model.generate(input_ids, do_sample=True, max_length=50, pad_token_id=pad_token_id)\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+<Tip warning={true}>\n+\n+Note this feature is supported on Intel CPUs.\n+\n+</Tip>\n\\ No newline at end of file"
        },
        {
            "sha": "b172c78513003ce13e64b8ded7464d5f58517068",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -21,6 +21,7 @@\n     \"awq\": [\n         \"fuse_awq_modules\",\n         \"post_init_awq_exllama_modules\",\n+        \"post_init_awq_ipex_modules\",\n         \"replace_quantization_scales\",\n         \"replace_with_awq_linear\",\n     ],\n@@ -115,6 +116,7 @@\n     from .awq import (\n         fuse_awq_modules,\n         post_init_awq_exllama_modules,\n+        post_init_awq_ipex_modules,\n         replace_quantization_scales,\n         replace_with_awq_linear,\n     )"
        },
        {
            "sha": "a945b7f781c4938fe2581a37c7812382a9fb157d",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 26,
            "deletions": 3,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -145,6 +145,10 @@ def replace_with_awq_linear(\n                 target_cls = WQLinear_ExllamaV2\n             else:\n                 raise ValueError(f\"Unrecognized Exllama version: {quantization_config.exllama_config['version']}\")\n+        elif quantization_config.version == AWQLinearVersion.IPEX:\n+            from awq.modules.linear.gemm_ipex import WQLinear_IPEX\n+\n+            target_cls = WQLinear_IPEX\n         else:\n             raise ValueError(f\"Unrecognized AWQ version: {quantization_config.version}\")\n     else:\n@@ -266,8 +270,11 @@ def fuse_awq_modules(model, quantization_config):\n         # Replace layer norms\n         _fuse_awq_layernorm(modules_to_fuse[\"layernorm\"], module, FasterTransformerRMSNorm)\n \n-        # Replace MLP layers\n-        _fuse_awq_mlp(model, name, modules_to_fuse[\"mlp\"], module, QuantFusedMLP)\n+        # Replace MLP layers if awq version is not ipex.\n+        if quantization_config.version != \"ipex\":\n+            _fuse_awq_mlp(model, name, modules_to_fuse[\"mlp\"], module, QuantFusedMLP)\n+        else:\n+            logger.info(\"The IPEX version AWQ does not support fuse mlp for now.\")\n \n         # Replace attention layers\n         attention_has_been_fused = _fuse_awq_attention_layers(\n@@ -372,7 +379,7 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n             The `QuantAttentionFused` class as it only supports that class\n             for now.\n     \"\"\"\n-    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV\n+    from awq.modules.linear import WQLinear_GEMM, WQLinear_GEMV, WQLinear_IPEX\n \n     module_has_been_fused = False\n \n@@ -389,6 +396,9 @@ def _fuse_awq_attention_layers(model, module, modules_to_fuse, current_module_na\n         elif isinstance(q_proj, WQLinear_GEMM):\n             linear_target_cls = WQLinear_GEMM\n             cat_dim = 1\n+        elif isinstance(q_proj, WQLinear_IPEX):\n+            linear_target_cls = WQLinear_IPEX\n+            cat_dim = 1\n         else:\n             raise ValueError(\"Unsupported q_proj type: {type(q_proj)}\")\n \n@@ -466,3 +476,16 @@ def post_init_awq_exllama_modules(model, exllama_config):\n         raise ValueError(f\"Unrecognized Exllama version: {exllama_config['version']}\")\n \n     return model\n+\n+\n+def post_init_awq_ipex_modules(model):\n+    \"\"\"\n+    Runs post init for IPEX layers which performs:\n+        - Weights packing, reordering and repacking\n+    \"\"\"\n+\n+    from awq.modules.linear.gemm_ipex import ipex_post_init\n+\n+    model = ipex_post_init(model)\n+\n+    return model"
        },
        {
            "sha": "3df4fd5147172cf76a25e1179db7aa5cc8e21cb4",
            "filename": "src/transformers/quantizers/quantizer_awq.py",
            "status": "modified",
            "additions": 30,
            "deletions": 12,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_awq.py?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -46,27 +46,40 @@ def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n     def validate_environment(self, device_map, **kwargs):\n-        if not torch.cuda.is_available():\n-            raise RuntimeError(\"GPU is required to run AWQ quantized model.\")\n-\n         if not is_auto_awq_available():\n             raise ImportError(\"Loading an AWQ quantized model requires auto-awq library (`pip install autoawq`)\")\n \n         if not is_accelerate_available():\n             raise ImportError(\"Loading an AWQ quantized model requires accelerate (`pip install accelerate`)\")\n \n-        if device_map is None:\n-            logger.warning_once(\n-                \"You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set \"\n-                \"your model on a GPU device in order to run your model.\"\n-            )\n-        elif device_map is not None:\n-            if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+        if self.quantization_config.version == AWQLinearVersion.IPEX:\n+            if (\n+                device_map is not None\n+                and isinstance(device_map, dict)\n+                and (torch.device(\"cpu\") not in device_map.values() or len(device_map.values()) > 1)\n+            ):\n                 raise ValueError(\n-                    \"You are attempting to load an AWQ model with a device_map that contains a CPU or disk device.\"\n-                    \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+                    \"You are attempting to load an IPEX version AWQ model with a device_map that contains more than CPU.\"\n+                    \" This is not supported. Please make sure only cpu in the device_map.\"\n+                )\n+        else:\n+            if not torch.cuda.is_available():\n+                raise RuntimeError(\n+                    \"GPU is required to run AWQ quantized model. You can use IPEX version AWQ if you have an Intel CPU\"\n                 )\n \n+            if device_map is None:\n+                logger.warning_once(\n+                    \"You have loaded an AWQ model on CPU and have a CUDA device available, make sure to set \"\n+                    \"your model on a GPU device in order to run your model.\"\n+                )\n+            elif device_map is not None:\n+                if isinstance(device_map, dict) and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n+                    raise ValueError(\n+                        \"You are attempting to load an AWQ model with a device_map that contains a CPU or disk device.\"\n+                        \" This is not supported. Please remove the CPU or disk device from the device_map.\"\n+                    )\n+\n     def update_torch_dtype(self, torch_dtype):\n         if torch_dtype is None:\n             torch_dtype = torch.float16\n@@ -106,6 +119,11 @@ def _process_model_after_weight_loading(self, model):\n \n             model = post_init_awq_exllama_modules(model, self.quantization_config.exllama_config)\n \n+        if self.quantization_config.version == AWQLinearVersion.IPEX:\n+            from ..integrations import post_init_awq_ipex_modules\n+\n+            model = post_init_awq_ipex_modules(model)\n+\n     def is_serializable(self, safe_serialization=None):\n         # AWQ through auto-awq has been always serializable, except if the model is fused.\n         if self.quantization_config.do_fuse:"
        },
        {
            "sha": "aa15e096f509d1520370503ca898a8047cf35256",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -51,6 +51,7 @@ class AWQLinearVersion(str, Enum):\n     GEMM = \"gemm\"\n     GEMV = \"gemv\"\n     EXLLAMA = \"exllama\"\n+    IPEX = \"ipex\"\n \n     @staticmethod\n     def from_str(version: str):\n@@ -61,6 +62,8 @@ def from_str(version: str):\n             return AWQLinearVersion.GEMV\n         elif version == \"exllama\":\n             return AWQLinearVersion.EXLLAMA\n+        elif version == \"ipex\":\n+            return AWQLinearVersion.IPEX\n         else:\n             raise ValueError(f\"Unknown AWQLinearVersion {version}\")\n \n@@ -830,18 +833,20 @@ def post_init(self):\n         r\"\"\"\n         Safety checker that arguments are correct\n         \"\"\"\n-        if not torch.cuda.is_available():\n-            raise ValueError(\"AWQ is only available on GPU\")\n-\n         if self.backend not in [AwqBackendPackingMethod.AUTOAWQ, AwqBackendPackingMethod.LLMAWQ]:\n             raise ValueError(\n                 f\"Only supported quantization backends in {AwqBackendPackingMethod.AUTOAWQ} and {AwqBackendPackingMethod.LLMAWQ} - not recognized backend {self.backend}\"\n             )\n \n         self.version = AWQLinearVersion.from_str(self.version)\n-        if self.version not in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA]:\n+        if self.version not in [\n+            AWQLinearVersion.GEMM,\n+            AWQLinearVersion.GEMV,\n+            AWQLinearVersion.EXLLAMA,\n+            AWQLinearVersion.IPEX,\n+        ]:\n             raise ValueError(\n-                f\"Only supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA] - not recognized version {self.version}\"\n+                f\"Only supported versions are in [AWQLinearVersion.GEMM, AWQLinearVersion.GEMV, AWQLinearVersion.EXLLAMA, AWQLinearVersion.IPEX] - not recognized version {self.version}\"\n             )\n \n         if self.backend == AwqBackendPackingMethod.LLMAWQ:"
        },
        {
            "sha": "780efe8aa4b465632365e151493b87bfaef29672",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/b916efcb3c615132d6278ccc46c48cf7e5ea7792/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b916efcb3c615132d6278ccc46c48cf7e5ea7792/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=b916efcb3c615132d6278ccc46c48cf7e5ea7792",
            "patch": "@@ -21,6 +21,7 @@\n from transformers.testing_utils import (\n     require_accelerate,\n     require_auto_awq,\n+    require_intel_extension_for_pytorch,\n     require_torch_gpu,\n     require_torch_multi_gpu,\n     slow,\n@@ -490,3 +491,31 @@ def test_load_quantized_model(self):\n             \"TechxGenus/starcoder2-3b-AWQ\", torch_dtype=torch.float16, device_map=\"cuda\"\n         )\n         self.assertTrue(isinstance(quantized_model.model.layers[0].mlp.act, ScaledActivation))\n+\n+\n+@slow\n+@require_auto_awq\n+@require_accelerate\n+@require_intel_extension_for_pytorch\n+class AwqIPEXTest(unittest.TestCase):\n+    def test_quantized_model_ipex(self):\n+        \"\"\"\n+        Simple test that checks if the quantized model is working properly with ipex backend\n+        \"\"\"\n+        quantization_config = AwqConfig(version=\"ipex\")\n+\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\",\n+            quantization_config=quantization_config,\n+            device_map=\"cpu\",\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ\")\n+        input_ids = tokenizer.encode(\"How to make a cake\", return_tensors=\"pt\")\n+        pad_token_id = tokenizer.eos_token_id\n+        output = model.generate(input_ids, do_sample=False, max_length=20, pad_token_id=pad_token_id)\n+        print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+        expected_output = (\n+            \"How to make a cake with a round tin?\\nHow to make a cake with a round tin?\\n1. Preheat the oven to 180Â°\"\n+        )\n+        self.assertIn(tokenizer.decode(output[0], skip_special_tokens=True), expected_output)"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 138,
        "deletions": 20
    }
}