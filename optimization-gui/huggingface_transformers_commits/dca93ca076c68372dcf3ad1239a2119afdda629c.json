{
    "author": "kibitzing",
    "message": "Fix step shifting when accumulate gradient (#33673)\n\n* replace total_batched_samples with step while counting grad accum step\r\n\r\n* remove unused variable\r\n\r\n* simplify condition for update step\r\n\r\n* fix format by ruff\r\n\r\n* simplify update step condition using accelerator.sync_gradients\r\n\r\n* simplify update condition using do_sync_step\r\n\r\n* remove print for test\r\n\r\n---------\r\n\r\nCo-authored-by: Zach Mueller <muellerzr@gmail.com>",
    "sha": "dca93ca076c68372dcf3ad1239a2119afdda629c",
    "files": [
        {
            "sha": "30caa2de260cb762e51404b47538b30296ab9f41",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/dca93ca076c68372dcf3ad1239a2119afdda629c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dca93ca076c68372dcf3ad1239a2119afdda629c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=dca93ca076c68372dcf3ad1239a2119afdda629c",
            "patch": "@@ -2404,7 +2404,6 @@ def _inner_training_loop(\n         if args.eval_on_start:\n             self._evaluate(trial, ignore_keys_for_eval, skip_scheduler=True)\n \n-        total_batched_samples = 0\n         for epoch in range(epochs_trained, num_train_epochs):\n             epoch_dataloader = train_dataloader\n             if hasattr(epoch_dataloader, \"set_epoch\"):\n@@ -2447,13 +2446,7 @@ def _inner_training_loop(\n                 batch_samples, num_items_in_batch = self.get_batch_samples(epoch_iterator, num_batches)\n                 for inputs in batch_samples:\n                     step += 1\n-                    total_batched_samples += 1\n-                    is_last_step_and_steps_less_than_grad_acc = (\n-                        steps_in_epoch <= args.gradient_accumulation_steps and (step + 1) == steps_in_epoch\n-                    )\n-                    do_sync_step = is_last_step_and_steps_less_than_grad_acc or (\n-                        total_batched_samples % args.gradient_accumulation_steps == 0\n-                    )\n+                    do_sync_step = (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == steps_in_epoch\n                     # Since we perform prefetching, we need to manually set sync_gradients\n                     if not do_sync_step:\n                         self.accelerator.gradient_state._set_sync_gradients(False)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 1,
        "deletions": 8
    }
}