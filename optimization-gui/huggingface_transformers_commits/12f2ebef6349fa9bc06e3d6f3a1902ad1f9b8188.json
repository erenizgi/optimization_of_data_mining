{
    "author": "yonigozlan",
    "message": "Support custom dosctrings in modular (#36726)\n\n* Override docstrings in modular if not none\n\n* Update doc",
    "sha": "12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
    "files": [
        {
            "sha": "badeab0214ae6da205ea25082b6d575e2c3b4b98",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
            "patch": "@@ -78,7 +78,7 @@ class RobertaModel(BertModel):\n     super().__init__(config)\n     self.embeddings = RobertaEmbeddings(config)\n \n-      \n+\n # The model heads now only need to redefine the model inside to `RobertaModel`\n class RobertaForMaskedLM(BertForMaskedLM):\n   def __init__(self, config):\n@@ -546,7 +546,7 @@ This makes it very easy to switch decorators and makes it explicit that the only\n \n ## Docstring variables\n \n-If an object defined in both the modular and modeling file from which it inherits, the modular definition has precedence unless for assignments containing the pattern `DOCSTRING`. These variables are typically used in `MODEL_START_DOCSTRING` and `MODEL_INPUT_DOCSTRING` in the modeling files. They are big blocks of docstrings and the linter rewrites the names everywhere. For this reason, assignments containing the `DOCSTRING` variable always uses the definition found in the source file instead of the modular file.\n+If an object defined in both the modular and modeling file from which it inherits, the modular definition has precedence unless for assignments containing the pattern `DOCSTRING`. These variables are typically used in `MODEL_START_DOCSTRING` and `MODEL_INPUT_DOCSTRING` in the modeling files. They are big blocks of docstrings and the linter rewrites the names everywhere. For this reason, assignments containing the `DOCSTRING` variable can use the definition found in the source file without copying the whole docstring, by simply setting the variable to `None` in the modular file.\n \n This is very useful if you need the variable reference somewhere but you don't want to clutter the modular file with docstrings which are always the same. The example code below allows you to automatically use the same docstrings from [Mistral](./model_doc/mistral) in [Starcoder2](./model_doc/starcoder2).\n \n@@ -561,6 +561,8 @@ class Starcoder2Model(MistralModel):\n         ...\n ```\n \n+Setting the variable to anything other than `None` will override the docstring, so that you can customize the docstrings if needed.\n+\n ## Special naming\n \n The linter automatically renames everything when inheriting from a class. For consistency, you should always use the same class name prefix when inheriting from different classes from the same file.\n@@ -586,7 +588,7 @@ We detected multiple prefix names when inheriting from transformers.models.llama\n If there are automatic dependencies with a prefix, but you want another one, explicitly rename the classes locally with a `pass` class as shown in the following.\n \n ```py\n-class Emu3TextMLP(LlamaMLP):                                 \n+class Emu3TextMLP(LlamaMLP):\n     pass\n ```\n "
        },
        {
            "sha": "d96fdbb4d666355630163cccb537a851a9ff47cb",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
            "patch": "@@ -57,7 +57,7 @@\n \n logger = logging.get_logger(__name__)\n \n-GEMMA3_INPUTS_DOCSTRING = \"\"\n+GEMMA3_INPUTS_DOCSTRING = None  # Will be picked up by modular\n \n \n class Gemma3TextConfig(Gemma2Config):"
        },
        {
            "sha": "fce0ae86e02f0d0ff1ee45f26d362aa1cf9311aa",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
            "patch": "@@ -625,10 +625,10 @@ def _init_weights(self, module):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):\n             The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`GotOcr2Processor`] uses\n-            [`CLIPImageProcessor`] for processing images).\n+            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses\n+            [`GotOcr2ImageProcessor`] for processing images.\n         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -667,13 +667,6 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n-        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n-            The index of the layer to select the vision feature. If multiple indices are provided,\n-            the vision feature of the corresponding indices will be concatenated to form the\n-            vision features.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`.\n         use_cache (`bool`, *optional*):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n             `past_key_values`)."
        },
        {
            "sha": "db7ca86631d14fecd65904292b5b68672b080daf",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
            "patch": "@@ -290,6 +290,10 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses\n+            [`GotOcr2ImageProcessor`] for processing images.\n         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -331,10 +335,6 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n         use_cache (`bool`, *optional*):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n             `past_key_values`).\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -343,10 +343,10 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        pixel_values (`torch.FloatTensor` of shape `(seq_length, num_channels * image_size * image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`GotOcr2Processor`] uses\n-            [`GotOcr2ImageProcessor`] for processing images.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n "
        },
        {
            "sha": "aedbf696d5b268c3d8a42a1d229c17c3b863504f",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=12f2ebef6349fa9bc06e3d6f3a1902ad1f9b8188",
            "patch": "@@ -537,6 +537,9 @@ def forward(...):\n # Top-level variables that match the following patterns will always use the value in the `modular_xxx.py` file\n ASSIGNMENTS_REGEX_TO_KEEP = [r\"_CHECKPOINT\", r\"_EXPECTED\", r\"_FOR_DOC\"]\n \n+# Top-level variables that match the following patterns will use the value in the `modular_xxx.py` file only if they are not None\n+ASSIGNMENTS_REGEX_TO_KEEP_IF_NOT_NONE = [r\"_DOCSTRING\"]\n+\n \n class ClassDependencyMapper(CSTVisitor):\n     \"\"\"A visitor which is designed to analyze a single class node to get all its dependencies that are shared with the set of\n@@ -854,13 +857,17 @@ def _merge_assignments(self, assignments: dict[str, cst.CSTNode], object_mapping\n         \"\"\"Update the global nodes with the assignment from the modular file.\n \n         Merging rule: if any assignment with the same name was redefined in the modular, we use it and its dependencies ONLY if it matches\n-        a pattern in `ASSIGNMENTS_REGEX_TO_KEEP`. Otherwise, we use the original value and dependencies. This rule was chosen to avoid having to rewrite the\n-        big docstrings.\n+        a pattern in `ASSIGNMENTS_REGEX_TO_KEEP_IF_NOT_NONE` and its value is not None, or if it matches a pattern in `ASSIGNMENTS_REGEX_TO_KEEP.\n+        Otherwise, we use the original value and dependencies. This rule was chosen to avoid having to rewrite the big docstrings.\n         \"\"\"\n         for assignment, node in assignments.items():\n             should_keep = any(re.search(pattern, assignment) for pattern in ASSIGNMENTS_REGEX_TO_KEEP)\n \n-            if should_keep or assignment not in self.assignments:\n+            should_keep_if_not_none = any(\n+                re.search(pattern, assignment) for pattern in ASSIGNMENTS_REGEX_TO_KEEP_IF_NOT_NONE\n+            ) and not (hasattr(node.body[0].value, \"value\") and node.body[0].value.value == \"None\")\n+\n+            if should_keep or should_keep_if_not_none or assignment not in self.assignments:\n                 self.assignments[assignment] = node\n                 if assignment in object_mapping:\n                     self.object_dependency_mapping[assignment] = object_mapping[assignment]"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 27,
        "deletions": 25
    }
}