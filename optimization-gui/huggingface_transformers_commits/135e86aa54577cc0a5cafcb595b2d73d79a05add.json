{
    "author": "ArthurZucker",
    "message": "Remove read_video and run",
    "sha": "135e86aa54577cc0a5cafcb595b2d73d79a05add",
    "files": [
        {
            "sha": "25e201a6e48d83f8444e55e00c410524a6e003a7",
            "filename": "read_video.py",
            "status": "removed",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/88b95e6179ad330b65f402daaf70565f1a89a0e3/read_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88b95e6179ad330b65f402daaf70565f1a89a0e3/read_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/read_video.py?ref=88b95e6179ad330b65f402daaf70565f1a89a0e3",
            "patch": "@@ -1,77 +0,0 @@\n-import numpy as np\n-import cv2\n-import requests\n-from yt_dlp import YoutubeDL\n-from contextlib import redirect_stdout\n-from pathlib import Path\n-import io\n-import imageio.v3 as iio\n-\n-\n-url = \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\"\n-vid = cv2.VideoCapture(url)\n-# ret, frame = vid.read()\n-\n-while(True):\n-    # Capture frame-by-frame\n-    ret, frame = vid.read()\n-    #print cap.isOpened(), ret\n-    if frame is not None:\n-        pass\n-        # print(frame.shape)\n-    else:\n-        break\n-\n-print(vid.isOpened(), frame is not None)\n-\n-buffer = io.BytesIO(requests.get(url).content)\n-video = buffer.getvalue()\n-frames = iio.imread(video, index=None)\n-print(frames.shape)\n-\n-\n-\n-\n-\n-youtube_id = \"https://www.youtube.com/watch?v=BaW_jenozKc\"\n-\n-ctx = {\n-    \"outtmpl\": \"-\",\n-    'logtostderr': True\n-}\n-\n-buffer = io.BytesIO()\n-with redirect_stdout(buffer), YoutubeDL(ctx) as foo:\n-    foo.download([youtube_id])\n-# Path(f\"vi.mp4\").write_bytes(buffer.getvalue())\n-\n-video = buffer.getvalue()\n-print(type(video))\n-frames = iio.imread(video, index=None)\n-print(frames.shape)\n-\n-\n-import decord\n-file_obj = io.BytesIO(video)\n-container = decord.VideoReader(file_obj)\n-print(container[2].shape)\n-\n-# print(np.frombuffer(video, dtype=np.uint8).shape)\n-# img_array = np.asarray(bytearray(video), dtype=np.uint8)\n-# im = cv2.imdecode(img_array, cv2.IMREAD_UNCHANGED)\n-\n-\n-\n-import av\n-\n-file_obj = io.BytesIO(video)\n-container = av.open(file_obj)\n-container.seek(0)\n-frames = []\n-for i, frame in enumerate(container.decode(video=0)):\n-    if i > 10:\n-        break\n-    if i >= 0:\n-        frames.append(frame)\n-out = np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-print(out.shape)"
        },
        {
            "sha": "b79ba1ecf3fb9f5c7f5e37b9f2ff5d5821da9057",
            "filename": "run.py",
            "status": "removed",
            "additions": 0,
            "deletions": 107,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/88b95e6179ad330b65f402daaf70565f1a89a0e3/run.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88b95e6179ad330b65f402daaf70565f1a89a0e3/run.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/run.py?ref=88b95e6179ad330b65f402daaf70565f1a89a0e3",
            "patch": "@@ -1,107 +0,0 @@\n-import av\n-import torch\n-import decord\n-from decord import VideoReader, cpu\n-\n-import numpy as np\n-from PIL import Image\n-from huggingface_hub import hf_hub_download\n-from transformers import LlavaNextVideoProcessor, LlavaNextVideoForConditionalGeneration, SiglipImageProcessor\n-\n-model_id = \"/raid/raushan/llava-next-video-qwen-7b\"\n-\n-model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n-    model_id, \n-    torch_dtype=torch.bfloat16, \n-    low_cpu_mem_usage=True, \n-).to(0)\n-\n-processor = LlavaNextVideoProcessor.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n-img_proc = SiglipImageProcessor.from_pretrained(\"google/siglip-so400m-patch14-384\")\n-\n-image = Image.open(\"/raid/raushan/image.png\")\n-\n-\n-def load_video(video_path, max_frames_num,fps=1,force_sample=False):\n-\n-    vr = VideoReader(video_path)\n-    total_frame_num = len(vr)\n-    video_time = total_frame_num / vr.get_avg_fps()\n-    fps = round(vr.get_avg_fps()/fps)\n-    frame_idx = [i for i in range(0, len(vr), fps)]\n-    frame_time = [i/fps for i in frame_idx]\n-    if len(frame_idx) > max_frames_num or force_sample:\n-        sample_fps = max_frames_num\n-        uniform_sampled_frames = np.linspace(0, total_frame_num - 1, sample_fps, dtype=int)\n-        frame_idx = uniform_sampled_frames.tolist()\n-        frame_time = [i/vr.get_avg_fps() for i in frame_idx]\n-    frame_time = \",\".join([f\"{i:.2f}s\" for i in frame_time])\n-    spare_frames = vr.get_batch(frame_idx).asnumpy()\n-    print(spare_frames.shape)\n-    return spare_frames,frame_time,video_time\n-\n-\n-def read_video_pyav(container, indices):\n-    '''\n-    Decode the video with PyAV decoder.\n-    Args:\n-        container (`av.container.input.InputContainer`): PyAV container.\n-        indices (`List[int]`): List of frame indices to decode.\n-    Returns:\n-        result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n-    '''\n-    frames = []\n-    container.seek(0)\n-    start_index = indices[0]\n-    end_index = indices[-1]\n-    for i, frame in enumerate(container.decode(video=0)):\n-        if i > end_index:\n-            break\n-        if i >= start_index and i in indices:\n-            frames.append(frame)\n-    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n-\n-\n-# define a chat history and use `apply_chat_template` to get correctly formatted prompt\n-# Each value in \"content\" has to be a list of dicts with types (\"text\", \"image\", \"video\") \n-# <|im_start|>system\n-# You are a helpful assistant.<|im_end|>\n-# <|im_start|>user\n-# <image>Time farmes are this moments and we ahev 64 frames\n-# Please describe this video in detail.<|im_end|>\n-# <|im_start|>assistant\n-\n-conversation = [\n-    {\n-\n-        \"role\": \"system\",\n-        \"content\": [\n-            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"},\n-            ],\n-    },\n-    {\n-\n-        \"role\": \"user\",\n-        \"content\": [\n-            {\"type\": \"text\", \"text\": \"The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.\"},\n-            {\"type\": \"video\"},\n-            ],\n-    },\n-]\n-\n-prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n-prompt = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\n<video>The video lasts for 19.97 seconds, and 64 frames are uniformly sampled from it. These frames are located at 0.00s,0.30s,0.60s,0.93s,1.23s,1.57s,1.87s,2.20s,2.50s,2.83s,3.13s,3.47s,3.77s,4.10s,4.40s,4.73s,5.03s,5.37s,5.67s,6.00s,6.30s,6.63s,6.93s,7.27s,7.57s,7.90s,8.20s,8.53s,8.83s,9.17s,9.47s,9.80s,10.10s,10.43s,10.73s,11.07s,11.37s,11.70s,12.00s,12.33s,12.63s,12.97s,13.27s,13.60s,13.90s,14.23s,14.53s,14.87s,15.17s,15.50s,15.80s,16.13s,16.43s,16.77s,17.07s,17.40s,17.70s,18.03s,18.33s,18.67s,18.97s,19.30s,19.60s,19.93s.Please answer the following questions related to this video.\\nPlease describe this video in detail.<|im_end|>\\n<|im_start|>assistant\"\n-\n-video_path = \"/raid/raushan/karate.mp4\" # hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\")\n-container = av.open(video_path)\n-\n-# sample uniformly 8 frames from the video, can sample more for longer videos\n-total_frames = container.streams.video[0].frames\n-indices = np.arange(0, total_frames, total_frames / 64).astype(int)\n-clip = read_video_pyav(container, indices)\n-\n-clip, frame_time,video_time = load_video(video_path, max_frames_num=64, force_sample=True)\n-inputs_video = processor(text=prompt, videos=clip, return_tensors=\"pt\").to(device=model.device, dtype=torch.bfloat16)\n-\n-output = model.generate(**inputs_video, max_new_tokens=100, do_sample=False)\n-print(processor.decode(output[0][2:], skip_special_tokens=True))"
        }
    ],
    "stats": {
        "total": 184,
        "additions": 0,
        "deletions": 184
    }
}