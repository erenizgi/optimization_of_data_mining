{
    "author": "Cyrilvallez",
    "message": "Import `expand_device_map` instead of redefining it (#41608)\n\nremove it",
    "sha": "c4210796e073f1a85b8140b7cecba0d5d6d0c71c",
    "files": [
        {
            "sha": "adf0dd5440581302b2f00dbda7a2b4dd5aca1026",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4210796e073f1a85b8140b7cecba0d5d6d0c71c/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4210796e073f1a85b8140b7cecba0d5d6d0c71c/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c4210796e073f1a85b8140b7cecba0d5d6d0c71c",
            "patch": "@@ -54,6 +54,7 @@\n     accelerate_disk_offload,\n     accelerate_dispatch,\n     check_and_set_device_map,\n+    expand_device_map,\n     find_tied_parameters,\n     init_empty_weights,\n )\n@@ -5298,18 +5299,6 @@ def unwrap_model(model: nn.Module, recursive: bool = False) -> nn.Module:\n             return model\n \n \n-def expand_device_map(device_map, param_names):\n-    \"\"\"\n-    Expand a device map to return the correspondence parameter name to device.\n-    \"\"\"\n-    new_device_map = {}\n-    for module, device in device_map.items():\n-        new_device_map.update(\n-            {p: device for p in param_names if p == module or p.startswith(f\"{module}.\") or module == \"\"}\n-        )\n-    return new_device_map\n-\n-\n def is_accelerator_device(device: Union[str, int, torch.device]) -> bool:\n     \"\"\"Check if the device is an accelerator. We need to function, as device_map can be \"disk\" as well, which is not\n     a proper `torch.device`."
        }
    ],
    "stats": {
        "total": 13,
        "additions": 1,
        "deletions": 12
    }
}