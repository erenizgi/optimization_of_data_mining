{
    "author": "NielsRogge",
    "message": "[Pixtral] Improve docs, rename model (#33491)\n\n* Improve docs, rename model\r\n\r\n* Fix style\r\n\r\n* Update repo id",
    "sha": "06e27e3dc08b51e0d564c70738ead3627c324c0c",
    "files": [
        {
            "sha": "ed5c67b7b5cb341052fe6d474fa0fbfb30a0db1b",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -255,7 +255,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                          [Phi3](model_doc/phi3)                          |       ✅        |         ❌         |      ❌      |\n |                       [PhoBERT](model_doc/phobert)                       |       ✅        |         ✅         |      ✅      |\n |                    [Pix2Struct](model_doc/pix2struct)                    |       ✅        |         ❌         |      ❌      |\n-|                       [Pixtral](model_doc/pixtral)                       |       ❌        |         ❌         |      ❌      |\n+|                       [Pixtral](model_doc/pixtral)                       |       ✅        |         ❌         |      ❌      |\n |                        [PLBart](model_doc/plbart)                        |       ✅        |         ❌         |      ❌      |\n |                    [PoolFormer](model_doc/poolformer)                    |       ✅        |         ❌         |      ❌      |\n |                     [Pop2Piano](model_doc/pop2piano)                     |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "c21938698db04e2ba3ec04d13dcb716ebd809bbf",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -18,20 +18,22 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Pixtral model was released by the Mistral AI team on [Vllm](https://github.com/vllm-project/vllm/pull/8377), where a version of the code can be found!\n-\n+The Pixtral model was released by the Mistral AI team on [vLLM](https://github.com/vllm-project/vllm/pull/8377), where a version of the code can be found!\n \n Tips:\n \n-- Pixtral is a multimodal model, the main contribution is the 2d ROPE on the images, and support for arbitrary image size (the images are not padded together nor are they resized)\n-- This model follows the `Llava` familiy, meaning image embeddings are placed instead of the `[IMG]` token placeholders.\n+- Pixtral is a multimodal model, taking images and text as input, and producing text as output.\n+- This model follows the [Llava](llava) family, meaning image embeddings are placed instead of the `[IMG]` token placeholders. The model uses [`PixtralVisionModel`] for its vision encoder, and [`MistralForCausalLM`] for its language decoder.\n+- The main contribution is the 2d ROPE (rotary postiion embeddings) on the images, and support for arbitrary image sizes (the images are not padded together nor are they resized).\n - The format for one or mulitple prompts is the following:\n ```\n \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n ```\n Then, the processor will replace each `[IMG]` token with  a number of `[IMG]` token that depends on the height and the width of the image. Each *row* of the image is separated by a `[IMG_BREAK]` token, and each image is separated by a  `[IMG_END]` token.\n \n-This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ)\n+This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/vllm-project/vllm/pull/8377).\n+\n+## Usage\n \n Here is an example of how to run it:\n \n@@ -83,9 +85,9 @@ Each image captures a different scene, from a close-up of a dog to expansive nat\n \n [[autodoc]] PixtralVisionConfig\n \n-## PixtralModel\n+## PixtralVisionModel\n \n-[[autodoc]] PixtralModel\n+[[autodoc]] PixtralVisionModel\n     - forward\n \n ## PixtralImageProcessor"
        },
        {
            "sha": "2d14e6364f74be1ad0801142372f6547108a8d51",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -2994,7 +2994,7 @@\n             \"Pix2StructVisionModel\",\n         ]\n     )\n-    _import_structure[\"models.pixtral\"].extend([\"PixtralModel\", \"PixtralPreTrainedModel\"])\n+    _import_structure[\"models.pixtral\"].extend([\"PixtralVisionModel\", \"PixtralPreTrainedModel\"])\n     _import_structure[\"models.plbart\"].extend(\n         [\n             \"PLBartForCausalLM\",\n@@ -7486,8 +7486,8 @@\n             Pix2StructVisionModel,\n         )\n         from .models.pixtral import (\n-            PixtralModel,\n             PixtralPreTrainedModel,\n+            PixtralVisionModel,\n         )\n         from .models.plbart import (\n             PLBartForCausalLM,"
        },
        {
            "sha": "dce849b1b4bfe8c3d1b3d00010a7dfb31ed1faa8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -195,7 +195,7 @@\n         (\"persimmon\", \"PersimmonModel\"),\n         (\"phi\", \"PhiModel\"),\n         (\"phi3\", \"Phi3Model\"),\n-        (\"pixtral\", \"PixtralModel\"),\n+        (\"pixtral\", \"PixtralVisionModel\"),\n         (\"plbart\", \"PLBartModel\"),\n         (\"poolformer\", \"PoolFormerModel\"),\n         (\"prophetnet\", \"ProphetNetModel\"),"
        },
        {
            "sha": "128fd3ebe0485a97d270dcf953174623f87c8263",
            "filename": "src/transformers/models/pixtral/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2F__init__.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -29,7 +29,7 @@\n     pass\n else:\n     _import_structure[\"modeling_pixtral\"] = [\n-        \"PixtralModel\",\n+        \"PixtralVisionModel\",\n         \"PixtralPreTrainedModel\",\n     ]\n \n@@ -53,8 +53,8 @@\n         pass\n     else:\n         from .modeling_pixtral import (\n-            PixtralModel,\n             PixtralPreTrainedModel,\n+            PixtralVisionModel,\n         )\n \n     try:"
        },
        {
            "sha": "32325a929411ba58c6ec3d5f83ae73dc7c8f77fe",
            "filename": "src/transformers/models/pixtral/configuration_pixtral.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fconfiguration_pixtral.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -22,9 +22,9 @@\n \n class PixtralVisionConfig(PretrainedConfig):\n     r\"\"\"\n-    This is the configuration class to store the configuration of a [`PixtralModel`]. It is used to instantiate an\n-    Pixtral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n-    with the defaults will yield a similar configuration to that of the Pixtral-9B.\n+    This is the configuration class to store the configuration of a [`PixtralVisionModel`]. It is used to instantiate an\n+    Pixtral vision encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to the vision encoder used by Pixtral-12B.\n \n     e.g. [pixtral-hf/pixtral-9b](https://huggingface.co/pixtral-hf/pixtral-9b)\n \n@@ -52,19 +52,17 @@ class PixtralVisionConfig(PretrainedConfig):\n             Dropout probability for the attention layers.\n         rope_theta (`float`, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings.\n-        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n-            Whether to tie the word embeddings with the input embeddings.\n \n     Example:\n \n     ```python\n-    >>> from transformers import PixtralModel, PixtralVisionConfig, CLIPVisionConfig, LlamaConfig\n+    >>> from transformers import PixtralVisionModel, PixtralVisionConfig\n \n-    >>> # Initializing a Pixtral 12B style configuration\n+    >>> # Initializing a Pixtral-12B style configuration\n     >>> config = PixtralVisionConfig()\n \n-    >>> # Initializing a model from the pixtral 12B style configuration\n-    >>> model = PixtralModel(configuration)\n+    >>> # Initializing a model (with randomly initialized weights) from the configuration\n+    >>> model = PixtralVisionModel(configuration)\n \n     >>> # Accessing the model configuration\n     >>> configuration = model.config\n@@ -84,7 +82,6 @@ def __init__(\n         hidden_act=\"gelu\",\n         attention_dropout=0.0,\n         rope_theta=10000.0,\n-        tie_word_embeddings=False,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -99,5 +96,4 @@ def __init__(\n         self.attention_dropout = attention_dropout\n         self.hidden_act = hidden_act\n         self.rope_theta = rope_theta\n-        self.tie_word_embeddings = tie_word_embeddings\n         self.head_dim = hidden_size // num_attention_heads"
        },
        {
            "sha": "06b9701a75661a870fb06bce84656cb9cd7ec80e",
            "filename": "src/transformers/models/pixtral/modeling_pixtral.py",
            "status": "modified",
            "additions": 14,
            "deletions": 24,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fmodeling_pixtral.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -1,5 +1,5 @@\n # coding=utf-8\n-# Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n+# Copyright 2024 Mistral and the HuggingFace Inc. team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -48,15 +48,13 @@ def position_ids_in_meshgrid(patch_embeds_list, max_width):\n class PixtralRotaryEmbedding(nn.Module):\n     \"\"\"\n     The key with pixtral embedding is just that you have a frequency for each pixel positions.\n-    If you have height x width pixels (or embedding pixels)\n+    If you have height x width pixels (or embedding pixels), then the frequency used for ROPE\n+    is given by indexing the pre_computed frequency on the width and height.\n \n-    then the frequency used for ROPE is given by indexing the pre_computed frequency on the\n-    width and height.\n+    What you output is of dimension (batch, height * width, dim) with dim the embed dim.\n \n-    What you output is of dimension batch, height * width, dim with dim the embed dim.\n-\n-    This simply means that for each image hidden states, you are going to add\n-    a corresponding positional embedding, based on it's index in the grid.\n+    This simply means that for each image hidden state, you are going to add\n+    a corresponding positional embedding, based on its index in the grid.\n     \"\"\"\n \n     def __init__(self, config, device):\n@@ -319,9 +317,7 @@ def forward(\n         r\"\"\"\n         Args:\n             inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n-                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n-                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n-                than the model's internal embedding lookup matrix.\n+                Embeddings which serve as input to the Transformer.\n             attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -392,17 +388,13 @@ def forward(\n     and behavior.\n \n     Parameters:\n-        config ([`PixtralVisionConfig`] or [`PixtralVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+        config ([`PixtralVisionConfig`]):\n+            Model configuration class with all the parameters of the vision encoder. Initializing with a config file does not\n             load the weights associated with the model, only the configuration. Check out the\n             [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n \"\"\"\n \n \n-@add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n-    PIXTRAL_START_DOCSTRING,\n-)\n class PixtralPreTrainedModel(PreTrainedModel):\n     config_class = PixtralVisionConfig\n     base_model_prefix = \"model\"\n@@ -412,9 +404,6 @@ class PixtralPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n \n     def _init_weights(self, module):\n-        # important: this ported version of Pixtral isn't meant for training from scratch - only\n-        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n-        # https://github.com/haotian-liu/LLaVA/tree/main/pixtral should serve for that purpose\n         std = (\n             self.config.initializer_range\n             if hasattr(self.config, \"initializer_range\")\n@@ -433,8 +422,9 @@ def _init_weights(self, module):\n \n PIXTRAL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n-        pixel_values: list of N_img images of variable sizes,\n-                each of shape (C, H, W)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See [`AutoImageProcessor.__call__`]\n+            for details.\n         output_attentions (`bool`, *optional*):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more detail.\n@@ -463,10 +453,10 @@ def generate_block_attention_mask(patch_embeds_list, tensor):\n \n \n @add_start_docstrings(\n-    \"\"\"The PIXTRAL model which consists of a vision backbone and a language model.\"\"\",\n+    \"The bare Pixtral vision encoder outputting raw hidden-states without any specific head on top.\",\n     PIXTRAL_START_DOCSTRING,\n )\n-class PixtralModel(PixtralPreTrainedModel):\n+class PixtralVisionModel(PixtralPreTrainedModel):\n     base_model_prefix = \"vision_encoder\"\n \n     def __init__(self, config):"
        },
        {
            "sha": "0866cccd655db8a3ed3650510ca5ec8d7125f376",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -7102,14 +7102,14 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class PixtralModel(metaclass=DummyObject):\n+class PixtralPreTrainedModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n-class PixtralPreTrainedModel(metaclass=DummyObject):\n+class PixtralVisionModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n \n     def __init__(self, *args, **kwargs):"
        },
        {
            "sha": "9a128f6ad2882335f935486e8e3ab39856e12ad8",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/06e27e3dc08b51e0d564c70738ead3627c324c0c/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/06e27e3dc08b51e0d564c70738ead3627c324c0c/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=06e27e3dc08b51e0d564c70738ead3627c324c0c",
            "patch": "@@ -21,8 +21,8 @@\n \n from transformers import (\n     AutoProcessor,\n-    PixtralModel,\n     PixtralVisionConfig,\n+    PixtralVisionModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -46,7 +46,7 @@\n     from PIL import Image\n \n \n-class PixtralModelTester:\n+class PixtralVisionModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -107,7 +107,7 @@ def get_config(self):\n         )\n \n     def create_and_check_model(self, config, pixel_values):\n-        model = PixtralModel(config=config)\n+        model = PixtralVisionModel(config=config)\n         model.to(torch_device)\n         model.eval()\n         with torch.no_grad():\n@@ -120,7 +120,7 @@ def create_and_check_model(self, config, pixel_values):\n         self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n \n     def create_and_check_model_with_projection(self, config, pixel_values):\n-        model = PixtralModel(config=config)\n+        model = PixtralVisionModel(config=config)\n         model.to(torch_device)\n         model.eval()\n         with torch.no_grad():\n@@ -140,17 +140,17 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class PixtralModelModelTest(ModelTesterMixin, unittest.TestCase):\n+class PixtralVisionModelModelTest(ModelTesterMixin, unittest.TestCase):\n     \"\"\"\n-    Model tester for `PixtralModel`.\n+    Model tester for `PixtralVisionModel`.\n     \"\"\"\n \n-    all_model_classes = (PixtralModel,) if is_torch_available() else ()\n+    all_model_classes = (PixtralVisionModel,) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n \n     def setUp(self):\n-        self.model_tester = PixtralModelTester(self)\n+        self.model_tester = PixtralVisionModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=PixtralVisionConfig, has_text_modality=False)\n \n     @unittest.skip(\"model does not support input embeds\")\n@@ -261,7 +261,7 @@ def test_determinism(self):\n \n \n @require_torch\n-class PixtralModelIntegrationTest(unittest.TestCase):\n+class PixtralVisionModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"hf-internal-testing/pixtral-12b\")\n \n@@ -273,7 +273,7 @@ def tearDown(self):\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n-        model = PixtralModel.from_pretrained(\"hf-internal-testing/pixtral-12b\", load_in_4bit=True)\n+        model = PixtralVisionModel.from_pretrained(\"hf-internal-testing/pixtral-12b\", load_in_4bit=True)\n \n         prompt = \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n         image_file = \"https://pixtral-vl.github.io/static/images/view.jpg\""
        }
    ],
    "stats": {
        "total": 108,
        "additions": 48,
        "deletions": 60
    }
}