{
    "author": "winglian",
    "message": "Fixes needed for n-d parallelism and TP (#39562)\n\nHandle non-DTensors cases in TP Layers\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "a44dcbe513e3e073271e0b8e369b75aca51affae",
    "files": [
        {
            "sha": "d765e0b6849410c5f88f9c4500cd45d85dbc197f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a44dcbe513e3e073271e0b8e369b75aca51affae/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a44dcbe513e3e073271e0b8e369b75aca51affae/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=a44dcbe513e3e073271e0b8e369b75aca51affae",
            "patch": "@@ -487,7 +487,7 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n \n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n-        return outputs.to_local() if use_local_output else outputs\n+        return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n \n     def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n         param = param[...].to(param_casting_dtype)\n@@ -556,7 +556,7 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         if outputs.placements != output_layouts:\n             outputs = outputs.redistribute(placements=output_layouts, async_op=False)\n         # back to local tensor\n-        return outputs.to_local() if use_local_output else outputs\n+        return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n \n \n class PackedColwiseParallel(ColwiseParallel):\n@@ -650,7 +650,7 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         if hasattr(mod, \"_bias\"):\n             outputs += mod._bias\n         # back to local tensor if use_local_output is True\n-        return outputs.to_local() if use_local_output else outputs\n+        return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         module._distribute_module_applied = True"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}