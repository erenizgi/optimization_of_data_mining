{
    "author": "cyyever",
    "message": "Unify is_torchvision_v2_available with is_torchvision_available (#41227)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "919a4845fb4f510c73e911c4ee05802fbbebf64f",
    "files": [
        {
            "sha": "241c12923bdb578c7d1a9ba665b04b00ffe9946b",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -39,12 +39,7 @@\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import (\n-    TensorType,\n-    filter_out_non_signature_kwargs,\n-    is_vision_available,\n-    logging,\n-)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n \n \n if is_vision_available():"
        },
        {
            "sha": "c04e006e358d5fcc157cedabc3ba18137cf2eea6",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -21,6 +21,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -39,13 +40,7 @@\n     pil_torch_interpolation_mapping,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import TensorType, auto_docstring\n \n \n class DeepseekVLHybridFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):"
        },
        {
            "sha": "d9a85654e901576c61c83c6b3ce3c3ea54d3aeaf",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -29,11 +29,7 @@\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    TransformersKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-)\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_deepseek_vl_hybrid import DeepseekVLHybridConfig\n "
        },
        {
            "sha": "18b416a57df2ee7895accfa27149f11a02c35271",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -16,6 +16,7 @@\n \n import torch\n import torch.nn as nn\n+from torchvision.transforms.v2 import functional as F\n \n from ...cache_utils import Cache\n from ...image_processing_utils_fast import (\n@@ -53,7 +54,6 @@\n     auto_docstring,\n     can_return_tuple,\n     filter_out_non_signature_kwargs,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel\n@@ -70,12 +70,6 @@\n from ..sam.modeling_sam import SamLayerNorm, SamVisionNeck\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "892ddd7c3d6f5ffca8332c503be07f1c1042a341",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -25,6 +25,7 @@\n from typing import TYPE_CHECKING, Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_base import BatchFeature\n from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n@@ -39,17 +40,12 @@\n     is_torch_tensor,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available, requires_backends\n+from ...utils import TensorType, auto_docstring, requires_backends\n \n \n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n \n class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\""
        },
        {
            "sha": "34eb08f39b684cfca10624b73a5669b9d9577632",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -32,7 +32,6 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n     requires_backends,\n )\n from ..beit.image_processing_beit_fast import BeitImageProcessorFast\n@@ -41,10 +40,7 @@\n if TYPE_CHECKING:\n     from ...modeling_outputs import DepthEstimatorOutput\n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from torchvision.transforms.v2 import functional as F\n \n \n def get_resize_output_image_size("
        },
        {
            "sha": "11872cb67bf3a8806c08c08ab698a99cbbfaa006",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -22,6 +22,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_processing_utils_fast import (\n@@ -41,13 +42,7 @@\n     get_image_size,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import TensorType, auto_docstring\n \n \n class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):"
        },
        {
            "sha": "b4f64dee8e041096dc7b648f23341e3eb5c11a1c",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -18,6 +18,7 @@\n \n import torch\n from torch import nn\n+from torchvision.transforms.v2 import functional as F\n \n from transformers.models.llava_next.image_processing_llava_next_fast import LlavaNextImageProcessorFast\n from transformers.models.llava_next_video.modeling_llava_next_video import (\n@@ -50,16 +51,10 @@\n     TensorType,\n     auto_docstring,\n     can_return_tuple,\n-    is_torchvision_v2_available,\n     logging,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "417fc800ea88b59782d8fcc18bc326c37466fc54",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -23,6 +23,7 @@\n from typing import TYPE_CHECKING, Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import BaseImageProcessorFast, BatchFeature, DefaultFastImageProcessorKwargs\n from ...image_transforms import center_to_corners_format, group_images_by_shape, reorder_images\n@@ -35,16 +36,10 @@\n     SizeDict,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n+from ...utils import TensorType, auto_docstring\n from .image_processing_owlv2 import _scale_boxes, box_iou\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n if TYPE_CHECKING:\n     from .modeling_owlv2 import Owlv2ObjectDetectionOutput\n "
        },
        {
            "sha": "66acd2088399c74751edea26e7f307f380890926",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -18,6 +18,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n@@ -37,17 +38,10 @@\n from ...utils import (\n     TensorType,\n     auto_docstring,\n-    is_torchvision_v2_available,\n )\n from ..owlvit.image_processing_owlvit_fast import OwlViTImageProcessorFast\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n \n "
        },
        {
            "sha": "1dfffd31082c85b708bfb31227c52da918bf3e3b",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -8,6 +8,7 @@\n from typing import Any, Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -36,12 +37,6 @@\n from .image_processing_rt_detr import get_size_with_aspect_ratio\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class RTDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     r\"\"\"\n     format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):"
        },
        {
            "sha": "61bd055144f0504aaac3ab256b3565f82411a87d",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -2,6 +2,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from transformers.models.detr.image_processing_detr_fast import DetrFastImageProcessorKwargs, DetrImageProcessorFast\n \n@@ -22,18 +23,11 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    is_torchvision_v2_available,\n     logging,\n     requires_backends,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION,)"
        },
        {
            "sha": "79d5b015f889933acd7c569deffcb806e5e04985",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -39,10 +39,7 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import compile_compatible_method_lru_cache\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-)\n+from ...utils import ModelOutput, auto_docstring\n from ...utils.generic import OutputRecorder, TransformersKwargs\n from ..auto import AutoModel\n from .configuration_sam2_video import Sam2VideoConfig, Sam2VideoMaskDecoderConfig, Sam2VideoPromptEncoderConfig"
        },
        {
            "sha": "b95a9f778251a6ced837d74c5ba9ae343fc608f8",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -36,8 +36,6 @@\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n-    is_torchvision_available,\n-    is_torchvision_v2_available,\n     logging,\n )\n from ...utils.generic import OutputRecorder, TransformersKwargs\n@@ -59,12 +57,6 @@\n from ..sam2.processing_sam2 import Sam2Processor\n \n \n-if is_torchvision_available() and is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "11dfa3c42ab1cb291298c73bdf60c88c9b813ad7",
            "filename": "src/transformers/models/segformer/image_processing_segformer_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fimage_processing_segformer_fast.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -22,6 +22,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n@@ -40,13 +41,7 @@\n     is_torch_tensor,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, auto_docstring, is_torchvision_v2_available\n-\n-\n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n+from ...utils import TensorType, auto_docstring\n \n \n class SegformerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n@@ -135,9 +130,7 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F.InterpolationMode.NEAREST,\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT,\n                 }\n             )\n             processed_segmentation_maps = self._preprocess("
        },
        {
            "sha": "831d046fd9a72d61913121cd69c2a8a178bc8491",
            "filename": "src/transformers/models/segformer/modular_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsegformer%2Fmodular_segformer.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -17,6 +17,7 @@\n from typing import Optional, Union\n \n import torch\n+from torchvision.transforms.v2 import functional as F\n \n from transformers.models.beit.image_processing_beit_fast import BeitFastImageProcessorKwargs, BeitImageProcessorFast\n \n@@ -36,16 +37,9 @@\n from ...processing_utils import Unpack\n from ...utils import (\n     TensorType,\n-    is_torchvision_v2_available,\n )\n \n \n-if is_torchvision_v2_available():\n-    from torchvision.transforms.v2 import functional as F\n-else:\n-    from torchvision.transforms import functional as F\n-\n-\n class SegformerFastImageProcessorKwargs(BeitFastImageProcessorKwargs):\n     pass\n \n@@ -96,9 +90,7 @@ def _preprocess_image_like_inputs(\n                     \"do_normalize\": False,\n                     \"do_rescale\": False,\n                     # Nearest interpolation is used for segmentation maps instead of BILINEAR.\n-                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT\n-                    if is_torchvision_v2_available()\n-                    else F.InterpolationMode.NEAREST,\n+                    \"interpolation\": F.InterpolationMode.NEAREST_EXACT,\n                 }\n             )\n             processed_segmentation_maps = self._preprocess("
        },
        {
            "sha": "07519ee865ac4957371ca82d57269fd7604dcde4",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/919a4845fb4f510c73e911c4ee05802fbbebf64f/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=919a4845fb4f510c73e911c4ee05802fbbebf64f",
            "patch": "@@ -400,11 +400,7 @@ def is_torchvision_available() -> bool:\n \n \n def is_torchvision_v2_available() -> bool:\n-    if not is_torchvision_available():\n-        return False\n-\n-    # NOTE: We require torchvision>=0.15 as v2 transforms are available from this version: https://pytorch.org/vision/stable/transforms.html#v1-or-v2-which-one-should-i-use\n-    return version.parse(_torchvision_version) >= version.parse(\"0.15\")\n+    return is_torchvision_available()\n \n \n def is_galore_torch_available() -> Union[tuple[bool, str], bool]:"
        }
    ],
    "stats": {
        "total": 136,
        "additions": 23,
        "deletions": 113
    }
}