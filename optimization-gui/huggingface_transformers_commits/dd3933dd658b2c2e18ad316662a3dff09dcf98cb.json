{
    "author": "Cyrilvallez",
    "message": "Simplify keep_in_fp32_modules logic (#36722)\n\n* better regex everywhere\n\n* fix\n\n* Update test_modeling_instructblip.py\n\n* BC with explanations this time otherwise it makes no sense at all\n\n* Update test_modeling_instructblip.py\n\n* style\n\n* CIs\n\n* update _keep_in_fp32_modules in blip2\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* style\n\n* CIs\n\n* add check\n\n* trigger CIs\n\n* Update modeling_utils.py\n\n* trigger CIs",
    "sha": "dd3933dd658b2c2e18ad316662a3dff09dcf98cb",
    "files": [
        {
            "sha": "7747003aeb4ac5f7ffc47701f61381fd3b2a00a2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 45,
            "deletions": 27,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=dd3933dd658b2c2e18ad316662a3dff09dcf98cb",
            "patch": "@@ -716,7 +716,7 @@ def _infer_parameter_dtype(\n     model: \"PreTrainedModel\",\n     param_name: str,\n     empty_param: torch.Tensor,\n-    keep_in_fp32_modules: Optional[List[str]] = None,\n+    keep_in_fp32_regex: Optional[re.Pattern] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n ) -> Union[bool, Optional[torch.dtype]]:\n     try:\n@@ -733,7 +733,7 @@ def _infer_parameter_dtype(\n     is_param_float8_e4m3fn = is_torch_e4m3fn_available and empty_param.dtype == torch.float8_e4m3fn\n     if empty_param.dtype.is_floating_point and not is_param_float8_e4m3fn:\n         # First fp32 if part of the exception list\n-        if keep_in_fp32_modules is not None and keep_in_fp32_modules.search(param_name):\n+        if keep_in_fp32_regex is not None and keep_in_fp32_regex.search(param_name):\n             casting_dtype = torch.float32\n         # Then dtype that was instantiated in the meta model -- note that this respects subconfigs dtypes\n         elif hf_quantizer is not None:\n@@ -757,7 +757,7 @@ def _load_state_dict_into_meta_model(\n     cpu_offload_index: Optional[Dict] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n     is_safetensors: bool = False,\n-    keep_in_fp32_modules: Optional[List[str]] = None,\n+    keep_in_fp32_regex: Optional[re.Pattern] = None,\n     unexpected_keys: Optional[List[str]] = None,  # passing `unexpected` for cleanup from quantization items\n     device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n ) -> Tuple[Optional[Dict], Optional[Dict]]:\n@@ -795,7 +795,7 @@ def _load_state_dict_into_meta_model(\n             model,\n             param_name,\n             empty_param,\n-            keep_in_fp32_modules,\n+            keep_in_fp32_regex,\n             hf_quantizer,\n         )\n \n@@ -1284,7 +1284,7 @@ def _get_device_map(\n     max_memory: Optional[Dict],\n     hf_quantizer: Optional[HfQuantizer],\n     torch_dtype: Optional[torch.dtype],\n-    keep_in_fp32_modules: Optional[List[str]],\n+    keep_in_fp32_regex: Optional[re.Pattern],\n ) -> Dict:\n     \"\"\"Compute the final `device_map` to use if we passed a value in ['auto', 'balanced', 'balanced_low_0', 'sequential'].\n     Otherwise, we check for any device inconsistencies in the device_map.\n@@ -1293,13 +1293,9 @@ def _get_device_map(\n         special_dtypes = {}\n         if hf_quantizer is not None:\n             special_dtypes.update(hf_quantizer.get_special_dtypes_update(model, torch_dtype))\n-        if keep_in_fp32_modules is not None:\n+        if keep_in_fp32_regex is not None:\n             special_dtypes.update(\n-                {\n-                    name: torch.float32\n-                    for name, _ in model.named_parameters()\n-                    if any(m in name for m in keep_in_fp32_modules)\n-                }\n+                {name: torch.float32 for name, _ in model.named_parameters() if keep_in_fp32_regex.search(name)}\n             )\n \n         target_dtype = torch_dtype\n@@ -1911,6 +1907,23 @@ def post_init(self):\n         self.init_weights()\n         self._backward_compatibility_gradient_checkpointing()\n \n+        # Make sure the modules correctly exist if the flag is active\n+        if self._keep_in_fp32_modules is not None:\n+            all_parameters = {name for name, _ in self.named_parameters() if len(name) > 0}\n+            unique_module_names = set()\n+            # Get all unique module names in the module graph, without the prefixes\n+            for param in all_parameters:\n+                unique_module_names.update(\n+                    [name for name in param.split(\".\") if not name.isnumeric() and name not in [\"weight\", \"bias\"]]\n+                )\n+            # Check that every module in the keep_in_fp32 list is part of the module graph\n+            for module in self._keep_in_fp32_modules:\n+                if module not in unique_module_names:\n+                    raise ValueError(\n+                        f\"{module} was specified in the `_keep_in_fp32_modules` list, but is not part of the modules in\"\n+                        f\" {self.__class__.__name__}\"\n+                    )\n+\n         # If current model is a base model, attach `base_model_tp_plan` and `base_model_pp_plan` from config\n         if self.base_model is self:\n             self._pp_plan = (\n@@ -4412,15 +4425,23 @@ def from_pretrained(\n         config = model.config\n \n         # Find fp32 modules if needed\n-        keep_in_fp32_modules = None\n-        if model._keep_in_fp32_modules is not None:\n-            if is_accelerate_available() and not is_deepspeed_zero3_enabled():\n-                low_cpu_mem_usage = True\n-            keep_in_fp32_modules = model._keep_in_fp32_modules if len(model._keep_in_fp32_modules) > 0 else None\n+        keep_in_fp32_regex = None\n+        # The _keep_in_fp32_modules flag is only used to avoid bf16 -> fp16 casting precision issues. It was introduced\n+        # in case of force loading a model that should stay bf16 in fp16 (which includes a few quantizers as this is a pre-processing\n+        # step for e.g. bitsandbytes). See https://github.com/huggingface/transformers/issues/20287 for details.\n+        if model._keep_in_fp32_modules is not None and (\n+            torch_dtype == torch.float16 or getattr(hf_quantizer, \"use_keep_in_fp32_modules\", False)\n+        ):\n+            # Only the path with `low_cpu_mem_usage` will check every param for the correct dtype\n+            low_cpu_mem_usage = True\n+            # We need to match exact layers, so we add either `.` on each side, or start/end of string\n+            keep_in_fp32_regex = re.compile(\n+                \"|\".join([rf\"((^|\\.){module}($|\\.))\" for module in model._keep_in_fp32_modules])\n+            )\n \n         if hf_quantizer is not None:\n             hf_quantizer.preprocess_model(\n-                model=model, device_map=device_map, keep_in_fp32_modules=keep_in_fp32_modules\n+                model=model, device_map=device_map, keep_in_fp32_modules=model._keep_in_fp32_modules\n             )\n \n             # We store the original dtype for quantized models as we cannot easily retrieve it\n@@ -4431,9 +4452,7 @@ def from_pretrained(\n \n         # Prepare the full device map\n         if device_map is not None:\n-            device_map = _get_device_map(\n-                model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_modules\n-            )\n+            device_map = _get_device_map(model, device_map, max_memory, hf_quantizer, torch_dtype, keep_in_fp32_regex)\n \n         # Finalize model weight initialization\n         if from_tf:\n@@ -4465,7 +4484,7 @@ def from_pretrained(\n                 offload_state_dict=offload_state_dict,\n                 dtype=torch_dtype,\n                 hf_quantizer=hf_quantizer,\n-                keep_in_fp32_modules=keep_in_fp32_modules,\n+                keep_in_fp32_regex=keep_in_fp32_regex,\n                 device_mesh=device_mesh,\n                 key_mapping=key_mapping,\n                 weights_only=weights_only,\n@@ -4674,7 +4693,7 @@ def _load_pretrained_model(\n         offload_state_dict: Optional[bool] = None,\n         dtype: Optional[torch.dtype] = None,\n         hf_quantizer: Optional[HfQuantizer] = None,\n-        keep_in_fp32_modules: Optional[List[str]] = None,\n+        keep_in_fp32_regex: Optional[re.Pattern] = None,\n         device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n         key_mapping: Optional[Dict[str, str]] = None,\n         weights_only: bool = True,\n@@ -4736,10 +4755,9 @@ def _load_pretrained_model(\n             model._initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n \n         # Set some modules to fp32 if needed\n-        if keep_in_fp32_modules is not None:\n-            keep_in_fp32_modules = re.compile(\"|\".join([re.escape(module) for module in keep_in_fp32_modules]))\n+        if keep_in_fp32_regex is not None:\n             for name, param in model.named_parameters():\n-                if keep_in_fp32_modules.search(name):\n+                if keep_in_fp32_regex.search(name):\n                     # param = param.to(torch.float32) does not work here as only in the local scope.\n                     param.data = param.data.to(torch.float32)\n \n@@ -4894,7 +4912,7 @@ def _load_pretrained_model(\n                         cpu_offload_index=cpu_offload_index,\n                         hf_quantizer=hf_quantizer,\n                         is_safetensors=is_offloaded_safetensors,\n-                        keep_in_fp32_modules=keep_in_fp32_modules,\n+                        keep_in_fp32_regex=keep_in_fp32_regex,\n                         unexpected_keys=unexpected_keys,\n                         device_mesh=device_mesh,\n                     )\n@@ -4951,7 +4969,7 @@ def _load_pretrained_model(\n                 }\n                 for name, param in parameters_to_initialize.items():\n                     # First move data to correct\n-                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_modules)\n+                    to_contiguous, casting_dtype = _infer_parameter_dtype(model, name, param, keep_in_fp32_regex)\n                     shard_and_distribute_module(\n                         model,\n                         param.to(tp_device),"
        },
        {
            "sha": "ab5a2a9abd65ab82f99629816f3e69a9db2f1a6b",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=dd3933dd658b2c2e18ad316662a3dff09dcf98cb",
            "patch": "@@ -419,7 +419,6 @@ class Blip2PreTrainedModel(PreTrainedModel):\n         \"OPTDecoderLayer\",\n     ]\n     _skip_keys_device_placement = \"past_key_values\"\n-    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n@@ -1448,6 +1447,7 @@ def forward(\n class Blip2Model(Blip2PreTrainedModel):\n     config_class = Blip2Config\n     main_input_name = \"pixel_values\"\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)\n@@ -2019,6 +2019,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n+    _keep_in_fp32_modules = [\"query_tokens\"]\n \n     def __init__(self, config: Blip2Config):\n         super().__init__(config)"
        },
        {
            "sha": "e9d325460d55e46d4bbe836ebbcf11069d8518fd",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd3933dd658b2c2e18ad316662a3dff09dcf98cb/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=dd3933dd658b2c2e18ad316662a3dff09dcf98cb",
            "patch": "@@ -791,15 +791,12 @@ def test_inference_flant5_xl(self):\n             num_beams=5,\n             max_length=256,\n             min_length=1,\n-            top_p=0.9,\n             repetition_penalty=1.5,\n             length_penalty=1.0,\n             temperature=1,\n         )\n         generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n \n-        expected_outputs = [0, 37, 1023, 9850, 7, 3, 9, 388, 3575, 53, 4954, 30, 8, 223, 13, 3, 9, 4459, 4049, 16, 8, 2214, 13, 3, 9, 3164, 690, 2815, 5, 37, 388, 19, 5119, 3, 9, 4459, 8677, 28, 3, 9, 2756, 4459, 6177, 6, 11, 3, 88, 19, 338, 46, 3575, 53, 1476, 12, 743, 112, 2491, 5, 37, 1023, 19, 7225, 788, 12, 8, 685, 24, 34, 1267, 3, 9, 388, 3575, 53, 4954, 30, 8, 223, 13, 3, 9, 4049, 16, 8, 2214, 13, 3, 9, 3164, 690, 2815, 5, 94, 19, 487, 24, 8, 388, 19, 1119, 12, 1097, 540, 57, 692, 112, 10428, 30, 8, 223, 13, 8, 4049, 6, 68, 34, 19, 92, 487, 24, 3, 88, 19, 1119, 12, 1097, 97, 57, 692, 112, 10428, 30, 8, 223, 13, 8, 4049, 16, 8, 2214, 13, 3, 9, 3164, 690, 2815, 5, 3, 13865, 13, 8, 1053, 21, 8, 388, 31, 7, 2874, 6, 34, 19, 964, 24, 3, 88, 19, 1119, 12, 1097, 97, 57, 692, 112, 10428, 30, 8, 223, 13, 8, 4049, 16, 8, 2214, 13, 3, 9, 3164, 690, 2815, 5, 1]  # fmt: skip\n-\n         expected_outputs = [0, 37, 7225, 1023, 9850, 7, 3, 9, 388, 3575, 53, 4954, 30, 8, 223, 13, 3, 9, 4459, 4049, 16, 8, 2214, 13, 3, 9, 3164, 690, 2815, 5, 37, 388, 19, 5119, 3, 9, 4459, 8677, 28, 46, 3575, 53, 1476, 5223, 12, 34, 6, 15495, 24, 3, 88, 19, 692, 112, 293, 10428, 44, 234, 1066, 145, 338, 3, 9, 50, 1106, 3522, 144, 42, 2192, 7919, 31, 7, 5, 37, 1023, 92, 1267, 3, 9, 381, 13, 119, 3203, 16, 8, 2458, 6, 379, 14264, 6, 9256, 7, 6, 11, 11718, 7, 5, 1]  # fmt: skip\n \n         self.assertEqual(outputs[0].tolist(), expected_outputs)"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 47,
        "deletions": 31
    }
}