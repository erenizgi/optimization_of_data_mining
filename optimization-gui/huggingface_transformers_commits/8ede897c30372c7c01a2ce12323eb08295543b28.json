{
    "author": "SunMarc",
    "message": "restrict cache allocator to non quantized model (#36428)",
    "sha": "8ede897c30372c7c01a2ce12323eb08295543b28",
    "files": [
        {
            "sha": "db318156acaad37638010050df96dcf24c199553",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ede897c30372c7c01a2ce12323eb08295543b28/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ede897c30372c7c01a2ce12323eb08295543b28/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8ede897c30372c7c01a2ce12323eb08295543b28",
            "patch": "@@ -4839,7 +4839,8 @@ def _load_pretrained_model(\n         model.expected_keys = expected_keys\n         if device_map is not None:\n             expanded_device_map = expand_device_map(device_map, original_loaded_keys, start_prefix)\n-            caching_allocator_warmup(model, expanded_device_map, dtype)\n+            if hf_quantizer is None:\n+                caching_allocator_warmup(model_to_load, expanded_device_map, dtype)\n \n         if device_map is not None and is_safetensors:\n             param_device_map = expanded_device_map"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}