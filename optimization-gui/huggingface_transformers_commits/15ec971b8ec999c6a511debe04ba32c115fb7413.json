{
    "author": "zucchini-nlp",
    "message": "Prepare processors for VideoLLMs (#36149)\n\n* allow processor to preprocess conversation + video metadata\r\n\r\n* allow callable\r\n\r\n* add test\r\n\r\n* fix test\r\n\r\n* nit: fix\r\n\r\n* add metadata frames_indices\r\n\r\n* Update src/transformers/processing_utils.py\r\n\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\r\n\r\n* Update src/transformers/processing_utils.py\r\n\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\r\n\r\n* port updates from Orr and add one more test\r\n\r\n* Update src/transformers/processing_utils.py\r\n\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\r\n\r\n* typo\r\n\r\n* as dataclass\r\n\r\n* style\r\n\r\n* docstring + maek sure tests green\r\n\r\n---------\r\n\r\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "15ec971b8ec999c6a511debe04ba32c115fb7413",
    "files": [
        {
            "sha": "101de2c78a7ed92cbf48ad431dc4f44d18c32f16",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 165,
            "deletions": 90,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/15ec971b8ec999c6a511debe04ba32c115fb7413/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15ec971b8ec999c6a511debe04ba32c115fb7413/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=15ec971b8ec999c6a511debe04ba32c115fb7413",
            "patch": "@@ -18,7 +18,7 @@\n from contextlib import redirect_stdout\n from dataclasses import dataclass\n from io import BytesIO\n-from typing import TYPE_CHECKING, Dict, Iterable, List, Optional, Tuple, Union\n+from typing import TYPE_CHECKING, Callable, Dict, Iterable, List, Optional, Tuple, Union\n \n import numpy as np\n import requests\n@@ -126,6 +126,14 @@ class AnnotionFormat(ExplicitEnum):\n     COCO_PANOPTIC = AnnotationFormat.COCO_PANOPTIC.value\n \n \n+@dataclass\n+class VideoMetadata:\n+    total_num_frames: int\n+    fps: float\n+    duration: float\n+    video_backend: str\n+\n+\n AnnotationType = Dict[str, Union[int, str, List[Dict]]]\n \n \n@@ -541,133 +549,165 @@ def load_image(image: Union[str, \"PIL.Image.Image\"], timeout: Optional[float] =\n     return image\n \n \n-def get_uniform_frame_indices(total_num_frames: int, num_frames: Optional[int] = None):\n+def default_sample_indices_fn(metadata: VideoMetadata, num_frames=None, fps=None, **kwargs):\n     \"\"\"\n-    Creates a numpy array for uniform sampling of `num_frame` frames from `total_num_frames`\n-    when loading a video.\n+    A default sampling function that replicates the logic used in get_uniform_frame_indices,\n+    while optionally handling `fps` if `num_frames` is not provided.\n \n     Args:\n-        total_num_frames (`int`):\n-            Total number of frames that a video has.\n+        metadata (`VideoMetadata`):\n+            `VideoMetadata` object containing metadat about the video, such as \"total_num_frames\" or \"fps\".\n         num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. If not specified, all frames are sampled.\n+            Number of frames to sample uniformly.\n+        fps (`int`, *optional*):\n+            Desired frames per second. Takes priority over num_frames if both are provided.\n \n     Returns:\n-        np.ndarray: np array of frame indices that will be sampled.\n+        `np.ndarray`: Array of frame indices to sample.\n     \"\"\"\n+    total_num_frames = metadata.total_num_frames\n+    video_fps = metadata.fps\n+\n+    # If num_frames is not given but fps is, calculate num_frames from fps\n+    if num_frames is None and fps is not None:\n+        num_frames = int(total_num_frames / video_fps * fps)\n+        if num_frames > total_num_frames:\n+            raise ValueError(\n+                f\"When loading the video with fps={fps}, we computed num_frames={num_frames} \"\n+                f\"which exceeds total_num_frames={total_num_frames}. Check fps or video metadata.\"\n+            )\n+\n     if num_frames is not None:\n-        indices = np.arange(0, total_num_frames, total_num_frames / num_frames).astype(int)\n+        indices = np.arange(0, total_num_frames, total_num_frames / num_frames, dtype=int)\n     else:\n-        indices = np.arange(0, total_num_frames).astype(int)\n+        indices = np.arange(0, total_num_frames, dtype=int)\n     return indices\n \n \n-def read_video_opencv(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n+def read_video_opencv(\n+    video_path: str,\n+    sample_indices_fn: Callable,\n+    **kwargs,\n+):\n     \"\"\"\n-    Decode the video with open-cv decoder.\n+    Decode a video using the OpenCV backend.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n-            If not specified and `fps==None`, all frames are sampled.\n-        fps (`int`, *optional*):\n-            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-            If not specified and `num_frames==None`, all frames are sampled.\n+        sample_indices_fn (`Callable`):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniform sampling with fps is performed.\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+        Tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - `VideoMetadata` object.\n     \"\"\"\n     video = cv2.VideoCapture(video_path)\n     total_num_frames = int(video.get(cv2.CAP_PROP_FRAME_COUNT))\n     video_fps = video.get(cv2.CAP_PROP_FPS)\n-    if num_frames is None and fps is not None:\n-        num_frames = int(total_num_frames / video_fps * fps)\n-        if num_frames > total_num_frames:\n-            raise ValueError(\n-                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n-                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n-            )\n-    indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n+    duration = total_num_frames / video_fps if video_fps else 0\n+    metadata = VideoMetadata(\n+        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"opencv\"\n+    )\n+    indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n     index = 0\n     frames = []\n     while video.isOpened():\n         success, frame = video.read()\n+        if not success:\n+            break\n         if index in indices:\n             height, width, channel = frame.shape\n+            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n             frames.append(frame[0:height, 0:width, 0:channel])\n         if success:\n             index += 1\n         if index >= total_num_frames:\n             break\n \n     video.release()\n-    return np.stack(frames)\n+    metadata.frames_indices = indices\n+    return np.stack(frames), metadata\n \n \n-def read_video_decord(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n+def read_video_decord(\n+    video_path: str,\n+    sample_indices_fn: Optional[Callable] = None,\n+    **kwargs,\n+):\n     \"\"\"\n-    Decode the video with Decord decoder.\n+    Decode a video using the Decord backend.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n-            If not specified and `fps==None`, all frames are sampled.\n-        fps (`int`, *optional*):\n-            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-            If not specified and `num_frames==None`, all frames are sampled.\n+        sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniform sampling with fps is performed.\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+        Tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - `VideoMetadata` object.\n     \"\"\"\n     vr = VideoReader(uri=video_path, ctx=cpu(0))  # decord has problems with gpu\n     video_fps = vr.get_avg_fps()\n     total_num_frames = len(vr)\n-    if num_frames is None and fps is not None:\n-        num_frames = int(total_num_frames / video_fps * fps)\n-        if num_frames > total_num_frames:\n-            raise ValueError(\n-                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n-                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n-            )\n-    indices = get_uniform_frame_indices(total_num_frames=total_num_frames, num_frames=num_frames)\n+    duration = total_num_frames / video_fps if video_fps else 0\n+    metadata = VideoMetadata(\n+        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"decord\"\n+    )\n+\n+    indices = sample_indices_fn(metadata=metadata, **kwargs)\n+\n     frames = vr.get_batch(indices).asnumpy()\n-    return frames\n+    metadata.frames_indices = indices\n+    return frames, metadata\n \n \n-def read_video_pyav(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n+def read_video_pyav(\n+    video_path: str,\n+    sample_indices_fn: Callable,\n+    **kwargs,\n+):\n     \"\"\"\n     Decode the video with PyAV decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n-            If not specified and `fps==None`, all frames are sampled.\n-        fps (`int`, *optional*):\n-            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-            If not specified and `num_frames==None`, all frames are sampled.\n+        sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniform sampling with fps is performed.\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+        Tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - `VideoMetadata` object.\n     \"\"\"\n     container = av.open(video_path)\n-\n     total_num_frames = container.streams.video[0].frames\n     video_fps = container.streams.video[0].average_rate  # should we better use `av_guess_frame_rate`?\n-    if num_frames is None and fps is not None:\n-        num_frames = int(total_num_frames / video_fps * fps)\n-        if num_frames > total_num_frames:\n-            raise ValueError(\n-                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n-                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n-            )\n-    indices = get_uniform_frame_indices(total_num_frames, num_frames=num_frames)\n+    duration = total_num_frames / video_fps if video_fps else 0\n+    metadata = VideoMetadata(\n+        total_num_frames=int(total_num_frames), fps=float(video_fps), duration=float(duration), video_backend=\"pyav\"\n+    )\n+    indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n     frames = []\n     container.seek(0)\n@@ -677,48 +717,58 @@ def read_video_pyav(video_path: str, num_frames: Optional[int] = None, fps: Opti\n             break\n         if i >= 0 and i in indices:\n             frames.append(frame)\n-    return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+\n+    video = np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n+    metadata.frames_indices = indices\n+    return video, metadata\n \n \n-def read_video_torchvision(video_path: str, num_frames: Optional[int] = None, fps: Optional[int] = None):\n+def read_video_torchvision(\n+    video_path: str,\n+    sample_indices_fn: Callable,\n+    **kwargs,\n+):\n     \"\"\"\n     Decode the video with torchvision decoder.\n \n     Args:\n         video_path (`str`):\n             Path to the video file.\n-        num_frames (`int`, *optional*):\n-            Number of frames to sample uniformly. Should be passed only when `fps=None`.\n-            If not specified and `fps==None`, all frames are sampled.\n-        fps (`int`, *optional*):\n-            Number of frames to sample per second. Should be passed only when `num_frames=None`.\n-            If not specified and `num_frames==None`, all frames are sampled.\n+        sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniform sampling with fps is performed.\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        np.ndarray: np array of decoded frames of shape (num_frames, height, width, 3).\n+        Tuple[`np.array`, `VideoMetadata`]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - `VideoMetadata` object.\n     \"\"\"\n     video, _, info = torchvision_io.read_video(\n         video_path,\n         start_pts=0.0,\n         end_pts=None,\n         pts_unit=\"sec\",\n-        output_format=\"TCHW\",\n+        output_format=\"THWC\",\n     )\n     video_fps = info[\"video_fps\"]\n-    total_num_frames = video.size(0) - 1\n-    if num_frames is None and fps is not None:\n-        num_frames = int(total_num_frames / video_fps * fps)\n-        if num_frames > total_num_frames:\n-            raise ValueError(\n-                f\"When loading the video with fps={fps}, we identified that num_frames ({num_frames}) > total_frames ({total_num_frames}) .\"\n-                f\"Make sure that fps of a video is less than the requested fps for loading. Detected video_fps={video_fps}\"\n-            )\n+    total_num_frames = video.size(0)\n+    duration = total_num_frames / video_fps if video_fps else 0\n+    metadata = VideoMetadata(\n+        total_num_frames=int(total_num_frames),\n+        fps=float(video_fps),\n+        duration=float(duration),\n+        video_backend=\"torchvision\",\n+    )\n \n-    if num_frames is not None:\n-        idx = torch.linspace(0, video.size(0) - 1, num_frames, dtype=torch.int64)\n-        return video[idx]\n+    indices = sample_indices_fn(metadata=metadata, **kwargs)\n \n-    return video\n+    video = video[indices].contiguous().numpy()\n+    metadata.frames_indices = indices\n+    return video, metadata\n \n \n VIDEO_DECODERS = {\n@@ -734,6 +784,8 @@ def load_video(\n     num_frames: Optional[int] = None,\n     fps: Optional[int] = None,\n     backend: str = \"opencv\",\n+    sample_indices_fn: Optional[Callable] = None,\n+    **kwargs,\n ) -> np.array:\n     \"\"\"\n     Loads `video` to a numpy array.\n@@ -748,13 +800,36 @@ def load_video(\n             If not specified and `num_frames==None`, all frames are sampled.\n         backend (`str`, *optional*, defaults to `\"opencv\"`):\n             The backend to use when loading the video. Can be any of [\"decord\", \"pyav\", \"opencv\", \"torchvision\"]. Defaults to \"opencv\".\n+        sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniformt sampling with fps is performed, otherwise `sample_indices_fn` has priority over other args.\n+            The function expects at input the all args along with all kwargs passed to `load_video` and should output valid\n+            indices at which the video should be sampled. For example:\n+\n+            Example:\n+            def sample_indices_fn(metadata, **kwargs):\n+                return np.linspace(0, metadata.total_num_frames - 1, num_frames, dtype=int)\n \n     Returns:\n-        `np.array`: A numpy array of shape (num_frames, channels, height, width).\n+        Tuple[`np.array`, Dict]: A tuple containing:\n+            - Numpy array of frames in RGB (shape: [num_frames, height, width, 3]).\n+            - Metadata dictionary.\n     \"\"\"\n \n-    if fps is not None and num_frames is not None:\n-        raise ValueError(\"`num_frames` and `fps` are mutually exclusive arguments, please use only one!\")\n+    # If `sample_indices_fn` is given, we can accept any args as those might be needed by custom `sample_indices_fn`\n+    if fps is not None and num_frames is not None and sample_indices_fn is None:\n+        raise ValueError(\n+            \"`num_frames`, `fps`, and `sample_indices_fn` are mutually exclusive arguments, please use only one!\"\n+        )\n+\n+    # If user didn't pass a sampling function, create one on the fly with default logic\n+    if sample_indices_fn is None:\n+\n+        def sample_indices_fn_func(metadata, **fn_kwargs):\n+            return default_sample_indices_fn(metadata, num_frames=num_frames, fps=fps, **fn_kwargs)\n+\n+        sample_indices_fn = sample_indices_fn_func\n \n     if video.startswith(\"https://www.youtube.com\") or video.startswith(\"http://www.youtube.com\"):\n         if not is_yt_dlp_available():\n@@ -796,8 +871,8 @@ def load_video(\n         )\n \n     video_decoder = VIDEO_DECODERS[backend]\n-    video = video_decoder(file_obj, num_frames=num_frames, fps=fps)\n-    return video\n+    video, metadata = video_decoder(file_obj, sample_indices_fn, **kwargs)\n+    return video, metadata\n \n \n def load_images("
        },
        {
            "sha": "5b1e45259f2cc321748539565864f8d5918cee0d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 135,
            "deletions": 37,
            "changes": 172,
            "blob_url": "https://github.com/huggingface/transformers/blob/15ec971b8ec999c6a511debe04ba32c115fb7413/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15ec971b8ec999c6a511debe04ba32c115fb7413/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=15ec971b8ec999c6a511debe04ba32c115fb7413",
            "patch": "@@ -24,13 +24,21 @@\n import typing\n import warnings\n from pathlib import Path\n-from typing import Any, Dict, List, Optional, Tuple, TypedDict, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, TypedDict, Union\n \n import numpy as np\n import typing_extensions\n \n from .dynamic_module_utils import custom_object_save\n-from .image_utils import ChannelDimension, is_valid_image, is_vision_available, load_image, load_video\n+from .image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    VideoInput,\n+    is_valid_image,\n+    is_vision_available,\n+    load_image,\n+    load_video,\n+)\n \n \n if is_vision_available():\n@@ -339,14 +347,10 @@ class CustomProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n-class ChatTemplateKwargs(TypedDict, total=False):\n+class TokenizerChatTemplateKwargs(TypedDict, total=False):\n     \"\"\"\n-    Keyword arguments for processor chat templates.\n+    Keyword arguments for tokenizer's `apply_chat_template`, when it is called from within a processor.\n \n-    tokenize (`bool`, *optional*, defaults to `False`):\n-        Whether to tokenize the output or not.\n-    return_dict (`bool`, defaults to `False`):\n-        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n     tools (`List[Dict]`, *optional*):\n         A list of tools (callable functions) that will be accessible to the model. If the template does not\n         support function calling, this argument will have no effect. Each tool should be passed as a JSON Schema,\n@@ -373,6 +377,23 @@ class ChatTemplateKwargs(TypedDict, total=False):\n         Whether to return a mask of the assistant generated tokens. For tokens generated by the assistant,\n         the mask will contain 1. For user and system tokens, the mask will contain 0.\n         This functionality is only available for chat templates that support it via the `{% generation %}` keyword.\n+    \"\"\"\n+\n+    tools: Optional[List[Dict]] = None\n+    documents: Optional[List[Dict[str, str]]] = None\n+    add_generation_prompt: Optional[bool] = False\n+    continue_final_message: Optional[bool] = False\n+    return_assistant_tokens_mask: Optional[bool] = False\n+\n+\n+class ProcessorChatTemplateKwargs(TokenizerChatTemplateKwargs, total=False):\n+    \"\"\"\n+    Keyword arguments for processor chat templates.\n+\n+    tokenize (`bool`, *optional*, defaults to `False`):\n+        Whether to tokenize the output or not.\n+    return_dict (`bool`, defaults to `False`):\n+        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n     num_frames (`int`, *optional*):\n         Number of frames to sample uniformly. If not passed, the whole video is loaded.\n     video_load_backend (`str`, *optional*, defaults to `\"pyav\"`):\n@@ -382,22 +403,28 @@ class ChatTemplateKwargs(TypedDict, total=False):\n     video_fps (`int`, *optional*):\n         Number of frames to sample per second. Should be passed only when `num_frames=None`.\n         If not specified and `num_frames==None`, all frames are sampled.\n+    sample_indices_fn (`Callable`, *optional*):\n+            A callable function that will return indices at which the video should be sampled. If the video has to be loaded using\n+            by a different sampling technique than provided by `num_frames` or `fps` arguments, one should provide their own `sample_indices_fn`.\n+            If not provided, simple uniformt sampling with fps is performed, otherwise `sample_indices_fn` has priority over other args.\n+            The function expects at input the all args along with all kwargs passed to `load_video` and should output valid\n+            indices at which the video should be sampled. For example:\n+\n+            def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n+                # add you sampling logic here ...\n+                return np.linspace(start_idx, end_idx, num_frames, dtype=int)\n     \"\"\"\n \n     tokenize: Optional[bool] = False\n     return_dict: Optional[bool] = False\n-    tools: Optional[List[Dict]] = None\n-    documents: Optional[List[Dict[str, str]]] = None\n-    add_generation_prompt: Optional[bool] = False\n-    continue_final_message: Optional[bool] = False\n-    return_assistant_tokens_mask: Optional[bool] = False\n     num_frames: Optional[int] = None\n     video_load_backend: Optional[str] = \"pyav\"\n     video_fps: Optional[int] = None\n+    sample_indices_fn: Optional[Callable] = None\n \n \n class AllKwargsForChatTemplate(\n-    TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs, ChatTemplateKwargs\n+    TextKwargs, ImagesKwargs, VideosKwargs, AudioKwargs, CommonKwargs, ProcessorChatTemplateKwargs\n ): ...\n \n \n@@ -1165,9 +1192,43 @@ def __call__(\n             )\n         return {arg_name: arg_value for arg_value, arg_name in zip(args, self.optional_call_args)}\n \n+    def _process_messages_for_chat_template(\n+        self,\n+        conversation: List[List[Dict[str, str]]],\n+        batch_images: List[ImageInput],\n+        batch_videos: List[VideoInput],\n+        batch_video_metadata: List[List[Dict[str, any]]],\n+        **chat_template_kwargs: Unpack[AllKwargsForChatTemplate],\n+    ):\n+        \"\"\"\n+        Used within `apply_chat_template` when a model has a special way to process conversation history. For example,\n+        video models might want to specify in the prompt the duration of video or which frame indices at which timestamps\n+        were sampled. This information cannot be accessed before the video is loaded.\n+\n+        For most models it is a no-op, and must be overriden by model processors which require special processing.\n+\n+        Args:\n+            conversation (`List[Dict, str, str]`):\n+                The conversation to process. Always comes in batched format.\n+            batch_images (`List[List[ImageInput]]`):\n+                Batch of images that were loaded from url/path defined in the conversation. The images\n+                are ordered in the same way as in the conversation. Comes in nested list format, one list of `PIL` images\n+                per batch.\n+            batch_videos (`List[List[ImageInput]]`):\n+                Batch of videos that were loaded from url/path defined in the conversation. The videos\n+                are ordered in the samm way as in the conversation. Comes in nested list format, one list of 4D video arrays\n+                per batch.\n+            batch_video_metadata (`List[List[Dict[[str, any]]]]`):\n+                Batch of metadata returned from loading videos. That includes video fps, duration and total number of framer in original video.\n+                Metadata are ordered in the same way as `batch_videos`. Comes in nested list format, one list of 4D video arrays\n+                per batch.\n+\n+        \"\"\"\n+        return conversation\n+\n     def apply_chat_template(\n         self,\n-        conversation: Union[List[Dict[str, str]]],\n+        conversation: Union[List[Dict[str, str]], List[List[Dict[str, str]]]],\n         chat_template: Optional[str] = None,\n         **kwargs: Unpack[AllKwargsForChatTemplate],\n     ) -> str:\n@@ -1190,7 +1251,7 @@ def apply_chat_template(\n         ]\n \n         Args:\n-            conversation (`List[Dict, str, str]`):\n+            conversation (`Union[List[Dict, [str, str]], List[List[Dict[str, str]]]]`):\n                 The conversation to format.\n             chat_template (`Optional[str]`, *optional*):\n                 The Jinja template to use for formatting the conversation. If not provided, the tokenizer's\n@@ -1207,39 +1268,42 @@ def apply_chat_template(\n                     \"https://huggingface.co/docs/transformers/main/en/chat_templating for more information.\"\n                 )\n \n+        # Fill two sets of kwargs that should be used by tokenizer's `apply_chat_template`\n+        # and for multimodal chat template\n+        tokenizer_template_kwargs = {}\n+        for tokenizer_key in TokenizerChatTemplateKwargs.__annotations__.keys():\n+            tokenizer_value = getattr(TokenizerChatTemplateKwargs, tokenizer_key, None)\n+            value = kwargs.pop(tokenizer_key, tokenizer_value)\n+            tokenizer_template_kwargs[tokenizer_key] = value\n+\n         chat_template_kwargs = {}\n-        for key in ChatTemplateKwargs.__annotations__.keys():\n-            value = kwargs.pop(key, getattr(ChatTemplateKwargs, key))\n+        for key in ProcessorChatTemplateKwargs.__annotations__.keys():\n+            processor_value = getattr(ProcessorChatTemplateKwargs, key, None)\n+            value = kwargs.pop(key, processor_value)\n             chat_template_kwargs[key] = value\n \n-        # Pop kwargs that should not be used by tokenizer's `apply_chat_template`\n-        tokenize = chat_template_kwargs.pop(\"tokenize\")\n-        return_dict = chat_template_kwargs.pop(\"return_dict\")\n-        num_frames = chat_template_kwargs.pop(\"num_frames\")\n-        video_fps = chat_template_kwargs.pop(\"video_fps\")\n-        video_load_backend = chat_template_kwargs.pop(\"video_load_backend\")\n-\n-        prompt = self.tokenizer.apply_chat_template(\n-            conversation,\n-            chat_template=chat_template,\n-            tokenize=False,\n-            return_dict=False,\n-            **chat_template_kwargs,\n-        )\n-\n         if isinstance(conversation, (list, tuple)) and (\n             isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n         ):\n-            conversations = conversation\n             is_batched = True\n+            conversations = conversation\n         else:\n-            conversations = [conversation]\n             is_batched = False\n+            conversations = [conversation]\n+\n+        num_frames = chat_template_kwargs.get(\"num_frames\")\n+        video_fps = chat_template_kwargs.get(\"video_fps\")\n+        video_load_backend = chat_template_kwargs.get(\"video_load_backend\")\n+        tokenize = chat_template_kwargs.get(\"tokenize\")\n+        return_dict = chat_template_kwargs.get(\"return_dict\")\n+        sample_indices_fn = chat_template_kwargs.get(\"sample_indices_fn\")\n \n         if tokenize:\n             batch_images, batch_videos = [], []\n+            batch_video_metadata = []\n             for conversation in conversations:\n                 images, videos = [], []\n+                video_metadata = []\n                 for message in conversation:\n                     visuals = [content for content in message[\"content\"] if content[\"type\"] in [\"image\", \"video\"]]\n                     image_fnames = [\n@@ -1261,17 +1325,51 @@ def apply_chat_template(\n                             video = [np.array(load_image(image_fname)).T for image_fname in fname]\n                             # create a 4D video because `load_video` always returns a 4D array\n                             video = np.stack(video)\n+                            metadata = None\n+                            logger.warning(\n+                                \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n+                                \"If you model applies special processing based on metadata, please load the whole video and let the model sample frames.\"\n+                            )\n                         else:\n-                            video = load_video(fname, num_frames=num_frames, fps=video_fps, backend=video_load_backend)\n+                            video, metadata = load_video(\n+                                fname,\n+                                num_frames=num_frames,\n+                                fps=video_fps,\n+                                backend=video_load_backend,\n+                                sample_indices_fn=sample_indices_fn,\n+                            )\n                         videos.append(video)\n+                        video_metadata.append(metadata)\n \n-                # Currently all processors can accept accept nested list of batches, but not flat list of visuals\n+                # Currently all processors can accept nested list of batches, but not flat list of visuals\n                 # So we'll make a batched list of images and let the processor handle it\n                 if images:\n                     batch_images.append(images)\n                 if videos:\n                     batch_videos.append(videos)\n+                    batch_video_metadata.append(video_metadata)\n+\n+            # Process conversation with video/image information if needed. Then convert into a prompt using Jinja template\n+            conversations = self._process_messages_for_chat_template(\n+                conversations,\n+                batch_images=batch_images,\n+                batch_videos=batch_videos,\n+                batch_video_metadata=batch_video_metadata,\n+                **chat_template_kwargs,\n+            )\n \n+        prompt = self.tokenizer.apply_chat_template(\n+            conversations,\n+            chat_template=chat_template,\n+            tokenize=False,\n+            return_dict=False,\n+            **tokenizer_template_kwargs,\n+        )\n+\n+        if not is_batched:\n+            prompt = prompt[0]\n+\n+        if tokenize:\n             # Tokenizer's `apply_chat_template` never adds special tokens when tokenizing\n             # But processor's `apply_chat_template` didn't have an option to tokenize, so users had to format the prompt\n             # and pass it to the processor. Users thus never worried about special tokens relying on processor hadnling"
        },
        {
            "sha": "28aff79c7e56205f8185c0f189a79366f4f97b6a",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 118,
            "deletions": 1,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/15ec971b8ec999c6a511debe04ba32c115fb7413/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/15ec971b8ec999c6a511debe04ba32c115fb7413/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=15ec971b8ec999c6a511debe04ba32c115fb7413",
            "patch": "@@ -22,6 +22,7 @@\n from typing import Optional\n \n import numpy as np\n+from huggingface_hub import hf_hub_download\n \n from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.processing_utils import Unpack\n@@ -538,7 +539,7 @@ def test_prepare_and_validate_optional_call_args(self):\n \n     def test_chat_template_save_loading(self):\n         processor = self.get_processor()\n-        signature = inspect.signature(processor.__call__)\n+        signature = inspect.signature(processor.__init__)\n         if \"chat_template\" not in {*signature.parameters.keys()}:\n             self.skipTest(\"Processor doesn't accept chat templates at input\")\n \n@@ -858,3 +859,119 @@ def test_chat_template_video(self):\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n+\n+    @require_av\n+    def test_chat_template_video_custom_sampling(self):\n+        \"\"\"\n+        Tests that models can pass their custom callables to sample video indices.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\n+                            \"type\": \"video\",\n+                            \"path\": video_file_path,\n+                        },\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def dummmy_sample_indices_fn(metadata, **fn_kwargs):\n+            # sample only the first two frame always\n+            return [0, 1]\n+\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            sample_indices_fn=dummmy_sample_indices_fn,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 2)\n+\n+    @require_av\n+    def test_chat_template_video_special_processing(self):\n+        \"\"\"\n+        Tests that models can use their own preprocessing to preprocess conversations.\n+        \"\"\"\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(\"Processor doesn't accept videos at input\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            [\n+                {\n+                    \"role\": \"user\",\n+                    \"content\": [\n+                        {\"type\": \"video\", \"path\": video_file_path},\n+                        {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n+                    ],\n+                },\n+            ]\n+        ]\n+\n+        def _process_messages_for_chat_template(\n+            conversation,\n+            batch_images,\n+            batch_videos,\n+            batch_video_metadata,\n+            **chat_template_kwargs,\n+        ):\n+            # Let us just always return a dummy prompt\n+            new_msg = [\n+                [\n+                    {\n+                        \"role\": \"user\",\n+                        \"content\": [\n+                            {\"type\": \"video\"},  # no need to use path, video is loaded already by this moment\n+                            {\"type\": \"text\", \"text\": \"Dummy prompt for preprocess testing\"},\n+                        ],\n+                    },\n+                ]\n+            ]\n+            return new_msg\n+\n+        processor._process_messages_for_chat_template = _process_messages_for_chat_template\n+        out_dict_with_video = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+        )\n+        self.assertTrue(self.videos_input_name in out_dict_with_video)\n+\n+        # Check with `in` because we don't know how each template formats the prompt with BOS/EOS/etc\n+        formatted_text = processor.batch_decode(out_dict_with_video[\"input_ids\"], skip_special_tokens=True)[0]\n+        self.assertTrue(\"Dummy prompt for preprocess testing\" in formatted_text)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 243)"
        }
    ],
    "stats": {
        "total": 546,
        "additions": 418,
        "deletions": 128
    }
}