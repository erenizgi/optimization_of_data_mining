{
    "author": "yonigozlan",
    "message": "Fix convert weights for InternVL (#38233)\n\nFix internvl convert weights",
    "sha": "c24d18bbae5e54832f4874607368f568483bd215",
    "files": [
        {
            "sha": "6d4ffe7befafddc423a58d41934a260137da0a13",
            "filename": "src/transformers/models/internvl/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c24d18bbae5e54832f4874607368f568483bd215/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c24d18bbae5e54832f4874607368f568483bd215/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2F__init__.py?ref=c24d18bbae5e54832f4874607368f568483bd215",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_internvl import *\n     from .modeling_internvl import *\n     from .processing_internvl import *\n+    from .video_processing_internvl import *\n else:\n     import sys\n "
        },
        {
            "sha": "fa6d4bc9e5263e04aa258946f24ab9519278bf88",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 25,
            "deletions": 8,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c24d18bbae5e54832f4874607368f568483bd215/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c24d18bbae5e54832f4874607368f568483bd215/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=c24d18bbae5e54832f4874607368f568483bd215",
            "patch": "@@ -28,6 +28,7 @@\n     InternVLConfig,\n     InternVLForConditionalGeneration,\n     InternVLProcessor,\n+    InternVLVideoProcessor,\n     InternVLVisionConfig,\n     LlamaConfig,\n     Qwen2Config,\n@@ -56,7 +57,7 @@\n # fmt: off\n ORIGINAL_TO_CONVERTED_KEY_MAPPING_VISION = {\n     # Vision encoder mapping\n-    r\"vision_model\":                                r\"vision_tower\",\n+    r\"vision_model\":                                r\"model.vision_tower\",\n     r\"layers\":                                      r\"layer\",\n     r\"class_embedding\":                             r\"cls_token\",\n     r\"position_embedding\":                          r\"position_embeddings\",\n@@ -71,22 +72,28 @@\n }\n \n ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_LLAMA = {\n-    # Vision encoder mapping\n+    r\"language_model.model.\":                       r\"model.language_model.\",\n     r\"tok_embeddings\":                              r\"embed_tokens\",\n     r\"attention.wo\":                                r\"self_attn.o_proj\",\n     r\"feed_forward.w1\":                             r\"mlp.gate_proj\",\n     r\"feed_forward.w2\":                             r\"mlp.down_proj\",\n     r\"feed_forward.w3\":                             r\"mlp.up_proj\",\n     r\"attention_norm\":                              r\"input_layernorm\",\n     r\"ffn_norm\":                                    r\"post_attention_layernorm\",\n-    r\"output\":                                      r\"lm_head\",\n+    r\"language_model.output\":                       r\"lm_head\",\n+}\n+\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_QWEN2 = {\n+    # Vision encoder mapping\n+    r\"language_model.model.\":                       r\"model.language_model.\",\n+    r\"language_model.lm_head\":                       r\"lm_head\",\n }\n \n ORIGINAL_TO_CONVERTED_KEY_MAPPING_MULTI = {\n     # Vision encoder mapping\n-    r\"mlp1.0\":                                 r\"multi_modal_projector.layer_norm\",\n-    r\"mlp1.1\":                                 r\"multi_modal_projector.linear_1\",\n-    r\"mlp1.3\":                                 r\"multi_modal_projector.linear_2\",\n+    r\"mlp1.0\":                                 r\"model.multi_modal_projector.layer_norm\",\n+    r\"mlp1.1\":                                 r\"model.multi_modal_projector.linear_1\",\n+    r\"mlp1.3\":                                 r\"model.multi_modal_projector.linear_2\",\n }\n \n \n@@ -98,7 +105,7 @@\n         \"{% else %}\"\n             \"{% for content in message['content'] %}\"\n                 \"{% if content['type'] == 'image' %}\"\n-                    \"{{ '<image>\\n' }}\"\n+                    \"{{ '<IMG_CONTEXT>\\n' }}\"\n                 \"{% elif content['type'] == 'video' %}\"\n                     \"{{ '<video>\\n' }}\"\n                 \"{% elif content['type'] == 'text' %}\"\n@@ -134,6 +141,9 @@ def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None, path: O\n         if LM_TYPE_CORRESPONDENCE[path] == \"llama\":\n             for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_LLAMA.items():\n                 new_text = re.sub(pattern, replacement, new_text)\n+        elif LM_TYPE_CORRESPONDENCE[path] == \"qwen2\":\n+            for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_QWEN2.items():\n+                new_text = re.sub(pattern, replacement, new_text)\n         output_dict.update(dict(zip(old_text_language.split(\"\\n\"), new_text.split(\"\\n\"))))\n         old_text_multi = \"\\n\".join(\n             [\n@@ -276,8 +286,14 @@ def write_model(\n         model.push_to_hub(hub_dir, use_temp_dir=True)\n \n     image_processor = GotOcr2ImageProcessorFast.from_pretrained(model_path)\n+    video_processor = InternVLVideoProcessor.from_pretrained(model_path)\n     tokenizer = AutoTokenizer.from_pretrained(model_path)\n-    processor = InternVLProcessor(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n+    processor = InternVLProcessor(\n+        image_processor=image_processor,\n+        video_processor=video_processor,\n+        tokenizer=tokenizer,\n+        chat_template=chat_template,\n+    )\n     processor.save_pretrained(model_path)\n     if push_to_hub:\n         processor.push_to_hub(hub_dir, use_temp_dir=True)\n@@ -349,6 +365,7 @@ def write_tokenizer(\n                 \"start_image_token\": \"<img>\",\n                 \"end_image_token\": \"</img>\",\n                 \"context_image_token\": \"<IMG_CONTEXT>\",\n+                \"video_token\": \"<video>\",\n             },\n         )\n "
        }
    ],
    "stats": {
        "total": 34,
        "additions": 26,
        "deletions": 8
    }
}