{
    "author": "gante",
    "message": "[`chat`] generate parameterization powered by `GenerationConfig` and UX-related changes (#38047)\n\n* accept arbitrary kwargs\n\n* move user commands to a separate fn\n\n* work with generation config files\n\n* rm cmmt\n\n* docs\n\n* base generate flag doc section\n\n* nits\n\n* nits\n\n* nits\n\n* no <br>\n\n* better basic args description",
    "sha": "8efe3a9d7783a763b1bac2e7907325235152c6db",
    "files": [
        {
            "sha": "d8a711b48e5f313f510033748e9a19111f9133a7",
            "filename": "README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8efe3a9d7783a763b1bac2e7907325235152c6db/README.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8efe3a9d7783a763b1bac2e7907325235152c6db/README.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/README.md?ref=8efe3a9d7783a763b1bac2e7907325235152c6db",
            "patch": "@@ -120,7 +120,7 @@ To chat with a model, the usage pattern is the same. The only difference is you\n > [!TIP]\n > You can also chat with a model directly from the command line.\n > ```shell\n-> transformers chat --model_name_or_path Qwen/Qwen2.5-0.5B-Instruct\n+> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n > ```\n \n ```py"
        },
        {
            "sha": "94bb9fd591ca88ab5cc65d3f192f79c70f6b4c44",
            "filename": "docs/source/en/conversations.md",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8efe3a9d7783a763b1bac2e7907325235152c6db/docs%2Fsource%2Fen%2Fconversations.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8efe3a9d7783a763b1bac2e7907325235152c6db/docs%2Fsource%2Fen%2Fconversations.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fconversations.md?ref=8efe3a9d7783a763b1bac2e7907325235152c6db",
            "patch": "@@ -27,7 +27,7 @@ This guide shows you how to quickly start chatting with Transformers from the co\n \n ## transformers CLI\n \n-Chat with a model directly from the command line as shown below. It launches an interactive session with a model. Enter `clear` to reset the conversation, `exit` to terminate the session, and `help` to display all the command options.\n+After you've [installed Transformers](./installation.md), chat with a model directly from the command line as shown below. It launches an interactive session with a model, with a few base commands listed at the start of the session.\n \n ```bash\n transformers chat Qwen/Qwen2.5-0.5B-Instruct\n@@ -37,6 +37,12 @@ transformers chat Qwen/Qwen2.5-0.5B-Instruct\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers-chat-cli.png\"/>\n </div>\n \n+You can launch the CLI with arbitrary `generate` flags, with the format `arg_1=value_1 arg_2=value_2 ...`\n+\n+```bash\n+transformers chat Qwen/Qwen2.5-0.5B-Instruct do_sample=False max_new_tokens=10\n+```\n+\n For a full list of options, run the command below.\n \n ```bash"
        },
        {
            "sha": "a191cdb4634296e20066d51d69b740a5fe3246ba",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 21,
            "deletions": 3,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/8efe3a9d7783a763b1bac2e7907325235152c6db/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8efe3a9d7783a763b1bac2e7907325235152c6db/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=8efe3a9d7783a763b1bac2e7907325235152c6db",
            "patch": "@@ -20,9 +20,13 @@ rendered properly in your Markdown viewer.\n \n Text generation is the most popular application for large language models (LLMs). A LLM is trained to generate the next word (token) given some initial text (prompt) along with its own generated outputs up to a predefined length or when it reaches an end-of-sequence (`EOS`) token.\n \n-In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities.\n+In Transformers, the [`~GenerationMixin.generate`] API handles text generation, and it is available for all models with generative capabilities. This guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n \n-This guide will show you the basics of text generation with [`~GenerationMixin.generate`] and some common pitfalls to avoid.\n+> [!TIP]\n+> You can also chat with a model directly from the command line. ([reference](./conversations.md#transformers-cli))\n+> ```shell\n+> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n+> ```\n \n ## Default generate\n \n@@ -134,6 +138,20 @@ outputs = model.generate(**inputs, generation_config=generation_config)\n print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n ```\n \n+## Common Options\n+\n+[`~GenerationMixin.generate`] is a powerful tool that can be heavily customized. This can be daunting for a new users. This section contains a list of popular generation options that you can define in most text generation tools in Transformers: [`~GenerationMixin.generate`], [`GenerationConfig`], `pipelines`, the `chat` CLI, ...\n+\n+| Option name | Type | Simplified description |\n+|---|---|---|\n+| `max_new_tokens` | `int` | Controls the maximum generation length. Be sure to define it, as it usually defaults to a small value. |\n+| `do_sample` | `bool` | Defines whether generation will sample the next token (`True`), or is greedy instead (`False`). Most use cases should set this flag to `True`. Check [this guide](./generation_strategies.md) for more information. |\n+| `temperature` | `float` | How unpredictable the next selected token will be. High values (`>0.8`) are good for creative tasks, low values (e.g. `<0.4`) for tasks that require \"thinking\". Requires `do_sample=True`. |\n+| `num_beams` | `int` | When set to `>1`, activates the beam search algorithm. Beam search is good on input-grounded tasks. Check [this guide](./generation_strategies.md) for more information. |\n+| `repetition_penalty` | `float` | Set it to `>1.0` if you're seeing the model repeat itself often. Larger values apply a larger penalty. |\n+| `eos_token_id` | `List[int]` | The token(s) that will cause generation to stop. The default value is usually good, but you can specify a different token. |\n+\n+\n ## Pitfalls\n \n The section below covers some common issues you may encounter during text generation and how to solve them.\n@@ -286,4 +304,4 @@ Take a look below for some more specific and specialized text generation librari\n - [SynCode](https://github.com/uiuc-focal-lab/syncode): a library for context-free grammar guided generation (JSON, SQL, Python).\n - [Text Generation Inference](https://github.com/huggingface/text-generation-inference): a production-ready server for LLMs.\n - [Text generation web UI](https://github.com/oobabooga/text-generation-webui): a Gradio web UI for text generation.\n-- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo): additional logits processors for controlling text generation.\n\\ No newline at end of file\n+- [logits-processor-zoo](https://github.com/NVIDIA/logits-processor-zoo): additional logits processors for controlling text generation."
        },
        {
            "sha": "5c9bd76bdb09690453514482a1be513afd4622ad",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 298,
            "deletions": 168,
            "changes": 466,
            "blob_url": "https://github.com/huggingface/transformers/blob/8efe3a9d7783a763b1bac2e7907325235152c6db/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8efe3a9d7783a763b1bac2e7907325235152c6db/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=8efe3a9d7783a763b1bac2e7907325235152c6db",
            "patch": "@@ -13,12 +13,12 @@\n # limitations under the License.\n \n \n-import copy\n import json\n import os\n import platform\n import string\n import time\n+import warnings\n from argparse import ArgumentParser, Namespace\n from dataclasses import dataclass, field\n from threading import Thread\n@@ -42,7 +42,13 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TextIteratorStreamer\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        AutoTokenizer,\n+        BitsAndBytesConfig,\n+        GenerationConfig,\n+        TextIteratorStreamer,\n+    )\n \n \n ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n@@ -64,25 +70,16 @@\n     \"socks\": {\"text\": \"Why is it important to eat socks after meditating?\"},\n }\n \n-SUPPORTED_GENERATION_KWARGS = [\n-    \"max_new_tokens\",\n-    \"do_sample\",\n-    \"num_beams\",\n-    \"temperature\",\n-    \"top_p\",\n-    \"top_k\",\n-    \"repetition_penalty\",\n-]\n-\n # Printed at the start of a chat session\n HELP_STRING_MINIMAL = \"\"\"\n \n **TRANSFORMERS CHAT INTERFACE**\n \n Chat interface to try out a model. Besides chatting with the model, here are some basic commands:\n-- **help**: shows all available commands\n-- **clear**: clears the current conversation and starts a new one\n-- **exit**: closes the interface\n+- **!help**: shows all available commands\n+- **!status**: shows the current status of the model and generation settings\n+- **!clear**: clears the current conversation and starts a new one\n+- **!exit**: closes the interface\n \"\"\"\n \n \n@@ -92,18 +89,32 @@\n **TRANSFORMERS CHAT INTERFACE HELP**\n \n Full command list:\n-- **help**: shows this help message\n-- **clear**: clears the current conversation and starts a new one\n-- **example {{NAME}}**: loads example named `{{NAME}}` from the config and uses it as the user input. Available example\n-names: `{\"`, `\".join(DEFAULT_EXAMPLES.keys())}`\n-- **set {{SETTING_NAME}}={{SETTING_VALUE}};**: changes the system prompt or generation settings (multiple settings are\n-separated by a ';'). Available settings: `{\"`, `\".join(SUPPORTED_GENERATION_KWARGS)}`\n-- **reset**: same as clear but also resets the generation configs to defaults if they have been changed by **set**\n-- **save {{SAVE_NAME}} (optional)**: saves the current chat and settings to file by default to\n+- **!help**: shows this help message\n+- **!clear**: clears the current conversation and starts a new one\n+- **!status**: shows the current status of the model and generation settings\n+- **!example {{NAME}}**: loads example named `{{NAME}}` from the config and uses it as the user input.\n+Available example names: `{\"`, `\".join(DEFAULT_EXAMPLES.keys())}`\n+- **!set {{ARG_1}}={{VALUE_1}} {{ARG_2}}={{VALUE_2}}** ...: changes the system prompt or generation settings (multiple\n+settings are separated by a space). Accepts the same flags and format as the `generate_flags` CLI argument.\n+If you're a new user, check this basic flag guide: https://huggingface.co/docs/transformers/llm_tutorial#common-options\n+- **!save {{SAVE_NAME}} (optional)**: saves the current chat and settings to file by default to\n `./chat_history/{{MODEL_NAME}}/chat_{{DATETIME}}.yaml` or `{{SAVE_NAME}}` if provided\n-- **exit**: closes the interface\n+- **!exit**: closes the interface\n \"\"\"\n \n+# format: (optional CLI arg being deprecated, its current default, corresponding `generate` flag)\n+_DEPRECATION_MAP = [\n+    (\"max_new_tokens\", 256, \"max_new_tokens\"),\n+    (\"do_sample\", True, \"do_sample\"),\n+    (\"num_beams\", 1, \"num_beams\"),\n+    (\"temperature\", 1.0, \"temperature\"),\n+    (\"top_k\", 50, \"top_k\"),\n+    (\"top_p\", 1.0, \"top_p\"),\n+    (\"repetition_penalty\", 1.0, \"repetition_penalty\"),\n+    (\"eos_tokens\", None, \"eos_token_id\"),\n+    (\"eos_token_ids\", None, \"eos_token_id\"),\n+]\n+\n \n class RichInterface:\n     def __init__(self, model_name: Optional[str] = None, user_name: Optional[str] = None):\n@@ -181,6 +192,14 @@ def print_help(self, minimal: bool = False):\n         self._console.print(Markdown(HELP_STRING_MINIMAL if minimal else HELP_STRING))\n         self._console.print()\n \n+    def print_status(self, model_name: str, generation_config: GenerationConfig, model_kwargs: dict):\n+        \"\"\"Prints the status of the model and generation settings to the console.\"\"\"\n+        self._console.print(f\"[bold blue]Model: {model_name}\\n\")\n+        if model_kwargs:\n+            self._console.print(f\"[bold blue]Model kwargs: {model_kwargs}\")\n+        self._console.print(f\"[bold blue]{generation_config}\")\n+        self._console.print()\n+\n \n @dataclass\n class ChatArguments:\n@@ -207,6 +226,17 @@ class ChatArguments:\n     examples_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to a yaml file with examples.\"})\n \n     # Generation settings\n+    generation_config: Optional[str] = field(\n+        default=None,\n+        metadata={\n+            \"help\": (\n+                \"Path to a local generation config file or to a HuggingFace repo containing a \"\n+                \"`generation_config.json` file. Other generation settings passed as CLI arguments will be applied on \"\n+                \"top of this generation config.\"\n+            ),\n+        },\n+    )\n+    # Deprecated CLI args start here\n     max_new_tokens: int = field(default=256, metadata={\"help\": \"Maximum number of tokens to generate.\"})\n     do_sample: bool = field(default=True, metadata={\"help\": \"Whether to sample outputs during generation.\"})\n     num_beams: int = field(default=1, metadata={\"help\": \"Number of beams for beam search.\"})\n@@ -222,6 +252,7 @@ class ChatArguments:\n         default=None,\n         metadata={\"help\": \"EOS token IDs to stop the generation. If multiple they should be comma separated.\"},\n     )\n+    # Deprecated CLI args end here\n \n     # Model loading\n     model_revision: str = field(\n@@ -280,23 +311,66 @@ def register_subcommand(parser: ArgumentParser):\n \n         group = chat_parser.add_argument_group(\"Positional arguments\")\n         group.add_argument(\n-            \"model_name_or_path_positional\", type=str, nargs=\"?\", default=None, help=\"Name of the pre-trained model.\"\n+            \"model_name_or_path_positional\", type=str, default=None, help=\"Name of the pre-trained model.\"\n+        )\n+        group.add_argument(\n+            \"generate_flags\",\n+            type=str,\n+            default=None,\n+            help=(\n+                \"Flags to pass to `generate`, using a space as a separator between flags. Accepts booleans, numbers, \"\n+                \"and lists of integers, more advanced parameterization should be set through --generation-config. \"\n+                \"Example: `transformers chat <model_repo> max_new_tokens=100 do_sample=False eos_token_id=[1,2]`. \"\n+                \"If you're a new user, check this basic flag guide: https://huggingface.co/docs/transformers/llm_tutorial#common-options\"\n+            ),\n+            nargs=\"*\",\n         )\n-\n         chat_parser.set_defaults(func=chat_command_factory)\n \n     def __init__(self, args):\n-        args.model_name_or_path = args.model_name_or_path_positional or args.model_name_or_path\n+        args = self._handle_deprecated_args(args)\n+        self.args = args\n \n-        if args.model_name_or_path is None:\n+    def _handle_deprecated_args(self, args: ChatArguments) -> ChatArguments:\n+        \"\"\"\n+        Handles deprecated arguments and their deprecation cycle. To be removed after we fully migrated to the new\n+        args.\n+        \"\"\"\n+        has_warnings = False\n+\n+        # 1. Model as a positional argument\n+        args.model_name_or_path_positional = args.model_name_or_path_positional or args.model_name_or_path\n+        if args.model_name_or_path_positional is None:\n             raise ValueError(\n                 \"One of the following must be provided:\"\n-                \"\\n- The positional argument containing the model repo;\"\n-                \"\\n- the optional --model_name_or_path argument, containing the model repo\"\n-                \"\\ne.g. transformers chat <model_repo> or transformers chat --model_name_or_path <model_repo>\"\n+                \"\\n- The positional argument containing the model repo, e.g. `transformers chat <model_repo>`\"\n+                \"\\n- the optional --model_name_or_path argument, containing the model repo (deprecated)\"\n+            )\n+        elif args.model_name_or_path is not None:\n+            has_warnings = True\n+            warnings.warn(\n+                \"The --model_name_or_path argument is deprecated will be removed in v4.54.0. Use the positional \"\n+                \"argument instead, e.g. `transformers chat <model_repo>`.\",\n+                FutureWarning,\n             )\n+        # 2. Named generate option args\n+        for deprecated_arg, default_value, new_arg in _DEPRECATION_MAP:\n+            value = getattr(args, deprecated_arg)\n+            if value != default_value:\n+                has_warnings = True\n+                warnings.warn(\n+                    f\"The --{deprecated_arg} argument is deprecated will be removed in v4.54.0. There are two \"\n+                    \"alternative solutions to specify this generation option: \\n\"\n+                    \"1. Pass `--generation-config <path_to_file/Hub repo>` to specify a generation config.\\n\"\n+                    \"2. Pass `generate` flags through positional arguments, e.g. `transformers chat <model_repo> \"\n+                    f\"{new_arg}={value}`\",\n+                    FutureWarning,\n+                )\n \n-        self.args = args\n+        if has_warnings:\n+            print(\"\\n(Press enter to continue)\")\n+            input()\n+        return args\n \n     # -----------------------------------------------------------------------------------------------------------------\n     # Chat session methods\n@@ -319,7 +393,7 @@ def save_chat(chat, args: ChatArguments, filename: Optional[str] = None) -> str:\n \n         if filename is None:\n             time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n-            filename = f\"{args.model_name_or_path}/chat_{time_str}.json\"\n+            filename = f\"{args.model_name_or_path_positional}/chat_{time_str}.json\"\n             filename = os.path.join(folder, filename)\n \n         os.makedirs(os.path.dirname(filename), exist_ok=True)\n@@ -338,50 +412,95 @@ def clear_chat_history(system_prompt: Optional[str] = None) -> list[dict]:\n \n     # -----------------------------------------------------------------------------------------------------------------\n     # Input parsing methods\n-    @staticmethod\n-    def parse_settings(\n-        user_input: str, current_args: ChatArguments, interface: RichInterface\n-    ) -> tuple[ChatArguments, bool]:\n-        \"\"\"Parses the settings from the user input into the CLI arguments.\"\"\"\n-        settings = user_input[4:].strip().split(\";\")\n-        settings = [(setting.split(\"=\")[0], setting[len(setting.split(\"=\")[0]) + 1 :]) for setting in settings]\n-        settings = dict(settings)\n-        error = False\n-\n-        for name in settings:\n-            if hasattr(current_args, name):\n-                try:\n-                    if isinstance(getattr(current_args, name), bool):\n-                        if settings[name] == \"True\":\n-                            settings[name] = True\n-                        elif settings[name] == \"False\":\n-                            settings[name] = False\n-                        else:\n-                            raise ValueError\n-                    else:\n-                        settings[name] = type(getattr(current_args, name))(settings[name])\n-                except ValueError:\n-                    error = True\n-                    interface.print_color(\n-                        text=f\"Cannot cast setting {name} (={settings[name]}) to {type(getattr(current_args, name))}.\",\n-                        color=\"red\",\n-                    )\n-            else:\n-                interface.print_color(text=f\"There is no '{name}' setting.\", color=\"red\")\n+    def parse_generate_flags(self, generate_flags: list[str]) -> dict:\n+        \"\"\"Parses the generate flags from the user input into a dictionary of `generate` kwargs.\"\"\"\n+        if len(generate_flags) == 0:\n+            return {}\n+\n+        # Assumption: `generate_flags` is a list of strings, each string being a `flag=value` pair, that can be parsed\n+        # into a json string if we:\n+        # 1. Add quotes around each flag name\n+        generate_flags_as_dict = {'\"' + flag.split(\"=\")[0] + '\"': flag.split(\"=\")[1] for flag in generate_flags}\n+\n+        # 2. Handle types:\n+        # 2. a. booleans should be lowercase, None should be null\n+        generate_flags_as_dict = {\n+            k: v.lower() if v.lower() in [\"true\", \"false\"] else v for k, v in generate_flags_as_dict.items()\n+        }\n+        generate_flags_as_dict = {k: \"null\" if v == \"None\" else v for k, v in generate_flags_as_dict.items()}\n+\n+        # 2. b. strings should be quoted\n+        def is_number(s: str) -> bool:\n+            return s.replace(\".\", \"\", 1).isdigit()\n+\n+        generate_flags_as_dict = {k: f'\"{v}\"' if not is_number(v) else v for k, v in generate_flags_as_dict.items()}\n+        # 2. c. [no processing needed] lists are lists of ints because `generate` doesn't take lists of strings :)\n+        # We also mention in the help message that we only accept lists of ints for now.\n+\n+        # 3. Join the the result into a comma separated string\n+        generate_flags_string = \", \".join([f\"{k}: {v}\" for k, v in generate_flags_as_dict.items()])\n+\n+        # 4. Add the opening/closing brackets\n+        generate_flags_string = \"{\" + generate_flags_string + \"}\"\n+\n+        # 5. Remove quotes around boolean/null and around lists\n+        generate_flags_string = generate_flags_string.replace('\"null\"', \"null\")\n+        generate_flags_string = generate_flags_string.replace('\"true\"', \"true\")\n+        generate_flags_string = generate_flags_string.replace('\"false\"', \"false\")\n+        generate_flags_string = generate_flags_string.replace('\"[', \"[\")\n+        generate_flags_string = generate_flags_string.replace(']\"', \"]\")\n \n-        if error:\n-            interface.print_color(\n-                text=\"There was an issue parsing the settings. No settings have been changed.\",\n-                color=\"red\",\n+        # 6. Replace the `=` with `:`\n+        generate_flags_string = generate_flags_string.replace(\"=\", \":\")\n+\n+        try:\n+            processed_generate_flags = json.loads(generate_flags_string)\n+        except json.JSONDecodeError:\n+            raise ValueError(\n+                \"Failed to convert `generate_flags` into a valid JSON object.\"\n+                \"\\n`generate_flags` = {generate_flags}\"\n+                \"\\nConverted JSON string = {generate_flags_string}\"\n             )\n-        else:\n-            for name in settings:\n-                setattr(current_args, name, settings[name])\n-                interface.print_color(text=f\"Set {name} to {settings[name]}.\", color=\"green\")\n+        return processed_generate_flags\n \n-            time.sleep(1.5)  # so the user has time to read the changes\n+    def get_generation_parameterization(\n+        self, args: ChatArguments, tokenizer: AutoTokenizer\n+    ) -> tuple[GenerationConfig, dict]:\n+        \"\"\"\n+        Returns a GenerationConfig object holding the generation parameters for the CLI command.\n+        \"\"\"\n+        # No generation config arg provided -> use base generation config, apply CLI defaults\n+        if args.generation_config is None:\n+            generation_config = GenerationConfig()\n+            # Apply deprecated CLI args on top of the default generation config\n+            pad_token_id, eos_token_ids = self.parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n+            deprecated_kwargs = {\n+                \"max_new_tokens\": args.max_new_tokens,\n+                \"do_sample\": args.do_sample,\n+                \"num_beams\": args.num_beams,\n+                \"temperature\": args.temperature,\n+                \"top_k\": args.top_k,\n+                \"top_p\": args.top_p,\n+                \"repetition_penalty\": args.repetition_penalty,\n+                \"pad_token_id\": pad_token_id,\n+                \"eos_token_id\": eos_token_ids,\n+            }\n+            generation_config.update(**deprecated_kwargs)\n+        # generation config arg provided -> use it as the base parameterization\n+        else:\n+            if \".json\" in args.generation_config:  # is a local file\n+                dirname = os.path.dirname(args.generation_config)\n+                filename = os.path.basename(args.generation_config)\n+                generation_config = GenerationConfig.from_pretrained(dirname, filename)\n+            else:\n+                generation_config = GenerationConfig.from_pretrained(args.generation_config)\n \n-        return current_args, not error\n+        # Finally: parse and apply `generate_flags`\n+        parsed_generate_flags = self.parse_generate_flags(args.generate_flags)\n+        model_kwargs = generation_config.update(**parsed_generate_flags)\n+        # `model_kwargs` contain non-generation flags in `parsed_generate_flags` that should be passed directly to\n+        # `generate`\n+        return generation_config, model_kwargs\n \n     @staticmethod\n     def parse_eos_tokens(\n@@ -406,36 +525,6 @@ def parse_eos_tokens(\n \n         return pad_token_id, all_eos_token_ids\n \n-    @staticmethod\n-    def is_valid_setting_command(s: str) -> bool:\n-        # First check the basic structure\n-        if not s.startswith(\"set \") or \"=\" not in s:\n-            return False\n-\n-        # Split into individual assignments\n-        assignments = [a.strip() for a in s[4:].split(\";\") if a.strip()]\n-\n-        for assignment in assignments:\n-            # Each assignment should have exactly one '='\n-            if assignment.count(\"=\") != 1:\n-                return False\n-\n-            key, value = assignment.split(\"=\", 1)\n-            key = key.strip()\n-            value = value.strip()\n-            if not key or not value:\n-                return False\n-\n-            # Keys can only have alphabetic characters, spaces and underscores\n-            if not set(key).issubset(ALLOWED_KEY_CHARS):\n-                return False\n-\n-            # Values can have just about anything that isn't a semicolon\n-            if not set(value).issubset(ALLOWED_VALUE_CHARS):\n-                return False\n-\n-        return True\n-\n     # -----------------------------------------------------------------------------------------------------------------\n     # Model loading and performance automation methods\n     @staticmethod\n@@ -460,7 +549,7 @@ def get_quantization_config(model_args: ChatArguments) -> Optional[\"BitsAndBytes\n \n     def load_model_and_tokenizer(self, args: ChatArguments) -> tuple[AutoModelForCausalLM, AutoTokenizer]:\n         tokenizer = AutoTokenizer.from_pretrained(\n-            args.model_name_or_path,\n+            args.model_name_or_path_positional,\n             revision=args.model_revision,\n             trust_remote_code=args.trust_remote_code,\n         )\n@@ -475,14 +564,96 @@ def load_model_and_tokenizer(self, args: ChatArguments) -> tuple[AutoModelForCau\n             \"quantization_config\": quantization_config,\n         }\n         model = AutoModelForCausalLM.from_pretrained(\n-            args.model_name_or_path, trust_remote_code=args.trust_remote_code, **model_kwargs\n+            args.model_name_or_path_positional, trust_remote_code=args.trust_remote_code, **model_kwargs\n         )\n \n         if getattr(model, \"hf_device_map\", None) is None:\n             model = model.to(args.device)\n \n         return model, tokenizer\n \n+    # -----------------------------------------------------------------------------------------------------------------\n+    # User commands\n+    def handle_non_exit_user_commands(\n+        self,\n+        user_input: str,\n+        args: ChatArguments,\n+        interface: RichInterface,\n+        examples: dict[str, dict[str, str]],\n+        generation_config: GenerationConfig,\n+        model_kwargs: dict,\n+        chat: list[dict],\n+    ) -> tuple[list[dict], GenerationConfig, dict]:\n+        \"\"\"\n+        Handles all user commands except for `!exit`. May update the chat history (e.g. reset it) or the\n+        generation config (e.g. set a new flag).\n+        \"\"\"\n+\n+        if user_input == \"!clear\":\n+            chat = self.clear_chat_history(args.system_prompt)\n+            interface.clear()\n+\n+        elif user_input == \"!help\":\n+            interface.print_help()\n+\n+        elif user_input.startswith(\"!save\") and len(user_input.split()) < 2:\n+            split_input = user_input.split()\n+\n+            if len(split_input) == 2:\n+                filename = split_input[1]\n+            else:\n+                filename = None\n+            filename = self.save_chat(chat, args, filename)\n+            interface.print_color(text=f\"Chat saved in {filename}!\", color=\"green\")\n+\n+        elif user_input.startswith(\"!set\"):\n+            # splits the new args into a list of strings, each string being a `flag=value` pair (same format as\n+            # `generate_flags`)\n+            new_generate_flags = user_input[4:].strip()\n+            new_generate_flags = new_generate_flags.split()\n+            # sanity check: each member in the list must have an =\n+            for flag in new_generate_flags:\n+                if \"=\" not in flag:\n+                    interface.print_color(\n+                        text=(\n+                            f\"Invalid flag format, missing `=` after `{flag}`. Please use the format \"\n+                            \"`arg_1=value_1 arg_2=value_2 ...`.\"\n+                        ),\n+                        color=\"red\",\n+                    )\n+                    break\n+            else:\n+                # parses the new args into a dictionary of `generate` kwargs, and updates the corresponding variables\n+                parsed_new_generate_flags = self.parse_generate_flags(new_generate_flags)\n+                new_model_kwargs = generation_config.update(**parsed_new_generate_flags)\n+                model_kwargs.update(**new_model_kwargs)\n+\n+        elif user_input.startswith(\"!example\") and len(user_input.split()) == 2:\n+            example_name = user_input.split()[1]\n+            if example_name in examples:\n+                interface.clear()\n+                chat = []\n+                interface.print_user_message(examples[example_name][\"text\"])\n+                chat.append({\"role\": \"user\", \"content\": examples[example_name][\"text\"]})\n+            else:\n+                example_error = (\n+                    f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n+                )\n+                interface.print_color(text=example_error, color=\"red\")\n+\n+        elif user_input == \"!status\":\n+            interface.print_status(\n+                model_name=args.model_name_or_path_positional,\n+                generation_config=generation_config,\n+                model_kwargs=model_kwargs,\n+            )\n+\n+        else:\n+            interface.print_color(text=f\"'{user_input}' is not a valid command. Showing help message.\", color=\"red\")\n+            interface.print_help()\n+\n+        return chat, generation_config, model_kwargs\n+\n     # -----------------------------------------------------------------------------------------------------------------\n     # Main logic\n     def run(self):\n@@ -498,79 +669,45 @@ def run(self):\n             with open(args.examples_path) as f:\n                 examples = yaml.safe_load(f)\n \n-        current_args = copy.deepcopy(args)\n-\n         if args.user is None:\n             user = self.get_username()\n         else:\n             user = args.user\n \n         model, tokenizer = self.load_model_and_tokenizer(args)\n         generation_streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True, skip_prompt=True)\n+        generation_config, model_kwargs = self.get_generation_parameterization(args, tokenizer)\n \n-        pad_token_id, eos_token_ids = self.parse_eos_tokens(tokenizer, args.eos_tokens, args.eos_token_ids)\n-\n-        interface = RichInterface(model_name=args.model_name_or_path, user_name=user)\n+        interface = RichInterface(model_name=args.model_name_or_path_positional, user_name=user)\n         interface.clear()\n-        chat = self.clear_chat_history(current_args.system_prompt)\n+        chat = self.clear_chat_history(args.system_prompt)\n \n         # Starts the session with a minimal help message at the top, so that a user doesn't get stuck\n         interface.print_help(minimal=True)\n         while True:\n             try:\n                 user_input = interface.input()\n \n-                if user_input == \"clear\":\n-                    chat = self.clear_chat_history(current_args.system_prompt)\n-                    interface.clear()\n-                    continue\n-\n-                if user_input == \"help\":\n-                    interface.print_help()\n-                    continue\n-\n-                if user_input == \"exit\":\n-                    break\n-\n-                if user_input == \"reset\":\n-                    interface.clear()\n-                    current_args = copy.deepcopy(args)\n-                    chat = self.clear_chat_history(current_args.system_prompt)\n-                    continue\n-\n-                if user_input.startswith(\"save\") and len(user_input.split()) < 2:\n-                    split_input = user_input.split()\n-\n-                    if len(split_input) == 2:\n-                        filename = split_input[1]\n-                    else:\n-                        filename = None\n-                    filename = self.save_chat(chat, current_args, filename)\n-                    interface.print_color(text=f\"Chat saved in {filename}!\", color=\"green\")\n-                    continue\n-\n-                if self.is_valid_setting_command(user_input):\n-                    current_args, success = self.parse_settings(user_input, current_args, interface)\n-                    if success:\n-                        chat = []\n-                        interface.clear()\n-                        continue\n-\n-                if user_input.startswith(\"example\") and len(user_input.split()) == 2:\n-                    example_name = user_input.split()[1]\n-                    if example_name in examples:\n-                        interface.clear()\n-                        chat = []\n-                        interface.print_user_message(examples[example_name][\"text\"])\n-                        user_input = examples[example_name][\"text\"]\n+                # User commands\n+                if user_input.startswith(\"!\"):\n+                    # `!exit` is special, it breaks the loop\n+                    if user_input == \"!exit\":\n+                        break\n                     else:\n-                        example_error = (\n-                            f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n+                        chat, generation_config, model_kwargs = self.handle_non_exit_user_commands(\n+                            user_input=user_input,\n+                            args=args,\n+                            interface=interface,\n+                            examples=examples,\n+                            generation_config=generation_config,\n+                            model_kwargs=model_kwargs,\n+                            chat=chat,\n                         )\n-                        interface.print_color(text=example_error, color=\"red\")\n+                    # `!example` sends a user message to the model\n+                    if not user_input.startswith(\"!example\"):\n                         continue\n-\n-                chat.append({\"role\": \"user\", \"content\": user_input})\n+                else:\n+                    chat.append({\"role\": \"user\", \"content\": user_input})\n \n                 inputs = tokenizer.apply_chat_template(chat, return_tensors=\"pt\", add_generation_prompt=True).to(\n                     model.device\n@@ -580,15 +717,8 @@ def run(self):\n                     \"inputs\": inputs,\n                     \"attention_mask\": attention_mask,\n                     \"streamer\": generation_streamer,\n-                    \"max_new_tokens\": current_args.max_new_tokens,\n-                    \"do_sample\": current_args.do_sample,\n-                    \"num_beams\": current_args.num_beams,\n-                    \"temperature\": current_args.temperature,\n-                    \"top_k\": current_args.top_k,\n-                    \"top_p\": current_args.top_p,\n-                    \"repetition_penalty\": current_args.repetition_penalty,\n-                    \"pad_token_id\": pad_token_id,\n-                    \"eos_token_id\": eos_token_ids,\n+                    \"generation_config\": generation_config,\n+                    **model_kwargs,\n                 }\n \n                 thread = Thread(target=model.generate, kwargs=generation_kwargs)"
        }
    ],
    "stats": {
        "total": 500,
        "additions": 327,
        "deletions": 173
    }
}