{
    "author": "Cyrilvallez",
    "message": "Fix tied weight for Bart (for BC) (#42355)\n\n* fix\n\n* fix\n\n* break copied from\n\n* fix copied from",
    "sha": "2f7747c39092fd931c85fe5f9f2da06456455aa4",
    "files": [
        {
            "sha": "925bbe8ac3729572084be7c1872087f33e4026e6",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 60,
            "deletions": 0,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f7747c39092fd931c85fe5f9f2da06456455aa4/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f7747c39092fd931c85fe5f9f2da06456455aa4/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=2f7747c39092fd931c85fe5f9f2da06456455aa4",
            "patch": "@@ -897,6 +897,21 @@ def __init__(self, config: BartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n+        if self.config.tie_word_embeddings:\n+            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n+            # need check here, see issue #36247\n+            if missing_keys is not None:\n+                if \"shared.weight\" in missing_keys and \"decoder.embed_tokens.weight\" not in missing_keys:\n+                    self.encoder.embed_tokens.weight = self.decoder.embed_tokens.weight\n+                    self.shared.weight = self.decoder.embed_tokens.weight\n+                    missing_keys.discard(\"encoder.embed_token.weight\")\n+                    missing_keys.discard(\"shared.weight\")\n+\n+        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n+        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n+\n     def get_input_embeddings(self):\n         return self.shared\n \n@@ -1034,6 +1049,21 @@ def __init__(self, config: BartConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n+        if self.config.tie_word_embeddings:\n+            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n+            # need check here, see issue #36247\n+            if missing_keys is not None:\n+                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n+                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n+                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n+                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n+                    missing_keys.discard(\"model.shared.weight\")\n+\n+        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n+        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n+\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n@@ -1212,6 +1242,21 @@ def __init__(self, config: BartConfig, **kwargs):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n+        if self.config.tie_word_embeddings:\n+            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n+            # need check here, see issue #36247\n+            if missing_keys is not None:\n+                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n+                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n+                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n+                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n+                    missing_keys.discard(\"model.shared.weight\")\n+\n+        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n+        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n+\n     @auto_docstring\n     def forward(\n         self,\n@@ -1343,6 +1388,21 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def tie_weights(self, missing_keys: Optional[set[str]] = None, recompute_mapping: bool = True):\n+        \"\"\"We need to overload here to handle the wrong key saved in some main checkpoints.\"\"\"\n+        if self.config.tie_word_embeddings:\n+            # Some model checkpoints like \"facebook/bart-large-cnn\"'s embedding weight is in decoder.embed_tokens,\n+            # need check here, see issue #36247\n+            if missing_keys is not None:\n+                if \"model.shared.weight\" in missing_keys and \"model.decoder.embed_tokens.weight\" not in missing_keys:\n+                    self.model.encoder.embed_tokens.weight = self.model.decoder.embed_tokens.weight\n+                    self.model.shared.weight = self.model.decoder.embed_tokens.weight\n+                    missing_keys.discard(\"model.encoder.embed_token.weight\")\n+                    missing_keys.discard(\"model.shared.weight\")\n+\n+        # needs to be done after, otherwise it raises an Error because the correct weights are not present\n+        super().tie_weights(missing_keys=missing_keys, recompute_mapping=recompute_mapping)\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b42d5ce3740983871c16d5b8deb37e84eb7380b5",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/2f7747c39092fd931c85fe5f9f2da06456455aa4/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2f7747c39092fd931c85fe5f9f2da06456455aa4/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=2f7747c39092fd931c85fe5f9f2da06456455aa4",
            "patch": "@@ -2183,14 +2183,14 @@ def forward(\n     The BigBirdPegasus Model with a language modeling head. Can be used for summarization.\n     \"\"\"\n )\n-# Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n class BigBirdPegasusForConditionalGeneration(BigBirdPegasusPreTrainedModel, GenerationMixin):\n     base_model_prefix = \"model\"\n     _tied_weights_keys = {\n         \"lm_head.weight\": \"model.shared.weight\",\n     }\n     _keys_to_ignore_on_load_missing = [\"final_logits_bias\"]\n \n+    # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.__init__ with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n     def __init__(self, config: BigBirdPegasusConfig):\n         super().__init__(config)\n         self.model = BigBirdPegasusModel(config)\n@@ -2200,13 +2200,15 @@ def __init__(self, config: BigBirdPegasusConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.resize_token_embeddings with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n     def resize_token_embeddings(\n         self, new_num_tokens: int, pad_to_multiple_of: Optional[int] = None, mean_resizing: bool = True\n     ) -> nn.Embedding:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens, pad_to_multiple_of, mean_resizing)\n         self._resize_final_logits_bias(new_embeddings.weight.shape[0])\n         return new_embeddings\n \n+    # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration._resize_final_logits_bias with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n     def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n         old_num_tokens = self.final_logits_bias.shape[-1]\n         if new_num_tokens <= old_num_tokens:\n@@ -2217,7 +2219,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n     @auto_docstring\n-    # Ignore copy\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -2327,6 +2328,7 @@ def forward(\n             encoder_attentions=outputs.encoder_attentions,\n         )\n \n+    # Copied from transformers.models.bart.modeling_bart.BartForConditionalGeneration.prepare_decoder_input_ids_from_labels with Bart->BigBirdPegasus, BART->BIGBIRD_PEGASUS\n     def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n "
        }
    ],
    "stats": {
        "total": 66,
        "additions": 64,
        "deletions": 2
    }
}