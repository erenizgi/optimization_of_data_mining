{
    "author": "sbucaille",
    "message": "Update SuperPoint model card (#38896)\n\n* docs: first draft to more standard SuperPoint documentation\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* docs: reverted changes on Auto classes\n\n* docs: addressed the rest of the comments\n\n* docs: remove outdated reference to keypoint detection task guide in SuperPoint documentation\n\n* Update superpoint.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "f171e7e884f4435a372b0690a50db251bc4302a8",
    "files": [
        {
            "sha": "31f40e5a374ecc578f901855a0f41a3c4b0fe0cd",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 85,
            "deletions": 85,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/f171e7e884f4435a372b0690a50db251bc4302a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f171e7e884f4435a372b0690a50db251bc4302a8/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=f171e7e884f4435a372b0690a50db251bc4302a8",
            "patch": "@@ -10,48 +10,35 @@ specific language governing permissions and limitations under the License.\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n \n-\n -->\n \n-# SuperPoint\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\" >\n+    </div>\n </div>\n \n-## Overview\n-\n-The SuperPoint model was proposed\n-in [SuperPoint: Self-Supervised Interest Point Detection and Description](https://huggingface.co/papers/1712.07629) by Daniel\n-DeTone, Tomasz Malisiewicz and Andrew Rabinovich.\n-\n-This model is the result of a self-supervised training of a fully-convolutional network for interest point detection and\n-description. The model is able to detect interest points that are repeatable under homographic transformations and\n-provide a descriptor for each point. The use of the model in its own is limited, but it can be used as a feature\n-extractor for other tasks such as homography estimation, image matching, etc.\n-\n-The abstract from the paper is the following:\n+# SuperPoint\n \n-*This paper presents a self-supervised framework for training interest point detectors and descriptors suitable for a\n-large number of multiple-view geometry problems in computer vision. As opposed to patch-based neural networks, our\n-fully-convolutional model operates on full-sized images and jointly computes pixel-level interest point locations and\n-associated descriptors in one forward pass. We introduce Homographic Adaptation, a multi-scale, multi-homography\n-approach for boosting interest point detection repeatability and performing cross-domain adaptation (e.g.,\n-synthetic-to-real). Our model, when trained on the MS-COCO generic image dataset using Homographic Adaptation, is able\n-to repeatedly detect a much richer set of interest points than the initial pre-adapted deep model and any other\n-traditional corner detector. The final system gives rise to state-of-the-art homography estimation results on HPatches\n-when compared to LIFT, SIFT and ORB.*\n+[SuperPoint](https://huggingface.co/papers/1712.07629) is the result of self-supervised training of a fully-convolutional network for interest point detection and description. The model is able to detect interest points that are repeatable under homographic transformations and provide a descriptor for each point. Usage on it's own is limited, but it can be used as a feature extractor for other tasks such as homography estimation and image matching.\n \n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/superpoint_architecture.png\"\n alt=\"drawing\" width=\"500\"/>\n \n-<small> SuperPoint overview. Taken from the <a href=\"https://huggingface.co/papers/1712.07629v4\">original paper.</a> </small>\n+You can find all the original SuperPoint checkpoints under the [Magic Leap Community](https://huggingface.co/magic-leap-community) organization.\n \n-## Usage tips\n+> [!TIP]\n+> This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n+>\n+> Click on the SuperPoint models in the right sidebar for more examples of how to apply SuperPoint to different computer vision tasks.\n \n-Here is a quick example of using the model to detect interest points in an image:\n \n-```python\n+\n+The example below demonstrates how to detect interest points in an image with the [`AutoModel`] class.\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n+\n+```py\n from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n import torch\n from PIL import Image\n@@ -64,67 +51,76 @@ processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\"\n model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n \n inputs = processor(image, return_tensors=\"pt\")\n-outputs = model(**inputs)\n-```\n-\n-The outputs contain the list of keypoint coordinates with their respective score and description (a 256-long vector).\n-\n-You can also feed multiple images to the model. Due to the nature of SuperPoint, to output a dynamic number of keypoints,\n-you will need to use the mask attribute to retrieve the respective information :\n-\n-```python\n-from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n-import torch\n-from PIL import Image\n-import requests\n-\n-url_image_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-image_1 = Image.open(requests.get(url_image_1, stream=True).raw)\n-url_image_2 = \"http://images.cocodataset.org/test-stuff2017/000000000568.jpg\"\n-image_2 = Image.open(requests.get(url_image_2, stream=True).raw)\n-\n-images = [image_1, image_2]\n-\n-processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\n-model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n-\n-inputs = processor(images, return_tensors=\"pt\")\n-outputs = model(**inputs)\n-image_sizes = [(image.height, image.width) for image in images]\n-outputs = processor.post_process_keypoint_detection(outputs, image_sizes)\n-\n-for output in outputs:\n-    for keypoints, scores, descriptors in zip(output[\"keypoints\"], output[\"scores\"], output[\"descriptors\"]):\n-        print(f\"Keypoints: {keypoints}\")\n-        print(f\"Scores: {scores}\")\n-        print(f\"Descriptors: {descriptors}\")\n-```\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n-You can then print the keypoints on the image of your choice to visualize the result:\n-```python\n-import matplotlib.pyplot as plt\n-\n-plt.axis(\"off\")\n-plt.imshow(image_1)\n-plt.scatter(\n-    outputs[0][\"keypoints\"][:, 0],\n-    outputs[0][\"keypoints\"][:, 1],\n-    c=outputs[0][\"scores\"] * 100,\n-    s=outputs[0][\"scores\"] * 50,\n-    alpha=0.8\n-)\n-plt.savefig(f\"output_image.png\")\n+# Post-process to get keypoints, scores, and descriptors\n+image_size = (image.height, image.width)\n+processed_outputs = processor.post_process_keypoint_detection(outputs, [image_size])\n ```\n-![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/ZtFmphEhx8tcbEQqOolyE.png)\n \n-This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n-The original code can be found [here](https://github.com/magicleap/SuperPointPretrainedNetwork).\n+</hfoption>\n+</hfoptions>\n+\n+## Notes\n+\n+- SuperPoint outputs a dynamic number of keypoints per image, which makes it suitable for tasks requiring variable-length feature representations.\n+\n+    ```py\n+    from transformers import AutoImageProcessor, SuperPointForKeypointDetection\n+    import torch\n+    from PIL import Image\n+    import requests\n+    processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superpoint\")\n+    model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/superpoint\")\n+    url_image_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image_1 = Image.open(requests.get(url_image_1, stream=True).raw)\n+    url_image_2 = \"http://images.cocodataset.org/test-stuff2017/000000000568.jpg\"\n+    image_2 = Image.open(requests.get(url_image_2, stream=True).raw)\n+    images = [image_1, image_2]\n+    inputs = processor(images, return_tensors=\"pt\")\n+    # Example of handling dynamic keypoint output\n+    outputs = model(**inputs)\n+    keypoints = outputs.keypoints  # Shape varies per image\n+    scores = outputs.scores        # Confidence scores for each keypoint\n+    descriptors = outputs.descriptors  # 256-dimensional descriptors\n+    mask = outputs.mask # Value of 1 corresponds to a keypoint detection\n+    ```\n+\n+- The model provides both keypoint coordinates and their corresponding descriptors (256-dimensional vectors) in a single forward pass.\n+- For batch processing with multiple images, you need to use the mask attribute to retrieve the respective information for each image. You can use the `post_process_keypoint_detection` from the `SuperPointImageProcessor` to retrieve the each image information.\n+\n+    ```py\n+    # Batch processing example\n+    images = [image1, image2, image3]\n+    inputs = processor(images, return_tensors=\"pt\")\n+    outputs = model(**inputs)\n+    image_sizes = [(img.height, img.width) for img in images]\n+    processed_outputs = processor.post_process_keypoint_detection(outputs, image_sizes)\n+    ```\n+\n+- You can then print the keypoints on the image of your choice to visualize the result:\n+    ```py\n+    import matplotlib.pyplot as plt\n+    plt.axis(\"off\")\n+    plt.imshow(image_1)\n+    plt.scatter(\n+        outputs[0][\"keypoints\"][:, 0],\n+        outputs[0][\"keypoints\"][:, 1],\n+        c=outputs[0][\"scores\"] * 100,\n+        s=outputs[0][\"scores\"] * 50,\n+        alpha=0.8\n+    )\n+    plt.savefig(f\"output_image.png\")\n+    ```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/ZtFmphEhx8tcbEQqOolyE.png\">\n+</div>\n \n ## Resources\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SuperPoint. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n-\n-- A notebook showcasing inference and visualization with SuperPoint can be found [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb). ðŸŒŽ\n+- Refer to this [noteboook](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/SuperPoint/Inference_with_SuperPoint_to_detect_interest_points_in_an_image.ipynb) for an inference and visualization example.\n \n ## SuperPointConfig\n \n@@ -137,8 +133,12 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n - preprocess\n - post_process_keypoint_detection\n \n+<frameworkcontent>\n+<pt>\n ## SuperPointForKeypointDetection\n \n [[autodoc]] SuperPointForKeypointDetection\n \n - forward\n+\n+</pt>"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 85,
        "deletions": 85
    }
}