{
    "author": "Rocketknight1",
    "message": "Add a fix for custom code tokenizers in pipelines (#32300)\n\n* Add a fix for the case when tokenizers are passed as a string\r\n\r\n* Support image processors and feature extractors as well\r\n\r\n* Reverting load_feature_extractor and load_image_processor\r\n\r\n* Add test\r\n\r\n* Test is torch-only\r\n\r\n* Add tests for preprocessors and feature extractors and move test\r\n\r\n* Extremely experimental fix\r\n\r\n* Revert that change, wrong branch!\r\n\r\n* Typo!\r\n\r\n* Split tests",
    "sha": "9956c2bc984d0bf65c52b22b98b882317e7264af",
    "files": [
        {
            "sha": "87baa6b99a120f6cbd3dc84026c962afa72481a4",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9956c2bc984d0bf65c52b22b98b882317e7264af/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9956c2bc984d0bf65c52b22b98b882317e7264af/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=9956c2bc984d0bf65c52b22b98b882317e7264af",
            "patch": "@@ -904,7 +904,11 @@ def pipeline(\n \n     model_config = model.config\n     hub_kwargs[\"_commit_hash\"] = model.config._commit_hash\n-    load_tokenizer = type(model_config) in TOKENIZER_MAPPING or model_config.tokenizer_class is not None\n+    load_tokenizer = (\n+        type(model_config) in TOKENIZER_MAPPING\n+        or model_config.tokenizer_class is not None\n+        or isinstance(tokenizer, str)\n+    )\n     load_feature_extractor = type(model_config) in FEATURE_EXTRACTOR_MAPPING or feature_extractor is not None\n     load_image_processor = type(model_config) in IMAGE_PROCESSOR_MAPPING or image_processor is not None\n "
        },
        {
            "sha": "f4aa1a27f505d732233dc6690a44f4cedac62da1",
            "filename": "tests/pipelines/test_pipelines_common.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/9956c2bc984d0bf65c52b22b98b882317e7264af/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9956c2bc984d0bf65c52b22b98b882317e7264af/tests%2Fpipelines%2Ftest_pipelines_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_common.py?ref=9956c2bc984d0bf65c52b22b98b882317e7264af",
            "patch": "@@ -26,10 +26,13 @@\n from requests.exceptions import HTTPError\n \n from transformers import (\n+    AutomaticSpeechRecognitionPipeline,\n     AutoModelForSequenceClassification,\n     AutoTokenizer,\n     DistilBertForSequenceClassification,\n+    MaskGenerationPipeline,\n     TextClassificationPipeline,\n+    TextGenerationPipeline,\n     TFAutoModelForSequenceClassification,\n     pipeline,\n )\n@@ -859,6 +862,42 @@ def new_forward(*args, **kwargs):\n \n         self.assertEqual(self.COUNT, 1)\n \n+    @require_torch\n+    def test_custom_code_with_string_tokenizer(self):\n+        # This test checks for an edge case - tokenizer loading used to fail when using a custom code model\n+        # with a separate tokenizer that was passed as a repo name rather than a tokenizer object.\n+        # See https://github.com/huggingface/transformers/issues/31669\n+        text_generator = pipeline(\n+            \"text-generation\",\n+            model=\"Rocketknight1/fake-custom-model-test\",\n+            tokenizer=\"Rocketknight1/fake-custom-model-test\",\n+            trust_remote_code=True,\n+        )\n+\n+        self.assertIsInstance(text_generator, TextGenerationPipeline)  # Assert successful loading\n+\n+    @require_torch\n+    def test_custom_code_with_string_feature_extractor(self):\n+        speech_recognizer = pipeline(\n+            \"automatic-speech-recognition\",\n+            model=\"Rocketknight1/fake-custom-wav2vec2\",\n+            feature_extractor=\"Rocketknight1/fake-custom-wav2vec2\",\n+            trust_remote_code=True,\n+        )\n+\n+        self.assertIsInstance(speech_recognizer, AutomaticSpeechRecognitionPipeline)  # Assert successful loading\n+\n+    @require_torch\n+    def test_custom_code_with_string_preprocessor(self):\n+        mask_generator = pipeline(\n+            \"mask-generation\",\n+            model=\"Rocketknight1/fake-custom-sam\",\n+            processor=\"Rocketknight1/fake-custom-sam\",\n+            trust_remote_code=True,\n+        )\n+\n+        self.assertIsInstance(mask_generator, MaskGenerationPipeline)  # Assert successful loading\n+\n \n @require_torch\n @is_staging_test"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 44,
        "deletions": 1
    }
}