{
    "author": "ArthurZucker",
    "message": "[`FlexAttention`] Update gemma2 (#34942)\n\n* update tests\n\n* now maybe this fixes the previous fialing tests!\n\n* nit default\n\n* Update src/transformers/models/gemma2/modular_gemma2.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fix-copies\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "4c1388f48e72dde96b666f8fe24f74bf4056e410",
    "files": [
        {
            "sha": "5504caf148413901f89db2e7d5c6e3cf6659dc99",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1388f48e72dde96b666f8fe24f74bf4056e410/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1388f48e72dde96b666f8fe24f74bf4056e410/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=4c1388f48e72dde96b666f8fe24f74bf4056e410",
            "patch": "@@ -247,9 +247,12 @@ def tanh_softcap(score, b, h, q_idx, kv_idx):\n         return_lse=output_attentions,\n     )\n     if not output_attentions:\n-        return attn_output, None\n+        attn_weights = None\n     else:\n-        return attn_output[0], attn_output[1]\n+        attn_output, attn_weights = attn_output\n+\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n \n \n def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n@@ -280,6 +283,7 @@ def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n         is_causal=is_causal,\n         scale=config.scaling,\n     )\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, None\n \n \n@@ -362,7 +366,7 @@ def forward(\n \n         if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n             logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n-            attention_type = \"eager\"\n+            attention_type = \"flex_attention\"\n         else:\n             attention_type = self.config._attn_implementation\n "
        },
        {
            "sha": "87e090aa8195cbc7558dd4317a459880fb824e0d",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1388f48e72dde96b666f8fe24f74bf4056e410/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1388f48e72dde96b666f8fe24f74bf4056e410/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=4c1388f48e72dde96b666f8fe24f74bf4056e410",
            "patch": "@@ -290,9 +290,12 @@ def tanh_softcap(score, b, h, q_idx, kv_idx):\n         return_lse=output_attentions,\n     )\n     if not output_attentions:\n-        return attn_output, None\n+        attn_weights = None\n     else:\n-        return attn_output[0], attn_output[1]\n+        attn_output, attn_weights = attn_output\n+\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    return attn_output, attn_weights\n \n \n def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n@@ -323,6 +326,7 @@ def sdpa_attention_forward(config, query, key, value, mask, **_kwargs):\n         is_causal=is_causal,\n         scale=config.scaling,\n     )\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n     return attn_output, None\n \n \n@@ -405,7 +409,7 @@ def forward(\n \n         if output_attentions and self.config._attn_implementation in [\"sdpa\", \"flash_attention_2\"]:\n             logger.warning_once(\"Setting `attention_type` to `flex_attention` because `output_attentions=True`\")\n-            attention_type = \"eager\"\n+            attention_type = \"flex_attention\"\n         else:\n             attention_type = self.config._attn_implementation\n "
        },
        {
            "sha": "d65c961bcf65f31f4abbe07310992dd2a8045490",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c1388f48e72dde96b666f8fe24f74bf4056e410/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c1388f48e72dde96b666f8fe24f74bf4056e410/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=4c1388f48e72dde96b666f8fe24f74bf4056e410",
            "patch": "@@ -385,7 +385,7 @@ def test_model_9b_bf16_flex_attention(self):\n         model = AutoModelForCausalLM.from_pretrained(\n             model_id, low_cpu_mem_usage=True, torch_dtype=torch.bfloat16, attn_implementation=\"flex_attention\"\n         ).to(torch_device)\n-\n+        assert model.config._attn_implementation == \"flex_attention\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(torch_device)\n "
        }
    ],
    "stats": {
        "total": 22,
        "additions": 15,
        "deletions": 7
    }
}