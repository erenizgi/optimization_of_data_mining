{
    "author": "ManukyanD",
    "message": "Fix Gemma2 synced multi-GPU generation (#35232)\n\n* Fix Gemma2 synced multi-GPU generation\n\n* Fix import ordering in modular_gemma2.py",
    "sha": "4831a94ee7d897c15f2dbf93dd039d36d9fdc61a",
    "files": [
        {
            "sha": "eef2f1a6304e1e563e1dcffe4d728ad8b670370f",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4831a94ee7d897c15f2dbf93dd039d36d9fdc61a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4831a94ee7d897c15f2dbf93dd039d36d9fdc61a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=4831a94ee7d897c15f2dbf93dd039d36d9fdc61a",
            "patch": "@@ -41,6 +41,7 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -936,8 +937,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        },
        {
            "sha": "351f083f813dbd34fe042f29eb3fc286e7e7545d",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/4831a94ee7d897c15f2dbf93dd039d36d9fdc61a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4831a94ee7d897c15f2dbf93dd039d36d9fdc61a/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=4831a94ee7d897c15f2dbf93dd039d36d9fdc61a",
            "patch": "@@ -29,7 +29,7 @@\n )\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import is_torchdynamo_compiling, logging\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n     GemmaForCausalLM,\n@@ -668,8 +668,13 @@ def prepare_inputs_for_generation(\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         # Exception 1: when passing input_embeds, input_ids may be missing entries\n         # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n+        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n+        #              (we can't check exception 3 while compiling)\n         if past_key_values is not None:\n-            if inputs_embeds is not None:  # Exception 1\n+            if (\n+                inputs_embeds is not None  # Exception 1\n+                or (is_torchdynamo_compiling() or cache_position[-1] >= input_ids.shape[1])  # Exception 3\n+            ):\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]"
        }
    ],
    "stats": {
        "total": 17,
        "additions": 14,
        "deletions": 3
    }
}