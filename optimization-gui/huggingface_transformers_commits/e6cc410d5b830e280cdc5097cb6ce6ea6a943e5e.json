{
    "author": "zucchini-nlp",
    "message": "Remove flakiness in VLMs  (#36242)\n\n* fix\n\n* nit\n\n* no logits processor needed\n\n* two more tests on assisted decoding",
    "sha": "e6cc410d5b830e280cdc5097cb6ce6ea6a943e5e",
    "files": [
        {
            "sha": "23190ebe8515845bf699a548900206d0aca1ab2f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 10,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/e6cc410d5b830e280cdc5097cb6ce6ea6a943e5e/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e6cc410d5b830e280cdc5097cb6ce6ea6a943e5e/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e6cc410d5b830e280cdc5097cb6ce6ea6a943e5e",
            "patch": "@@ -113,6 +113,10 @@\n from transformers.utils import is_sklearn_available\n \n \n+# TODO: raushan remove this when VLMs start accepting input embeds\n+VLM_CLASS_NAMES = [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\", \"gotocr2\", \"qwen2vl\", \"qwen2_5_vl\"]\n+\n+\n class GenerationTesterMixin:\n     input_name = \"input_ids\"\n     model_tester = None\n@@ -1258,6 +1262,7 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                     \"blip2\",  # overridden `generate()`\n                     \"instructblip\",\n                     \"instructblipvideo\",\n+                    *VLM_CLASS_NAMES,  # shouldn't suggest image tokens\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n@@ -1411,7 +1416,8 @@ def test_assisted_decoding_sample(self):\n                 \"return_dict_in_generate\": True,\n                 \"use_cache\": True,\n             }\n-            output_assisted = model.generate(**generation_kwargs, **inputs_dict)\n+            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n+            output_assisted = model.generate(**generation_kwargs, **inputs_dict, **logits_processor_kwargs)\n \n             self._check_generate_outputs(output_assisted, config, use_cache=True)\n \n@@ -1690,16 +1696,15 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n             #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n             #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n             pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower()\n-                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\", \"gotocr2\"]\n+                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n             )\n             if pixel_values_is_mutually_exclusive:\n                 inputs_dict.pop(\"pixel_values\", None)\n                 inputs_dict.pop(\"pixel_values_videos\", None)\n                 inputs_dict.pop(\"pixel_values_images\", None)\n             #   2.C - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n             has_complex_embeds_computation = any(\n-                model_name in model_class.__name__.lower() for model_name in [\"moshi\", \"qwen2vl\", \"qwen2_5_vl\"]\n+                model_name in model_class.__name__.lower() for model_name in [\"moshi\"]\n             )\n             # 3 - `inputs_dict` doesn't contain `attention_mask`. When `attention_mask` is not passed to generate,\n             # we infer it from `input_ids`. The last test case will fail if there is a pad token in the original input.\n@@ -1769,8 +1774,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n             #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n             pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower()\n-                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n+                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n             )\n             if pixel_values_is_mutually_exclusive:\n                 inputs_dict.pop(\"pixel_values\", None)\n@@ -1929,8 +1933,7 @@ def test_generate_continue_from_inputs_embeds(self):\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n             pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower()\n-                for model_name in [\"llava\", \"idefics2\", \"idefics3\", \"mllama\", \"paligemma\", \"emu3\"]\n+                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n             )\n             if pixel_values_is_mutually_exclusive:\n                 inputs_dict.pop(\"pixel_values\", None)\n@@ -2311,11 +2314,14 @@ def test_assisted_decoding_with_logits_to_keep(self):\n                 \"return_dict_in_generate\": True,\n                 \"output_scores\": True,\n             }\n+            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n \n             # Setting logits_to_keep at 0 keeps all logits (old behavior)\n-            with_all_logits = model.generate(**generation_kwargs, **inputs_dict, logits_to_keep=0)\n+            with_all_logits = model.generate(\n+                **generation_kwargs, **inputs_dict, **logits_processor_kwargs, logits_to_keep=0\n+            )\n             # By default, logits_to_keep is automatically set to 1 if not provided (new behavior)\n-            without_all_logits = model.generate(**inputs_dict, **generation_kwargs)\n+            without_all_logits = model.generate(**inputs_dict, **generation_kwargs, **logits_processor_kwargs)\n \n             self._check_similar_generate_outputs(with_all_logits, without_all_logits)\n "
        }
    ],
    "stats": {
        "total": 26,
        "additions": 16,
        "deletions": 10
    }
}