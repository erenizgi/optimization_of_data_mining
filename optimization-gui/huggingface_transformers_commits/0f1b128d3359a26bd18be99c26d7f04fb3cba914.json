{
    "author": "manueldeprada",
    "message": "‚ö†Ô∏è üî¥ Add ministral model (#40247)\n\n* add ministral model\n\n* docs, tests\n\n* nits\n\n* fix tests\n\n* run modular after merge\n\n* opsie\n\n* integration tests\n\n* again\n\n* fff\n\n* dtype\n\n* rerun modular\n\n* arthur review\n\n* ops\n\n* review",
    "sha": "0f1b128d3359a26bd18be99c26d7f04fb3cba914",
    "files": [
        {
            "sha": "240d912f10b6717f39c4571ff2e28436a656fa07",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -585,6 +585,8 @@\n         title: MegatronGPT2\n       - local: model_doc/minimax\n         title: MiniMax\n+      - local: model_doc/ministral\n+        title: Ministral\n       - local: model_doc/mistral\n         title: Mistral\n       - local: model_doc/mixtral"
        },
        {
            "sha": "07692c6163e5bafc657db093910e243c3e19915f",
            "filename": "docs/source/en/model_doc/ministral.md",
            "status": "added",
            "additions": 86,
            "deletions": 0,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fministral.md?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,86 @@\n+<!--Copyright 2024 Mistral AI and The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"Tensor parallelism\" src=\"https://img.shields.io/badge/Tensor%20parallelism-06b6d4?style=flat&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Ministral\n+\n+[Ministral](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410) is a 8B parameter language model that extends the Mistral architecture with alternating attention pattern. Unlike Mistral, that uses either full attention or sliding window attention consistently, Ministral alternates between full attention and sliding window attention layers, in a pattern of 1 full attention layer followed by 3 sliding window attention layers. This allows for a 128K context length support.\n+\n+This architecture turns out to coincide with Qwen2, with the main difference being the presence of biases in attention projections in Ministral.\n+\n+\n+You can find the Ministral checkpoints under the [Mistral AI](https://huggingface.co/mistralai) organization.\n+\n+## Usage\n+\n+The example below demonstrates how to use Ministral for text generation:\n+\n+```python\n+>>> import torch\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\")\n+\n+>>> messages = [\n+...     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n+...     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n+...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n+... ]\n+\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+\n+>>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n+>>> tokenizer.batch_decode(generated_ids)[0]\n+\"Mayonnaise can be made as follows: (...)\"\n+```\n+\n+## MinistralConfig\n+\n+[[autodoc]] MinistralConfig\n+\n+## MinistralModel\n+\n+[[autodoc]] MinistralModel\n+    - forward\n+\n+## MinistralForCausalLM\n+\n+[[autodoc]] MinistralForCausalLM\n+    - forward\n+\n+## MinistralForSequenceClassification\n+\n+[[autodoc]] MinistralForSequenceClassification\n+    - forward\n+\n+## MinistralForTokenClassification\n+\n+[[autodoc]] MinistralForTokenClassification\n+    - forward\n+\n+## MinistralForQuestionAnswering\n+\n+[[autodoc]] MinistralForQuestionAnswering\n+- forward\n\\ No newline at end of file"
        },
        {
            "sha": "1e70e0e7b4d72ded306c99c2f593829e950bfd34",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -208,6 +208,7 @@\n     from .mgp_str import *\n     from .mimi import *\n     from .minimax import *\n+    from .ministral import *\n     from .mistral import *\n     from .mistral3 import *\n     from .mixtral import *"
        },
        {
            "sha": "0d6981d685ed5a0a4e255cfdf9735ccc474ba175",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -250,6 +250,7 @@\n         (\"mgp-str\", \"MgpstrConfig\"),\n         (\"mimi\", \"MimiConfig\"),\n         (\"minimax\", \"MiniMaxConfig\"),\n+        (\"ministral\", \"MinistralConfig\"),\n         (\"mistral\", \"MistralConfig\"),\n         (\"mistral3\", \"Mistral3Config\"),\n         (\"mixtral\", \"MixtralConfig\"),\n@@ -682,6 +683,7 @@\n         (\"mgp-str\", \"MGP-STR\"),\n         (\"mimi\", \"Mimi\"),\n         (\"minimax\", \"MiniMax\"),\n+        (\"ministral\", \"Ministral\"),\n         (\"mistral\", \"Mistral\"),\n         (\"mistral3\", \"Mistral3\"),\n         (\"mixtral\", \"Mixtral\"),\n@@ -1307,6 +1309,13 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             config_class.register_for_auto_class()\n             return config_class.from_pretrained(pretrained_model_name_or_path, **kwargs)\n         elif \"model_type\" in config_dict:\n+            # Apply heuristic: if model_type is mistral but layer_types is present, treat as ministral\n+            if config_dict[\"model_type\"] == \"mistral\" and \"layer_types\" in config_dict:\n+                logger.info(\n+                    \"Detected mistral model with layer_types, treating as ministral for alternating attention compatibility. \"\n+                )\n+                config_dict[\"model_type\"] = \"ministral\"\n+\n             try:\n                 config_class = CONFIG_MAPPING[config_dict[\"model_type\"]]\n             except KeyError:"
        },
        {
            "sha": "a4b9434f24b9ac95fac3f4986253d7f0db49cfc8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -250,6 +250,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mgp-str\", \"MgpstrForSceneTextRecognition\"),\n         (\"mimi\", \"MimiModel\"),\n         (\"minimax\", \"MiniMaxModel\"),\n+        (\"ministral\", \"MinistralModel\"),\n         (\"mistral\", \"MistralModel\"),\n         (\"mistral3\", \"Mistral3Model\"),\n         (\"mixtral\", \"MixtralModel\"),\n@@ -685,6 +686,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mega\", \"MegaForCausalLM\"),\n         (\"megatron-bert\", \"MegatronBertForCausalLM\"),\n         (\"minimax\", \"MiniMaxForCausalLM\"),\n+        (\"ministral\", \"MinistralForCausalLM\"),\n         (\"mistral\", \"MistralForCausalLM\"),\n         (\"mixtral\", \"MixtralForCausalLM\"),\n         (\"mllama\", \"MllamaForCausalLM\"),\n@@ -1238,6 +1240,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mega\", \"MegaForSequenceClassification\"),\n         (\"megatron-bert\", \"MegatronBertForSequenceClassification\"),\n         (\"minimax\", \"MiniMaxForSequenceClassification\"),\n+        (\"ministral\", \"MinistralForSequenceClassification\"),\n         (\"mistral\", \"MistralForSequenceClassification\"),\n         (\"mixtral\", \"MixtralForSequenceClassification\"),\n         (\"mobilebert\", \"MobileBertForSequenceClassification\"),\n@@ -1337,6 +1340,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mega\", \"MegaForQuestionAnswering\"),\n         (\"megatron-bert\", \"MegatronBertForQuestionAnswering\"),\n         (\"minimax\", \"MiniMaxForQuestionAnswering\"),\n+        (\"ministral\", \"MinistralForQuestionAnswering\"),\n         (\"mistral\", \"MistralForQuestionAnswering\"),\n         (\"mixtral\", \"MixtralForQuestionAnswering\"),\n         (\"mobilebert\", \"MobileBertForQuestionAnswering\"),\n@@ -1452,6 +1456,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mega\", \"MegaForTokenClassification\"),\n         (\"megatron-bert\", \"MegatronBertForTokenClassification\"),\n         (\"minimax\", \"MiniMaxForTokenClassification\"),\n+        (\"ministral\", \"MinistralForTokenClassification\"),\n         (\"mistral\", \"MistralForTokenClassification\"),\n         (\"mixtral\", \"MixtralForTokenClassification\"),\n         (\"mobilebert\", \"MobileBertForTokenClassification\"),"
        },
        {
            "sha": "688faf00c4ea22bf0d7afe332f46e1e9cb4f84d6",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -788,7 +788,7 @@ def tokenizer_class_from_name(class_name: str) -> Union[type[Any], None]:\n     for module_name, tokenizers in TOKENIZER_MAPPING_NAMES.items():\n         if class_name in tokenizers:\n             module_name = model_type_to_module_name(module_name)\n-            if module_name in [\"mistral\", \"mixtral\"] and class_name == \"MistralCommonTokenizer\":\n+            if module_name in [\"mistral\", \"mixtral\", \"ministral\"] and class_name == \"MistralCommonTokenizer\":\n                 module = importlib.import_module(\".tokenization_mistral_common\", \"transformers\")\n             else:\n                 module = importlib.import_module(f\".{module_name}\", \"transformers.models\")"
        },
        {
            "sha": "348f0aa811c5bf89fa61384f2dd8bf9f205257e9",
            "filename": "src/transformers/models/ministral/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2F__init__.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2023 Mistral AI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_ministral import *\n+    from .modeling_ministral import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e6e20b4c6fd88ff5d7e0481d44ac80ce72037adc",
            "filename": "src/transformers/models/ministral/configuration_ministral.py",
            "status": "added",
            "additions": 163,
            "deletions": 0,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fconfiguration_ministral.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,163 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/ministral/modular_ministral.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ministral.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+from ...configuration_utils import PretrainedConfig\n+\n+\n+class MinistralConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MinistralModel`]. It is used to instantiate an\n+    Ministral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Ministral-8B-Instruct-2410.\n+\n+    [mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)\n+    [mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the Ministral model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MinistralModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 14336):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `8`.\n+        head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n+            The attention head dimension.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to `4096*32`):\n+            The maximum sequence length that this model might ever be used with. Ministral's sliding window attention\n+            allows sequence of up to 4096*32 tokens.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            The id of the padding token.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            The id of the \"beginning-of-sequence\" token.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            The id of the \"end-of-sequence\" token.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Sliding window attention window size. If not specified, will default to `4096`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+\n+    ```python\n+    >>> from transformers import MinistralModel, MinistralConfig\n+\n+    >>> # Initializing a Ministral 8B style configuration\n+    >>> configuration = MinistralConfig()\n+\n+    >>> # Initializing a model from the Ministral 8B style configuration\n+    >>> model = MinistralModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ministral\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    # Default tensor parallel plan for base model `MinistralModel`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=4096,\n+        intermediate_size=14336,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=8,\n+        head_dim=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096 * 32,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        sliding_window=4096,\n+        attention_dropout=0.0,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.sliding_window = sliding_window\n+        self.head_dim = head_dim\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_dropout = attention_dropout\n+        self.layer_types = layer_types\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n+            ] * num_hidden_layers\n+\n+\n+__all__ = [\"MinistralConfig\"]"
        },
        {
            "sha": "91ea520c6167fe57182219d6bfdf01a72233880f",
            "filename": "src/transformers/models/ministral/modeling_ministral.py",
            "status": "added",
            "additions": 495,
            "deletions": 0,
            "changes": 495,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodeling_ministral.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,495 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/ministral/modular_ministral.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_ministral.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import (\n+    GenericForQuestionAnswering,\n+    GenericForSequenceClassification,\n+    GenericForTokenClassification,\n+    GradientCheckpointingLayer,\n+)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_ministral import MinistralConfig\n+\n+\n+class MinistralMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class MinistralAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+        # Match Mistral: q/k/v do not have bias\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,  # main diff with Llama\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class MinistralRMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps: float = 1e-6) -> None:\n+        \"\"\"\n+        MinistralRMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class MinistralDecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: MinistralConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = MinistralAttention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = MinistralMLP(config)\n+        self.input_layernorm = MinistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = MinistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class MinistralPreTrainedModel(PreTrainedModel):\n+    config: MinistralConfig\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"MinistralDecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": MinistralDecoderLayer,\n+        \"attentions\": MinistralAttention,\n+    }\n+\n+\n+class MinistralRotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: MinistralConfig, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class MinistralModel(MinistralPreTrainedModel):\n+    def __init__(self, config: MinistralConfig):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [MinistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = MinistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = MinistralRotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+@auto_docstring\n+class MinistralForCausalLM(MinistralPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = MinistralModel(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MinistralForCausalLM\n+\n+        >>> model = MinistralForCausalLM.from_pretrained(\"meta-ministral/Ministral-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-ministral/Ministral-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class MinistralForSequenceClassification(GenericForSequenceClassification, MinistralPreTrainedModel):\n+    pass\n+\n+\n+class MinistralForTokenClassification(GenericForTokenClassification, MinistralPreTrainedModel):\n+    pass\n+\n+\n+class MinistralForQuestionAnswering(GenericForQuestionAnswering, MinistralPreTrainedModel):\n+    base_model_prefix = \"transformer\"  # For BC, where `transformer` was used instead of `model`\n+\n+\n+__all__ = [\n+    \"MinistralPreTrainedModel\",\n+    \"MinistralModel\",\n+    \"MinistralForCausalLM\",\n+    \"MinistralForSequenceClassification\",\n+    \"MinistralForTokenClassification\",\n+    \"MinistralForQuestionAnswering\",\n+]"
        },
        {
            "sha": "f0b0d52d6954270059506c2fb0881b6883c5d006",
            "filename": "src/transformers/models/ministral/modular_ministral.py",
            "status": "added",
            "additions": 297,
            "deletions": 0,
            "changes": 297,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral%2Fmodular_ministral.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,297 @@\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring\n+from ...utils.generic import check_model_inputs\n+from ..mistral.configuration_mistral import MistralConfig\n+from ..qwen2.modeling_qwen2 import (\n+    Qwen2Attention,\n+    Qwen2DecoderLayer,\n+    Qwen2ForCausalLM,\n+    Qwen2ForQuestionAnswering,\n+    Qwen2ForSequenceClassification,\n+    Qwen2ForTokenClassification,\n+    Qwen2MLP,\n+    Qwen2Model,\n+    Qwen2PreTrainedModel,\n+    Qwen2RMSNorm,\n+    Qwen2RotaryEmbedding,\n+)\n+\n+\n+class MinistralConfig(MistralConfig, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MinistralModel`]. It is used to instantiate an\n+    Ministral model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the Ministral-8B-Instruct-2410.\n+\n+    [mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)\n+    [mistralai/Ministral-8B-Instruct-2410](https://huggingface.co/mistralai/Ministral-8B-Instruct-2410)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the Ministral model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`MinistralModel`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 14336):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_key_value_heads (`int`, *optional*, defaults to 8):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to `8`.\n+        head_dim (`int`, *optional*, defaults to `hidden_size // num_attention_heads`):\n+            The attention head dimension.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to `4096*32`):\n+            The maximum sequence length that this model might ever be used with. Ministral's sliding window attention\n+            allows sequence of up to 4096*32 tokens.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*):\n+            The id of the padding token.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            The id of the \"beginning-of-sequence\" token.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            The id of the \"end-of-sequence\" token.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether the model's input and output word embeddings should be tied.\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Sliding window attention window size. If not specified, will default to `4096`.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+\n+    ```python\n+    >>> from transformers import MinistralModel, MinistralConfig\n+\n+    >>> # Initializing a Ministral 8B style configuration\n+    >>> configuration = MinistralConfig()\n+\n+    >>> # Initializing a model from the Ministral 8B style configuration\n+    >>> model = MinistralModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"ministral\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=4096,\n+        intermediate_size=14336,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=8,\n+        head_dim=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=4096 * 32,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=None,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        sliding_window=4096,\n+        attention_dropout=0.0,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        PretrainedConfig.__init__(\n+            self,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.sliding_window = sliding_window\n+        self.head_dim = head_dim\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_dropout = attention_dropout\n+        self.layer_types = layer_types\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if self.sliding_window is not None else \"full_attention\"\n+            ] * num_hidden_layers\n+\n+\n+class MinistralMLP(Qwen2MLP):\n+    pass\n+\n+\n+class MinistralAttention(Qwen2Attention):\n+    def __init__(self, config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        # Match Mistral: q/k/v do not have bias\n+        self.q_proj = nn.Linear(config.hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+\n+\n+class MinistralRMSNorm(Qwen2RMSNorm):\n+    pass\n+\n+\n+class MinistralDecoderLayer(Qwen2DecoderLayer):\n+    pass\n+\n+\n+class MinistralPreTrainedModel(Qwen2PreTrainedModel):\n+    pass\n+\n+\n+class MinistralRotaryEmbedding(Qwen2RotaryEmbedding):\n+    pass\n+\n+\n+class MinistralModel(Qwen2Model):\n+    def __init__(self, config: MinistralConfig):\n+        super().__init__(config)\n+        del self.has_sliding_layers\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+        )\n+\n+\n+class MinistralForCausalLM(Qwen2ForCausalLM):\n+    pass\n+\n+\n+class MinistralForSequenceClassification(Qwen2ForSequenceClassification):\n+    pass\n+\n+\n+class MinistralForTokenClassification(Qwen2ForTokenClassification):\n+    pass\n+\n+\n+class MinistralForQuestionAnswering(Qwen2ForQuestionAnswering):\n+    pass\n+\n+\n+__all__ = [\n+    \"MinistralConfig\",\n+    \"MinistralPreTrainedModel\",\n+    \"MinistralModel\",\n+    \"MinistralForCausalLM\",\n+    \"MinistralForSequenceClassification\",\n+    \"MinistralForTokenClassification\",\n+    \"MinistralForQuestionAnswering\",\n+]"
        },
        {
            "sha": "e9f66b1a2fbe9117108b949107cd42246a7760b5",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -157,6 +157,11 @@ def __init__(\n         self.rope_theta = rope_theta\n         self.attention_dropout = attention_dropout\n \n+        if \"layer_types\" in kwargs:\n+            logger.warning_once(\n+                \"Detected Mistral model with layer_types. Consider using AutoModel or Ministral classes instead to enable alternating attention compatibility.\"\n+            )\n+\n         super().__init__(\n             pad_token_id=pad_token_id,\n             bos_token_id=bos_token_id,"
        },
        {
            "sha": "9f274cf3aba47dacb405d4347166d0972882767f",
            "filename": "src/transformers/utils/fx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Futils%2Ffx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/src%2Ftransformers%2Futils%2Ffx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Ffx.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -148,6 +148,7 @@ def _generate_supported_model_class_names(\n     \"marian\",\n     \"mbart\",\n     \"megatron-bert\",\n+    \"ministral\",\n     \"mistral\",\n     \"mixtral\",\n     \"mobilebert\","
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/ministral/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/tests%2Fmodels%2Fministral%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/tests%2Fmodels%2Fministral%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2F__init__.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914"
        },
        {
            "sha": "ff62ec1c438aba8e917ce7926a0c2933e8dc2d8b",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "added",
            "additions": 272,
            "deletions": 0,
            "changes": 272,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f1b128d3359a26bd18be99c26d7f04fb3cba914/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f1b128d3359a26bd18be99c26d7f04fb3cba914/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=0f1b128d3359a26bd18be99c26d7f04fb3cba914",
            "patch": "@@ -0,0 +1,272 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Ministral model.\"\"\"\n+\n+import gc\n+import logging\n+import unittest\n+\n+import pytest\n+\n+from transformers import AutoTokenizer, GenerationConfig, MinistralConfig, is_torch_available\n+from transformers.testing_utils import (\n+    backend_empty_cache,\n+    cleanup,\n+    require_bitsandbytes,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        MinistralForCausalLM,\n+        MinistralForQuestionAnswering,\n+        MinistralForSequenceClassification,\n+        MinistralForTokenClassification,\n+        MinistralModel,\n+    )\n+\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class MinistralModelTester(CausalLMModelTester):\n+    config_class = MinistralConfig\n+    if is_torch_available():\n+        base_model_class = MinistralModel\n+        causal_lm_class = MinistralForCausalLM\n+        sequence_class = MinistralForSequenceClassification\n+        token_class = MinistralForTokenClassification\n+        question_answering_class = MinistralForQuestionAnswering\n+\n+\n+@require_torch\n+class MinistralModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            MinistralModel,\n+            MinistralForCausalLM,\n+            MinistralForSequenceClassification,\n+            MinistralForTokenClassification,\n+            MinistralForQuestionAnswering,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = MinistralModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": MinistralModel,\n+            \"text-classification\": MinistralForSequenceClassification,\n+            \"token-classification\": MinistralForTokenClassification,\n+            \"text-generation\": MinistralForCausalLM,\n+            \"question-answering\": MinistralForQuestionAnswering,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    # TODO (ydshieh): Check this. See https://app.circleci.com/pipelines/github/huggingface/transformers/79245/workflows/9490ef58-79c2-410d-8f51-e3495156cf9c/jobs/1012146\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        return True\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        self.skipTest(reason=\"Ministral flash attention does not support right padding\")\n+\n+\n+@require_torch\n+class MinistralIntegrationTest(unittest.TestCase):\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_8b_logits(self):\n+        input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n+        model = AutoModelForCausalLM.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\", device_map=\"auto\")\n+        assert isinstance(model, MinistralForCausalLM)\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        with torch.no_grad():\n+            out = model(input_ids).logits.float().cpu()\n+        # Expected mean on dim = -1\n+        EXPECTED_MEAN = torch.tensor([[-1.5029, -7.2815, 4.5190, 0.5930, -5.2526, 3.0765, -0.6314, 1.8068]])\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # slicing logits[0, 0, 0:30]\n+        EXPECTED_SLICE = torch.tensor([-3.9446, -3.9466,  0.6383, -3.9466, -3.9468, -3.9448, -3.9462, -3.9455,\n+                                                    -3.9451, -0.8244, -3.9472, -3.9458, -3.9460, -3.9406, -3.9462, -3.9462,\n+                                                    -3.9458, -3.9462, -3.9463, -3.9461, -3.9448, -3.9451, -3.9462, -3.9458,\n+                                                    -3.9455, -3.9452, -3.9458, -3.9469, -3.9460, -3.9464])  # fmt: skip\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @slow\n+    def test_model_8b_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"My favourite condiment is 100% natural, 100% organic, 100% free of\"\n+        prompt = \"My favourite condiment is \"\n+        tokenizer = AutoTokenizer.from_pretrained(\"Mistralai/Ministral-8B-Instruct-2410\")\n+        model = MinistralForCausalLM.from_pretrained(\"Mistralai/Ministral-8B-Instruct-2410\", device_map=\"auto\")\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @require_bitsandbytes\n+    @slow\n+    @require_flash_attn\n+    @pytest.mark.flash_attn_test\n+    def test_model_8b_long_prompt(self):\n+        EXPECTED_OUTPUT_TOKEN_IDS = [36850, 4112]\n+        # An input with 4097 tokens that is above the size of the sliding window\n+        input_ids = [1] + [306, 338] * 2048\n+        model = MinistralForCausalLM.from_pretrained(\n+            \"Mistralai/Ministral-8B-Instruct-2410\",\n+            device_map=\"auto\",\n+            dtype=torch.bfloat16,\n+            attn_implementation=\"flash_attention_2\",\n+        )\n+        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n+        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n+\n+        # Assisted generation\n+        assistant_model = model\n+        assistant_model.generation_config.num_assistant_tokens = 2\n+        assistant_model.generation_config.num_assistant_tokens_schedule = \"constant\"\n+        generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n+        print(generated_ids[0][-2:].tolist())\n+        self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n+\n+        del assistant_model\n+        del model\n+        backend_empty_cache(torch_device)\n+        gc.collect()\n+\n+    @slow\n+    @unittest.skip(\"not working with Ministral\")\n+    @pytest.mark.torch_export_test\n+    def test_export_text_with_hybrid_cache(self):\n+        # TODO: Exportability is not working\n+        from transformers.testing_utils import is_torch_greater_or_equal\n+\n+        if not is_torch_greater_or_equal(\"2.6.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")\n+\n+        from transformers.integrations.executorch import TorchExportableModuleForDecoderOnlyLM\n+\n+        model_id = \"Mistralai/Ministral-8B-Instruct-2410\"\n+        model = MinistralForCausalLM.from_pretrained(\n+            model_id,\n+            generation_config=GenerationConfig(\n+                use_cache=True,\n+                cache_implementation=\"static\",\n+                cache_config={\n+                    \"batch_size\": 1,\n+                    \"max_cache_len\": 50,\n+                },\n+            ),\n+        )\n+\n+        # Export + HybridCache\n+        model.eval()\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exported_program = exportable_module.export(\n+            input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n+            cache_position=torch.tensor([0], dtype=torch.long, device=model.device),\n+        )\n+        logging.info(f\"\\nExported program: {exported_program}\")\n+\n+        # Test generation with the exported model\n+        prompt = \"My favourite condiment is \"\n+        max_new_tokens_to_generate = 20\n+        # Generate text with the exported model\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        export_generated_text = TorchExportableModuleForDecoderOnlyLM.generate(\n+            exported_program, tokenizer, prompt, max_new_tokens=max_new_tokens_to_generate\n+        )\n+        logging.info(f\"\\nExport generated texts: '{export_generated_text}'\")\n+\n+        input_text = tokenizer(prompt, return_tensors=\"pt\")\n+        with torch.no_grad():\n+            eager_outputs = model.generate(\n+                **input_text,\n+                max_new_tokens=max_new_tokens_to_generate,\n+                do_sample=False,  # Use greedy decoding to match the exported model\n+                cache_implementation=\"static\",\n+            )\n+\n+        eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)\n+        logging.info(f\"\\nEager generated texts: '{eager_generated_text}'\")\n+\n+        self.assertEqual(export_generated_text, eager_generated_text)\n+\n+    @require_flash_attn\n+    @slow\n+    def test_past_sliding_window_generation(self):\n+        try:\n+            from datasets import load_dataset\n+        except ImportError:\n+            self.skipTest(\"datasets not found\")\n+\n+        model = MinistralForCausalLM.from_pretrained(\n+            \"mistralai/Ministral-8B-Instruct-2410\",\n+            device_map=\"auto\",\n+            load_in_4bit=True,\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Ministral-8B-Instruct-2410\", legacy=False)\n+\n+        wiki = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=\"validation\")\n+        chunks = [x[\"text\"] for x in wiki.select(range(550)) if x[\"text\"].strip()]\n+        real_corpus = \"\\n\".join(chunks)\n+        prompt = f\"<s>[INST]{real_corpus} Question: Based on the text, at which depth of the continental shelf does H. Gammarus live?[/INST]\"\n+\n+        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n+        input_length = inputs.input_ids.shape[1]  # around 33k tokens > 32k sliding window\n+        outputs = model.generate(**inputs, max_new_tokens=100, do_sample=False)\n+        output_text = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)\n+        print(output_text)\n+        self.assertEqual(\n+            output_text,\n+            \" H. Gammarus lives on the continental shelf at depths of 0 ‚Äì 150 metres ( 0 ‚Äì 492 ft ) , although not normally deeper than 50 m ( 160 ft ) .\",\n+        )"
        }
    ],
    "stats": {
        "total": 1365,
        "additions": 1364,
        "deletions": 1
    }
}