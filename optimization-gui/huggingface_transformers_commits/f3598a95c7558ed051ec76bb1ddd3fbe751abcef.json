{
    "author": "yao-matrix",
    "message": "extend more trainer test cases to XPU, all pass (#39652)\n\nextend more trainer test cases to XPU\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "f3598a95c7558ed051ec76bb1ddd3fbe751abcef",
    "files": [
        {
            "sha": "3c2cc0ce892d9b6eecd67657484e284802c56258",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f3598a95c7558ed051ec76bb1ddd3fbe751abcef/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f3598a95c7558ed051ec76bb1ddd3fbe751abcef/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=f3598a95c7558ed051ec76bb1ddd3fbe751abcef",
            "patch": "@@ -2520,7 +2520,7 @@ def test_apollo_lr_display_with_scheduler(self):\n         self.assertTrue(len(decreasing_lrs) > len(increasing_lrs))\n \n     @require_torch_optimi\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_stable_adamw(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2539,7 +2539,7 @@ def test_stable_adamw(self):\n         _ = trainer.train()\n \n     @require_torch_optimi\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_stable_adamw_extra_args(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2561,7 +2561,7 @@ def test_stable_adamw_extra_args(self):\n         _ = trainer.train()\n \n     @require_torch_optimi\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_stable_adamw_lr_display_without_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2586,7 +2586,7 @@ def test_stable_adamw_lr_display_without_scheduler(self):\n         self.assertEqual(trainer.get_learning_rates(), [learning_rate, learning_rate])\n \n     @require_torch_optimi\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_stable_adamw_lr_display_with_scheduler(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2615,19 +2615,19 @@ def test_stable_adamw_lr_display_with_scheduler(self):\n         logs = trainer.state.log_history[1:][:-1]\n \n         # reach given learning rate peak and end with 0 lr\n-        self.assertTrue(logs[num_warmup_steps - 2][\"learning_rate\"] == learning_rate)\n-        self.assertTrue(logs[-1][\"learning_rate\"] == 0)\n+        self.assertTrue(logs[num_warmup_steps - 1][\"learning_rate\"] == learning_rate)\n+        self.assertTrue(np.allclose(logs[-1][\"learning_rate\"], 0, atol=5e-6))\n \n         # increasing and decreasing pattern of lrs\n         increasing_lrs = [\n             logs[i][\"learning_rate\"] < logs[i + 1][\"learning_rate\"]\n             for i in range(len(logs))\n-            if i < num_warmup_steps - 2\n+            if i < num_warmup_steps - 1\n         ]\n         decreasing_lrs = [\n             logs[i][\"learning_rate\"] > logs[i + 1][\"learning_rate\"]\n             for i in range(len(logs) - 1)\n-            if i >= num_warmup_steps - 2\n+            if i >= num_warmup_steps - 1\n         ]\n \n         self.assertTrue(all(increasing_lrs))"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 8,
        "deletions": 8
    }
}