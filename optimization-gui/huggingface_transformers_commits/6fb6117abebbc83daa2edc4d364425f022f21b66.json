{
    "author": "SunMarc",
    "message": "Revert \"Fix DeepSpeed mixed precision precedence over Accelerate defaults\" (#41124)\n\n* Revert \"Fix DeepSpeed mixed precision precedence over Accelerate defaults (#3â€¦\"\n\nThis reverts commit df67cd35f0ca1a1cbf7147b2576db31b16200cf4.\n\n* fix",
    "sha": "6fb6117abebbc83daa2edc4d364425f022f21b66",
    "files": [
        {
            "sha": "daa7e885d272dccc6dd71fc60273e10e11031101",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fb6117abebbc83daa2edc4d364425f022f21b66/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fb6117abebbc83daa2edc4d364425f022f21b66/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=6fb6117abebbc83daa2edc4d364425f022f21b66",
            "patch": "@@ -130,58 +130,11 @@ def fill_match(self, ds_key_long, hf_val, hf_key=None, must_match=True):\n \n     fill_only = partialmethod(fill_match, must_match=False)\n \n-    def override_training_args_from_deepspeed(self, args):\n-        \"\"\"\n-        Override TrainingArguments based on DeepSpeed config values to ensure compatibility.\n-\n-        This method ensures that the DeepSpeed config takes precedence over TrainingArguments\n-        defaults when there are conflicts, particularly for mixed precision settings.\n-\n-        Args:\n-            args: TrainingArguments object to potentially modify\n-        \"\"\"\n-        # Check precision settings in DeepSpeed config and override TrainingArguments accordingly\n-        # Only override defaults, not explicit user settings\n-\n-        # Check if user explicitly set precision options (we assume defaults are False)\n-        user_set_fp16 = args.fp16 is True\n-        user_set_bf16 = args.bf16 is True\n-\n-        if self.is_true(\"fp16.enabled\"):\n-            # DeepSpeed config explicitly enables fp16\n-            if not user_set_fp16 and not user_set_bf16:\n-                # User didn't explicitly set either, so apply DeepSpeed config\n-                args.fp16 = True\n-                args.bf16 = False\n-            elif user_set_bf16 and not user_set_fp16:\n-                # User explicitly chose bf16, but DeepSpeed config wants fp16\n-                # This is a potential conflict - let user choice win but log a warning\n-                pass  # Keep user's bf16=True, fp16=False\n-        elif self.is_true(\"bf16.enabled\"):\n-            # DeepSpeed config explicitly enables bf16\n-            if not user_set_fp16 and not user_set_bf16:\n-                # User didn't explicitly set either, so apply DeepSpeed config\n-                args.bf16 = True\n-                args.fp16 = False\n-            elif user_set_fp16 and not user_set_bf16:\n-                # User explicitly chose fp16, but DeepSpeed config wants bf16\n-                # This is a potential conflict - let user choice win but log a warning\n-                pass  # Keep user's fp16=True, bf16=False\n-        elif self.is_false(\"fp16.enabled\") and self.is_false(\"bf16.enabled\"):\n-            # Both are explicitly disabled in DeepSpeed config\n-            if not user_set_fp16 and not user_set_bf16:\n-                # User didn't explicitly set either, so apply DeepSpeed config (fp32)\n-                args.fp16 = False\n-                args.bf16 = False\n-\n     def trainer_config_process(self, args, auto_find_batch_size=False):\n         \"\"\"\n         Adjust the config with `TrainingArguments` values. This stage is run during `TrainingArguments` object\n         creation.\n         \"\"\"\n-        # First, override TrainingArguments based on DeepSpeed config to ensure compatibility\n-        self.override_training_args_from_deepspeed(args)\n-\n         # DeepSpeed does:\n         # train_batch_size = world_size * train_micro_batch_size_per_gpu * gradient_accumulation_steps\n         train_batch_size = args.world_size * args.per_device_train_batch_size * args.gradient_accumulation_steps"
        },
        {
            "sha": "160611472a5945321fecf4c23e9d822889168ec0",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 7,
            "deletions": 11,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fb6117abebbc83daa2edc4d364425f022f21b66/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fb6117abebbc83daa2edc4d364425f022f21b66/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=6fb6117abebbc83daa2edc4d364425f022f21b66",
            "patch": "@@ -1688,8 +1688,13 @@ def __post_init__(self):\n                         torch.backends.cudnn.allow_tf32 = False\n                 # no need to assert on else\n \n-        # NOTE: Mixed precision environment variable setting moved to after DeepSpeed processing\n-        # to ensure DeepSpeed config can override TrainingArguments defaults\n+        # if training args is specified, it will override the one specified in the accelerate config\n+        mixed_precision_dtype = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n+        if self.fp16:\n+            mixed_precision_dtype = \"fp16\"\n+        elif self.bf16:\n+            mixed_precision_dtype = \"bf16\"\n+        os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n \n         if self.report_to is None:\n             logger.info(\n@@ -1876,15 +1881,6 @@ def __post_init__(self):\n             self.deepspeed_plugin.set_mixed_precision(mixed_precision)\n             self.deepspeed_plugin.set_deepspeed_weakref()\n \n-        # Set mixed precision environment variable after DeepSpeed processing\n-        # This ensures DeepSpeed config overrides have been applied to fp16/bf16 settings\n-        mixed_precision_dtype = os.environ.get(\"ACCELERATE_MIXED_PRECISION\", \"no\")\n-        if self.fp16:\n-            mixed_precision_dtype = \"fp16\"\n-        elif self.bf16:\n-            mixed_precision_dtype = \"bf16\"\n-        os.environ[\"ACCELERATE_MIXED_PRECISION\"] = mixed_precision_dtype\n-\n         if self.use_cpu:\n             self.dataloader_pin_memory = False\n "
        },
        {
            "sha": "99b1450a0d59501a34172687e221c2682e0ddb30",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/6fb6117abebbc83daa2edc4d364425f022f21b66/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6fb6117abebbc83daa2edc4d364425f022f21b66/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=6fb6117abebbc83daa2edc4d364425f022f21b66",
            "patch": "@@ -1431,50 +1431,3 @@ def test_clm_from_config_zero3_fp16(self):\n         with CaptureStderr() as cs:\n             execute_subprocess_async(cmd, env=self.get_env())\n         self.assertIn(\"Detected DeepSpeed ZeRO-3\", cs.err)\n-\n-\n-@require_deepspeed\n-class TestDeepSpeedMixedPrecisionPrecedence(TestCasePlus):\n-    \"\"\"Test DeepSpeed mixed precision precedence over Accelerate defaults.\"\"\"\n-\n-    def setUp(self):\n-        super().setUp()\n-        unset_hf_deepspeed_config()\n-\n-    def tearDown(self):\n-        super().tearDown()\n-        unset_hf_deepspeed_config()\n-\n-    def test_deepspeed_fp16_overrides_defaults(self):\n-        \"\"\"Test that DeepSpeed fp16 config overrides TrainingArguments defaults\"\"\"\n-        from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig\n-\n-        args = TrainingArguments(output_dir=\"./test_output\", fp16=False, bf16=False)\n-        ds_config = {\"fp16\": {\"enabled\": True}, \"bf16\": {\"enabled\": False}, \"zero_optimization\": {\"stage\": 2}}\n-        hf_ds_config = HfTrainerDeepSpeedConfig(ds_config)\n-        hf_ds_config.trainer_config_process(args)\n-        self.assertTrue(args.fp16)\n-        self.assertFalse(args.bf16)\n-\n-    def test_deepspeed_bf16_overrides_defaults(self):\n-        \"\"\"Test that DeepSpeed bf16 config overrides TrainingArguments defaults\"\"\"\n-        from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig\n-\n-        args = TrainingArguments(output_dir=\"./test_output\", fp16=False, bf16=False)\n-        ds_config = {\"fp16\": {\"enabled\": False}, \"bf16\": {\"enabled\": True}, \"zero_optimization\": {\"stage\": 2}}\n-        hf_ds_config = HfTrainerDeepSpeedConfig(ds_config)\n-        hf_ds_config.trainer_config_process(args)\n-        self.assertTrue(args.bf16)\n-        self.assertFalse(args.fp16)\n-\n-    def test_user_explicit_settings_preserved(self):\n-        \"\"\"Test that explicit user settings are preserved over DeepSpeed config\"\"\"\n-        from transformers.integrations.deepspeed import HfTrainerDeepSpeedConfig\n-\n-        args = TrainingArguments(output_dir=\"./test_output\", fp16=True, bf16=False)  # User explicit\n-        ds_config = {\"fp16\": {\"enabled\": False}, \"bf16\": {\"enabled\": True}, \"zero_optimization\": {\"stage\": 2}}\n-        hf_ds_config = HfTrainerDeepSpeedConfig(ds_config)\n-        hf_ds_config.trainer_config_process(args)\n-        # User's explicit choice should be preserved\n-        self.assertTrue(args.fp16)\n-        self.assertFalse(args.bf16)"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 7,
        "deletions": 105
    }
}