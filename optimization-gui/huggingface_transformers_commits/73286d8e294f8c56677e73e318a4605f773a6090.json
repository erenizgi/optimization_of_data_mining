{
    "author": "Cyrilvallez",
    "message": "Fix HybridChunedCache & Llama4 (#38299)\n\n* Update cache_utils.py\n\n* Update cache_utils.py",
    "sha": "73286d8e294f8c56677e73e318a4605f773a6090",
    "files": [
        {
            "sha": "e97cac65f250a0381f4abc51db8319f7268b450f",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/73286d8e294f8c56677e73e318a4605f773a6090/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/73286d8e294f8c56677e73e318a4605f773a6090/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=73286d8e294f8c56677e73e318a4605f773a6090",
            "patch": "@@ -1967,7 +1967,8 @@ def __init__(\n         else:\n             self.sliding_window = config.sliding_window\n         self.max_cache_len = max_cache_len\n-        self._sliding_window_max_len = min(self.sliding_window, max_cache_len)\n+        # Sliding layers can't be larger than the overall max cache len\n+        self.sliding_window = min(config.sliding_window, self.max_cache_len)\n         self.max_batch_size = max_batch_size\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self._dtype = dtype\n@@ -1989,7 +1990,7 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         num_key_value_heads = key_states.shape[1]\n         device = key_states.device\n         global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n-        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self._sliding_window_max_len, self.head_dim)\n+        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self.sliding_window, self.head_dim)\n         # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n         # breaks when updating the cache.\n         cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape\n@@ -2163,7 +2164,7 @@ def initialise_cache_layer(self, layer_idx, key_states):\n         device = key_states.device if self.is_sliding[layer_idx] else self.offload_device\n         pin_memory = not self.is_sliding[layer_idx]\n         global_cache_shape = (self.max_batch_size, num_key_value_heads, self.max_cache_len, self.head_dim)\n-        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self._sliding_window_max_len, self.head_dim)\n+        sliding_cache_shape = (self.max_batch_size, num_key_value_heads, self.sliding_window, self.head_dim)\n         # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n         # breaks when updating the cache.\n         cache_shape = sliding_cache_shape if self.is_sliding[layer_idx] else global_cache_shape"
        }
    ],
    "stats": {
        "total": 7,
        "additions": 4,
        "deletions": 3
    }
}