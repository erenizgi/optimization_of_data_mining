{
    "author": "yao-matrix",
    "message": "enable csm integration cases on xpu, all passed (#38140)\n\n* enable csm test cases on XPU, all passed\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>",
    "sha": "0173a99e734a1dd853f46e1736d89800d4af16b4",
    "files": [
        {
            "sha": "be4ab6a0e2a01a55aa1c1655972dc244fb02bf68",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0173a99e734a1dd853f46e1736d89800d4af16b4/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0173a99e734a1dd853f46e1736d89800d4af16b4/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=0173a99e734a1dd853f46e1736d89800d4af16b4",
            "patch": "@@ -30,7 +30,7 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -430,7 +430,7 @@ def _load_conversation(self):\n         return ds[0]\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_1b_model_integration_generate(self):\n         \"\"\"\n         Tests the generated tokens match the ones from the original model implementation.\n@@ -474,7 +474,7 @@ def test_1b_model_integration_generate(self):\n         torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_1b_model_integration_generate_no_audio(self):\n         \"\"\"\n         Tests the generated tokens match the ones from the original model implementation.\n@@ -535,7 +535,7 @@ def test_1b_model_integration_generate_no_audio(self):\n         torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_1b_model_integration_generate_multiple_audio(self):\n         \"\"\"\n         Test the generated tokens match the ones from the original model implementation.\n@@ -594,7 +594,7 @@ def test_1b_model_integration_generate_multiple_audio(self):\n         torch.testing.assert_close(output_tokens.cpu(), EXPECTED_OUTPUT_TOKENS)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_1b_model_integration_generate_batched(self):\n         \"\"\"\n         Test the generated tokens match the ones from the original model implementation."
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}