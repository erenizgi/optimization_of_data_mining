{
    "author": "vasqu",
    "message": "[`FA`] Remaining Cleanup (#40424)\n\n* fa cleanup\n\n* flaky tests\n\n* readd removed test and changeup comments to reflect the purpose\n\n* flaky tests",
    "sha": "7e1aee4db6fffa357bb67bf7975420bf532b2341",
    "files": [
        {
            "sha": "47aaedc99fbadcc3a096c228e31920a24a25decb",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 53,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e1aee4db6fffa357bb67bf7975420bf532b2341/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e1aee4db6fffa357bb67bf7975420bf532b2341/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=7e1aee4db6fffa357bb67bf7975420bf532b2341",
            "patch": "@@ -313,17 +313,13 @@ def _upad_input(\n     )\n \n \n-def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool = True):\n+def prepare_fa_kwargs_from_position_ids(position_ids):\n     \"\"\"\n-    This function returns all the necessary kwargs to call `flash_attn_varlen_func`\n-    extracted from position_ids. The `position_ids` can be either packed sequence or\n-    the usual padded position ids, for example in inference time.\n+    This function returns all the necessary kwargs to call `flash_attn_varlen_func` extracted from position_ids.\n \n     Arguments:\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n-        is_packed_sequence (`bool`, *optional*, defaults to `True`):\n-            Whether the input position ids are a packed sequence or not.\n \n     Return:\n         (cu_seqlens_q, cu_seqlens_k) (`tuple[int]`):\n@@ -333,52 +329,35 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query,\n             `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n-    # If the lengths are not equal, most probably we are in decoding stage with cache\n-    # In that case the position ids will not always start with `0` and we need a better way to infer\n-    # cumulative seq lengths.\n     tensor_kwargs = {\"dtype\": torch.int32, \"device\": position_ids.device}\n-    if not is_packed_sequence:\n-        last_position_ids = position_ids[:, -1]\n-        q_len = (\n-            torch.ones(position_ids.size(0), **tensor_kwargs)\n-            if position_ids.shape[-1] == 1\n-            else last_position_ids.add(1)\n-        )\n-        cu_seq_lens_q = torch.cat([torch.zeros(1, **tensor_kwargs), q_len.cumsum(0).to(torch.int32)], 0)\n-        cu_seq_lens_k = torch.cat(\n-            [torch.zeros(1, **tensor_kwargs), last_position_ids.add(1).cumsum(0).to(torch.int32)], 0\n-        )\n \n-        max_length_q = int(q_len.max())\n-        max_length_k = int(last_position_ids.max()) + 1\n-    else:\n-        position_ids = position_ids.view(-1)\n-        indices_q = (position_ids == 0).nonzero().view(-1)\n+    position_ids = position_ids.view(-1)\n+    indices_q = (position_ids == 0).nonzero().view(-1)\n \n-        cu_seq_lens_q = torch.cat(\n-            (\n-                indices_q.to(**tensor_kwargs),\n-                torch.tensor(position_ids.size(), **tensor_kwargs),\n-            )\n+    cu_seq_lens_q = torch.cat(\n+        (\n+            indices_q.to(**tensor_kwargs),\n+            torch.tensor(position_ids.size(), **tensor_kwargs),\n         )\n-        cu_seq_lens_k = cu_seq_lens_q\n-\n-        # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n-        # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n-        # for some models (e.g. qwen2-vl).\n-        max_length_q = cu_seq_lens_q.diff().max()\n-        # NOTE: With torch compile, this will cause a graph break if you don't set\n-        # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n-        # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n-        # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n-        # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n-        max_length_q = max_length_q.item()\n-        max_length_k = max_length_q\n+    )\n+    cu_seq_lens_k = cu_seq_lens_q\n+\n+    # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n+    # We should use cu_seq_lens instead of position_ids to get the max length since position_ids is not always increasing\n+    # for some models (e.g. qwen2-vl).\n+    max_length_q = cu_seq_lens_q.diff().max()\n+    # NOTE: With torch compile, this will cause a graph break if you don't set\n+    # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n+    # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n+    # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n+    # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n+    max_length_q = max_length_q.item()\n+    max_length_k = max_length_q\n \n     return (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k)\n \n \n-def _prepare_from_posids(query, key, value, position_ids, query_length):\n+def _prepare_from_posids(query, key, value, position_ids):\n     \"\"\"\n     This function returns necessary arguments to call `flash_attn_varlen_func`.\n     All three query, key, value states will be flattened.\n@@ -394,8 +373,6 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n             Value state with padding. Shape: (batch_size, kv_seq_len, num_key_value_heads, head_dim).\n         position_ids (`torch.Tensor`):\n             Boolean or int tensor of shape (batch_size, sequence_length), 1 means valid and 0 means not valid.\n-        query_length (`int`):\n-            Sequence length of the input queries.\n \n     Return:\n         query (`torch.Tensor`):\n@@ -409,16 +386,11 @@ def _prepare_from_posids(query, key, value, position_ids, query_length):\n         (max_seqlen_in_batch_q, max_seqlen_in_batch_k) (`tuple[int]`):\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n-    kv_length = key.shape[1]\n-    is_packed_sequence = query_length == kv_length\n-\n     query = query.contiguous().view(-1, query.size(-2), query.size(-1))\n     key = key.contiguous().view(-1, key.size(-2), key.size(-1))\n     value = value.contiguous().view(-1, value.size(-2), value.size(-1))\n \n-    (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(\n-        position_ids, is_packed_sequence=is_packed_sequence\n-    )\n+    (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = prepare_fa_kwargs_from_position_ids(position_ids)\n \n     return (query, key, value, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k))\n \n@@ -660,7 +632,7 @@ def _flash_attention_forward(\n     elif is_fa_with_varlen_kwargs or is_fa_with_position_ids:\n         if cu_seq_lens_q is None or cu_seq_lens_k is None:\n             q, k, v, (cu_seq_lens_q, cu_seq_lens_k), (max_length_q, max_length_k) = _prepare_from_posids(\n-                query_states, key_states, value_states, position_ids, query_length=query_length\n+                query_states, key_states, value_states, position_ids\n             )\n         else:\n             q = query_states.reshape(-1, query_states.size(-2), query_states.size(-1))"
        },
        {
            "sha": "304568cbdaca5982244da019fb8a27e8970eb968",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e1aee4db6fffa357bb67bf7975420bf532b2341/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e1aee4db6fffa357bb67bf7975420bf532b2341/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=7e1aee4db6fffa357bb67bf7975420bf532b2341",
            "patch": "@@ -4313,8 +4313,10 @@ def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa\n     @mark.flash_attn_test\n     def test_flash_attention_2_continue_generate_with_position_ids(self):\n         \"\"\"\n-        Tests that the given attention implementation can work with packed sequences and infers the mask\n-        from position ids. This test requires the model to use new attention mask API which handles packing.\n+        Tests whether flash attention can continue its generation from given position ids.\n+\n+        NOTE: This serves as regression check as we had instances where flash attention entered the varlen\n+        path here. It should now always enter the base `flash_fn`.\n         \"\"\"\n \n         max_new_tokens = 2"
        }
    ],
    "stats": {
        "total": 84,
        "additions": 29,
        "deletions": 55
    }
}