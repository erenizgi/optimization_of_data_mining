{
    "author": "JuanFKurucz",
    "message": "Fix documentation reference to pytorch max memory allocated (#42350)",
    "sha": "2e0457e6074d82e1d36b041aa271bb55025e36a3",
    "files": [
        {
            "sha": "af220c6efca952d03c537b2915df9793d992d5d0",
            "filename": "docs/source/ar/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fllm_tutorial_optimization.md?ref=2e0457e6074d82e1d36b041aa271bb55025e36a3",
            "patch": "@@ -98,7 +98,7 @@ def bytes_to_giga_bytes(bytes):\n   return bytes / 1024 / 1024 / 1024\n ```\n \n-دعونا نستدعي [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html) لقياس ذروة تخصيص ذاكرة وحدة معالجة الرسومات (GPU).\n+دعونا نستدعي [`torch.cuda.memory.max_memory_allocated`](https://docs.pytorch.org/docs/stable/generated/torch.cuda.memory.max_memory_allocated.html) لقياس ذروة تخصيص ذاكرة وحدة معالجة الرسومات (GPU).\n \n ```python\n bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
        },
        {
            "sha": "b186fcdeba1716049aa36eef8c916e9d32c48312",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=2e0457e6074d82e1d36b041aa271bb55025e36a3",
            "patch": "@@ -111,7 +111,7 @@ def bytes_to_giga_bytes(bytes):\n   return bytes / 1024 / 1024 / 1024\n ```\n \n-Let's call [`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html) to measure the peak GPU memory allocation.\n+Let's call [`torch.cuda.memory.max_memory_allocated`](https://docs.pytorch.org/docs/stable/generated/torch.cuda.memory.max_memory_allocated.html) to measure the peak GPU memory allocation.\n \n ```python\n bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
        },
        {
            "sha": "010aaabd809c03ab07730b21cfedede242d334aa",
            "filename": "docs/source/ko/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2e0457e6074d82e1d36b041aa271bb55025e36a3/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fllm_tutorial_optimization.md?ref=2e0457e6074d82e1d36b041aa271bb55025e36a3",
            "patch": "@@ -106,7 +106,7 @@ def bytes_to_giga_bytes(bytes):\n   return bytes / 1024 / 1024 / 1024\n ```\n \n-[`torch.cuda.max_memory_allocated`](https://pytorch.org/docs/stable/generated/torch.cuda.max_memory_allocated.html)를 호출하여 최대 GPU 메모리 할당을 측정해 보겠습니다.\n+[`torch.cuda.memory.max_memory_allocated`](https://docs.pytorch.org/docs/stable/generated/torch.cuda.memory.max_memory_allocated.html)를 호출하여 최대 GPU 메모리 할당을 측정해 보겠습니다.\n \n ```python\n bytes_to_giga_bytes(torch.cuda.max_memory_allocated())"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}