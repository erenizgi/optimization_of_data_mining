{
    "author": "ydshieh",
    "message": "Make `test_generate_with_static_cache` even less flaky (#34995)\n\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
    "files": [
        {
            "sha": "2f523ed36d983ff7ae5c8918befc38206b560bde",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 48,
            "deletions": 0,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
            "patch": "@@ -14,6 +14,7 @@\n \n import collections\n import contextlib\n+import copy\n import doctest\n import functools\n import gc\n@@ -1396,6 +1397,53 @@ def assert_screenout(out, what):\n     assert match_str != -1, f\"expecting to find {what} in output: f{out_pr}\"\n \n \n+def set_model_tester_for_less_flaky_test(test_case):\n+    if hasattr(test_case.model_tester, \"num_hidden_layers\"):\n+        test_case.model_tester.num_hidden_layers = 1\n+    if (\n+        hasattr(test_case.model_tester, \"vision_config\")\n+        and \"num_hidden_layers\" in test_case.model_tester.vision_config\n+    ):\n+        test_case.model_tester.vision_config = copy.deepcopy(test_case.model_tester.vision_config)\n+        test_case.model_tester.vision_config[\"num_hidden_layers\"] = 1\n+    if hasattr(test_case.model_tester, \"text_config\") and \"num_hidden_layers\" in test_case.model_tester.text_config:\n+        test_case.model_tester.text_config = copy.deepcopy(test_case.model_tester.text_config)\n+        test_case.model_tester.text_config[\"num_hidden_layers\"] = 1\n+\n+\n+def set_config_for_less_flaky_test(config):\n+    target_attrs = [\n+        \"rms_norm_eps\",\n+        \"layer_norm_eps\",\n+        \"norm_eps\",\n+        \"norm_epsilon\",\n+        \"layer_norm_epsilon\",\n+        \"batch_norm_eps\",\n+    ]\n+    for target_attr in target_attrs:\n+        setattr(config, target_attr, 1.0)\n+\n+    # norm layers (layer/group norm, etc.) could cause flaky tests when the tensors have very small variance.\n+    # (We don't need the original epsilon values to check eager/sdpa matches)\n+    attrs = [\"text_config\", \"vision_config\", \"text_encoder\", \"audio_encoder\", \"decoder\"]\n+    for attr in attrs:\n+        if hasattr(config, attr):\n+            for target_attr in target_attrs:\n+                setattr(getattr(config, attr), target_attr, 1.0)\n+\n+\n+def set_model_for_less_flaky_test(model):\n+    # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n+    target_names = (\"LayerNorm\", \"GroupNorm\", \"BatchNorm\", \"RMSNorm\", \"BatchNorm2d\", \"BatchNorm1d\")\n+    target_attrs = [\"eps\", \"epsilon\", \"variance_epsilon\"]\n+    if is_torch_available() and isinstance(model, torch.nn.Module):\n+        for module in model.modules():\n+            if type(module).__name__.endswith(target_names):\n+                for attr in target_attrs:\n+                    if hasattr(module, attr):\n+                        setattr(module, attr, 1.0)\n+\n+\n class CaptureStd:\n     \"\"\"\n     Context manager to capture:"
        },
        {
            "sha": "4ac22e77779022d97e9c91ba72f3f486fd9b43e6",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
            "patch": "@@ -37,6 +37,9 @@\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n+    set_config_for_less_flaky_test,\n+    set_model_for_less_flaky_test,\n+    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -1921,11 +1924,13 @@ def test_generate_with_static_cache(self):\n         Tests that generating with static cache give almost same results as with dynamic cache, and the output cache\n         has the expected shapes\n         \"\"\"\n+        set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n             if not model_class._supports_static_cache:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            set_config_for_less_flaky_test(config)\n             main_input = inputs_dict[model_class.main_input_name]\n \n             if config.is_encoder_decoder:\n@@ -1938,6 +1943,8 @@ def test_generate_with_static_cache(self):\n \n             for dtype in (torch.float32, torch.float16):\n                 model = model_class(config).to(torch_device).to(dtype).eval()\n+                set_model_for_less_flaky_test(model)\n+\n                 generation_kwargs = {\n                     \"max_new_tokens\": max_new_tokens,\n                     \"return_dict_in_generate\": True,  # Required to return `past_key_values`"
        },
        {
            "sha": "98b554be65fbf985e5afb90751c8669b4c2f572e",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
            "patch": "@@ -41,6 +41,9 @@\n     require_torch_gpu,\n     require_torch_sdpa,\n     require_torchaudio,\n+    set_config_for_less_flaky_test,\n+    set_model_for_less_flaky_test,\n+    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -516,8 +519,11 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n \n+        set_model_tester_for_less_flaky_test(self)\n+\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            set_config_for_less_flaky_test(config)\n             model = model_class(config)\n \n             is_encoder_decoder = model.config.is_encoder_decoder\n@@ -534,6 +540,9 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n+                set_model_for_less_flaky_test(model_eager)\n+                set_model_for_less_flaky_test(model_sdpa)\n+\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []\n@@ -1528,8 +1537,11 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n \n+        set_model_tester_for_less_flaky_test(self)\n+\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            set_config_for_less_flaky_test(config)\n             model = model_class(config)\n \n             is_encoder_decoder = model.config.is_encoder_decoder\n@@ -1546,6 +1558,9 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device)\n \n+                set_model_for_less_flaky_test(model_eager)\n+                set_model_for_less_flaky_test(model_sdpa)\n+\n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand\n                 fail_cases = []"
        },
        {
            "sha": "276375c7e85439128a32e0d55a3b70592486be55",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
            "patch": "@@ -840,7 +840,13 @@ def test_generation_languages(self):\n     def test_speech_generation(self):\n         config, input_speech, input_text = self.prepare_speech_and_text_input()\n \n+        from transformers.testing_utils import set_config_for_less_flaky_test, set_model_for_less_flaky_test\n+\n+        set_config_for_less_flaky_test(config)\n+\n         model = SeamlessM4Tv2Model(config=config)\n+        set_model_for_less_flaky_test(model)\n+\n         self.update_generation(model)\n         model.save_pretrained(self.tmpdirname)\n         model.to(torch_device)\n@@ -852,13 +858,23 @@ def test_speech_generation(self):\n         state_dict = model.state_dict()\n \n         text_model = SeamlessM4Tv2ForTextToSpeech.from_pretrained(self.tmpdirname)\n+        # Even if this component is loaded after `model.save_pretrained` which is after\n+        # `set_model_for_less_flaky_test(model)`, we still need to apply `set_model_for_less_flaky_test` here as the\n+        # `eps` attribute in the model's norm layers is not set from the config.\n+        set_model_for_less_flaky_test(text_model)\n+\n         self.update_generation(text_model)\n         text_model.to(torch_device)\n         text_model.eval()\n \n         output_text = self.factory_generation_speech_test(model, input_text)\n \n         speech_model = SeamlessM4Tv2ForSpeechToSpeech.from_pretrained(self.tmpdirname)\n+        # Even if this component is loaded after `model.save_pretrained` which is after\n+        # `set_model_for_less_flaky_test(model)`, we still need to apply `set_model_for_less_flaky_test` here as the\n+        # `eps` attribute in the model's norm layers is not set from the config.\n+        set_model_for_less_flaky_test(speech_model)\n+\n         self.update_generation(speech_model)\n         speech_model.to(torch_device)\n         speech_model.eval()"
        },
        {
            "sha": "929bbb13a56e806ff28d62a7a7474bdcb46fe00a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 7,
            "deletions": 32,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/504c4d36929b6bb8a8c2ecfad0f2625f4075f22a/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=504c4d36929b6bb8a8c2ecfad0f2625f4075f22a",
            "patch": "@@ -89,6 +89,9 @@\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n+    set_config_for_less_flaky_test,\n+    set_model_for_less_flaky_test,\n+    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n@@ -3976,34 +3979,11 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         def get_mean_reldiff(failcase, x, ref, atol, rtol):\n             return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n \n-        if hasattr(self.model_tester, \"num_hidden_layers\"):\n-            self.model_tester.num_hidden_layers = 1\n-        if hasattr(self.model_tester, \"vision_config\") and \"num_hidden_layers\" in self.model_tester.vision_config:\n-            self.model_tester.vision_config = copy.deepcopy(self.model_tester.vision_config)\n-            self.model_tester.vision_config[\"num_hidden_layers\"] = 1\n-        if hasattr(self.model_tester, \"text_config\") and \"num_hidden_layers\" in self.model_tester.text_config:\n-            self.model_tester.text_config = copy.deepcopy(self.model_tester.text_config)\n-            self.model_tester.text_config[\"num_hidden_layers\"] = 1\n+        set_model_tester_for_less_flaky_test(self)\n \n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            config.rms_norm_eps = 1.0\n-            config.layer_norm_eps = 1.0\n-            config.norm_eps = 1.0\n-            config.norm_epsilon = 1.0\n-            config.layer_norm_epsilon = 1.0\n-\n-            # norm layers (layer/group norm, etc.) could cause flaky tests when the tensors have very small variance.\n-            # (We don't need the original epsilon values to check eager/sdpa matches)\n-            for attr in [\"text_config\", \"vision_config\", \"text_encoder\", \"audio_encoder\", \"decoder\"]:\n-                if hasattr(config, attr):\n-                    getattr(config, attr).rms_norm_eps = 1.0\n-                    getattr(config, attr).layer_norm_eps = 1.0\n-                    getattr(config, attr).norm_eps = 1.0\n-                    getattr(config, attr).norm_epsilon = 1.0\n-                    getattr(config, attr).layer_norm_epsilon = 1.0\n-\n+            set_config_for_less_flaky_test(config)\n             model = model_class(config)\n             # FIXME: we deactivate boolean mask for models using \"use_mask_token\" in their constructors.\n             # These models support masking only in the case `use_mask_token=True`. Otherwise they cannot consume an input mask.\n@@ -4029,13 +4009,8 @@ def get_mean_reldiff(failcase, x, ref, atol, rtol):\n                 )\n                 model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n \n-                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n-                for x in model_eager.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n-                for x in model_sdpa.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n+                set_model_for_less_flaky_test(model_eager)\n+                set_model_for_less_flaky_test(model_sdpa)\n \n                 # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n                 # but it would be nicer to have an efficient way to use parameterized.expand"
        }
    ],
    "stats": {
        "total": 125,
        "additions": 93,
        "deletions": 32
    }
}