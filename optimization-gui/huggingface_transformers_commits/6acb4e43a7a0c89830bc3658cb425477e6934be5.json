{
    "author": "gallilmaimon",
    "message": "Support BatchNorm in Hubert pos_conv_emb as in fairseq (#34389)\n\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\n\r\n* Correct the new defaults (#34377)\r\n\r\n* Correct the new defaults\r\n\r\n* CIs\r\n\r\n* add check\r\n\r\n* Update utils.py\r\n\r\n* Update utils.py\r\n\r\n* Add the max_length in generate test checking shape without passing length\r\n\r\n* style\r\n\r\n* CIs\r\n\r\n* fix fx CI issue\r\n\r\n* [auto. ping] Avoid sending empty info + add more team members (#34383)\r\n\r\n* update\r\n\r\n* update\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\n\r\n* Fix glm  (#34388)\r\n\r\n* Fix duplicated\r\n\r\n* fix import\r\n\r\n* Use non nested images and batched text Idefics2/3  (#34222)\r\n\r\n* add support for non nested images and add tests\r\n\r\n* add tests error scenario\r\n\r\n* fix style\r\n\r\n* added single and no image to error tests\r\n\r\n* Fix onnx non-expotable inplace aten op (#34376)\r\n\r\n* fix onnx non-expotable inplace op\r\n\r\n* mistral, qwen2, qwen2_vl, starcoder2\r\n\r\n* fixup copies\r\n\r\n* Fix right padding in LLaVA models (#34305)\r\n\r\n* fix right pad llavas\r\n\r\n* device mismatch\r\n\r\n* no filter (#34391)\r\n\r\n* no filter\r\n\r\n* no filter\r\n\r\n* no filter\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\n\r\n* SynthID: better example (#34372)\r\n\r\n* better example\r\n\r\n* Update src/transformers/generation/configuration_utils.py\r\n\r\n* Update src/transformers/generation/logits_process.py\r\n\r\n* nits\r\n\r\n* Tests: upgrade `test_eager_matches_sdpa_generate` (#34386)\r\n\r\n* Fix bnb training test failure (#34414)\r\n\r\n* Fix bnb training test: compatibility with OPTSdpaAttention\r\n\r\n* Avoid check expected exception when it is on CUDA (#34408)\r\n\r\n* update\r\n\r\n* update\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\n\r\n* Fix typos in agents_advanced.md (#34405)\r\n\r\n* [docs] Cache implementations (#34325)\r\n\r\ncache\r\n\r\n* [run-slow] hubert\r\n\r\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\nAdd conversion integration test, and make batchnorm explicit variable\r\n\r\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\nfix make fixup styling changes\r\n\r\n* [run-slow] hubert\r\n\r\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\n\r\n* [run-slow] hubert\r\n\r\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\nAdd conversion integration test, and make batchnorm explicit variable\r\n\r\n* Support BatchNorm in Hubert pos_conv_emb as in fairseq\r\nfix make fixup styling changes\r\n\r\n* [run-slow] hubert\r\n\r\n* [run-slow] hubert\r\n\r\n---------\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\r\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\r\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\r\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>\r\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>\r\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\r\nCo-authored-by: Matthew Douglas <38992547+matthewdouglas@users.noreply.github.com>\r\nCo-authored-by: Rudy Delouya <rudy.delouya@gmail.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: Yoach Lacombe <52246514+ylacombe@users.noreply.github.com>",
    "sha": "6acb4e43a7a0c89830bc3658cb425477e6934be5",
    "files": [
        {
            "sha": "9f488b198889572865327b8a0b17b9e412d5c073",
            "filename": "src/transformers/models/hubert/configuration_hubert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconfiguration_hubert.py?ref=6acb4e43a7a0c89830bc3658cb425477e6934be5",
            "patch": "@@ -94,6 +94,8 @@ class HubertConfig(PretrainedConfig):\n             embeddings layer.\n         num_conv_pos_embedding_groups (`int`, *optional*, defaults to 16):\n             Number of groups of 1D convolutional positional embeddings layer.\n+        conv_pos_batch_norm (`bool`, *optional*, defaults to `False`):\n+            Whether to use batch norm instead of weight norm in conv_pos\n         do_stable_layer_norm (`bool`, *optional*, defaults to `False`):\n             Whether do apply *stable* layer norm architecture of the Transformer encoder. `do_stable_layer_norm is\n             True` corresponds to applying layer norm before the attention layer, whereas `do_stable_layer_norm is\n@@ -182,6 +184,7 @@ def __init__(\n         conv_bias=False,\n         num_conv_pos_embeddings=128,\n         num_conv_pos_embedding_groups=16,\n+        conv_pos_batch_norm=False,\n         do_stable_layer_norm=False,\n         apply_spec_augment=True,\n         mask_time_prob=0.05,\n@@ -209,6 +212,7 @@ def __init__(\n         self.conv_bias = conv_bias\n         self.num_conv_pos_embeddings = num_conv_pos_embeddings\n         self.num_conv_pos_embedding_groups = num_conv_pos_embedding_groups\n+        self.conv_pos_batch_norm = conv_pos_batch_norm\n         self.num_feat_extract_layers = len(self.conv_dim)\n         self.num_hidden_layers = num_hidden_layers\n         self.intermediate_size = intermediate_size"
        },
        {
            "sha": "4966340493f35cf3ba05dc8ce1f40e294e164573",
            "filename": "src/transformers/models/hubert/convert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fconvert_hubert_original_pytorch_checkpoint_to_pytorch.py?ref=6acb4e43a7a0c89830bc3658cb425477e6934be5",
            "patch": "@@ -38,7 +38,8 @@\n \n MAPPING = {\n     \"post_extract_proj\": \"feature_projection.projection\",\n-    \"encoder.pos_conv.0\": \"encoder.pos_conv_embed.conv\",\n+    \"encoder.pos_conv.0\": \"encoder.pos_conv_embed.batch_norm\",\n+    \"encoder.pos_conv.1\": \"encoder.pos_conv_embed.conv\",\n     \"self_attn.k_proj\": \"encoder.layers.*.attention.k_proj\",\n     \"self_attn.v_proj\": \"encoder.layers.*.attention.v_proj\",\n     \"self_attn.q_proj\": \"encoder.layers.*.attention.q_proj\",\n@@ -76,6 +77,12 @@ def set_recursively(hf_pointer, key, value, full_name, weight_type):\n         hf_pointer.weight_v.data = value\n     elif weight_type == \"bias\":\n         hf_pointer.bias.data = value\n+    elif weight_type == \"running_mean\":\n+        hf_pointer.running_mean.data = value\n+    elif weight_type == \"running_var\":\n+        hf_pointer.running_var.data = value\n+    elif weight_type == \"num_batches_tracked\":\n+        hf_pointer.num_batches_tracked.data = value\n     else:\n         hf_pointer.data = value\n \n@@ -116,6 +123,12 @@ def recursively_load_weights(fairseq_model, hf_model, is_finetuned):\n                         weight_type = \"weight\"\n                     elif \"bias\" in name:\n                         weight_type = \"bias\"\n+                    elif \"running_mean\" in name:\n+                        weight_type = \"running_mean\"\n+                    elif \"running_var\" in name:\n+                        weight_type = \"running_var\"\n+                    elif \"num_batches_tracked\" in name:\n+                        weight_type = \"num_batches_tracked\"\n                     else:\n                         weight_type = None\n                     set_recursively(hf_model, mapped_key, value, name, weight_type)"
        },
        {
            "sha": "03904a6abfa08bf1c1ae52b3f1f5081c8c55d222",
            "filename": "src/transformers/models/hubert/modeling_hubert.py",
            "status": "modified",
            "additions": 22,
            "deletions": 18,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6acb4e43a7a0c89830bc3658cb425477e6934be5/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_hubert.py?ref=6acb4e43a7a0c89830bc3658cb425477e6934be5",
            "patch": "@@ -260,7 +260,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-# Copied from transformers.models.wav2vec2.modeling_wav2vec2.Wav2Vec2PositionalConvEmbedding with Wav2Vec2->Hubert\n class HubertPositionalConvEmbedding(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -272,32 +271,37 @@ def __init__(self, config):\n             groups=config.num_conv_pos_embedding_groups,\n         )\n \n-        weight_norm = nn.utils.weight_norm\n-        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n-            weight_norm = nn.utils.parametrizations.weight_norm\n+        self.batch_norm = None\n+        if config.conv_pos_batch_norm:\n+            self.batch_norm = nn.BatchNorm1d(config.hidden_size)\n+        else:\n+            weight_norm = nn.utils.weight_norm\n+            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+                weight_norm = nn.utils.parametrizations.weight_norm\n \n-        if is_deepspeed_zero3_enabled():\n-            import deepspeed\n+            if is_deepspeed_zero3_enabled():\n+                import deepspeed\n \n-            with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n-            if hasattr(self.conv, \"parametrizations\"):\n-                weight_g = self.conv.parametrizations.weight.original0\n-                weight_v = self.conv.parametrizations.weight.original1\n+                with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n+                    self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+                if hasattr(self.conv, \"parametrizations\"):\n+                    weight_g = self.conv.parametrizations.weight.original0\n+                    weight_v = self.conv.parametrizations.weight.original1\n+                else:\n+                    weight_g = self.conv.weight_g\n+                    weight_v = self.conv.weight_v\n+                deepspeed.zero.register_external_parameter(self, weight_v)\n+                deepspeed.zero.register_external_parameter(self, weight_g)\n             else:\n-                weight_g = self.conv.weight_g\n-                weight_v = self.conv.weight_v\n-            deepspeed.zero.register_external_parameter(self, weight_v)\n-            deepspeed.zero.register_external_parameter(self, weight_g)\n-        else:\n-            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n         self.padding = HubertSamePadLayer(config.num_conv_pos_embeddings)\n         self.activation = ACT2FN[config.feat_extract_activation]\n \n     def forward(self, hidden_states):\n         hidden_states = hidden_states.transpose(1, 2)\n-\n+        if self.batch_norm is not None:\n+            hidden_states = self.batch_norm(hidden_states)\n         hidden_states = self.conv(hidden_states)\n         hidden_states = self.padding(hidden_states)\n         hidden_states = self.activation(hidden_states)"
        },
        {
            "sha": "191d2f8c88c380f5ecf140671d06fc9c0d14ad1b",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 37,
            "deletions": 0,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/6acb4e43a7a0c89830bc3658cb425477e6934be5/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6acb4e43a7a0c89830bc3658cb425477e6934be5/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=6acb4e43a7a0c89830bc3658cb425477e6934be5",
            "patch": "@@ -943,3 +943,40 @@ def test_inference_distilhubert(self):\n         self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))\n         self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))\n         self.assertTrue(abs(outputs.sum() - expected_output_sum) < 0.1)\n+\n+    def test_inference_hubert_25hz(self):\n+        model = HubertModel.from_pretrained(\"slprl/mhubert-base-25hz\").to(torch_device)\n+\n+        sample = self._load_datasamples(1)\n+        input_speech = torch.tensor(sample[0], dtype=torch.float, device=torch_device).unsqueeze(0)\n+\n+        with torch.no_grad():\n+            outputs = model(input_speech, output_hidden_states=True).hidden_states[11]\n+\n+        # expected outputs taken from the original textlesslib implementation by:\n+        # model = SpeechEncoder.by_name(dense_model_name='mhubert-base-25hz', quantizer_model_name='kmeans',\n+        # vocab_size=500, deduplicate=False, need_f0=False)\n+        # model(wav)['dense']\n+        expected_outputs_first = torch.tensor(\n+            [\n+                [0.0267, 0.1776, -0.1706, -0.4559],\n+                [-0.2430, -0.2943, -0.1864, -0.1187],\n+                [-0.1812, -0.4239, -0.1916, -0.0858],\n+                [-0.1495, -0.4758, -0.4036, 0.0302],\n+            ],\n+            device=torch_device,\n+        )\n+        expected_outputs_last = torch.tensor(\n+            [\n+                [0.3366, -0.2734, -0.1415, -0.3055],\n+                [0.2329, -0.3580, -0.1421, -0.3197],\n+                [0.1631, -0.4301, -0.1965, -0.2956],\n+                [0.3342, -0.2185, -0.2253, -0.2363],\n+            ],\n+            device=torch_device,\n+        )\n+        expected_output_sum = 1681.7603\n+\n+        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))\n+        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))\n+        self.assertTrue(abs(outputs.sum() - expected_output_sum) < 0.1)"
        }
    ],
    "stats": {
        "total": 96,
        "additions": 77,
        "deletions": 19
    }
}