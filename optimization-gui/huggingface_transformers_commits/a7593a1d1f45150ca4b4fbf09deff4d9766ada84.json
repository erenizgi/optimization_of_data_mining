{
    "author": "Vixel2006",
    "message": "[BugFix] QA pipeline edge case: `align_to_words=True` in `QuestionAnsweringPipeline` can lead to duplicate answers (#38761)\n\n* fixing the problem align_to_words=True leading to duplicate solutions\n\n* adding tests\n\n* some fixes\n\n* some fixes\n\n* changing the handle_duplicate_answers=False by default\n\n* some fixese\n\n* some fixes\n\n* make the duplicate handling the default behaviour and merge duplicates\n\n* make the duplicate handling the default behaviour",
    "sha": "a7593a1d1f45150ca4b4fbf09deff4d9766ada84",
    "files": [
        {
            "sha": "4e4fc2fc279ffb8eb869b065629d3d850cc71aca",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7593a1d1f45150ca4b4fbf09deff4d9766ada84/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7593a1d1f45150ca4b4fbf09deff4d9766ada84/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=a7593a1d1f45150ca4b4fbf09deff4d9766ada84",
            "patch": "@@ -595,20 +595,27 @@ def postprocess(\n                 # - we start by finding the right word containing the token with `token_to_word`\n                 # - then we convert this word in a character span with `word_to_chars`\n                 sequence_index = 1 if question_first else 0\n+\n                 for s, e, score in zip(starts, ends, scores):\n                     s = s - offset\n                     e = e - offset\n \n                     start_index, end_index = self.get_indices(enc, s, e, sequence_index, align_to_words)\n \n-                    answers.append(\n-                        {\n-                            \"score\": score.item(),\n-                            \"start\": start_index,\n-                            \"end\": end_index,\n-                            \"answer\": example.context_text[start_index:end_index],\n-                        }\n-                    )\n+                    target_answer = example.context_text[start_index:end_index]\n+                    answer = self.get_answer(answers, target_answer)\n+\n+                    if answer:\n+                        answer[\"score\"] += score.item()\n+                    else:\n+                        answers.append(\n+                            {\n+                                \"score\": score.item(),\n+                                \"start\": start_index,\n+                                \"end\": end_index,\n+                                \"answer\": example.context_text[start_index:end_index],\n+                            }\n+                        )\n \n         if handle_impossible_answer:\n             answers.append({\"score\": min_null_score, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n@@ -617,6 +624,12 @@ def postprocess(\n             return answers[0]\n         return answers\n \n+    def get_answer(self, answers: List[Dict], target: str) -> Optional[Dict]:\n+        for answer in answers:\n+            if answer[\"answer\"].lower() == target.lower():\n+                return answer\n+        return None\n+\n     def get_indices(\n         self, enc: \"tokenizers.Encoding\", s: int, e: int, sequence_index: int, align_to_words: bool\n     ) -> Tuple[int, int]:"
        },
        {
            "sha": "fbd70b2a0997d223c8d8d6b333ac69c5cdb9df57",
            "filename": "tests/pipelines/test_pipelines_question_answering.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a7593a1d1f45150ca4b4fbf09deff4d9766ada84/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a7593a1d1f45150ca4b4fbf09deff4d9766ada84/tests%2Fpipelines%2Ftest_pipelines_question_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_question_answering.py?ref=a7593a1d1f45150ca4b4fbf09deff4d9766ada84",
            "patch": "@@ -138,7 +138,11 @@ def run_pipeline_test(self, question_answerer, _):\n             question=\"Where was HuggingFace founded ?\", context=\"HuggingFace was founded in Paris.\", top_k=20\n         )\n         self.assertEqual(\n-            outputs, [{\"answer\": ANY(str), \"start\": ANY(int), \"end\": ANY(int), \"score\": ANY(float)} for i in range(20)]\n+            outputs,\n+            [\n+                {\"answer\": ANY(str), \"start\": ANY(int), \"end\": ANY(int), \"score\": ANY(float)}\n+                for i in range(len(outputs))\n+            ],\n         )\n         for single_output in outputs:\n             compare_pipeline_output_to_hub_spec(single_output, QuestionAnsweringOutputElement)\n@@ -279,6 +283,19 @@ def test_small_model_long_context_cls_slow(self):\n         )\n         self.assertEqual(nested_simplify(outputs), {\"score\": 0.988, \"start\": 0, \"end\": 0, \"answer\": \"\"})\n \n+    @require_torch\n+    def test_duplicate_handling(self):\n+        question_answerer = pipeline(\"question-answering\", model=\"deepset/tinyroberta-squad2\")\n+\n+        outputs = question_answerer(\n+            question=\"Who is the chancellor of Germany?\",\n+            context=\"Angela Merkel was the chancellor of Germany.\",\n+            top_k=10,\n+        )\n+\n+        answers = [output[\"answer\"] for output in outputs]\n+        self.assertEqual(len(answers), len(set(answers)), \"There are duplicate answers in the outputs.\")\n+\n     @require_tf\n     def test_small_model_tf(self):\n         question_answerer = pipeline("
        }
    ],
    "stats": {
        "total": 48,
        "additions": 39,
        "deletions": 9
    }
}