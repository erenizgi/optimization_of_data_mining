{
    "author": "jiqing-feng",
    "message": "add gpt2 test on XPU (#37028)\n\n* add gpt2 test on XPU\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* auto dtype has been fixed\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* convert model to train mode\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4",
    "files": [
        {
            "sha": "2d40e90104326f0c53ef69088303a9a665461c60",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4",
            "patch": "@@ -626,7 +626,6 @@ def test_training(self):\n \n \n @apply_skip_if_not_implemented\n-@unittest.skipIf(torch_device == \"xpu\", reason=\"XPU has precision issue on gpt model, will test it once fixed\")\n class Bnb4BitGPT2Test(Bnb4BitTest):\n     model_name = \"openai-community/gpt2-xl\"\n     EXPECTED_RELATIVE_DIFFERENCE = 3.3191854854152187"
        },
        {
            "sha": "26191baa4e9d8990f0c48886a684f83b41fabf6d",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=3a6ab46a0b85479d6fb0d6ce0bff2e48b4751ac4",
            "patch": "@@ -889,6 +889,7 @@ def test_training(self):\n \n         # Step 1: freeze all parameters\n         model = AutoModelForCausalLM.from_pretrained(self.model_name, load_in_8bit=True)\n+        model.train()\n \n         if torch.cuda.is_available():\n             self.assertEqual(set(model.hf_device_map.values()), {torch.cuda.current_device()})\n@@ -914,14 +915,9 @@ def test_training(self):\n         batch = self.tokenizer(\"Test batch \", return_tensors=\"pt\").to(torch_device)\n \n         # Step 4: Check if the gradient is not None\n-        if torch_device in {\"xpu\", \"cpu\"}:\n-            # XPU and CPU finetune do not support autocast for now.\n+        with torch.autocast(torch_device):\n             out = model.forward(**batch)\n             out.logits.norm().backward()\n-        else:\n-            with torch.autocast(torch_device):\n-                out = model.forward(**batch)\n-                out.logits.norm().backward()\n \n         for module in model.modules():\n             if isinstance(module, LoRALayer):\n@@ -932,7 +928,6 @@ def test_training(self):\n \n \n @apply_skip_if_not_implemented\n-@unittest.skipIf(torch_device == \"xpu\", reason=\"XPU has precision issue on gpt model, will test it once fixed\")\n class MixedInt8GPT2Test(MixedInt8Test):\n     model_name = \"openai-community/gpt2-xl\"\n     EXPECTED_RELATIVE_DIFFERENCE = 1.8720077507258357"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 2,
        "deletions": 8
    }
}