{
    "author": "Cyrilvallez",
    "message": "[loading] Really initialize on meta device for huge perf gains (#42941)\n\n* use meta device directly\n\n* style\n\n* move back non-persistent\n\n* fix\n\n* make helper\n\n* fix it\n\n* use native param dtype\n\n* make tensors buffers\n\n* style\n\n* fix\n\n* oupsi\n\n* add a test and fix\n\n* fix\n\n* create timm integration to reinit non-persistemnt buffers....\n\n* style\n\n* style\n\n* more\n\n* better\n\n* add doc\n\n* more timm stuff\n\n* more\n\n* fix\n\n* small change\n\n* no actually it was fine before",
    "sha": "bb9357ffe89252437286478c98cbd302708cb9f6",
    "files": [
        {
            "sha": "deb195e25a2bdee55a4c437a034c37fe8dfb502a",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -21,7 +21,6 @@\n import os\n import re\n from collections import OrderedDict, defaultdict\n-from contextlib import contextmanager\n from typing import TYPE_CHECKING\n \n from safetensors import safe_open\n@@ -55,114 +54,6 @@\n logger = logging.get_logger(__name__)\n \n \n-@contextmanager\n-def init_empty_weights(include_buffers: bool = False):\n-    \"\"\"\n-    A context manager under which models are initialized with all parameters on the meta device, therefore creating an\n-    empty model. Useful when just initializing the model would blow the available RAM.\n-\n-    Args:\n-        include_buffers (`bool`, *optional*):\n-            Whether or not to also put all buffers on the meta device while initializing.\n-\n-    Example:\n-\n-    ```python\n-    import torch.nn as nn\n-    from accelerate import init_empty_weights\n-\n-    # Initialize a model with 100 billions parameters in no time and without using any RAM.\n-    with init_empty_weights():\n-        tst = nn.Sequential(*[nn.Linear(10000, 10000) for _ in range(1000)])\n-    ```\n-\n-    <Tip warning={true}>\n-\n-    Any model created under this context manager has no weights. As such you can't do something like\n-    `model.to(some_device)` with it. To load weights inside your empty model, see [`load_checkpoint_and_dispatch`].\n-    Make sure to overwrite the default device_map param for [`load_checkpoint_and_dispatch`], otherwise dispatch is not\n-    called.\n-\n-    </Tip>\n-    \"\"\"\n-    with init_on_device(torch.device(\"meta\"), include_buffers=include_buffers) as f:\n-        yield f\n-\n-\n-@contextmanager\n-def init_on_device(device: \"torch.device\", include_buffers: bool = False):\n-    \"\"\"\n-    A context manager under which models are initialized with all parameters on the specified device.\n-\n-    Args:\n-        device (`torch.device`):\n-            Device to initialize all parameters on.\n-        include_buffers (`bool`, *optional*):\n-            Whether or not to also put all buffers on the meta device while initializing.\n-\n-    Example:\n-\n-    ```python\n-    import torch.nn as nn\n-    from accelerate import init_on_device\n-\n-    with init_on_device(device=torch.device(\"cuda\")):\n-        tst = nn.Linear(100, 100)  # on `cuda` device\n-    ```\n-    \"\"\"\n-    if include_buffers:\n-        with device:\n-            yield\n-        return\n-\n-    old_register_parameter = nn.Module.register_parameter\n-    if include_buffers:\n-        old_register_buffer = nn.Module.register_buffer\n-\n-    def register_empty_parameter(module, name, param):\n-        old_register_parameter(module, name, param)\n-        if param is not None:\n-            param_cls = type(module._parameters[name])\n-            kwargs = module._parameters[name].__dict__\n-            kwargs[\"requires_grad\"] = param.requires_grad\n-            module._parameters[name] = param_cls(module._parameters[name].to(device), **kwargs)\n-\n-    def register_empty_buffer(module, name, buffer, persistent=True):\n-        old_register_buffer(module, name, buffer, persistent=persistent)\n-        if buffer is not None:\n-            module._buffers[name] = module._buffers[name].to(device)\n-\n-    # Patch tensor creation\n-    if include_buffers:\n-        tensor_constructors_to_patch = {\n-            torch_function_name: getattr(torch, torch_function_name)\n-            for torch_function_name in [\"empty\", \"zeros\", \"ones\", \"full\"]\n-        }\n-    else:\n-        tensor_constructors_to_patch = {}\n-\n-    def patch_tensor_constructor(fn):\n-        def wrapper(*args, **kwargs):\n-            kwargs[\"device\"] = device\n-            return fn(*args, **kwargs)\n-\n-        return wrapper\n-\n-    try:\n-        nn.Module.register_parameter = register_empty_parameter\n-        if include_buffers:\n-            nn.Module.register_buffer = register_empty_buffer\n-        for torch_function_name in tensor_constructors_to_patch:\n-            setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n-        yield\n-    finally:\n-        nn.Module.register_parameter = old_register_parameter\n-        if include_buffers:\n-            nn.Module.register_buffer = old_register_buffer\n-        for torch_function_name, old_torch_function in tensor_constructors_to_patch.items():\n-            setattr(torch, torch_function_name, old_torch_function)\n-\n-\n def check_and_set_device_map(device_map: \"torch.device | int | str | dict | None\") -> dict | str | None:\n     from ..modeling_utils import get_torch_context_manager_or_global_device\n "
        },
        {
            "sha": "816fb227daa478fcf04598921464b84be87079e4",
            "filename": "src/transformers/integrations/aqlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Faqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faqlm.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,13 +14,11 @@\n \"AQLM (Additive Quantization of Language Model) integration file\"\n \n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n+    import torch\n     import torch.nn as nn\n \n logger = logging.get_logger(__name__)\n@@ -46,7 +44,7 @@ def replace_with_aqlm_linear(model, modules_to_not_convert: list[str] | None = N\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 new_module = QuantizedLinear(\n                     module.in_features,"
        },
        {
            "sha": "494f7aef20f8d06cc419055e89f2349ff3b8f938",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,12 +16,9 @@\n from typing import Optional, Union\n \n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n     import torch\n     import torch.nn as nn\n@@ -97,7 +94,7 @@ def replace_with_awq_linear(\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 new_module = target_cls(\n                     bits=quantization_config.bits,"
        },
        {
            "sha": "fb30c17d6975e3143f7d57f812fa44da1d62df28",
            "filename": "src/transformers/integrations/bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fbitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitnet.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -1,10 +1,7 @@\n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n     import torch\n     import torch.nn as nn\n@@ -334,7 +331,7 @@ def replace_with_bitnet_linear(model, modules_to_not_convert: list[str] | None =\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 if quantization_config and quantization_config.linear_class == \"autobitlinear\":\n                     new_module = AutoBitLinear("
        },
        {
            "sha": "a1b087a808c933730406f3e3f3af1577a9b9e718",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -22,7 +22,6 @@\n \n if is_accelerate_available():\n     import accelerate\n-    from accelerate import init_empty_weights\n     from accelerate.hooks import add_hook_to_module, remove_hook_from_module\n \n logger = logging.get_logger(__name__)\n@@ -181,7 +180,7 @@ def replace_with_bnb_linear(\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n         new_module = None\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, (nn.Linear, Conv1D)):\n                 if isinstance(module, Conv1D):\n                     in_features, out_features = module.weight.shape\n@@ -293,7 +292,7 @@ def dequantize_and_replace(model, quantization_config=None, dtype=None):\n     target_cls = bnb.nn.Linear8bitLt if quant_method == \"llm_int8\" else bnb.nn.Linear4bit\n     for module_name, module in model.named_modules():\n         if isinstance(module, target_cls):\n-            with init_empty_weights():\n+            with torch.device(\"meta\"):\n                 bias = getattr(module, \"bias\", None)\n                 new_module = torch.nn.Linear(module.in_features, module.out_features, bias=bias is not None)\n             state = module.state if quant_method == \"llm_int8\" else None"
        },
        {
            "sha": "640caf9ed5e446d59f726894ff5bef0da75870a7",
            "filename": "src/transformers/integrations/eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Feetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feetq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,15 +14,13 @@\n # limitations under the License.\n from ..core_model_loading import ConversionOps\n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n \n \n if is_torch_available():\n     import torch\n     import torch.nn as nn\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n \n logger = logging.get_logger(__name__)\n \n@@ -108,7 +106,7 @@ def replace_with_eetq_linear(model, modules_to_not_convert: list[str] | None = N\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 new_module = EetqLinear(\n                     module.in_features, module.out_features, bias=module.bias is not None, **module_kwargs"
        },
        {
            "sha": "cfd78b9d57d7e10d26c6806b9a8384f6d4efc86a",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -15,7 +15,7 @@\n \n from ..core_model_loading import ConversionOps\n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n+from ..utils import is_torch_accelerator_available, is_torch_available, logging\n \n \n if is_torch_available():\n@@ -25,9 +25,6 @@\n     import triton.language as tl\n     from torch.nn import functional as F\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n logger = logging.get_logger(__name__)\n try:\n@@ -614,7 +611,7 @@ def replace_with_fp8_linear(\n         # we need this to correctly materialize the weights during quantization\n         module_kwargs = {} if pre_quantized else {\"dtype\": None}\n         new_module = None\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if module_name.endswith(\".experts\"):\n                 new_module = FP8Expert(\n                     config=model.config, block_size=quantization_config.weight_block_size, **module_kwargs"
        },
        {
            "sha": "15ef1befa9d65e112aec53deebc350f7279afdb1",
            "filename": "src/transformers/integrations/higgs.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fhiggs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhiggs.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,12 +16,9 @@\n from math import sqrt\n \n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_flute_available, is_hadamard_available, is_torch_available, logging\n+from ..utils import is_flute_available, is_hadamard_available, is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n     import torch\n     import torch.nn as nn\n@@ -569,7 +566,7 @@ def replace_with_higgs_linear(model, modules_to_not_convert: list[str] | None =\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 new_module = HiggsLinear(\n                     module.in_features,"
        },
        {
            "sha": "a20a9dae9afc70cf2632be4a1988a46ad116da52",
            "filename": "src/transformers/integrations/mxfp4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fmxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmxfp4.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -12,22 +12,16 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ..utils import is_accelerate_available, is_torch_available, is_torch_xpu_available, logging\n+from ..utils import is_torch_available, is_torch_xpu_available, logging\n \n \n if is_torch_available():\n     import torch\n     from torch import nn\n+from contextlib import contextmanager\n from typing import Optional\n \n from ..core_model_loading import ConversionOps\n-\n-\n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n-from contextlib import contextmanager\n-\n from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n \n \n@@ -620,7 +614,7 @@ def replace_with_mxfp4_linear(model, quantization_config=None, modules_to_not_co\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n         if module.__class__.__name__ == \"GptOssExperts\" and not quantization_config.dequantize:\n-            with init_empty_weights():\n+            with torch.device(\"meta\"):\n                 model.set_submodule(module_name, Mxfp4GptOssExperts(model.config))\n                 has_been_replaced = True\n         if module.__class__.__name__ == \"GptOssMLP\" and not quantization_config.dequantize:"
        },
        {
            "sha": "595cf3901979b0f8651255b6fa89891ce8863060",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -73,7 +73,6 @@ def replace_with_quanto_layers(\n             A list of modules to not convert. If a module name is in the list (e.g. `lm_head`), it will not be\n             converted.\n     \"\"\"\n-    from accelerate import init_empty_weights\n     from optimum.quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n \n     w_mapping = {\"float8\": qfloat8, \"int8\": qint8, \"int4\": qint4, \"int2\": qint2}\n@@ -83,7 +82,7 @@ def replace_with_quanto_layers(\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             new_module = None\n             if isinstance(module, nn.Linear):\n                 new_module = QLinear("
        },
        {
            "sha": "d7b2ae7d3ab1b328755c469063a9284d78cb7e62",
            "filename": "src/transformers/integrations/spqr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fspqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fspqr.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,13 +14,11 @@\n \"SpQR (Sparse-Quantized Representation) integration file\"\n \n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_spqr_available, is_torch_available, logging\n+from ..utils import is_spqr_available, is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n+    import torch\n     import torch.nn as nn\n \n logger = logging.get_logger(__name__)\n@@ -47,7 +45,7 @@ def replace_with_spqr_linear(model, modules_to_not_convert: list[str] | None = N\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 shapes = quantization_config.shapes\n "
        },
        {
            "sha": "dc51d76f225c5f3a579ea9f99e4c0ae519f1cc47",
            "filename": "src/transformers/integrations/timm.py",
            "status": "added",
            "additions": 216,
            "deletions": 0,
            "changes": 216,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Ftimm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Ftimm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftimm.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -0,0 +1,216 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+This integration has for unique goal to allow re-initialization of the non-persistent buffers of `timm` models.\n+Indeed, as we load models fully on meta by default, we need a way to get back the correct value of non-persistent buffers.\n+We assume that everything else, i.e. parameters and persistent buffers, will correctly reside in model checkpoints, so we\n+won't need to reinit them.\n+Do not rely on it, as we will work to integrate it directly in `timm`, to then remove this file without warning.\n+\"\"\"\n+\n+from math import comb\n+\n+import torch\n+\n+from .. import initialization as init\n+from ..utils import is_timm_available\n+\n+\n+if is_timm_available():\n+    from timm.layers import ndgrid\n+    from timm.layers.blur_pool import BlurPool2d\n+    from timm.layers.lambda_layer import LambdaLayer, rel_pos_indices\n+    from timm.layers.pos_embed_rel import (\n+        RelPosBias,\n+        RelPosBiasTf,\n+        RelPosMlp,\n+        gen_relative_log_coords,\n+        gen_relative_position_index,\n+        generate_lookup_tensor,\n+    )\n+    from timm.layers.pos_embed_sincos import (\n+        FourierEmbed,\n+        RotaryEmbedding,\n+        RotaryEmbeddingCat,\n+        RotaryEmbeddingDinoV3,\n+        RotaryEmbeddingMixed,\n+        freq_bands,\n+        pixel_freq_bands,\n+    )\n+    from timm.models.beit import Attention\n+    from timm.models.beit import gen_relative_position_index as beit_gen_relative_position_index\n+    from timm.models.efficientformer_v2 import Attention2d, Attention2dDownsample\n+    from timm.models.eva import EvaAttention\n+    from timm.models.levit import AttentionDownsample\n+    from timm.models.swin_transformer import SwinTransformerBlock, get_relative_position_index\n+    from timm.models.swin_transformer import WindowAttention as SwinWindowAttention\n+    from timm.models.swin_transformer_v2 import SwinTransformerV2Block\n+    from timm.models.swin_transformer_v2 import WindowAttention as Swin2WindowAttention\n+    from timm.models.swin_transformer_v2_cr import SwinTransformerV2CrBlock, WindowMultiHeadAttention\n+    from timm.models.vision_transformer import ParallelScalingBlock\n+\n+    # This one is very recent and is not necesarily in all versions we support (we require timm>=1.0.20)\n+    try:\n+        from timm.models.csatv2 import _DCT_MEAN, _DCT_VAR, LearnableDct2d\n+    except Exception:\n+        _DCT_MEAN, _DCT_VAR, LearnableDct2d = None, None, type(None)\n+\n+\n+def _maybe_reinit_non_persistent_buffer(module):\n+    \"\"\"Reinit the non-persistent buffers of `module` if it matches any timm Module which has any.\"\"\"\n+    # This is a loooong list of hardcoded combinations from timm, as the modules do not provide a nice way to do\n+    # it natively\n+    if isinstance(module, FourierEmbed):\n+        init.copy_(module.bands, pixel_freq_bands(module.max_res, module.num_bands))\n+    elif isinstance(module, RotaryEmbedding):\n+        if module.bands is not None:\n+            bands = (\n+                pixel_freq_bands(module.dim // 4, float(module.max_res), linear_bands=module.linear_bands)\n+                if module.in_pixels\n+                else freq_bands(module.dim // 4, temperature=module.temperature, step=1)\n+            )\n+            init.copy_(module.bands, bands)\n+        elif module.pos_embed_sin is not None:\n+            emb_sin, emb_cos = module._get_pos_embed_values(module.feat_shape)\n+            init.copy_(module.pos_embed_sin, emb_sin)\n+            init.copy_(module.pos_embed_cos, emb_cos)\n+    elif isinstance(module, RotaryEmbeddingCat):\n+        if module.bands is not None:\n+            bands = (\n+                pixel_freq_bands(module.dim // 4, float(module.max_res), linear_bands=module.linear_bands)\n+                if module.in_pixels\n+                else freq_bands(module.dim // 4, temperature=module.temperature, step=1)\n+            )\n+            init.copy_(module.bands, bands)\n+        elif module.pos_embed is not None:\n+            init.copy_(module.pos_embed, module._get_pos_embed_values(feat_shape=module.feat_shape))\n+    elif isinstance(module, RotaryEmbeddingMixed):\n+        if module.t_x is not None:\n+            t_x, t_y = module._get_grid_values(module.feat_shape)\n+            init.copy_(module.t_x, t_x)\n+            init.copy(module.t_y, t_y)\n+    elif isinstance(module, RotaryEmbeddingDinoV3):\n+        init.copy_(module.periods, module._compute_periods())\n+        if module.pos_embed_cached is not None:\n+            init.copy_(module.pos_embed_cached, module._create_embed(module.feat_shape, no_aug=True))\n+    elif isinstance(module, RelPosBias):\n+        has_class_token = module.relative_position_bias_table.shape[0] > (2 * module.window_size[0] - 1) * (\n+            2 * module.window_size[1] - 1\n+        )\n+        init.copy_(\n+            module.relative_position_index,\n+            gen_relative_position_index(module.window_size, class_token=has_class_token).view(-1),\n+        )\n+    elif isinstance(module, RelPosMlp):\n+        init.copy_(module.relative_position_index, gen_relative_position_index(module.window_size).view(-1))\n+        # This one is supposed to pass args `pretrained_window_size` as well to `gen_relative_log_coords`, but it's\n+        # not recorded as class attributes in `__init__` and we have no way to infer its value back as we do for `mode` here...\n+        # Let's hope it's always default value\n+        mode = \"cr\" if module.bias_gain is None else \"swin\"\n+        init.copy_(module.rel_coords_log, gen_relative_log_coords(module.window_size, mode=mode))\n+    elif isinstance(module, RelPosBiasTf):\n+        init.copy_(module.height_lookup, generate_lookup_tensor(module.window_size[0]))\n+        init.copy_(module.width_lookup, generate_lookup_tensor(module.window_size[1]))\n+    elif isinstance(module, LearnableDct2d):\n+        init.copy_(module.mean, torch.tensor(_DCT_MEAN))\n+        init.copy_(module.var, torch.tensor(_DCT_VAR))\n+        init.copy_(module.imagenet_mean, torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1))\n+        init.copy_(module.imagenet_std, torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1))\n+    elif isinstance(module, LambdaLayer):\n+        if module.rel_pos_indices is not None:\n+            rel_size = module.pos_enb.shape[:2]\n+            feat_size = [(s + 1) // 2 for s in rel_size]\n+            init.copy_(module.rel_pos_indices, rel_pos_indices(feat_size))\n+    elif isinstance(module, AttentionDownsample):\n+        k_pos = torch.stack(\n+            ndgrid(\n+                torch.arange(module.resolution[0], dtype=torch.long),\n+                torch.arange(module.resolution[1], dtype=torch.long),\n+            )\n+        ).flatten(1)\n+        q_pos = torch.stack(\n+            ndgrid(\n+                torch.arange(0, module.resolution[0], step=module.stride, dtype=torch.long),\n+                torch.arange(0, module.resolution[1], step=module.stride, dtype=torch.long),\n+            )\n+        ).flatten(1)\n+        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n+        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n+        init.copy_(module.attention_bias_idxs, rel_pos)\n+    elif isinstance(\n+        module,\n+        EvaAttention,\n+    ):\n+        if module.k_bias is not None:\n+            init.zeros_(module.k_bias)\n+    elif isinstance(module, ParallelScalingBlock):\n+        if module.qkv_bias is not None:\n+            init.zeros_(module.qkv_bias)\n+    elif isinstance(module, Attention):\n+        if module.k_bias is not None:\n+            init.zeros_(module.k_bias)\n+        if module.relative_position_index is not None:\n+            init.copy_(module.relative_position_index, beit_gen_relative_position_index(module.window_size))\n+    elif isinstance(module, SwinTransformerV2CrBlock):\n+        if module.attn_mask is not None:\n+            init.copy_(module.attn_mask, module.get_attn_mask())\n+    elif isinstance(module, WindowMultiHeadAttention):\n+        module._make_pair_wise_relative_positions()\n+    elif isinstance(module, BlurPool2d):\n+        coeffs = torch.tensor(\n+            [comb(module.filt_size - 1, k) for k in range(module.filt_size)], dtype=torch.float32\n+        ) / (2 ** (module.filt_size - 1))\n+        blur_filter = (coeffs[:, None] * coeffs[None, :])[None, None, :, :]\n+        if module.channels is not None:\n+            blur_filter = blur_filter.repeat(module.channels, 1, 1, 1)\n+        init.copy_(module.filt, blur_filter)\n+    elif isinstance(module, Swin2WindowAttention):\n+        module._make_pair_wise_relative_positions()\n+        if module.k_bias is not None:\n+            init.zeros_(module.k_bias)\n+    elif isinstance(module, SwinTransformerV2Block):\n+        if module.attn_mask is not None:\n+            init.copy_(module.attn_mask, module.get_attn_mask())\n+    elif isinstance(module, SwinWindowAttention):\n+        init.copy_(module.relative_position_index, get_relative_position_index(*module.window_size))\n+    elif isinstance(module, SwinTransformerBlock):\n+        if module.attn_mask is not None:\n+            init.copy_(module.attn_mask, module.get_attn_mask())\n+    elif isinstance(module, Attention2d):\n+        pos = torch.stack(\n+            ndgrid(\n+                torch.arange(module.resolution[0], dtype=torch.long),\n+                torch.arange(module.resolution[1], dtype=torch.long),\n+            )\n+        ).flatten(1)\n+        rel_pos = (pos[..., :, None] - pos[..., None, :]).abs()\n+        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n+        init.copy_(module.attention_bias_idxs, rel_pos)\n+    elif isinstance(module, Attention2dDownsample):\n+        k_pos = torch.stack(\n+            ndgrid(\n+                torch.arange(module.resolution[0], dtype=torch.long),\n+                torch.arange(module.resolution[1], dtype=torch.long),\n+            )\n+        ).flatten(1)\n+        q_pos = torch.stack(\n+            ndgrid(\n+                torch.arange(0, module.resolution[0], step=2, dtype=torch.long),\n+                torch.arange(0, module.resolution[1], step=2, dtype=torch.long),\n+            )\n+        ).flatten(1)\n+        rel_pos = (q_pos[..., :, None] - k_pos[..., None, :]).abs()\n+        rel_pos = (rel_pos[0] * module.resolution[1]) + rel_pos[1]\n+        init.copy_(module.attention_bias_idxs, rel_pos)"
        },
        {
            "sha": "941da476e3737b9fbef1eb617f5ad683910c90e3",
            "filename": "src/transformers/integrations/vptq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fintegrations%2Fvptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fvptq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,13 +14,11 @@\n \"VPTQ (Vector Post-Training Quantization) integration file\"\n \n from ..quantizers.quantizers_utils import should_convert_module\n-from ..utils import is_accelerate_available, is_torch_available, logging\n+from ..utils import is_torch_available, logging\n \n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n if is_torch_available():\n+    import torch\n     import torch.nn as nn\n \n logger = logging.get_logger(__name__)\n@@ -48,7 +46,7 @@ def replace_with_vptq_linear(model, modules_to_not_convert: list[str] | None = N\n     for module_name, module in model.named_modules():\n         if not should_convert_module(module_name, modules_to_not_convert):\n             continue\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             if isinstance(module, nn.Linear):\n                 layer_params = config_for_layers.get(module_name, None) or shared_layer_config.get(\n                     module_name.rsplit(\".\")[1], None"
        },
        {
            "sha": "c072c562fef4289b37acfb3f7644b3181d1992f6",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 28,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -25,7 +25,7 @@\n import warnings\n from abc import abstractmethod\n from collections import defaultdict\n-from collections.abc import Callable, Sequence\n+from collections.abc import Callable, Iterator, Sequence\n from contextlib import contextmanager\n from enum import Enum\n from functools import partial, wraps\n@@ -62,7 +62,6 @@\n     accelerate_dispatch,\n     check_and_set_device_map,\n     expand_device_map,\n-    init_empty_weights,\n     load_offloaded_parameter,\n )\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n@@ -497,10 +496,13 @@ def remove_tied_weights_from_state_dict(\n \n \n def _load_parameter_into_model(model: \"PreTrainedModel\", param_name: str, tensor: torch.Tensor):\n-    \"\"\"Cast a single parameter `param_name` into the `model`, with value `tensor`.\"\"\"\n-    module, param_type = get_module_from_name(model, param_name)\n-    # This will check potential shape mismatch if skipped before\n-    module.load_state_dict({param_type: tensor}, strict=False, assign=True)\n+    \"\"\"Cast a single parameter or buffer `param_name` into the `model`, with value `tensor`.\"\"\"\n+    parent, param_type = get_module_from_name(model, param_name)\n+    if param_type in parent._parameters and not isinstance(tensor, nn.Parameter):\n+        tensor = nn.Parameter(tensor, requires_grad=tensor.is_floating_point())\n+    # We need to use setattr here, as we set non-persistent buffers as well with this function (`load_state_dict`\n+    # does not allow to do it)\n+    setattr(parent, param_type, tensor)\n \n \n def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n@@ -2253,9 +2255,7 @@ def _initialize_weights(self, module):\n             return\n \n         self._init_weights(module)\n-        # If we are not currently under meta device (which would virtually skip `_init_weights`), mark as initialized\n-        if get_torch_context_manager_or_global_device() != torch.device(\"meta\"):\n-            module._is_hf_initialized = True\n+        module._is_hf_initialized = True\n \n     @torch.no_grad()\n     @init.guard_torch_init_functions()\n@@ -2987,11 +2987,12 @@ def init_weights(self):\n         Maybe initializes weights. If using a custom `PreTrainedModel`, you need to implement any\n         initialization logic in `_init_weights`.\n         \"\"\"\n-        if _init_weights:\n-            # Initialize weights\n-            self.initialize_weights()\n-            # Tie weights needs to be called here, but it can use the pre-computed `all_tied_weights_keys`\n-            self.tie_weights(recompute_mapping=False)\n+        if not _init_weights or get_torch_context_manager_or_global_device() == torch.device(\"meta\"):\n+            return\n+        # Initialize weights\n+        self.initialize_weights()\n+        # Tie weights needs to be called here, but it can use the pre-computed `all_tied_weights_keys`\n+        self.tie_weights(recompute_mapping=False)\n \n     def gradient_checkpointing_enable(self, gradient_checkpointing_kwargs=None):\n         \"\"\"\n@@ -3532,15 +3533,16 @@ def get_init_context(cls, dtype: torch.dtype, is_quantized: bool, _is_ds_init_ca\n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n \n-            init_contexts.append(no_init_weights())\n             # We cannot initialize the model on meta device with deepspeed when not quantized\n             if not is_quantized and not _is_ds_init_called:\n                 logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n-                init_contexts.extend([deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()])\n+                init_contexts.extend(\n+                    [no_init_weights(), deepspeed.zero.Init(config_dict_or_path=deepspeed_config()), set_zero3_state()]\n+                )\n             elif is_quantized:\n-                init_contexts.extend([init_empty_weights(), set_quantized_state()])\n+                init_contexts.extend([torch.device(\"meta\"), set_quantized_state()])\n         else:\n-            init_contexts.extend([no_init_weights(), init_empty_weights()])\n+            init_contexts.append(torch.device(\"meta\"))\n \n         return init_contexts\n \n@@ -4154,7 +4156,7 @@ def _load_pretrained_model(\n         # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n         # loading the weights as they are not in the loaded state dict)\n         miss_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n-        model._move_missing_keys_from_meta_to_cpu(miss_and_mismatched, dtype, hf_quantizer)\n+        model._move_missing_keys_from_meta_to_cpu(miss_and_mismatched, hf_quantizer)\n \n         # Correctly initialize the missing (and potentially mismatched) keys (all parameters without the `_is_hf_initialzed` flag)\n         model._initialize_missing_keys(is_quantized)\n@@ -4391,7 +4393,7 @@ def is_backend_compatible(cls):\n         return cls._supports_attention_backend\n \n     def _move_missing_keys_from_meta_to_cpu(\n-        self, missing_keys: list[str], dtype: torch.dtype, hf_quantizer: Optional[HfQuantizer]\n+        self, missing_keys: list[str], hf_quantizer: Optional[HfQuantizer]\n     ) -> None:\n         \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n         from meta device to cpu.\n@@ -4400,22 +4402,25 @@ def _move_missing_keys_from_meta_to_cpu(\n \n         # In this case we need to move everything back\n         if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n-            # We only do it for the parameters, as the buffers are not initialized on the meta device by default\n             for key, param in self.named_parameters():\n-                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n+                value = torch.empty_like(param, device=\"cpu\")\n+                _load_parameter_into_model(self, key, value)\n+            for key, buffer in self.named_buffers():\n+                value = torch.empty_like(buffer, device=\"cpu\")\n                 _load_parameter_into_model(self, key, value)\n             return\n \n-        model_state_dict = self.state_dict()\n         # The tied weight keys are in the \"missing\" usually, but they should not be moved (they will be tied anyway)\n         # This is especially important because if they are moved, they will lose the `_is_hf_initialized` flag, and they\n         # will be re-initialized for nothing (which can be quite long)\n         for key in missing_keys - self.all_tied_weights_keys.keys():\n-            param = model_state_dict[key]\n-            # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n-            if param.device == torch.device(\"meta\"):\n-                value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n-                _load_parameter_into_model(self, key, value)\n+            param = self.get_parameter_or_buffer(key)\n+            value = torch.empty_like(param, device=\"cpu\")\n+            _load_parameter_into_model(self, key, value)\n+        # We need to move back non-persistent buffers as well, as they are not part of loaded weights anyway\n+        for key, buffer in self.named_non_persistent_buffers():\n+            value = torch.empty_like(buffer, device=\"cpu\")\n+            _load_parameter_into_model(self, key, value)\n \n     def _initialize_missing_keys(self, is_quantized: bool) -> None:\n         \"\"\"\n@@ -4503,6 +4508,19 @@ def get_parameter_or_buffer(self, target: str):\n \n         raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")\n \n+    def named_non_persistent_buffers(\n+        self, recurse: bool = True, remove_duplicate: bool = True\n+    ) -> Iterator[tuple[str, torch.Tensor]]:\n+        \"\"\"Similar to `named_buffers`, but only yield non-persistent ones. It is handy as it's not perfectly straightforward\n+        to know if they are persistent or not\"\"\"\n+        for name, tensor in self.named_buffers(recurse=recurse, remove_duplicate=remove_duplicate):\n+            # We have to grab the parent here, as the attribute `_non_persistent_buffers_set` is on the immediate\n+            # parent only\n+            parent, buf_name = name.rsplit(\".\", 1) if \".\" in name else (\"\", name)\n+            parent = self.get_submodule(parent)\n+            if buf_name in parent._non_persistent_buffers_set:\n+                yield name, tensor\n+\n     def train(self, mode: bool = True):\n         out = super().train(mode)\n         if self.use_kernels:"
        },
        {
            "sha": "69277f17212b73d89e54362940b728ed484cdc4a",
            "filename": "src/transformers/models/chameleon/convert_chameleon_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fconvert_chameleon_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -19,7 +19,6 @@\n import requests\n import torch\n import yaml\n-from accelerate import init_empty_weights\n from PIL import Image\n \n from transformers import (\n@@ -373,7 +372,7 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         vq_config=vq_config,\n         vocabulary_map=vocabulary_map,\n     )\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = ChameleonForConditionalGeneration(config)\n \n     model.load_state_dict(state_dict, assign=True, strict=False)"
        },
        {
            "sha": "d968d22ab37519a0f57698e9e7ada67e5aaf364e",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,11 +14,13 @@\n # limitations under the License.\n \"\"\"PyTorch CodeGen model.\"\"\"\n \n+import math\n from typing import Optional, Union\n \n import torch\n from torch import nn\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -69,7 +71,7 @@ class CodeGenAttention(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n \n-        max_positions = config.max_position_embeddings\n+        self.max_positions = config.max_position_embeddings\n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n         self.layer_idx = layer_idx\n@@ -88,13 +90,15 @@ def __init__(self, config, layer_idx=None):\n                 f\"embed_dim must be divisible by num_attention_heads (got `embed_dim`: {self.embed_dim} and\"\n                 f\" `num_attention_heads`: {self.num_attention_heads}).\"\n             )\n-        self.scale_attn = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(torch.get_default_dtype())\n+        self.scale_attn = math.sqrt(self.head_dim)\n         self.qkv_proj = nn.Linear(self.embed_dim, self.embed_dim * 3, bias=False)\n \n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n         self.rotary_dim = config.rotary_dim\n-        pos_embd_dim = self.rotary_dim or self.embed_dim\n-        self.embed_positions = create_sinusoidal_positions(max_positions, pos_embd_dim)\n+        self.pos_embd_dim = self.rotary_dim or self.embed_dim\n+        self.register_buffer(\n+            \"embed_positions\", create_sinusoidal_positions(self.max_positions, self.pos_embd_dim), persistent=False\n+        )\n \n     def _split_heads(self, x, n_head, dim_head, mp_num):\n         reshaped = x.reshape(x.shape[:-1] + (n_head // mp_num, dim_head))\n@@ -279,6 +283,11 @@ class CodeGenPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, CodeGenAttention):\n+            init.copy_(module.embed_positions, create_sinusoidal_positions(module.max_positions, module.pos_embd_dim))\n+\n \n @auto_docstring\n class CodeGenModel(CodeGenPreTrainedModel):"
        },
        {
            "sha": "ab803a30716d3c210e4007195e6d5e00b57aa838",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 1,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutput\n@@ -187,6 +188,13 @@ class CTRLPreTrainedModel(PreTrainedModel):\n     config: CTRLConfig\n     base_model_prefix = \"transformer\"\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, CTRLModel):\n+            init.copy_(\n+                module.pos_encoding, positional_encoding(module.config.n_positions, module.d_model_size, torch.float)\n+            )\n+\n \n @auto_docstring\n class CTRLModel(CTRLPreTrainedModel):\n@@ -196,7 +204,9 @@ def __init__(self, config):\n         self.d_model_size = config.n_embd\n         self.num_layers = config.n_layer\n \n-        self.pos_encoding = positional_encoding(config.n_positions, self.d_model_size, torch.float)\n+        self.register_buffer(\n+            \"pos_encoding\", positional_encoding(config.n_positions, self.d_model_size, torch.float), persistent=False\n+        )\n \n         self.w = nn.Embedding(config.vocab_size, config.n_embd)\n "
        },
        {
            "sha": "350c75c03769efb168519a184eace1aded3e6edf",
            "filename": "src/transformers/models/deepseek_vl/convert_deepseek_vl_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fconvert_deepseek_vl_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -20,7 +20,6 @@\n \n import regex as re\n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import snapshot_download\n from huggingface_hub.errors import HFValidationError\n from safetensors.torch import load_file\n@@ -285,7 +284,7 @@ def convert_model(\n     # ------------------------------------------------------------\n \n     print(\"Creating empty model...\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = DeepseekVLForConditionalGeneration(config)\n \n     # Load and convert state dict"
        },
        {
            "sha": "cb58bda10d7e306bafe58c1d70c0c0c0f793e698",
            "filename": "src/transformers/models/deepseek_vl_hybrid/convert_deepseek_vl_hybrid_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fconvert_deepseek_vl_hybrid_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -20,7 +20,6 @@\n \n import regex as re\n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import snapshot_download\n from huggingface_hub.errors import HFValidationError\n from safetensors.torch import load_file\n@@ -323,7 +322,7 @@ def convert_model(\n     # ------------------------------------------------------------\n \n     print(\"Creating empty model...\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = DeepseekVLHybridForConditionalGeneration(config)\n \n     # Load and convert state dict"
        },
        {
            "sha": "60f323d8e79c6fc8aa62a4ecb16975d9cf925bee",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -19,7 +19,6 @@\n \n import requests\n import torch\n-from accelerate import init_empty_weights\n from PIL import Image\n \n from transformers import (\n@@ -288,7 +287,7 @@ def convert_model(vq_model_id, llm_model_id, output_dir, hub_model_id=None, test\n     )\n     config = Emu3Config(text_config=text_config, vocabulary_map=vocabulary_map)\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = Emu3ForConditionalGeneration(config=config)\n         model.generation_config = GenerationConfig(\n             do_sample=True,"
        },
        {
            "sha": "548c28327a80b73730f3b0fcabd92e622a74a9bf",
            "filename": "src/transformers/models/eomt/convert_eomt_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fconvert_eomt_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -21,7 +21,6 @@\n from typing import Optional\n \n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import snapshot_download\n \n from transformers import EomtConfig, EomtForUniversalSegmentation, EomtImageProcessorFast\n@@ -255,7 +254,7 @@ def convert_model(\n \n     # Initialize model with empty weights\n     print(\"Creating empty model...\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = EomtForUniversalSegmentation(config)\n \n     # Load and convert state dict"
        },
        {
            "sha": "97ce5d12d86d8c2cd848ef7343fffbe53e99b66f",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 10,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -727,19 +727,20 @@ def __init__(self, config: FastSpeech2ConformerConfig, module_config):\n         self.embed_dim = config.hidden_size\n         self.input_scale = math.sqrt(self.embed_dim)\n         self.dropout = nn.Dropout(p=module_config[\"positional_dropout_rate\"])\n-        self.pos_enc = None\n         self.max_len = 5000\n-        self.extend_pos_enc(torch.tensor(0.0).expand(1, self.max_len))\n+        self.register_buffer(\n+            \"pos_enc\", self.extend_pos_enc(torch.tensor(0.0).expand(1, self.max_len)), persistent=False\n+        )\n \n-    def extend_pos_enc(self, x):\n+    def extend_pos_enc(self, x, pos_enc=None):\n         \"\"\"Reset the positional encodings.\"\"\"\n-        if self.pos_enc is not None:\n+        if pos_enc is not None:\n             # self.pos_enc contains both positive and negative parts\n             # the length of self.pos_enc is 2 * input_len - 1\n-            if self.pos_enc.size(1) >= x.size(1) * 2 - 1:\n-                if self.pos_enc.dtype != x.dtype or self.pos_enc.device != x.device:\n-                    self.pos_enc = self.pos_enc.to(dtype=x.dtype, device=x.device)\n-                return\n+            if pos_enc.size(1) >= x.size(1) * 2 - 1:\n+                if pos_enc.dtype != x.dtype or pos_enc.device != x.device:\n+                    pos_enc = pos_enc.to(dtype=x.dtype, device=x.device)\n+                return pos_enc\n         # Suppose `i` means to the position of query vector and `j` means the\n         # position of key vector. We use position relative positions when keys\n         # are to the left (i>j) and negative relative positions otherwise (i<j).\n@@ -760,7 +761,7 @@ def extend_pos_enc(self, x):\n         pos_enc_positive = torch.flip(pos_enc_positive, [0]).unsqueeze(0)\n         pos_enc_negative = pos_enc_negative[1:].unsqueeze(0)\n         pos_enc = torch.cat([pos_enc_positive, pos_enc_negative], dim=1)\n-        self.pos_enc = pos_enc.to(device=x.device, dtype=x.dtype)\n+        return pos_enc.to(device=x.device, dtype=x.dtype)\n \n     def forward(self, feature_representation):\n         \"\"\"\n@@ -771,7 +772,7 @@ def forward(self, feature_representation):\n         Returns:\n             `torch.Tensor`: Encoded tensor (batch_size, time, `*`).\n         \"\"\"\n-        self.extend_pos_enc(feature_representation)\n+        self.pos_enc = self.extend_pos_enc(feature_representation, self.pos_enc)\n         hidden_states = feature_representation * self.input_scale\n         center_idx = self.pos_enc.size(1) // 2\n         pos_emb = self.pos_enc[:, center_idx - hidden_states.size(1) + 1 : center_idx + hidden_states.size(1)]\n@@ -1022,6 +1023,8 @@ def _init_weights(self, module):\n         elif isinstance(module, FastSpeech2ConformerAttention):\n             init.xavier_uniform_(module.pos_bias_u)\n             init.xavier_uniform_(module.pos_bias_v)\n+        elif isinstance(module, FastSpeech2ConformerRelPositionalEncoding):\n+            init.copy_(module.pos_enc, module.extend_pos_enc(torch.tensor(0.0).expand(1, module.max_len)))\n \n     def _set_gradient_checkpointing(self, module, value=False):\n         if isinstance(module, FastSpeech2ConformerEncoder):"
        },
        {
            "sha": "8d6698bf430f88e3dcbd9875c5f6b9deb9348a19",
            "filename": "src/transformers/models/gemma/convert_gemma_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconvert_gemma_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,7 +16,6 @@\n import warnings\n \n import torch\n-from accelerate import init_empty_weights\n \n from transformers import GemmaConfig, GemmaForCausalLM, GemmaTokenizer\n \n@@ -110,7 +109,7 @@ def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=tor\n     torch.set_default_dtype(dtype)\n \n     print(\"Loading the checkpoint in a Gemma model.\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = GemmaForCausalLM(config)\n     model.load_state_dict(state_dict, assign=True, strict=False)\n "
        },
        {
            "sha": "d2abd1bc31b7507097051ed151a95f1f7a4d555c",
            "filename": "src/transformers/models/gemma2/convert_gemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconvert_gemma2_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,7 +16,6 @@\n import warnings\n \n import torch\n-from accelerate import init_empty_weights\n \n from transformers import Gemma2Config, Gemma2ForCausalLM, GemmaTokenizer\n \n@@ -143,7 +142,7 @@ def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=tor\n     torch.set_default_dtype(dtype)\n \n     print(\"Loading the checkpoint in a Gemma2 model.\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = Gemma2ForCausalLM(config)\n     model.load_state_dict(state_dict, assign=True, strict=False)\n "
        },
        {
            "sha": "f5f2fd58413c55fc8dff633abf9c6510ed281f5e",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 13,
            "deletions": 4,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -14,12 +14,14 @@\n # limitations under the License.\n \"\"\"PyTorch GPT-J model.\"\"\"\n \n+import math\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n+from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n@@ -77,7 +79,7 @@ class GPTJAttention(nn.Module):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         self.config = config\n-        max_positions = config.max_position_embeddings\n+        self.max_positions = config.max_position_embeddings\n \n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n@@ -99,15 +101,17 @@ def __init__(self, config, layer_idx=None):\n                 f\"embed_dim must be divisible by num_attention_heads (got `embed_dim`: {self.embed_dim} and\"\n                 f\" `num_attention_heads`: {self.num_attention_heads}).\"\n             )\n-        self.scale_attn = torch.sqrt(torch.tensor(self.head_dim, dtype=torch.float32)).to(torch.get_default_dtype())\n+        self.scale_attn = math.sqrt(self.head_dim)\n \n         self.k_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n         self.v_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n         self.q_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n         self.out_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=False)\n         self.rotary_dim = config.rotary_dim\n-        pos_embd_dim = self.rotary_dim or self.embed_dim\n-        self.embed_positions = create_sinusoidal_positions(max_positions, pos_embd_dim)\n+        self.pos_embd_dim = self.rotary_dim or self.embed_dim\n+        self.register_buffer(\n+            \"embed_positions\", create_sinusoidal_positions(self.max_positions, self.pos_embd_dim), persistent=False\n+        )\n \n     def _split_heads(self, tensor, num_attention_heads, attn_head_size, rotary):\n         \"\"\"\n@@ -444,6 +448,11 @@ class GPTJPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _can_compile_fullgraph = True\n \n+    def _init_weights(self, module):\n+        super()._init_weights(module)\n+        if isinstance(module, GPTJAttention):\n+            init.copy_(module.embed_positions, create_sinusoidal_positions(module.max_positions, module.pos_embd_dim))\n+\n \n @auto_docstring\n class GPTJModel(GPTJPreTrainedModel):"
        },
        {
            "sha": "1aee7410a62c35e57f996411eb5a461945d6de84",
            "filename": "src/transformers/models/idefics2/convert_idefics2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconvert_idefics2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconvert_idefics2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fconvert_idefics2_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n \n import torch\n-from accelerate import init_empty_weights\n \n from transformers import (\n     AutoConfig,\n@@ -146,7 +145,7 @@ def convert_idefics2_hub_to_hf(original_model_id, output_hub_path, push_to_hub):\n \n     config = get_config(original_model_id)\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = Idefics2ForConditionalGeneration(config)\n \n     model.load_state_dict(state_dict, strict=True, assign=True)"
        },
        {
            "sha": "382a1d238abc3719afe4dfbc84d4a80864ca145b",
            "filename": "src/transformers/models/idefics3/convert_idefics3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconvert_idefics3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconvert_idefics3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fconvert_idefics3_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,7 +16,6 @@\n import json\n \n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download\n \n from transformers import (\n@@ -175,7 +174,7 @@ def convert_idefics3_hub_to_hf(original_model_id, output_hub_path, push_to_hub):\n     config = get_config(original_model_id)\n     print(config)\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = Idefics3ForConditionalGeneration(config)\n \n     model.load_state_dict(new_state_dict, strict=True, assign=True)"
        },
        {
            "sha": "2fcf0d25c0c0854e6124b53ace2e6d0f8ddb4c62",
            "filename": "src/transformers/models/janus/convert_janus_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fconvert_janus_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -28,7 +28,6 @@\n from typing import Optional\n \n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import snapshot_download\n \n from transformers import (\n@@ -403,7 +402,7 @@ def convert_model(\n \n     # Initialize model with empty weights\n     print(\"Creating empty model...\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = JanusForConditionalGeneration(config)\n \n     model.generation_config._from_model_config = False"
        },
        {
            "sha": "5f06dc6c3220fb4846136df41cf6ecda7dabe4a3",
            "filename": "src/transformers/models/layoutlmv3/modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fmodeling_layoutlmv3.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -212,6 +212,8 @@ def _init_weights(self, module):\n             if self.config.visual_embed:\n                 init.zeros_(module.cls_token)\n                 init.zeros_(module.pos_embed)\n+            if hasattr(module, \"visual_bbox\"):\n+                init.copy_(module.visual_bbox, module.create_visual_bbox(image_size=(module.size, module.size)))\n         elif isinstance(module, LayoutLMv3TextEmbeddings):\n             init.copy_(module.position_ids, torch.arange(module.position_ids.shape[-1]).expand((1, -1)))\n \n@@ -578,16 +580,18 @@ def __init__(self, config):\n             # when the input_size is larger in fine-tuning, we will interpolate the position embeddings in forward\n             self.patch_embed = LayoutLMv3PatchEmbeddings(config)\n \n-            size = int(config.input_size / config.patch_size)\n+            self.size = int(config.input_size / config.patch_size)\n             self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n-            self.pos_embed = nn.Parameter(torch.zeros(1, size * size + 1, config.hidden_size))\n+            self.pos_embed = nn.Parameter(torch.zeros(1, self.size * self.size + 1, config.hidden_size))\n             self.pos_drop = nn.Dropout(p=0.0)\n \n             self.LayerNorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n             self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n             if self.config.has_relative_attention_bias or self.config.has_spatial_attention_bias:\n-                self.init_visual_bbox(image_size=(size, size))\n+                self.register_buffer(\n+                    \"visual_bbox\", self.create_visual_bbox(image_size=(self.size, self.size)), persistent=False\n+                )\n \n             self.norm = nn.LayerNorm(config.hidden_size, eps=1e-6)\n \n@@ -601,7 +605,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def init_visual_bbox(self, image_size=(14, 14), max_len=1000):\n+    def create_visual_bbox(self, image_size=(14, 14), max_len=1000):\n         \"\"\"\n         Create the bounding boxes for the visual (patch) tokens.\n         \"\"\"\n@@ -622,7 +626,7 @@ def init_visual_bbox(self, image_size=(14, 14), max_len=1000):\n         ).view(-1, 4)\n \n         cls_token_box = torch.tensor([[0 + 1, 0 + 1, max_len - 1, max_len - 1]])\n-        self.visual_bbox = torch.cat([cls_token_box, visual_bbox], dim=0)\n+        return torch.cat([cls_token_box, visual_bbox], dim=0)\n \n     def calculate_visual_bbox(self, device, dtype, batch_size):\n         visual_bbox = self.visual_bbox.repeat(batch_size, 1, 1)"
        },
        {
            "sha": "d63df140a3d0936f0f37e2a35fe481080e406043",
            "filename": "src/transformers/models/llava_next/convert_llava_next_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fconvert_llava_next_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -31,7 +31,6 @@\n \n import requests\n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download, snapshot_download\n from PIL import Image\n from safetensors import safe_open\n@@ -145,7 +144,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         image_token_id=image_token_id,\n     )\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = LlavaNextForConditionalGeneration(config)\n \n     # load original state dict"
        },
        {
            "sha": "7b348f6f42234ea65c2a35ca7351a33de175a0d2",
            "filename": "src/transformers/models/llava_next_video/convert_llava_next_video_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconvert_llava_next_video_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -23,7 +23,6 @@\n from pathlib import Path\n \n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download, snapshot_download\n from safetensors import safe_open\n \n@@ -203,7 +202,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         image_token_id=image_token_id,\n     )\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = LlavaNextVideoForConditionalGeneration(config)\n \n     # load original state dict"
        },
        {
            "sha": "fde31a6ec2503c5ed6ff8be6dcb5d91a8ecdd836",
            "filename": "src/transformers/models/llava_onevision/convert_llava_onevision_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fconvert_llava_onevision_weights_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -26,7 +26,6 @@\n \n import requests\n import torch\n-from accelerate import init_empty_weights\n from huggingface_hub import hf_hub_download, snapshot_download\n from PIL import Image\n from safetensors import safe_open\n@@ -153,7 +152,7 @@ def convert_llava_to_hf(model_id, pytorch_dump_folder_path, push_to_hub=False):\n         use_image_newline_parameter=True,\n     )\n \n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = LlavaOnevisionForConditionalGeneration(config)\n \n     # load original state dict"
        },
        {
            "sha": "b5831c919afc7b798e2b33a22b156e539fc926fe",
            "filename": "src/transformers/models/owlv2/modeling_owlv2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodeling_owlv2.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch OWLv2 model.\"\"\"\n \n from dataclasses import dataclass\n-from functools import lru_cache\n from typing import Any, Optional, Union\n \n import torch\n@@ -603,6 +602,8 @@ def _init_weights(self, module: nn.Module):\n                 std=module.vision_embed_dim**-0.5 * factor,\n             )\n             init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n+        elif isinstance(module, Owlv2ForObjectDetection):\n+            init.copy_(module.box_bias, module.compute_box_bias(module.num_patches_height, module.num_patches_width))\n         if isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n@@ -1224,7 +1225,9 @@ def __init__(self, config: Owlv2Config):\n         self.config = config\n         self.num_patches_height = self.config.vision_config.image_size // self.config.vision_config.patch_size\n         self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n-        self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)\n+        self.register_buffer(\n+            \"box_bias\", self.compute_box_bias(self.num_patches_height, self.num_patches_width), persistent=False\n+        )\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -1261,7 +1264,6 @@ def objectness_predictor(self, image_features: torch.FloatTensor) -> torch.Float\n         objectness_logits = objectness_logits[..., 0]\n         return objectness_logits\n \n-    @lru_cache(maxsize=2)\n     # Copied from transformers.models.owlvit.modeling_owlvit.OwlViTForObjectDetection.compute_box_bias\n     def compute_box_bias(\n         self, num_patches_height: int, num_patches_width: int, feature_map: Optional[torch.FloatTensor] = None"
        },
        {
            "sha": "559e1ff534bf212559ede37e056ee62dcd22c70c",
            "filename": "src/transformers/models/owlvit/modeling_owlvit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fmodeling_owlvit.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch OWL-ViT model.\"\"\"\n \n from dataclasses import dataclass\n-from functools import lru_cache\n from typing import Any, Optional, Union\n \n import torch\n@@ -590,6 +589,8 @@ def _init_weights(self, module: nn.Module):\n                 std=module.vision_embed_dim**-0.5 * factor,\n             )\n             init.constant_(module.logit_scale, self.config.logit_scale_init_value)\n+        elif isinstance(module, OwlViTForObjectDetection):\n+            init.copy_(module.box_bias, module.compute_box_bias(module.num_patches_height, module.num_patches_width))\n         if isinstance(module, nn.LayerNorm):\n             init.zeros_(module.bias)\n             init.ones_(module.weight)\n@@ -1202,7 +1203,9 @@ def __init__(self, config: OwlViTConfig):\n         self.config = config\n         self.num_patches_height = self.config.vision_config.image_size // self.config.vision_config.patch_size\n         self.num_patches_width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n-        self.box_bias = self.compute_box_bias(self.num_patches_height, self.num_patches_width)\n+        self.register_buffer(\n+            \"box_bias\", self.compute_box_bias(self.num_patches_height, self.num_patches_width), persistent=False\n+        )\n \n         self.post_init()\n \n@@ -1223,7 +1226,6 @@ def normalize_grid_corner_coordinates(num_patches_height: int, num_patches_width\n \n         return box_coordinates\n \n-    @lru_cache(maxsize=2)\n     def compute_box_bias(\n         self, num_patches_height: int, num_patches_width: int, feature_map: Optional[torch.FloatTensor] = None\n     ) -> torch.Tensor:"
        },
        {
            "sha": "084fa9b5da79c6adb761445981f9d3fe0540291c",
            "filename": "src/transformers/models/recurrent_gemma/convert_recurrent_gemma_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fconvert_recurrent_gemma_to_hf.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -16,7 +16,6 @@\n import warnings\n \n import torch\n-from accelerate import init_empty_weights\n \n from transformers import GemmaTokenizer, RecurrentGemmaConfig, RecurrentGemmaForCausalLM\n \n@@ -127,7 +126,7 @@ def write_model(save_path, input_base_path, config, push_to_hub=False, dtype=tor\n     torch.set_default_dtype(dtype)\n \n     print(\"Loading the checkpoint in a Gemma model.\")\n-    with init_empty_weights():\n+    with torch.device(\"meta\"):\n         model = RecurrentGemmaForCausalLM(config)\n     model.load_state_dict(state_dict, assign=True, strict=True)\n "
        },
        {
            "sha": "4ac05951ac2bd607f7ae89893b53353c11e0480e",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -287,18 +287,17 @@ def __init__(self, config):\n         super().__init__()\n         self.max_len = config.max_source_positions\n         self.d_model = config.hidden_size\n-        self.pe = None\n-        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n+        self.register_buffer(\"pe\", self.extend_pe(torch.tensor(0.0).expand(1, self.max_len)), persistent=False)\n \n-    def extend_pe(self, x):\n+    def extend_pe(self, x, pe=None):\n         # Reset the positional encodings\n-        if self.pe is not None:\n+        if pe is not None:\n             # self.pe contains both positive and negative parts\n             # the length of self.pe is 2 * input_len - 1\n-            if self.pe.size(1) >= x.size(1) * 2 - 1:\n-                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n-                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n-                return\n+            if pe.size(1) >= x.size(1) * 2 - 1:\n+                if pe.dtype != x.dtype or pe.device != x.device:\n+                    pe = pe.to(dtype=x.dtype, device=x.device)\n+                return pe\n         # Suppose `i` is the position of query vector and `j` is the\n         # position of key vector. We use positive relative positions when keys\n         # are to the left (i>j) and negative relative positions otherwise (i<j).\n@@ -319,10 +318,10 @@ def extend_pe(self, x):\n         pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n         pe_negative = pe_negative[1:].unsqueeze(0)\n         pe = torch.cat([pe_positive, pe_negative], dim=1)\n-        self.pe = pe.to(device=x.device, dtype=x.dtype)\n+        return pe.to(device=x.device, dtype=x.dtype)\n \n     def forward(self, hidden_states: torch.Tensor):\n-        self.extend_pe(hidden_states)\n+        self.pe = self.extend_pe(hidden_states, self.pe)\n         start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n         end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n         relative_position_embeddings = self.pe[:, start_idx:end_idx]\n@@ -1395,6 +1394,8 @@ def _init_weights(self, module: nn.Module):\n             base = self.config.rotary_embedding_base\n             inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n             init.copy_(module.inv_freq, inv_freq)\n+        elif isinstance(module, SeamlessM4TConformerRelPositionalEmbedding):\n+            init.copy_(module.pe, module.extend_pe(torch.tensor(0.0).expand(1, module.max_len)))\n \n     def _compute_sub_sample_lengths_from_attention_mask(self, attention_mask):\n         kernel_size, stride = self.config.adaptor_kernel_size, self.config.adaptor_stride"
        },
        {
            "sha": "7de7f092b2852cc700e9c904c990dffd80ada572",
            "filename": "src/transformers/models/timm_backbone/modeling_timm_backbone.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_backbone%2Fmodeling_timm_backbone.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -27,6 +27,8 @@\n if is_timm_available():\n     import timm\n \n+    from ...integrations.timm import _maybe_reinit_non_persistent_buffer\n+\n \n if is_torch_available():\n     from torch import Tensor\n@@ -89,7 +91,6 @@ def __init__(self, config, **kwargs):\n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n         requires_backends(cls, [\"vision\", \"timm\"])\n-        from ...models.timm_backbone import TimmBackboneConfig\n \n         config = kwargs.pop(\"config\", TimmBackboneConfig())\n \n@@ -118,9 +119,9 @@ def unfreeze_batch_norm_2d(self):\n \n     @torch.no_grad()\n     def _init_weights(self, module):\n-        \"\"\"\n-        Empty init weights function to ensure compatibility of the class in the library.\n-        \"\"\"\n+        \"\"\"We need to at least re-init the non-persistent buffers if the model was initialized on meta device (we\n+        assume weights and persistent buffers will be part of checkpoint as we have no way to control timm inits)\"\"\"\n+        _maybe_reinit_non_persistent_buffer(module)\n \n     def forward(\n         self,"
        },
        {
            "sha": "d72de953b32f806374941462f6212efcc72776bb",
            "filename": "src/transformers/models/timm_wrapper/modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fmodeling_timm_wrapper.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -29,6 +29,8 @@\n if is_timm_available():\n     import timm\n \n+    from ...integrations.timm import _maybe_reinit_non_persistent_buffer\n+\n \n @dataclass\n @auto_docstring(\n@@ -109,10 +111,12 @@ def _init_weights(self, module):\n         Since model architectures may vary, we assume only the classifier requires\n         initialization, while all other weights should be loaded from the checkpoint.\n         \"\"\"\n-        if isinstance(module, (nn.Linear)):\n+        if isinstance(module, nn.Linear):\n             init.normal_(module.weight, mean=0.0, std=self.config.initializer_range)\n             if module.bias is not None:\n                 init.zeros_(module.bias)\n+        # Also, reinit all non-persistemt buffers if any!\n+        _maybe_reinit_non_persistent_buffer(module)\n \n     def _timm_model_supports_gradient_checkpointing(self):\n         \"\"\""
        },
        {
            "sha": "7d5c1a9720a56104d7780fbbd169c2e051877e51",
            "filename": "src/transformers/models/wav2vec2_bert/modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodeling_wav2vec2_bert.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -74,18 +74,17 @@ def __init__(self, config):\n         super().__init__()\n         self.max_len = config.max_source_positions\n         self.d_model = config.hidden_size\n-        self.pe = None\n-        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n+        self.register_buffer(\"pe\", self.extend_pe(torch.tensor(0.0).expand(1, self.max_len)), persistent=False)\n \n-    def extend_pe(self, x):\n+    def extend_pe(self, x, pe=None):\n         # Reset the positional encodings\n-        if self.pe is not None:\n+        if pe is not None:\n             # self.pe contains both positive and negative parts\n             # the length of self.pe is 2 * input_len - 1\n-            if self.pe.size(1) >= x.size(1) * 2 - 1:\n-                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n-                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n-                return\n+            if pe.size(1) >= x.size(1) * 2 - 1:\n+                if pe.dtype != x.dtype or pe.device != x.device:\n+                    pe = pe.to(dtype=x.dtype, device=x.device)\n+                return pe\n         # Suppose `i` is the position of query vector and `j` is the\n         # position of key vector. We use positive relative positions when keys\n         # are to the left (i>j) and negative relative positions otherwise (i<j).\n@@ -106,10 +105,10 @@ def extend_pe(self, x):\n         pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n         pe_negative = pe_negative[1:].unsqueeze(0)\n         pe = torch.cat([pe_positive, pe_negative], dim=1)\n-        self.pe = pe.to(device=x.device, dtype=x.dtype)\n+        return pe.to(device=x.device, dtype=x.dtype)\n \n     def forward(self, hidden_states: torch.Tensor):\n-        self.extend_pe(hidden_states)\n+        self.pe = self.extend_pe(hidden_states, self.pe)\n         start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n         end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n         relative_position_embeddings = self.pe[:, start_idx:end_idx]\n@@ -754,6 +753,8 @@ def _init_weights(self, module):\n             base = self.config.rotary_embedding_base\n             inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n             init.copy_(module.inv_freq, inv_freq)\n+        elif isinstance(module, Wav2Vec2BertRelPositionalEmbedding):\n+            init.copy_(module.pe, module.extend_pe(torch.tensor(0.0).expand(1, module.max_len)))\n \n     # Ignore copy\n     def _get_feat_extract_output_lengths("
        },
        {
            "sha": "a0a98ec35089cc2911f4c612374232eee168eec6",
            "filename": "src/transformers/models/wav2vec2_bert/modular_wav2vec2_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fmodular_wav2vec2_bert.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -626,6 +626,8 @@ def _init_weights(self, module):\n             base = self.config.rotary_embedding_base\n             inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n             init.copy_(module.inv_freq, inv_freq)\n+        elif isinstance(module, Wav2Vec2BertRelPositionalEmbedding):\n+            init.copy_(module.pe, module.extend_pe(torch.tensor(0.0).expand(1, module.max_len)))\n \n     # Ignore copy\n     def _get_feat_extract_output_lengths("
        },
        {
            "sha": "cca7ac82778926d50ae467ec0efc23450df5aab4",
            "filename": "src/transformers/models/wav2vec2_conformer/modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodeling_wav2vec2_conformer.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -164,18 +164,17 @@ def __init__(self, config):\n         super().__init__()\n         self.max_len = config.max_source_positions\n         self.d_model = config.hidden_size\n-        self.pe = None\n-        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n+        self.register_buffer(\"pe\", self.extend_pe(torch.tensor(0.0).expand(1, self.max_len)), persistent=False)\n \n-    def extend_pe(self, x):\n+    def extend_pe(self, x, pe=None):\n         # Reset the positional encodings\n-        if self.pe is not None:\n+        if pe is not None:\n             # self.pe contains both positive and negative parts\n             # the length of self.pe is 2 * input_len - 1\n-            if self.pe.size(1) >= x.size(1) * 2 - 1:\n-                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n-                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n-                return\n+            if pe.size(1) >= x.size(1) * 2 - 1:\n+                if pe.dtype != x.dtype or pe.device != x.device:\n+                    pe = pe.to(dtype=x.dtype, device=x.device)\n+                return pe\n         # Suppose `i` is the position of query vector and `j` is the\n         # position of key vector. We use positive relative positions when keys\n         # are to the left (i>j) and negative relative positions otherwise (i<j).\n@@ -196,10 +195,10 @@ def extend_pe(self, x):\n         pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n         pe_negative = pe_negative[1:].unsqueeze(0)\n         pe = torch.cat([pe_positive, pe_negative], dim=1)\n-        self.pe = pe.to(device=x.device, dtype=x.dtype)\n+        return pe.to(device=x.device, dtype=x.dtype)\n \n     def forward(self, hidden_states: torch.Tensor):\n-        self.extend_pe(hidden_states)\n+        self.pe = self.extend_pe(hidden_states, self.pe)\n         start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n         end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n         relative_position_embeddings = self.pe[:, start_idx:end_idx]\n@@ -903,6 +902,8 @@ def _init_weights(self, module):\n             base = self.config.rotary_embedding_base\n             inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n             init.copy_(module.inv_freq, inv_freq)\n+        elif isinstance(module, Wav2Vec2ConformerRelPositionalEmbedding):\n+            init.copy_(module.pe, module.extend_pe(torch.tensor(0.0).expand(1, module.max_len)))\n \n     def _get_feat_extract_output_lengths(\n         self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None"
        },
        {
            "sha": "64acaf620796c03dd74b4f3b7e5b96c9eeb344a0",
            "filename": "src/transformers/models/wav2vec2_conformer/modular_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 11,
            "deletions": 10,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_conformer%2Fmodular_wav2vec2_conformer.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -116,18 +116,17 @@ def __init__(self, config):\n         super().__init__()\n         self.max_len = config.max_source_positions\n         self.d_model = config.hidden_size\n-        self.pe = None\n-        self.extend_pe(torch.tensor(0.0).expand(1, self.max_len))\n+        self.register_buffer(\"pe\", self.extend_pe(torch.tensor(0.0).expand(1, self.max_len)), persistent=False)\n \n-    def extend_pe(self, x):\n+    def extend_pe(self, x, pe=None):\n         # Reset the positional encodings\n-        if self.pe is not None:\n+        if pe is not None:\n             # self.pe contains both positive and negative parts\n             # the length of self.pe is 2 * input_len - 1\n-            if self.pe.size(1) >= x.size(1) * 2 - 1:\n-                if self.pe.dtype != x.dtype or self.pe.device != x.device:\n-                    self.pe = self.pe.to(dtype=x.dtype, device=x.device)\n-                return\n+            if pe.size(1) >= x.size(1) * 2 - 1:\n+                if pe.dtype != x.dtype or pe.device != x.device:\n+                    pe = pe.to(dtype=x.dtype, device=x.device)\n+                return pe\n         # Suppose `i` is the position of query vector and `j` is the\n         # position of key vector. We use positive relative positions when keys\n         # are to the left (i>j) and negative relative positions otherwise (i<j).\n@@ -148,10 +147,10 @@ def extend_pe(self, x):\n         pe_positive = torch.flip(pe_positive, [0]).unsqueeze(0)\n         pe_negative = pe_negative[1:].unsqueeze(0)\n         pe = torch.cat([pe_positive, pe_negative], dim=1)\n-        self.pe = pe.to(device=x.device, dtype=x.dtype)\n+        return pe.to(device=x.device, dtype=x.dtype)\n \n     def forward(self, hidden_states: torch.Tensor):\n-        self.extend_pe(hidden_states)\n+        self.pe = self.extend_pe(hidden_states, self.pe)\n         start_idx = self.pe.size(1) // 2 - hidden_states.size(1) + 1\n         end_idx = self.pe.size(1) // 2 + hidden_states.size(1)\n         relative_position_embeddings = self.pe[:, start_idx:end_idx]\n@@ -602,6 +601,8 @@ def _init_weights(self, module):\n             base = self.config.rotary_embedding_base\n             inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float() / dim))\n             init.copy_(module.inv_freq, inv_freq)\n+        elif isinstance(module, Wav2Vec2ConformerRelPositionalEmbedding):\n+            init.copy_(module.pe, module.extend_pe(torch.tensor(0.0).expand(1, module.max_len)))\n \n     def _get_feat_extract_output_lengths(\n         self, input_lengths: Union[torch.LongTensor, int], add_adapter: Optional[bool] = None"
        },
        {
            "sha": "81aa971750c5b2811996239e372a8e69b7e12109",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -260,15 +260,13 @@ def is_serializable(self): ...\n     def is_trainable(self): ...\n \n     def _convert_model_for_quantization(self, model):\n-        from accelerate import init_empty_weights\n-\n         for name, module in model.named_modules():\n             module_class_name = module.__class__.__name__\n             if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION and (\n                 self.quantization_config.quant_method\n                 in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n             ):\n-                with init_empty_weights():\n+                with torch.device(\"meta\"):\n                     parent_module, name = get_module_from_name(model, name)\n                     parent_module._modules[name] = MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"module_name\"](\n                         model.config.get_text_config()"
        },
        {
            "sha": "5075745552e0d3d141609313c0f1ad27d41b057b",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -31,15 +31,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_aqlm_available, is_torch_available\n+from transformers.utils import is_aqlm_available, is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_accelerator\n class AqlmConfigTest(unittest.TestCase):\n@@ -112,7 +109,7 @@ def test_quantized_model_conversion(self):\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n         quantization_config = AqlmConfig()\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -129,7 +126,7 @@ def test_quantized_model_conversion(self):\n         self.assertEqual(nb_linears, nb_aqlm_linear)\n \n         # Try with `linear_weights_not_to_quantize`\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         model, _ = replace_with_aqlm_linear("
        },
        {
            "sha": "bb5bb72fe0a0b55905ffd607d979d8109bcbbca5",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -27,16 +27,13 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n from transformers.utils.quantization_config import AwqBackend\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_accelerator\n class AwqConfigTest(unittest.TestCase):\n@@ -154,7 +151,7 @@ def test_quantized_model_conversion(self):\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n         quantization_config = AwqConfig(bits=4)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -171,7 +168,7 @@ def test_quantized_model_conversion(self):\n         self.assertEqual(nb_linears, nb_awq_linear)\n \n         # Try with `modules_not_to_convert`\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         model, _ = replace_with_awq_linear("
        },
        {
            "sha": "aa4e2de9cf977c1f813869f5e332d01a6726bf40",
            "filename": "tests/quantization/bitnet_integration/test_bitnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbitnet_integration%2Ftest_bitnet.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -29,15 +29,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_accelerator\n class BitNetQuantConfigTest(unittest.TestCase):\n@@ -80,7 +77,7 @@ def test_replace_with_bitlinear(self):\n         model_id = \"facebook/opt-350m\"\n         config = AutoConfig.from_pretrained(model_id)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0"
        },
        {
            "sha": "90e99417fe967ce22ccf556f6a764fa151663096",
            "filename": "tests/quantization/bnb/test_mixed_int8.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_mixed_int8.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -148,21 +148,20 @@ def test_get_keys_to_not_convert(self):\n         r\"\"\"\n         Test the `get_keys_to_not_convert` function.\n         \"\"\"\n-        from accelerate import init_empty_weights\n \n         from transformers import AutoModelForMaskedLM, Blip2ForConditionalGeneration, MptForCausalLM, OPTForCausalLM\n         from transformers.quantizers.base import get_keys_to_not_convert\n \n         model_id = \"mosaicml/mpt-7b\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"72e5f594ce36f9cabfa2a9fd8f58b491eb467ee7\")\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = MptForCausalLM(config)\n         # The order of the keys does not matter, so we sort them before comparing, same for the other tests.\n         self.assertEqual(get_keys_to_not_convert(model).sort(), [\"lm_head\", \"transformer.wte\"].sort())\n \n         model_id = \"Salesforce/blip2-opt-2.7b\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"1ef7f63a8f0a144c13fdca8103eb7b4691c74cec\")\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = Blip2ForConditionalGeneration(config)\n         self.assertEqual(\n             get_keys_to_not_convert(model).sort(),\n@@ -171,13 +170,13 @@ def test_get_keys_to_not_convert(self):\n \n         model_id = \"facebook/opt-350m\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n         self.assertEqual(get_keys_to_not_convert(model).sort(), [\"lm_head\", \"model.decoder.embed_tokens\"].sort())\n \n         model_id = \"FacebookAI/roberta-large\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"716877d372b884cad6d419d828bac6c85b3b18d9\")\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = AutoModelForMaskedLM.from_config(config)\n         self.assertEqual(\n             get_keys_to_not_convert(model).sort(),"
        },
        {
            "sha": "7188ec64fd4275272b9ab0da3f37e06b648e1828",
            "filename": "tests/quantization/eetq_integration/test_eetq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -26,15 +26,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_gpu\n class EetqConfigTest(unittest.TestCase):\n@@ -101,7 +98,7 @@ def test_quantized_model_conversion(self):\n         model_id = \"facebook/opt-350m\"\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -118,7 +115,7 @@ def test_quantized_model_conversion(self):\n         self.assertEqual(nb_linears, nb_eetq_linear)\n \n         # Try with `modules_to_not_convert`\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n         model = replace_with_eetq_linear(model, modules_to_not_convert=[\"fc1\"])\n         nb_eetq_linear = 0"
        },
        {
            "sha": "ff03624467cb3636ea7b134d46bed795ab2df192",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -153,7 +153,7 @@ def test_quantized_model_conversion(self):\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n         quantization_config = FbgemmFp8Config()\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -169,7 +169,7 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears, nb_fbgemm_linear)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n         quantization_config = FbgemmFp8Config(modules_to_not_convert=[\"fc1\"])\n         model = replace_with_fbgemm_fp8_linear("
        },
        {
            "sha": "48bd079092f3678d9b360a0a1080c865ba4db078",
            "filename": "tests/quantization/finegrained_fp8/test_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffinegrained_fp8%2Ftest_fp8.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -30,15 +30,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @contextmanager\n def _patch_no_accelerator():\n@@ -145,7 +142,7 @@ def test_quantized_model_conversion(self):\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n         quantization_config = FineGrainedFP8Config()\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -158,7 +155,7 @@ def test_quantized_model_conversion(self):\n             if isinstance(module, FP8Linear):\n                 nb_fp8_linear += 1\n         self.assertEqual(nb_linears, nb_fp8_linear)\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n         quantization_config = FineGrainedFP8Config()\n         model = replace_with_fp8_linear(model, modules_to_not_convert=[\"fc1\"], quantization_config=quantization_config)"
        },
        {
            "sha": "12bf6bde1d2738382bcdc690f294613ecdc9eace",
            "filename": "tests/quantization/higgs/test_higgs.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fhiggs%2Ftest_higgs.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -26,15 +26,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_gpu\n class HiggsConfigTest(unittest.TestCase):\n@@ -102,7 +99,7 @@ def test_quantized_model_conversion(self):\n         config = AutoConfig.from_pretrained(model_id, revision=\"cb32f77e905cccbca1d970436fb0f5e6b58ee3c5\")\n         quantization_config = HiggsConfig()\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n \n         nb_linears = 0\n@@ -118,7 +115,7 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears - 1, nb_higgs_linear)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = OPTForCausalLM(config)\n         quantization_config = HiggsConfig(modules_to_not_convert=[\"fc1\"])\n         model, _ = replace_with_higgs_linear(model, quantization_config=quantization_config)"
        },
        {
            "sha": "8864ffdf9876638caa1517206b8e30198af8ef71",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -24,14 +24,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_optimum_quanto_available, is_torch_available\n+from transformers.utils import is_optimum_quanto_available, is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n \n if is_optimum_quanto_available():\n     from optimum.quanto import QLayerNorm, QLinear\n@@ -46,7 +44,7 @@ class QuantoTestIntegration(unittest.TestCase):\n \n     def setUp(self):\n         config = AutoConfig.from_pretrained(self.model_id)\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             self.model = AutoModelForCausalLM.from_config(config)\n         self.nb_linear = 0\n         self.nb_layernorm = 0"
        },
        {
            "sha": "11b60b78924040f64fb4fa82e2c641ef606754f5",
            "filename": "tests/quantization/spqr_integration/test_spqr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fspqr_integration%2Ftest_spqr.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -28,15 +28,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n @require_torch_gpu\n class SpQRConfigTest(unittest.TestCase):\n@@ -115,7 +112,7 @@ def test_quantized_model_conversion(self):\n         quantization_config = AutoConfig.from_pretrained(self.model_name, return_dict=False).quantization_config\n         quantization_config = SpQRConfig.from_dict(quantization_config)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = AutoModelForCausalLM.from_pretrained(pretrained_model_name_or_path=model_id, config=config)\n \n         nb_linears = 0"
        },
        {
            "sha": "604dc0e6bf6573328ddddc8323751de1c988f91a",
            "filename": "tests/quantization/vptq_integration/test_vptq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fvptq_integration%2Ftest_vptq.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -26,15 +26,12 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n-if is_accelerate_available():\n-    from accelerate import init_empty_weights\n-\n \n class VptqConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n@@ -162,7 +159,7 @@ def test_quantized_model_conversion(self):\n         layer_configs[\"model.decoder.project_in\"] = value\n         quantization_config = VptqConfig(config_for_layers=layer_configs, shared_layer_config=shared_layer_config)\n \n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = AutoModelForCausalLM.from_config(config)\n \n         nb_linears = 0\n@@ -179,7 +176,7 @@ def test_quantized_model_conversion(self):\n         self.assertEqual(nb_linears - 1, nb_vptq_linear)\n \n         # Try with `linear_weights_not_to_quantize`\n-        with init_empty_weights():\n+        with torch.device(\"meta\"):\n             model = AutoModelForCausalLM.from_config(config)\n         quantization_config = VptqConfig(config_for_layers=layer_configs, shared_layer_config=shared_layer_config)\n         model, _ = replace_with_vptq_linear("
        },
        {
            "sha": "5ff6c7e09096f3bfb915799882fe4d7854254321",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb9357ffe89252437286478c98cbd302708cb9f6/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=bb9357ffe89252437286478c98cbd302708cb9f6",
            "patch": "@@ -1195,6 +1195,42 @@ def test_init_weights_can_init_buffers(self):\n                 f\"them correctly if the model is on meta device):\\n{unique_bad_module_traceback}\",\n             )\n \n+    def test_all_tensors_are_parameter_or_buffer(self):\n+        \"\"\"Check that all tensors are registered as Parameter or Buffer, i.e. we don't have simple assignments such\n+        as `self.x = torch.tensor(...)` in a Module (as we cannot correctly recover from meta device if it's not registered as\n+        parameter/buffer)\"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            # apparently this model cannot correctly create its inputs and has to use another function....\n+            if \"modeling_perceiver.py\" in inspect.getfile(model_class):\n+                _, inputs_dict = self.model_tester.prepare_config_and_inputs_for_model_class(model_class)\n+\n+            # Initialize the model fully on meta device, then move everything to cpu and run `init_weights`\n+            with torch.device(\"meta\"):\n+                model = model_class(copy.deepcopy(config)).eval()\n+            # move everything randomly to cpu\n+            model.to_empty(device=\"cpu\")\n+            # Now, run all the inits\n+            model.init_weights()\n+\n+            # Try running a forward, to see if a tensor stayed on meta somewhere\n+            try:\n+                _ = model(**self._prepare_for_class(inputs_dict, model_class))\n+            except (RuntimeError, NotImplementedError) as e:\n+                # Re-raise a more friendly exception (unfortunately, we cannot know which tensor it was...)\n+                if \"Cannot copy out of meta tensor; no data!\" in str(\n+                    e\n+                ) or \"Tensor on device meta is not on the expected device cpu!\" in str(e):\n+                    raise ValueError(\n+                        \"A tensor is still on meta device. It means it was not properly registered as a Parameter or \"\n+                        \"Buffer.\\nMost of the time, it should be added as a non-persistent buffer if you don't want to include \"\n+                        \"it in the model's state dict. It can also be a scalar that was added as a torch.Tensor, consider making it \"\n+                        \"a Python scalar in this case and use it as such in forward\"\n+                    ) from e\n+                else:\n+                    raise e\n+\n     def test_torch_save_load(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         if config.__class__ not in MODEL_MAPPING:"
        }
    ],
    "stats": {
        "total": 846,
        "additions": 493,
        "deletions": 353
    }
}