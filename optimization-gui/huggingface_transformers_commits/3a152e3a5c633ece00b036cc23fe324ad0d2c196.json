{
    "author": "Cyrilvallez",
    "message": "Fix the check in flex test (#39548)\n\n* fix the check\n\n* fix flags\n\n* flags",
    "sha": "3a152e3a5c633ece00b036cc23fe324ad0d2c196",
    "files": [
        {
            "sha": "ad53d6a985385f135fcbd3c55248d9d89fb9d6b7",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -33,6 +33,9 @@ class ColPaliPreTrainedModel(PreTrainedModel):\n     config: ColPaliConfig\n     base_model_prefix = \"model\"\n     _no_split_modules = []\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = ("
        },
        {
            "sha": "684804ee377ac4184569968a32ae20829a8470f7",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -41,8 +41,9 @@ class ColQwen2PreTrainedModel(PreTrainedModel):\n     config: ColQwen2Config\n     base_model_prefix = \"model\"\n     _no_split_modules = []\n-    _supports_flash_attn = True\n     _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n \n     def _init_weights(self, module):\n         std = ("
        },
        {
            "sha": "0fcdc09b7b16289f05a0ba2ec1717b7a24502ea7",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -226,8 +226,7 @@ def __call__(\n \n \n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n+    pass\n \n \n @dataclass"
        },
        {
            "sha": "dc6f81945b37d10dd2a23fb0964fc5b12cd072ef",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -280,12 +280,11 @@ class GotOcr2PreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _skip_keys_device_placement = \"past_key_values\"\n-\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n+    _supports_flash_attn = False\n+    _supports_sdpa = False\n \n     _supports_static_cache = True\n-    _supports_flex_attn = True\n+    _supports_flex_attn = False\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):"
        },
        {
            "sha": "06b9fca29835ae3912086cf66a3a4c47c65ee8dc",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -286,6 +286,10 @@ class GotOcr2ModelOutputWithPast(LlavaModelOutputWithPast):\n \n \n class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n+    _supports_flash_attn = False\n+    _supports_sdpa = False\n+    _supports_flex_attn = False\n+\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n "
        },
        {
            "sha": "26f6a032d66a8ae619e4762771d68152ec1d7a84",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 11,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/3a152e3a5c633ece00b036cc23fe324ad0d2c196/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3a152e3a5c633ece00b036cc23fe324ad0d2c196/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=3a152e3a5c633ece00b036cc23fe324ad0d2c196",
            "patch": "@@ -4599,16 +4599,10 @@ def test_flex_attention_with_grads(self):\n             model = model_class(config).to(device=torch_device)\n \n             # If not all sub-models support flex, skip the test\n-            sub_models_supporting_flex = [\n-                module._supports_flex_attn\n-                for name, module in model.named_modules()\n-                if isinstance(module, PreTrainedModel) and name != \"\"\n-            ]\n-            supports_flex_all_modules = (all(sub_models_supporting_flex) and len(sub_models_supporting_flex) > 0) or (\n-                model._supports_flex_attn and len(sub_models_supporting_flex) == 0\n-            )\n-            if not supports_flex_all_modules:\n-                self.skipTest(reason=\"This model's submodels does not support flex attention\")\n+            if not all(\n+                submodel._supports_flex_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n+            ):\n+                self.skipTest(reason=\"At least some parts of this model do not support flex attention\")\n \n             def update_config_for_flex(config):\n                 # Flex Attention cannot use dropout\n@@ -4664,8 +4658,8 @@ def update_config_for_flex(config):\n                 sub_config = getattr(config, key)\n                 update_config_for_flex(sub_config)\n \n-            config._attn_implementation = \"flex_attention\"\n             model = model_class(config).to(device=torch_device)\n+            model.set_attn_implementation(\"flex_attention\")\n             self.assertTrue(model.config._attn_implementation == \"flex_attention\")\n \n             # Elaborate workaround for encoder-decoder models as some do not specify their main input"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 18,
        "deletions": 18
    }
}