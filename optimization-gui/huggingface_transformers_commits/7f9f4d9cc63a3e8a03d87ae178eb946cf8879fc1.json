{
    "author": "Cyrilvallez",
    "message": "Fix TP plans for MoE models (#42236)\n\n* start\n\n* more fixes",
    "sha": "7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
    "files": [
        {
            "sha": "101d699194fdea6a3e19d2356dce3ce190cebf3f",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -127,8 +127,9 @@ class DeepseekV2Config(PreTrainedConfig):\n         \"layers.*.self_attn.q_b_proj\": \"colwise\",\n         \"layers.*.self_attn.kv_b_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "17ab64923470beb5135aafd48e0fcc44e2ce0a95",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -142,8 +142,9 @@ class DeepseekV2Config(LlamaConfig):\n         \"layers.*.self_attn.q_b_proj\": \"colwise\",\n         \"layers.*.self_attn.kv_b_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.gate_up_proj\": \"colwise\",\n-        \"layers.*.mlp.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_colwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n \n     model_type = \"deepseek_v2\""
        },
        {
            "sha": "928a0e1fcf7a556b59037aeceb66f56bde78926c",
            "filename": "src/transformers/models/deepseek_v3/configuration_deepseek_v3.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fconfiguration_deepseek_v3.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -131,19 +131,16 @@ class DeepseekV3Config(PreTrainedConfig):\n \n     model_type = \"deepseek_v3\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n-    base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n-        \"layers.*.mlp.experts.*.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.experts.*\": \"local\",  # each expert is wrapped in a module list\n-        \"layers.*.mlp.shared_experts.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.shared_experts\": \"local\",\n-        \"layers.*.mlp.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp\": \"gather\",  # This is the only moment where results are gathered\n+    base_model_tp_plan = {\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "71393a7844ba4e24ca6a1c665b4ae81a22e1d352",
            "filename": "src/transformers/models/dots1/configuration_dots1.py",
            "status": "modified",
            "additions": 10,
            "deletions": 13,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fconfiguration_dots1.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -109,23 +109,20 @@ class Dots1Config(PreTrainedConfig):\n     model_type = \"dots1\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n-    base_model_tp_plan = {  # TODO: only replicate attention layers when > first_k_dense_replace\n+    base_model_tp_plan = {\n         \"layers.*.self_attn.q_proj\": \"colwise\",\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.experts.*\": \"local\",  # each expert is wrapped in a module list\n-        \"layers.*.mlp.shared_experts.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.shared_experts\": \"local\",\n-        \"layers.*.mlp.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp\": \"gather\",  # This is the only moment where results are gathered\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n \n     base_model_pp_plan = {"
        },
        {
            "sha": "66a299b04c00f853ba2122647a73f13cf9eb6e5f",
            "filename": "src/transformers/models/ernie4_5_moe/configuration_ernie4_5_moe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 15,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fconfiguration_ernie4_5_moe.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -122,21 +122,15 @@ class Ernie4_5_MoeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        # sequence parallel is pretty slow\n-        # \"norm.weight\": \"sequence_parallel\",\n-        # \"layers.*.input_layernorm.weight\": \"sequence_parallel\",\n-        # \"layers.*.post_attention_layernorm.weight\": \"sequence_parallel\",\n-        \"layers.*.mlp.shared_experts.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.shared_experts.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp.experts\": \"local\",\n-        \"layers.*.mlp.gate_proj\": \"local_colwise\",\n-        \"layers.*.mlp.up_proj\": \"local_colwise\",\n-        \"layers.*.mlp.down_proj\": \"local_rowwise\",\n-        \"layers.*.mlp\": \"gather\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n+        \"layers.*.mlp.shared_experts.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.up_proj\": \"colwise\",\n+        \"layers.*.mlp.shared_experts.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "635d398b46d97403064971bd0ce32331a7924c3a",
            "filename": "src/transformers/models/flex_olmo/configuration_flex_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fconfiguration_flex_olmo.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -115,9 +115,9 @@ class FlexOlmoConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "a25362a71f3587587af2f460286bdb63244d016b",
            "filename": "src/transformers/models/flex_olmo/modular_flex_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflex_olmo%2Fmodular_flex_olmo.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -125,9 +125,9 @@ class FlexOlmoConfig(OlmoeConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n         \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "aa1a16a95b373e51c2c844f0ea69e254cd747e3f",
            "filename": "src/transformers/models/glm4_moe/configuration_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fconfiguration_glm4_moe.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -121,9 +121,9 @@ class Glm4MoeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "0912f2289f2f135969c2830a62bdf71ea966f9b9",
            "filename": "src/transformers/models/glm4_moe/modular_glm4_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodular_glm4_moe.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -135,9 +135,9 @@ class Glm4MoeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "e99c2b8265c2b6d2e036aca0abb11e4a7a45aa30",
            "filename": "src/transformers/models/longcat_flash/configuration_longcat_flash.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fconfiguration_longcat_flash.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -129,9 +129,9 @@ class LongcatFlashConfig(PreTrainedConfig):\n         \"layers.*.mlps.*.gate_proj\": \"colwise\",\n         \"layers.*.mlps.*.up_proj\": \"colwise\",\n         \"layers.*.mlps.*.down_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n \n     base_model_pp_plan = {"
        },
        {
            "sha": "1e582de1bff86c8150dd9145c6bca5b8bed55748",
            "filename": "src/transformers/models/minimax/configuration_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fconfiguration_minimax.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -138,9 +138,9 @@ class MiniMaxConfig(PreTrainedConfig):\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.gate\": \"colwise_rep\",  # we need to replicate here to correctly route experts\n-        \"layers.*.mlp.experts.*.w1\": \"colwise\",\n-        \"layers.*.mlp.experts.*.w2\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.w3\": \"colwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "2f459f7709983b74c829c94fc27d47a562a6bf45",
            "filename": "src/transformers/models/minimax/modular_minimax.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodular_minimax.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -163,9 +163,9 @@ class MiniMaxConfig(PreTrainedConfig):\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.gate\": \"colwise_rep\",  # we need to replicate here to correctly route experts\n-        \"layers.*.mlp.experts.*.w1\": \"colwise\",\n-        \"layers.*.mlp.experts.*.w2\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.w3\": \"colwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "adc86a035bed9ae38dc2bce5ac72db1310959e04",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -115,16 +115,14 @@ class MixtralConfig(PreTrainedConfig):\n     model_type = \"mixtral\"\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     base_model_tp_plan = {\n-        \"layers.*.self_attn.q_proj\": \"local_colwise\",\n-        \"layers.*.self_attn.k_proj\": \"local_colwise\",\n-        \"layers.*.self_attn.v_proj\": \"local_colwise\",\n-        \"layers.*.self_attn.o_proj\": \"local_rowwise\",\n-        \"layers.*.self_attn\": \"gather\",\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n         \"layers.*.mlp.gate\": \"ep_router\",  # we need to replicate here to correctly route experts\n         \"layers.*.mlp.experts.gate_up_proj\": \"local_colwise\",\n         \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n         \"layers.*.mlp.experts\": \"gather\",\n-        # \"layers.*.mlp.experts.gate_up_proj\": \"local_packed_rowwise\" ? if you load from\n     }\n     base_model_pp_plan = {\n         \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),"
        },
        {
            "sha": "8bc756a172677f8927a0950e1ab04c05b166c001",
            "filename": "src/transformers/models/qwen3_moe/configuration_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fconfiguration_qwen3_moe.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -121,9 +121,9 @@ class Qwen3MoeConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        },
        {
            "sha": "83eb062cb6f8653ce3d0414e32172db03b732417",
            "filename": "src/transformers/models/qwen3_next/configuration_qwen3_next.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fconfiguration_qwen3_next.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -135,9 +135,9 @@ class Qwen3NextConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.shared_expert.gate_proj\": \"colwise\",\n         \"layers.*.mlp.shared_expert.up_proj\": \"colwise\",\n         \"layers.*.mlp.shared_expert.down_proj\": \"rowwise\","
        },
        {
            "sha": "c7746f420514aba3e60aaf1678b6696d00f2b018",
            "filename": "src/transformers/models/qwen3_omni_moe/configuration_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fconfiguration_qwen3_omni_moe.py?ref=7f9f4d9cc63a3e8a03d87ae178eb946cf8879fc1",
            "patch": "@@ -268,9 +268,9 @@ class Qwen3OmniMoeTextConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n@@ -712,9 +712,9 @@ class Qwen3OmniMoeTalkerTextConfig(PreTrainedConfig):\n         \"layers.*.self_attn.k_proj\": \"colwise\",\n         \"layers.*.self_attn.v_proj\": \"colwise\",\n         \"layers.*.self_attn.o_proj\": \"rowwise\",\n-        \"layers.*.mlp.experts.*.gate_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.up_proj\": \"colwise\",\n-        \"layers.*.mlp.experts.*.down_proj\": \"rowwise\",\n+        \"layers.*.mlp.experts.gate_up_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts.down_proj\": \"local_rowwise\",\n+        \"layers.*.mlp.experts\": \"gather\",\n         \"layers.*.mlp.gate_proj\": \"colwise\",\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\","
        }
    ],
    "stats": {
        "total": 156,
        "additions": 72,
        "deletions": 84
    }
}