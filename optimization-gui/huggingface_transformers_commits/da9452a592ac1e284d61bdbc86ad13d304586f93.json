{
    "author": "gante",
    "message": "[docs] delete more TF/Flax docs (#40289)\n\n* delete some TF docs\n\n* update documentation checks to ignore tf/flax\n\n* a few more removals\n\n* nit\n\n* Update utils/check_repo.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "da9452a592ac1e284d61bdbc86ad13d304586f93",
    "files": [
        {
            "sha": "ecd4e77fc5f78eb14818607f21dccdb647ccbf82",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -66,8 +66,6 @@ values. Here, for instance, it has two keys that are `sequences` and `scores`.\n We document here all output types.\n \n \n-### PyTorch\n-\n [[autodoc]] generation.GenerateDecoderOnlyOutput\n \n [[autodoc]] generation.GenerateEncoderDecoderOutput\n@@ -76,42 +74,12 @@ We document here all output types.\n \n [[autodoc]] generation.GenerateBeamEncoderDecoderOutput\n \n-### TensorFlow\n-\n-[[autodoc]] generation.TFGreedySearchEncoderDecoderOutput\n-\n-[[autodoc]] generation.TFGreedySearchDecoderOnlyOutput\n-\n-[[autodoc]] generation.TFSampleEncoderDecoderOutput\n-\n-[[autodoc]] generation.TFSampleDecoderOnlyOutput\n-\n-[[autodoc]] generation.TFBeamSearchEncoderDecoderOutput\n-\n-[[autodoc]] generation.TFBeamSearchDecoderOnlyOutput\n-\n-[[autodoc]] generation.TFBeamSampleEncoderDecoderOutput\n-\n-[[autodoc]] generation.TFBeamSampleDecoderOnlyOutput\n-\n-[[autodoc]] generation.TFContrastiveSearchEncoderDecoderOutput\n-\n-[[autodoc]] generation.TFContrastiveSearchDecoderOnlyOutput\n-\n-### FLAX\n-\n-[[autodoc]] generation.FlaxSampleOutput\n-\n-[[autodoc]] generation.FlaxGreedySearchOutput\n-\n-[[autodoc]] generation.FlaxBeamSearchOutput\n \n ## LogitsProcessor\n \n A [`LogitsProcessor`] can be used to modify the prediction scores of a language model head for\n generation.\n \n-### PyTorch\n \n [[autodoc]] AlternatingCodebooksLogitsProcessor\n     - __call__\n@@ -210,93 +178,6 @@ generation.\n     - __call__\n \n \n-### TensorFlow\n-\n-[[autodoc]] TFForcedBOSTokenLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFForcedEOSTokenLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFForceTokensLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFLogitsProcessorList\n-    - __call__\n-\n-[[autodoc]] TFLogitsWarper\n-    - __call__\n-\n-[[autodoc]] TFMinLengthLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFNoBadWordsLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFNoRepeatNGramLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFRepetitionPenaltyLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFSuppressTokensAtBeginLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFSuppressTokensLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] TFTemperatureLogitsWarper\n-    - __call__\n-\n-[[autodoc]] TFTopKLogitsWarper\n-    - __call__\n-\n-[[autodoc]] TFTopPLogitsWarper\n-    - __call__\n-\n-### FLAX\n-\n-[[autodoc]] FlaxForcedBOSTokenLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxForcedEOSTokenLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxForceTokensLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxLogitsProcessorList\n-    - __call__\n-\n-[[autodoc]] FlaxLogitsWarper\n-    - __call__\n-\n-[[autodoc]] FlaxMinLengthLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxSuppressTokensAtBeginLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxSuppressTokensLogitsProcessor\n-    - __call__\n-\n-[[autodoc]] FlaxTemperatureLogitsWarper\n-    - __call__\n-\n-[[autodoc]] FlaxTopKLogitsWarper\n-    - __call__\n-\n-[[autodoc]] FlaxTopPLogitsWarper\n-    - __call__\n-\n-[[autodoc]] FlaxWhisperTimeStampLogitsProcessor\n-    - __call__\n \n ## StoppingCriteria\n "
        },
        {
            "sha": "5d1e0cce918cf353d887bb25a96590a1aeab8af6",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -53,31 +53,3 @@ Most of those are only useful if you are studying the code of the models in the\n [[autodoc]] pytorch_utils.prune_conv1d_layer\n \n [[autodoc]] pytorch_utils.prune_linear_layer\n-\n-## TensorFlow custom layers\n-\n-[[autodoc]] modeling_tf_utils.TFConv1D\n-\n-[[autodoc]] modeling_tf_utils.TFSequenceSummary\n-\n-## TensorFlow loss functions\n-\n-[[autodoc]] modeling_tf_utils.TFCausalLanguageModelingLoss\n-\n-[[autodoc]] modeling_tf_utils.TFMaskedLanguageModelingLoss\n-\n-[[autodoc]] modeling_tf_utils.TFMultipleChoiceLoss\n-\n-[[autodoc]] modeling_tf_utils.TFQuestionAnsweringLoss\n-\n-[[autodoc]] modeling_tf_utils.TFSequenceClassificationLoss\n-\n-[[autodoc]] modeling_tf_utils.TFTokenClassificationLoss\n-\n-## TensorFlow Helper Functions\n-\n-[[autodoc]] modeling_tf_utils.get_initializer\n-\n-[[autodoc]] modeling_tf_utils.keras_serializable\n-\n-[[autodoc]] modeling_tf_utils.shape_list"
        },
        {
            "sha": "2941338375be9556d1f6493babdfa8a5bae9efa5",
            "filename": "docs/source/en/main_classes/data_collator.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fdata_collator.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -50,21 +50,18 @@ Examples of use can be found in the [example scripts](../examples) or [example n\n \n [[autodoc]] data.data_collator.DataCollatorForLanguageModeling\n     - numpy_mask_tokens\n-    - tf_mask_tokens\n     - torch_mask_tokens\n \n ## DataCollatorForWholeWordMask\n \n [[autodoc]] data.data_collator.DataCollatorForWholeWordMask\n     - numpy_mask_tokens\n-    - tf_mask_tokens\n     - torch_mask_tokens\n \n ## DataCollatorForPermutationLanguageModeling\n \n [[autodoc]] data.data_collator.DataCollatorForPermutationLanguageModeling\n     - numpy_mask_tokens\n-    - tf_mask_tokens\n     - torch_mask_tokens\n \n ## DataCollatorWithFlattening"
        },
        {
            "sha": "d7768a905ce07099c35640a2013bd8388765315c",
            "filename": "docs/source/en/main_classes/model.md",
            "status": "modified",
            "additions": 4,
            "deletions": 27,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fmodel.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -16,22 +16,15 @@ rendered properly in your Markdown viewer.\n \n # Models\n \n-The base classes [`PreTrainedModel`], [`TFPreTrainedModel`], and\n-[`FlaxPreTrainedModel`] implement the common methods for loading/saving a model either from a local\n-file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's AWS\n-S3 repository).\n+The base class [`PreTrainedModel`] implements the common methods for loading/saving a model either from a local\n+file or directory, or from a pretrained model configuration provided by the library (downloaded from HuggingFace's Hub).\n \n-[`PreTrainedModel`] and [`TFPreTrainedModel`] also implement a few methods which\n-are common among all the models to:\n+[`PreTrainedModel`] also implements a few methods which are common among all the models to:\n \n - resize the input token embeddings when new tokens are added to the vocabulary\n - prune the attention heads of the model.\n \n-The other methods that are common to each model are defined in [`~modeling_utils.ModuleUtilsMixin`]\n-(for the PyTorch models) and [`~modeling_tf_utils.TFModuleUtilsMixin`] (for the TensorFlow models) or\n-for text generation, [`~generation.GenerationMixin`] (for the PyTorch models),\n-[`~generation.TFGenerationMixin`] (for the TensorFlow models) and\n-[`~generation.FlaxGenerationMixin`] (for the Flax/JAX models).\n+The other methods that are common to each model are defined in [`~modeling_utils.ModuleUtilsMixin`] and [`~generation.GenerationMixin`].\n \n \n ## PreTrainedModel\n@@ -48,22 +41,6 @@ set this to `False`.\n \n [[autodoc]] modeling_utils.ModuleUtilsMixin\n \n-## TFPreTrainedModel\n-\n-[[autodoc]] TFPreTrainedModel\n-    - push_to_hub\n-    - all\n-\n-## TFModelUtilsMixin\n-\n-[[autodoc]] modeling_tf_utils.TFModelUtilsMixin\n-\n-## FlaxPreTrainedModel\n-\n-[[autodoc]] FlaxPreTrainedModel\n-    - push_to_hub\n-    - all\n-\n ## Pushing to the Hub\n \n [[autodoc]] utils.PushToHubMixin"
        },
        {
            "sha": "295f99e21d10489cf7a3b6a044a5869ced404c00",
            "filename": "docs/source/en/main_classes/output.md",
            "status": "modified",
            "additions": 0,
            "deletions": 132,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Foutput.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -187,135 +187,3 @@ documented on their corresponding model page.\n ## SampleTSPredictionOutput\n \n [[autodoc]] modeling_outputs.SampleTSPredictionOutput\n-\n-## TFBaseModelOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFBaseModelOutput\n-\n-## TFBaseModelOutputWithPooling\n-\n-[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPooling\n-\n-## TFBaseModelOutputWithPoolingAndCrossAttentions\n-\n-[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPoolingAndCrossAttentions\n-\n-## TFBaseModelOutputWithPast\n-\n-[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPast\n-\n-## TFBaseModelOutputWithPastAndCrossAttentions\n-\n-[[autodoc]] modeling_tf_outputs.TFBaseModelOutputWithPastAndCrossAttentions\n-\n-## TFSeq2SeqModelOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFSeq2SeqModelOutput\n-\n-## TFCausalLMOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFCausalLMOutput\n-\n-## TFCausalLMOutputWithCrossAttentions\n-\n-[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithCrossAttentions\n-\n-## TFCausalLMOutputWithPast\n-\n-[[autodoc]] modeling_tf_outputs.TFCausalLMOutputWithPast\n-\n-## TFMaskedLMOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFMaskedLMOutput\n-\n-## TFSeq2SeqLMOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFSeq2SeqLMOutput\n-\n-## TFNextSentencePredictorOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFNextSentencePredictorOutput\n-\n-## TFSequenceClassifierOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFSequenceClassifierOutput\n-\n-## TFSeq2SeqSequenceClassifierOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFSeq2SeqSequenceClassifierOutput\n-\n-## TFMultipleChoiceModelOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFMultipleChoiceModelOutput\n-\n-## TFTokenClassifierOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFTokenClassifierOutput\n-\n-## TFQuestionAnsweringModelOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFQuestionAnsweringModelOutput\n-\n-## TFSeq2SeqQuestionAnsweringModelOutput\n-\n-[[autodoc]] modeling_tf_outputs.TFSeq2SeqQuestionAnsweringModelOutput\n-\n-## FlaxBaseModelOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutput\n-\n-## FlaxBaseModelOutputWithPast\n-\n-[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPast\n-\n-## FlaxBaseModelOutputWithPooling\n-\n-[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPooling\n-\n-## FlaxBaseModelOutputWithPastAndCrossAttentions\n-\n-[[autodoc]] modeling_flax_outputs.FlaxBaseModelOutputWithPastAndCrossAttentions\n-\n-## FlaxSeq2SeqModelOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqModelOutput\n-\n-## FlaxCausalLMOutputWithCrossAttentions\n-\n-[[autodoc]] modeling_flax_outputs.FlaxCausalLMOutputWithCrossAttentions\n-\n-## FlaxMaskedLMOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxMaskedLMOutput\n-\n-## FlaxSeq2SeqLMOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqLMOutput\n-\n-## FlaxNextSentencePredictorOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxNextSentencePredictorOutput\n-\n-## FlaxSequenceClassifierOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxSequenceClassifierOutput\n-\n-## FlaxSeq2SeqSequenceClassifierOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqSequenceClassifierOutput\n-\n-## FlaxMultipleChoiceModelOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxMultipleChoiceModelOutput\n-\n-## FlaxTokenClassifierOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxTokenClassifierOutput\n-\n-## FlaxQuestionAnsweringModelOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxQuestionAnsweringModelOutput\n-\n-## FlaxSeq2SeqQuestionAnsweringModelOutput\n-\n-[[autodoc]] modeling_flax_outputs.FlaxSeq2SeqQuestionAnsweringModelOutput"
        },
        {
            "sha": "cb853f722e1dac68c6c07de69d73ccdcfbaf1b1d",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 16,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -19,12 +19,8 @@ rendered properly in your Markdown viewer.\n Each framework has a generate method for text generation implemented in their respective `GenerationMixin` class:\n \n - PyTorch [`~generation.GenerationMixin.generate`] is implemented in [`~generation.GenerationMixin`].\n-- TensorFlow [`~generation.TFGenerationMixin.generate`] is implemented in [`~generation.TFGenerationMixin`].\n-- Flax/JAX [`~generation.FlaxGenerationMixin.generate`] is implemented in [`~generation.FlaxGenerationMixin`].\n \n-Regardless of your framework of choice, you can parameterize the generate method with a [`~generation.GenerationConfig`]\n-class instance. Please refer to this class for the complete list of generation parameters, which control the behavior\n-of the generation method.\n+You can parameterize the generate method with a [`~generation.GenerationConfig`] class instance. Please refer to this class for the complete list of generation parameters, which control the behavior of the generation method.\n \n To learn how to inspect a model's generation configuration, what are the defaults, how to change the parameters ad hoc,\n and how to create and save a customized generation configuration, refer to the\n@@ -46,14 +42,3 @@ like token streaming.\n [[autodoc]] GenerationMixin\n \t- generate\n \t- compute_transition_scores\n-\n-## TFGenerationMixin\n-\n-[[autodoc]] TFGenerationMixin\n-\t- generate\n-\t- compute_transition_scores\n-\n-## FlaxGenerationMixin\n-\n-[[autodoc]] FlaxGenerationMixin\n-\t- generate"
        },
        {
            "sha": "497c6b0193114caabce7e14d569f11665a4d02c7",
            "filename": "docs/source/en/testing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftesting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftesting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftesting.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -502,7 +502,7 @@ Inside tests:\n ```python\n from transformers.testing_utils import get_gpu_count\n \n-n_gpu = get_gpu_count()  # works with torch and tf\n+n_gpu = get_gpu_count()\n ```\n \n ### Testing with a specific PyTorch backend or device"
        },
        {
            "sha": "ed992e8152d919dd4256e152efd1cdb45f58f570",
            "filename": "docs/source/en/training.md",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftraining.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -126,50 +126,6 @@ Finally, use [`~Trainer.push_to_hub`] to upload your model and tokenizer to the\n trainer.push_to_hub()\n ```\n \n-## TensorFlow\n-\n-[`Trainer`] is incompatible with Transformers TensorFlow models. Instead, fine-tune these models with [Keras](https://keras.io/) since they're implemented as a standard [tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-from datasets import load_dataset\n-from transformers import AutoTokenizer\n-\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n-dataset = load_dataset(\"yelp_review_full\")\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-\n-def tokenize(examples):\n-    return tokenizer(examples[\"text\"])\n-\n-dataset = dataset.map(tokenize)\n-```\n-\n-There are two methods to convert a dataset to [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset).\n-\n-- [`~TFPreTrainedModel.prepare_tf_dataset`] is the recommended way to create a [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) because you can inspect the model to figure out which columns to use as inputs and which columns to discard. This allows you to create a simpler, more performant dataset.\n-- [`~datasets.Dataset.to_tf_dataset`] is a more low-level method from the [Datasets](https://hf.co/docs/datasets/index) library that gives you more control over how a dataset is created by specifying the columns and label columns to use.\n-\n-Add the tokenizer to [`~TFPreTrainedModel.prepare_tf_dataset`] to pad each batch, and you can optionally shuffle the dataset. For more complicated preprocessing, pass the preprocessing function to the `collate_fn` parameter instead.\n-\n-```py\n-tf_dataset = model.prepare_tf_dataset(\n-    dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer\n-)\n-```\n-\n-Finally, [compile](https://keras.io/api/models/model_training_apis/#compile-method) and [fit](https://keras.io/api/models/model_training_apis/#fit-method) the model to start training.\n-\n-> [!TIP]\n-> It isn't necessary to pass a loss argument to [compile](https://keras.io/api/models/model_training_apis/#compile-method) because Transformers automatically chooses a loss that is appropriate for the task and architecture. However, you can always specify a loss argument if you want.\n-\n-```py\n-from tensorflow.keras.optimizers import Adam\n-\n-model.compile(optimizer=Adam(3e-5))\n-model.fit(tf_dataset)\n-```\n-\n ## Resources\n \n Refer to the Transformers [examples](https://github.com/huggingface/transformers/tree/main/examples) for more detailed training scripts on various tasks. You can also check out the [notebooks](./notebooks) for interactive examples."
        },
        {
            "sha": "7998881d3648d17139e5648aa9940d2e35d0fa7f",
            "filename": "docs/source/en/troubleshooting.md",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftroubleshooting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/docs%2Fsource%2Fen%2Ftroubleshooting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftroubleshooting.md?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -65,29 +65,6 @@ Refer to the Performance [guide](performance) for more details about memory-savi\n \n </Tip>\n \n-## Unable to load a saved TensorFlow model\n-\n-TensorFlow's [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) method will save the entire model - architecture, weights, training configuration - in a single file. However, when you load the model file again, you may run into an error because ðŸ¤— Transformers may not load all the TensorFlow-related objects in the model file. To avoid issues with saving and loading TensorFlow models, we recommend you:\n-\n-- Save the model weights as a `h5` file extension with [`model.save_weights`](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) and then reload the model with [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFPreTrainedModel\n->>> from tensorflow import keras\n-\n->>> model.save_weights(\"some_folder/tf_model.h5\")\n->>> model = TFPreTrainedModel.from_pretrained(\"some_folder\")\n-```\n-\n-- Save the model with [`~TFPretrainedModel.save_pretrained`] and load it again with [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFPreTrainedModel\n-\n->>> model.save_pretrained(\"path_to/model\")\n->>> model = TFPreTrainedModel.from_pretrained(\"path_to/model\")\n-```\n-\n ## ImportError\n \n Another common error you may encounter, especially if it is a newly released model, is `ImportError`:"
        },
        {
            "sha": "19253ea77ddcf513f852d133e070d74ad1a3a84d",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/da9452a592ac1e284d61bdbc86ad13d304586f93/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/da9452a592ac1e284d61bdbc86ad13d304586f93/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=da9452a592ac1e284d61bdbc86ad13d304586f93",
            "patch": "@@ -1019,6 +1019,7 @@ def find_all_documented_objects() -> list[str]:\n     \"LineByLineWithRefDataset\",\n     \"LineByLineWithSOPTextDataset\",\n     \"NerPipeline\",\n+    \"OwlViTFeatureExtractor\",\n     \"PretrainedBartModel\",\n     \"PretrainedFSMTModel\",\n     \"SingleSentenceClassificationProcessor\",\n@@ -1028,24 +1029,22 @@ def find_all_documented_objects() -> list[str]:\n     \"SquadFeatures\",\n     \"SquadV1Processor\",\n     \"SquadV2Processor\",\n-    \"TFAutoModelWithLMHead\",\n-    \"TFBartPretrainedModel\",\n     \"TextDataset\",\n     \"TextDatasetForNextSentencePrediction\",\n+    \"TFTrainingArguments\",\n     \"Wav2Vec2ForMaskedLM\",\n     \"Wav2Vec2Tokenizer\",\n     \"glue_compute_metrics\",\n     \"glue_convert_examples_to_features\",\n     \"glue_output_modes\",\n     \"glue_processors\",\n     \"glue_tasks_num_labels\",\n+    \"shape_list\",\n     \"squad_convert_examples_to_features\",\n     \"xnli_compute_metrics\",\n     \"xnli_output_modes\",\n     \"xnli_processors\",\n     \"xnli_tasks_num_labels\",\n-    \"TFTrainingArguments\",\n-    \"OwlViTFeatureExtractor\",\n ]\n \n # Exceptionally, some objects should not be documented after all rules passed.\n@@ -1146,7 +1145,13 @@ def check_all_objects_are_documented():\n     \"\"\"Check all models are properly documented.\"\"\"\n     documented_objs, documented_methods_map = find_all_documented_objects()\n     modules = transformers._modules\n-    objects = [c for c in dir(transformers) if c not in modules and not c.startswith(\"_\")]\n+    # the objects with the following prefixes are not requires to be in the docs\n+    ignore_prefixes = [\n+        \"_\",  # internal objects\n+        \"TF\",  # TF objects, support deprecated\n+        \"Flax\",  # Flax objects, support deprecated\n+    ]\n+    objects = [c for c in dir(transformers) if c not in modules and not any(c.startswith(p) for p in ignore_prefixes)]\n     undocumented_objs = [c for c in objects if c not in documented_objs and not ignore_undocumented(c)]\n     if len(undocumented_objs) > 0:\n         raise Exception("
        }
    ],
    "stats": {
        "total": 414,
        "additions": 16,
        "deletions": 398
    }
}