{
    "author": "gante",
    "message": "[tests] move generative tests away from `test_modeling_common.py` (#40854)\n\nmove tests",
    "sha": "e682f90f60ad854d68ab903b720d266c0578f781",
    "files": [
        {
            "sha": "8600f1dc265e52f02e64102f754f1a9e990126a4",
            "filename": "tests/causal_lm_tester.py",
            "status": "modified",
            "additions": 17,
            "deletions": 1,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Fcausal_lm_tester.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Fcausal_lm_tester.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcausal_lm_tester.py?ref=e682f90f60ad854d68ab903b720d266c0578f781",
            "patch": "@@ -18,7 +18,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers import PretrainedConfig, set_seed\n+from transformers import AutoModelForCausalLM, PretrainedConfig, set_seed\n from transformers.testing_utils import (\n     is_flaky,\n     require_flash_attn,\n@@ -474,6 +474,22 @@ def test_flash_attn_2_equivalence(self):\n                 logits_fa = outputs_fa.hidden_states[-1]\n                 torch.testing.assert_close(logits_fa, logits, atol=3e-2, rtol=3e-2)\n \n+    def test_causal_lm_can_accept_training_kwargs(self):\n+        if not getattr(self.model_tester, \"is_training\", False):\n+            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            with torch.device(torch_device):\n+                model_eager = AutoModelForCausalLM.from_config(config, dtype=torch.float32)\n+\n+            model_eager.save_pretrained(tmpdir)\n+            model = AutoModelForCausalLM.from_pretrained(tmpdir, dtype=torch.float32, device_map=torch_device)\n+            inputs_dict[\"num_items_in_batch\"] = torch.tensor(inputs_dict[\"input_ids\"].shape[0])\n+            inputs_dict[\"labels\"] = inputs_dict[\"input_ids\"]\n+            _ = model(**inputs_dict, return_dict=False)\n+\n \n def _config_supports_rope_scaling(config: PretrainedConfig) -> bool:\n     \"\"\"Returns whether a certain model config supports RoPE scaling parameterization.\"\"\""
        },
        {
            "sha": "dbeade214410b8398ad98a9a3f5ca72f83c6afff",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 338,
            "deletions": 0,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=e682f90f60ad854d68ab903b720d266c0578f781",
            "patch": "@@ -61,6 +61,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoModelForCausalLM,\n@@ -70,6 +71,7 @@\n         AutoModelForVision2Seq,\n         BartForConditionalGeneration,\n         BartTokenizer,\n+        DataCollatorWithFlattening,\n         GPT2LMHeadModel,\n         GPT2Tokenizer,\n         ImageGPTForCausalImageModeling,\n@@ -1912,6 +1914,342 @@ def test_eager_matches_fa3_generate(self):\n         \"\"\"Tests that generate has equivalent outputs with FA3 and eager attention implementations.\"\"\"\n         self._test_attention_implementation(\"flash_attention_3\")\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    def test_flash_attention_2_continue_generate_with_position_ids(self):\n+        \"\"\"\n+        Tests whether flash attention can continue its generation from given position ids.\n+\n+        NOTE: This serves as regression check as we had instances where flash attention entered the varlen\n+        path here. It should now always enter the base `flash_fn`.\n+        \"\"\"\n+\n+        max_new_tokens = 2\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention.\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if config.is_encoder_decoder:\n+                self.skipTest(\"Model is an encoder-decoder\")\n+\n+            if not hasattr(config.get_text_config(), \"use_cache\"):\n+                self.skipTest(f\"{model_class.__name__} doesn't support caching\")\n+\n+            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n+                self.skipTest(\"Model dummy inputs should contain text input ids\")\n+\n+            # make sure that all models have enough positions for generation\n+            dummy_input_ids = inputs_dict[\"input_ids\"]\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        dtype=torch.bfloat16,\n+                        attn_implementation=\"flash_attention_2\",\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # Drop all keys except for `input_ids`. Hard to manipulate with multimodals/head_mask/etc\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+                dummy_position_ids = torch.arange(dummy_input_ids.shape[1], device=torch_device)\n+                dummy_position_ids = dummy_position_ids.unsqueeze(0).repeat(dummy_input_ids.shape[0], 1)\n+\n+                # Store cache for the input prompt\n+                output = model(dummy_input_ids, position_ids=dummy_position_ids, use_cache=True)\n+                if \"past_key_values\" not in output:\n+                    self.skipTest(\"This model doesn't return `past_key_values`\")\n+\n+                # create new input_ids and position_ids to continue generation re-using the cache\n+                new_input_ids = output.logits[:, -1, :].float().argmax(-1)[:, None]\n+                past_length = dummy_input_ids.shape[1]\n+                position_ids = torch.arange(past_length, past_length + new_input_ids.shape[1], device=torch_device)\n+                position_ids = position_ids.unsqueeze(0).repeat(new_input_ids.shape[0], 1)\n+\n+                output = model(\n+                    input_ids=new_input_ids,\n+                    past_key_values=output.past_key_values,\n+                    position_ids=position_ids,\n+                    use_cache=True,\n+                )\n+                next_token_logits = output.logits[:, -1, :].float()\n+\n+                generate_kwargs = {\n+                    \"pad_token_id\": -1,\n+                    \"eos_token_id\": -1,\n+                    \"forced_eos_token_id\": None,\n+                    \"use_cache\": True,\n+                    \"do_sample\": False,\n+                    \"return_dict_in_generate\": True,\n+                    \"output_logits\": True,\n+                    \"max_new_tokens\": max_new_tokens,\n+                }\n+                generation_out = model.generate(dummy_input_ids, **generate_kwargs)\n+                next_token_logits_from_generate = generation_out.logits[-1]\n+\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(next_token_logits_from_generate, next_token_logits, rtol=tol, atol=tol)\n+\n+    def attention_mask_padding_matches_padding_free_with_position_ids(\n+        self, attn_implementation: str, fa_kwargs: bool = False\n+    ):\n+        \"\"\"\n+        Tests that the given attention implementation can work with packed sequences and infers the mask\n+        from position ids. This test requires the model to use new attention mask API which handles packing.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        max_new_tokens = 30\n+        support_flag = {\n+            \"sdpa\": \"_supports_sdpa\",\n+            \"flash_attention_2\": \"_supports_flash_attn\",\n+            \"flash_attention_3\": \"_supports_flash_attn\",\n+        }\n+\n+        for model_class in self.all_generative_model_classes:\n+            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n+                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n+\n+            # can't infer if new attn mask API is supported by assume that only model with attention backend support it\n+            if not model_class._supports_attention_backend:\n+                self.skipTest(f\"{model_class.__name__} does not support new attention mask API\")\n+\n+            if model_class._is_stateful:  # non-transformer models most probably have no packing support\n+                self.skipTest(f\"{model_class.__name__} doesn't support packing!\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if config.is_encoder_decoder:\n+                self.skipTest(\"Model is an encoder-decoder\")\n+\n+            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n+                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n+\n+            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n+                self.skipTest(\"Model dummy inputs should contain text input ids\")\n+\n+            # make sure that all models have enough positions for generation\n+            dummy_input_ids = inputs_dict[\"input_ids\"]\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            if (not fa_kwargs) and \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                continue  # this model doesn't accept position ids as input\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Drop all keys except for the minimal set. Hard to manipulate with multimodals/head_mask/etc\n+                inputs_dict = {k: v for k, v in inputs_dict.items() if k in [\"input_ids\", \"attention_mask\"]}\n+\n+                # Ensure left padding, to adapt for some models\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                dummy_input_ids[~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        dtype=torch.bfloat16,\n+                        attn_implementation=attn_implementation,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                if fa_kwargs:\n+                    # flatten\n+                    features = [\n+                        {\"input_ids\": i[a.bool()].tolist()} for i, a in zip(dummy_input_ids, dummy_attention_mask)\n+                    ]\n+\n+                    # add position_ids + fa_kwargs\n+                    data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n+                    batch = data_collator(features)\n+                    padfree_inputs_dict = {\n+                        k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()\n+                    }\n+                else:\n+                    # create packed position_ids\n+                    position_ids = (\n+                        torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n+                        .long()\n+                        .unsqueeze(0)\n+                        .to(torch_device)\n+                    )\n+                    padfree_inputs_dict = {\n+                        \"input_ids\": dummy_input_ids[dummy_attention_mask.bool()].unsqueeze(0),\n+                        \"position_ids\": position_ids,\n+                    }\n+\n+                # We need to do simple forward without cache in order to trigger packed SDPA/flex/eager attention path\n+                res_padded = model(**inputs_dict, use_cache=False)\n+                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n+\n+                logits_padded = res_padded.logits[dummy_attention_mask.bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.bfloat16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n+    def test_eager_padding_matches_padding_free_with_position_ids(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"eager\")\n+\n+    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n+    @slow\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(\n+            attn_implementation=\"flash_attention_2\", fa_kwargs=True\n+        )\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_3\")\n+\n+    @require_flash_attn_3\n+    @require_torch_gpu\n+    @pytest.mark.flash_attn_3_test\n+    @slow\n+    def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        self.attention_mask_padding_matches_padding_free_with_position_ids(\n+            attn_implementation=\"flash_attention_3\", fa_kwargs=True\n+        )\n+\n+    def _get_custom_4d_mask_test_data(self):\n+        # Sequence in which all but the last token is the same\n+        input_ids = torch.tensor(\n+            [[10, 11, 12, 13], [10, 11, 12, 14], [10, 11, 12, 15]], device=torch_device, dtype=torch.int64\n+        )\n+        position_ids = torch.tensor([[0, 1, 2, 3]] * 3, device=torch_device, dtype=torch.int64)\n+\n+        # Combining common prefix with the unique ending tokens:\n+        input_ids_shared_prefix = torch.cat([input_ids[0][:-1], input_ids[:, -1]]).unsqueeze(0)\n+\n+        # Creating a 4D mask where each of the last 3 tokens do not attend to each other.\n+        mask_shared_prefix = torch.tensor(\n+            [\n+                [\n+                    [\n+                        [1, 0, 0, 0, 0, 0],\n+                        [1, 1, 0, 0, 0, 0],\n+                        [1, 1, 1, 0, 0, 0],\n+                        [1, 1, 1, 1, 0, 0],\n+                        [1, 1, 1, 0, 1, 0],\n+                        [1, 1, 1, 0, 0, 1],\n+                    ]\n+                ]\n+            ],\n+        )\n+        # inverting the attention mask\n+        mask_dtype = torch.float32\n+        min_dtype = torch.finfo(mask_dtype).min\n+        mask_shared_prefix = (mask_shared_prefix.eq(0.0)).to(dtype=mask_dtype, device=torch_device) * min_dtype\n+\n+        # Creating a position_ids tensor. note the repeating figures in the end.\n+        position_ids_shared_prefix = torch.tensor([[0, 1, 2, 3, 3, 3]], device=torch_device, dtype=torch.int64)\n+\n+        return input_ids, position_ids, input_ids_shared_prefix, mask_shared_prefix, position_ids_shared_prefix\n+\n+    def test_custom_4d_attention_mask(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        set_model_tester_for_less_flaky_test(self)\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._can_compile_fullgraph:\n+                self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            set_config_for_less_flaky_test(config)\n+            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n+                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n+            model = model_class(config).to(device=torch_device, dtype=torch.float32).eval()\n+            set_model_for_less_flaky_test(model)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                continue  # model doesn't accept position ids and probably has special way to model positions\n+\n+            (\n+                input_ids,\n+                position_ids,\n+                input_ids_shared_prefix,\n+                mask_shared_prefix,\n+                position_ids_shared_prefix,\n+            ) = self._get_custom_4d_mask_test_data()\n+\n+            logits = model.forward(input_ids, position_ids=position_ids).logits\n+            # logits.shape == torch.Size([3, 4, ...])\n+\n+            logits_shared_prefix = model(\n+                input_ids_shared_prefix,\n+                attention_mask=mask_shared_prefix,\n+                position_ids=position_ids_shared_prefix,\n+            )[0]\n+            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n+\n+            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n+            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n+\n+            # comparing softmax-normalized logits:\n+            normalized_0 = F.softmax(out_last_tokens, dim=-1)\n+            normalized_1 = F.softmax(out_shared_prefix_last_tokens, dim=-1)\n+            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n+\n+    def test_forward_with_logits_to_keep(self):\n+        for model_class in self.all_generative_model_classes:\n+            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n+                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n+\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+            batch_size, sequence_length = inputs[\"input_ids\"].shape[:2]\n+            vocab_size = config.get_text_config().vocab_size\n+            model = model_class(config).to(device=torch_device).eval()\n+            # some models have labels but `logits_to_keep` should not be used in train mode\n+            _ = inputs.pop(\"labels\", None)\n+\n+            # logits_to_keep=0 is a special case meaning \"keep all logits\"\n+            all_logits = model(**inputs, logits_to_keep=0).logits\n+            last_token_logits = model(**inputs, logits_to_keep=1).logits\n+\n+            # Assert all shapes are correct\n+            self.assertEqual(tuple(all_logits.shape), (batch_size, sequence_length, vocab_size))\n+            self.assertEqual(tuple(last_token_logits.shape), (batch_size, 1, vocab_size))\n+\n+            # Assert the last tokens are actually the same (except for the natural fluctuation due to order of FP ops)\n+            torch.testing.assert_close(all_logits[:, -1:, :], last_token_logits, rtol=1e-5, atol=1e-5)\n+\n     def _check_generate_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n         input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n         internal_batch_size = ("
        },
        {
            "sha": "f4c890e3ce156add264aab48b2fcf221cc6ba54c",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 8,
            "deletions": 495,
            "changes": 503,
            "blob_url": "https://github.com/huggingface/transformers/blob/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e682f90f60ad854d68ab903b720d266c0578f781/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e682f90f60ad854d68ab903b720d266c0578f781",
            "patch": "@@ -34,9 +34,7 @@\n \n from transformers import (\n     AutoModel,\n-    AutoModelForCausalLM,\n     AutoModelForSequenceClassification,\n-    DataCollatorWithFlattening,\n     PretrainedConfig,\n     PreTrainedModel,\n     is_torch_available,\n@@ -125,7 +123,6 @@\n \n if is_torch_available():\n     import torch\n-    import torch.nn.functional as F\n     from safetensors.torch import load_file as safe_load_file\n     from safetensors.torch import save_file as safe_save_file\n     from torch import nn\n@@ -247,6 +244,11 @@ def _can_output_attn(model):\n     for model_class in self.all_model_classes:\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         set_config_for_less_flaky_test(config)\n+\n+        # If it's a model with sliding window attention, let's test it with sliding window\n+        if hasattr(config, \"sliding_window\"):\n+            config.sliding_window = 2\n+\n         model = model_class(config)\n         # TODO: standardize the interfaces for musicgen models, see other todo in this test\n         if model.__class__.__name__ == \"MusicgenMelodyForConditionalGeneration\":\n@@ -1197,41 +1199,6 @@ def test_training(self):\n             loss = model(**inputs).loss\n             loss.backward()\n \n-    def test_causal_lm_can_accept_kwargs(self):\n-        if not getattr(self.model_tester, \"is_training\", False):\n-            self.skipTest(reason=\"ModelTester is not configured to run training tests\")\n-\n-        valid_model_class = False\n-        incompatible_models = (\n-            \"MusicgenForCausalLM\",\n-            \"MusicgenMelodyForCausalLM\",\n-            \"MllamaForCausalLM\",\n-            \"CpmAntForCausalLM\",\n-            \"GotOcr2ForConditionalGeneration\",\n-        )\n-        for model_class in self.all_model_classes:\n-            if (\n-                model_class.__name__ in get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES)\n-                and model_class.__name__ not in incompatible_models\n-            ):\n-                valid_model_class = True\n-        if not valid_model_class:\n-            self.skipTest(reason=\"No causal lm model classes found\")\n-        for model_class in self.all_model_classes:\n-            model_name = model_class.__name__\n-            if model_name in get_values(MODEL_FOR_CAUSAL_LM_MAPPING_NAMES) and model_name not in incompatible_models:\n-                config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-                with tempfile.TemporaryDirectory() as tmpdir:\n-                    with torch.device(torch_device):\n-                        model_eager = AutoModelForCausalLM.from_config(config, dtype=torch.float32)\n-\n-                    model_eager.save_pretrained(tmpdir)\n-                    model = AutoModelForCausalLM.from_pretrained(tmpdir, dtype=torch.float32, device_map=torch_device)\n-                    inputs_dict[\"num_items_in_batch\"] = torch.tensor(inputs_dict[\"input_ids\"].shape[0])\n-                    inputs_dict[\"labels\"] = inputs_dict[\"input_ids\"]\n-                    _ = model(**inputs_dict, return_dict=False)\n-\n     def test_training_gradient_checkpointing(self):\n         # Scenario - 1 default behaviour\n         self.check_training_gradient_checkpointing()\n@@ -2758,65 +2725,6 @@ def recursive_check(tuple_object, dict_object):\n                     model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True, \"output_attentions\": True}\n                 )\n \n-    # Don't copy this method to model specific test file!\n-    # TODO: remove this method once the issues are all fixed!\n-    def _make_attention_mask_non_null(self, inputs_dict):\n-        \"\"\"Make sure no sequence has all zeros as attention mask\"\"\"\n-\n-        for k in [\"attention_mask\", \"encoder_attention_mask\", \"decoder_attention_mask\"]:\n-            if k in inputs_dict:\n-                attention_mask = inputs_dict[k]\n-\n-                # Make sure no all 0s attention masks - to avoid failure at this moment.\n-                # Put `1` at the beginning of sequences to make it still work when combining causal attention masks.\n-                # TODO: remove this line once a fix regarding large negative values for attention mask is done.\n-                attention_mask = torch.cat(\n-                    [torch.ones_like(attention_mask[:, :1], dtype=attention_mask.dtype), attention_mask[:, 1:]], dim=-1\n-                )\n-\n-                # Here we make the first sequence with all 0s as attention mask.\n-                # Currently, this will fail for `TFWav2Vec2Model`. This is caused by the different large negative\n-                # values, like `1e-4`, `1e-9`, `1e-30` and `-inf` for attention mask across models/frameworks.\n-                # TODO: enable this block once the large negative values thing is cleaned up.\n-                # (see https://github.com/huggingface/transformers/issues/14859)\n-                # attention_mask = torch.cat(\n-                #     [torch.zeros_like(attention_mask[:1], dtype=attention_mask.dtype), attention_mask[1:]],\n-                #     dim=0\n-                # )\n-\n-                inputs_dict[k] = attention_mask\n-\n-    # Don't copy this method to model specific test file!\n-    # TODO: remove this method once the issues are all fixed!\n-    def _postprocessing_to_ignore_test_cases(self, tf_outputs, pt_outputs, model_class):\n-        \"\"\"For temporarily ignoring some failed test cases (issues to be fixed)\"\"\"\n-\n-        tf_keys = {k for k, v in tf_outputs.items() if v is not None}\n-        pt_keys = {k for k, v in pt_outputs.items() if v is not None}\n-\n-        key_differences = tf_keys.symmetric_difference(pt_keys)\n-\n-        if model_class.__name__ in [\n-            \"FlaubertWithLMHeadModel\",\n-            \"FunnelForPreTraining\",\n-            \"ElectraForPreTraining\",\n-            \"XLMWithLMHeadModel\",\n-        ]:\n-            for k in key_differences:\n-                if k in [\"loss\", \"losses\"]:\n-                    tf_keys.discard(k)\n-                    pt_keys.discard(k)\n-        elif model_class.__name__.startswith(\"GPT2\"):\n-            # `TFGPT2` has `past_key_values` as a tensor while `GPT2` has it as a tuple.\n-            tf_keys.discard(\"past_key_values\")\n-            pt_keys.discard(\"past_key_values\")\n-\n-        # create new outputs from the remaining fields\n-        new_tf_outputs = type(tf_outputs)(**{k: tf_outputs[k] for k in tf_keys})\n-        new_pt_outputs = type(pt_outputs)(**{k: pt_outputs[k] for k in pt_keys})\n-\n-        return new_tf_outputs, new_pt_outputs\n-\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -3485,7 +3393,7 @@ def _init_weights(self, module):\n                     )\n \n     def test_model_is_small(self):\n-        # Just a consistency check to make sure we are not running tests on 80M parameter models.\n+        # Just a consistency check to make sure we are not running tests on 1M parameter models.\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n@@ -3989,57 +3897,6 @@ def test_sdpa_can_compile_dynamic(self):\n                 with torch.no_grad():\n                     _ = model(**inputs_dict)\n \n-    def test_sdpa_matches_eager_sliding_window(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        WINDOW_ATTENTION_MODELS = [\"mistral\", \"mixtral\", \"minimax\", \"qwen2\", \"qwen_moe\", \"starcoder2\"]\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(f\"No generative model classes for {self.__class__.__name__}\")\n-\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._supports_sdpa:\n-                self.skipTest(reason=\"Model architecture does not support attentions\")\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            if config.model_type not in WINDOW_ATTENTION_MODELS:\n-                self.skipTest(f\"{config.model_type} does not use window attention\")\n-\n-            config.sliding_window = 2\n-\n-            dummy_input = inputs_dict[model_class.main_input_name]\n-            attention_mask = inputs_dict[\"attention_mask\"]\n-\n-            self.assertTrue(dummy_input.ndim == 2)\n-            self.assertTrue(dummy_input.shape[1] > 6)\n-\n-            with tempfile.TemporaryDirectory() as tmpdir:\n-                with torch.device(torch_device):\n-                    model_eager = AutoModelForCausalLM.from_config(\n-                        config, attn_implementation=\"eager\", dtype=torch.float32\n-                    )\n-\n-                model_eager.save_pretrained(tmpdir)\n-\n-                with torch.device(torch_device):\n-                    model_sdpa = AutoModelForCausalLM.from_pretrained(\n-                        tmpdir, attn_implementation=\"sdpa\", dtype=torch.float32\n-                    )\n-\n-                model_eager = model_eager.eval()\n-                model_sdpa = model_sdpa.eval()\n-\n-                with torch.no_grad():\n-                    with sdpa_kernel(enable_flash=False, enable_math=True, enable_mem_efficient=False):\n-                        res_eager = model_eager(**inputs_dict, return_dict=False)[0]\n-                        res_sdpa = model_sdpa(**inputs_dict, return_dict=False)[0]\n-\n-                # Only non-padding tokens are expected to match.\n-                self.assertTrue(\n-                    torch.allclose(res_eager[attention_mask == 1], res_sdpa[attention_mask == 1], rtol=1e-4, atol=1e-4)\n-                )\n-\n     def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n         \"\"\"\n         Tests if composite models can dispatch on flash attention if the sub-models support it.\n@@ -4123,7 +3980,7 @@ def test_flash_attn_2_fp32_ln(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        for model_class in self.all_generative_model_classes:\n+        for model_class in self.all_generative_model_classes:  # TODO: this test should run on all classes instead\n             if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4209,240 +4066,6 @@ def test_flash_attn_2_can_compile_with_attention_mask_None_without_graph_break(s\n \n         assert not loss.isnan().any()\n \n-    def attention_mask_padding_matches_padding_free_with_position_ids(\n-        self, attn_implementation: str, fa_kwargs: bool = False\n-    ):\n-        \"\"\"\n-        Tests that the given attention implementation can work with packed sequences and infers the mask\n-        from position ids. This test requires the model to use new attention mask API which handles packing.\n-        \"\"\"\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        max_new_tokens = 30\n-        support_flag = {\n-            \"sdpa\": \"_supports_sdpa\",\n-            \"flash_attention_2\": \"_supports_flash_attn\",\n-            \"flash_attention_3\": \"_supports_flash_attn\",\n-        }\n-\n-        for model_class in self.all_generative_model_classes:\n-            if attn_implementation != \"eager\" and not getattr(model_class, support_flag[attn_implementation]):\n-                self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n-\n-            # can't infer if new attn mask API is supported by assume that only model with attention backend support it\n-            if not model_class._supports_attention_backend:\n-                self.skipTest(f\"{model_class.__name__} does not support new attention mask API\")\n-\n-            if model_class._is_stateful:  # non-transformer models most probably have no packing support\n-                self.skipTest(f\"{model_class.__name__} doesn't support packing!\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            if config.is_encoder_decoder:\n-                self.skipTest(\"Model is an encoder-decoder\")\n-\n-            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n-                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n-\n-            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n-                self.skipTest(\"Model dummy inputs should contain text input ids\")\n-\n-            # make sure that all models have enough positions for generation\n-            dummy_input_ids = inputs_dict[\"input_ids\"]\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n-\n-            model = model_class(config)\n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                self.skipTest(\"Model does not support position_ids\")\n-\n-            if (not fa_kwargs) and \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                continue  # this model doesn't accept position ids as input\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-\n-                # Drop all keys except for the minimal set. Hard to manipulate with multimodals/head_mask/etc\n-                inputs_dict = {k: v for k, v in inputs_dict.items() if k in [\"input_ids\", \"attention_mask\"]}\n-\n-                # Ensure left padding, to adapt for some models\n-                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n-                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n-                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n-                dummy_input_ids[~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n-\n-                model = (\n-                    model_class.from_pretrained(\n-                        tmpdirname,\n-                        dtype=torch.bfloat16,\n-                        attn_implementation=attn_implementation,\n-                    )\n-                    .to(torch_device)\n-                    .eval()\n-                )\n-\n-                if fa_kwargs:\n-                    # flatten\n-                    features = [\n-                        {\"input_ids\": i[a.bool()].tolist()} for i, a in zip(dummy_input_ids, dummy_attention_mask)\n-                    ]\n-\n-                    # add position_ids + fa_kwargs\n-                    data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n-                    batch = data_collator(features)\n-                    padfree_inputs_dict = {\n-                        k: t.to(torch_device) if torch.is_tensor(t) else t for k, t in batch.items()\n-                    }\n-                else:\n-                    # create packed position_ids\n-                    position_ids = (\n-                        torch.cat([torch.arange(length) for length in dummy_attention_mask.sum(1).tolist()])\n-                        .long()\n-                        .unsqueeze(0)\n-                        .to(torch_device)\n-                    )\n-                    padfree_inputs_dict = {\n-                        \"input_ids\": dummy_input_ids[dummy_attention_mask.bool()].unsqueeze(0),\n-                        \"position_ids\": position_ids,\n-                    }\n-\n-                # We need to do simple forward without cache in order to trigger packed SDPA/flex/eager attention path\n-                res_padded = model(**inputs_dict, use_cache=False)\n-                res_padfree = model(**padfree_inputs_dict, use_cache=False)\n-\n-                logits_padded = res_padded.logits[dummy_attention_mask.bool()]\n-                logits_padfree = res_padfree.logits[0]\n-\n-                # acceptable numerical instability\n-                tol = torch.finfo(torch.bfloat16).eps\n-                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n-\n-    def test_eager_padding_matches_padding_free_with_position_ids(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"eager\")\n-\n-    def test_sdpa_padding_matches_padding_free_with_position_ids(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"sdpa\")\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_2\")\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(\n-            attn_implementation=\"flash_attention_2\", fa_kwargs=True\n-        )\n-\n-    @require_flash_attn_3\n-    @require_torch_gpu\n-    @mark.flash_attn_3_test\n-    @slow\n-    def test_flash_attention_3_padding_matches_padding_free_with_position_ids(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(attn_implementation=\"flash_attention_3\")\n-\n-    @require_flash_attn_3\n-    @require_torch_gpu\n-    @mark.flash_attn_3_test\n-    @slow\n-    def test_flash_attention_3_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n-        self.attention_mask_padding_matches_padding_free_with_position_ids(\n-            attn_implementation=\"flash_attention_3\", fa_kwargs=True\n-        )\n-\n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    def test_flash_attention_2_continue_generate_with_position_ids(self):\n-        \"\"\"\n-        Tests whether flash attention can continue its generation from given position ids.\n-\n-        NOTE: This serves as regression check as we had instances where flash attention entered the varlen\n-        path here. It should now always enter the base `flash_fn`.\n-        \"\"\"\n-\n-        max_new_tokens = 2\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._supports_flash_attn:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention.\")\n-\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            if config.is_encoder_decoder:\n-                self.skipTest(\"Model is an encoder-decoder\")\n-\n-            if not hasattr(config.get_text_config(), \"use_cache\"):\n-                self.skipTest(f\"{model_class.__name__} doesn't support caching\")\n-\n-            if \"input_ids\" not in inputs_dict or inputs_dict[\"input_ids\"].ndim != 2:\n-                self.skipTest(\"Model dummy inputs should contain text input ids\")\n-\n-            # make sure that all models have enough positions for generation\n-            dummy_input_ids = inputs_dict[\"input_ids\"]\n-            if hasattr(config, \"max_position_embeddings\"):\n-                config.max_position_embeddings = max_new_tokens + dummy_input_ids.shape[1] + 1\n-\n-            model = model_class(config)\n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                self.skipTest(\"Model does not support position_ids\")\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = (\n-                    model_class.from_pretrained(\n-                        tmpdirname,\n-                        dtype=torch.bfloat16,\n-                        attn_implementation=\"flash_attention_2\",\n-                    )\n-                    .to(torch_device)\n-                    .eval()\n-                )\n-\n-                # Drop all keys except for `input_ids`. Hard to manipulate with multimodals/head_mask/etc\n-                dummy_input_ids = inputs_dict[\"input_ids\"]\n-                dummy_position_ids = torch.arange(dummy_input_ids.shape[1], device=torch_device)\n-                dummy_position_ids = dummy_position_ids.unsqueeze(0).repeat(dummy_input_ids.shape[0], 1)\n-\n-                # Store cache for the input prompt\n-                output = model(dummy_input_ids, position_ids=dummy_position_ids, use_cache=True)\n-                if \"past_key_values\" not in output:\n-                    self.skipTest(\"This model doesn't return `past_key_values`\")\n-\n-                # create new input_ids and position_ids to continue generation re-using the cache\n-                new_input_ids = output.logits[:, -1, :].float().argmax(-1)[:, None]\n-                past_length = dummy_input_ids.shape[1]\n-                position_ids = torch.arange(past_length, past_length + new_input_ids.shape[1], device=torch_device)\n-                position_ids = position_ids.unsqueeze(0).repeat(new_input_ids.shape[0], 1)\n-\n-                output = model(\n-                    input_ids=new_input_ids,\n-                    past_key_values=output.past_key_values,\n-                    position_ids=position_ids,\n-                    use_cache=True,\n-                )\n-                next_token_logits = output.logits[:, -1, :].float()\n-\n-                generate_kwargs = {\n-                    \"pad_token_id\": -1,\n-                    \"eos_token_id\": -1,\n-                    \"forced_eos_token_id\": None,\n-                    \"use_cache\": True,\n-                    \"do_sample\": False,\n-                    \"return_dict_in_generate\": True,\n-                    \"output_logits\": True,\n-                    \"max_new_tokens\": max_new_tokens,\n-                }\n-                generation_out = model.generate(dummy_input_ids, **generate_kwargs)\n-                next_token_logits_from_generate = generation_out.logits[-1]\n-\n-                # acceptable numerical instability\n-                tol = torch.finfo(torch.bfloat16).eps\n-                torch.testing.assert_close(next_token_logits_from_generate, next_token_logits, rtol=tol, atol=tol)\n-\n     def flash_attn_from_config(self, attn_implementation: str):\n         r\"\"\"\n         Tests if the model can be loaded with `attn_implementation` from the config and if the\n@@ -4451,7 +4074,7 @@ def flash_attn_from_config(self, attn_implementation: str):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        for model_class in self.all_generative_model_classes:\n+        for model_class in self.all_generative_model_classes:  # TODO: this test should run on all classes instead\n             if not model_class._supports_flash_attn:\n                 self.skipTest(f\"{model_class.__name__} does not support {attn_implementation}\")\n \n@@ -4497,41 +4120,6 @@ def test_flash_attn_2_from_config(self):\n     def test_flash_attn_3_from_config(self):\n         self.flash_attn_from_config(attn_implementation=\"flash_attention_3\")\n \n-    def _get_custom_4d_mask_test_data(self):\n-        # Sequence in which all but the last token is the same\n-        input_ids = torch.tensor(\n-            [[10, 11, 12, 13], [10, 11, 12, 14], [10, 11, 12, 15]], device=torch_device, dtype=torch.int64\n-        )\n-        position_ids = torch.tensor([[0, 1, 2, 3]] * 3, device=torch_device, dtype=torch.int64)\n-\n-        # Combining common prefix with the unique ending tokens:\n-        input_ids_shared_prefix = torch.cat([input_ids[0][:-1], input_ids[:, -1]]).unsqueeze(0)\n-\n-        # Creating a 4D mask where each of the last 3 tokens do not attend to each other.\n-        mask_shared_prefix = torch.tensor(\n-            [\n-                [\n-                    [\n-                        [1, 0, 0, 0, 0, 0],\n-                        [1, 1, 0, 0, 0, 0],\n-                        [1, 1, 1, 0, 0, 0],\n-                        [1, 1, 1, 1, 0, 0],\n-                        [1, 1, 1, 0, 1, 0],\n-                        [1, 1, 1, 0, 0, 1],\n-                    ]\n-                ]\n-            ],\n-        )\n-        # inverting the attention mask\n-        mask_dtype = torch.float32\n-        min_dtype = torch.finfo(mask_dtype).min\n-        mask_shared_prefix = (mask_shared_prefix.eq(0.0)).to(dtype=mask_dtype, device=torch_device) * min_dtype\n-\n-        # Creating a position_ids tensor. note the repeating figures in the end.\n-        position_ids_shared_prefix = torch.tensor([[0, 1, 2, 3, 3, 3]], device=torch_device, dtype=torch.int64)\n-\n-        return input_ids, position_ids, input_ids_shared_prefix, mask_shared_prefix, position_ids_shared_prefix\n-\n     def test_sliding_window_mask(self):\n         \"\"\"Tests that we can control the sliding window attention behavior of a model.\"\"\"\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4586,58 +4174,6 @@ def test_sliding_window_mask(self):\n             for layer_attention in attentions_not_sliding:\n                 self.assertFalse((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n \n-    def test_custom_4d_attention_mask(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if len(self.all_generative_model_classes) == 0:\n-            self.skipTest(\n-                reason=\"Model architecture has no generative classes, and thus not necessarily supporting 4D masks\"\n-            )\n-\n-        set_model_tester_for_less_flaky_test(self)\n-\n-        for model_class in self.all_generative_model_classes:\n-            if not model_class._can_compile_fullgraph:\n-                self.skipTest(f\"{model_class.__name__} is not guaranteed to work with custom 4D attention masks\")\n-            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-            set_config_for_less_flaky_test(config)\n-            if getattr(config, \"sliding_window\", 0) is not None and getattr(config, \"sliding_window\", 0) > 0:\n-                self.skipTest(f\"{model_class.__name__} with sliding window attention is not supported by this test\")\n-            model = model_class(config).to(device=torch_device, dtype=torch.float32).eval()\n-            set_model_for_less_flaky_test(model)\n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                continue  # model doesn't accept position ids and probably has special way to model positions\n-\n-            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n-                continue  # this model doesn't accept position ids as input\n-\n-            (\n-                input_ids,\n-                position_ids,\n-                input_ids_shared_prefix,\n-                mask_shared_prefix,\n-                position_ids_shared_prefix,\n-            ) = self._get_custom_4d_mask_test_data()\n-\n-            logits = model.forward(input_ids, position_ids=position_ids).logits\n-            # logits.shape == torch.Size([3, 4, ...])\n-\n-            logits_shared_prefix = model(\n-                input_ids_shared_prefix,\n-                attention_mask=mask_shared_prefix,\n-                position_ids=position_ids_shared_prefix,\n-            )[0]\n-            # logits_shared_prefix.shape == torch.Size([1, 6, ...])\n-\n-            out_last_tokens = logits[:, -1, :]  # last tokens in each batch line\n-            out_shared_prefix_last_tokens = logits_shared_prefix[0, -3:, :]  # last three tokens\n-\n-            # comparing softmax-normalized logits:\n-            normalized_0 = F.softmax(out_last_tokens, dim=-1)\n-            normalized_1 = F.softmax(out_shared_prefix_last_tokens, dim=-1)\n-            torch.testing.assert_close(normalized_0, normalized_1, rtol=1e-3, atol=1e-3)\n-\n     @slow\n     @require_torch_accelerator\n     @pytest.mark.torch_compile_test\n@@ -4688,29 +4224,6 @@ def test_torch_compile_for_training(self):\n         for name, param in model._orig_mod.named_parameters():\n             torch.testing.assert_close(param.grad.detach().cpu(), params[name], rtol=1e-4, atol=1e-4)\n \n-    def test_forward_with_logits_to_keep(self):\n-        for model_class in self.all_generative_model_classes:\n-            if \"logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):\n-                self.skipTest(reason=\"This model does not support `logits_to_keep` argument.\")\n-\n-            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-            batch_size, sequence_length = inputs[\"input_ids\"].shape[:2]\n-            vocab_size = config.get_text_config().vocab_size\n-            model = model_class(config).to(device=torch_device).eval()\n-            # some models have labels but `logits_to_keep` should not be used in train mode\n-            _ = inputs.pop(\"labels\", None)\n-\n-            # logits_to_keep=0 is a special case meaning \"keep all logits\"\n-            all_logits = model(**inputs, logits_to_keep=0).logits\n-            last_token_logits = model(**inputs, logits_to_keep=1).logits\n-\n-            # Assert all shapes are correct\n-            self.assertEqual(tuple(all_logits.shape), (batch_size, sequence_length, vocab_size))\n-            self.assertEqual(tuple(last_token_logits.shape), (batch_size, 1, vocab_size))\n-\n-            # Assert the last tokens are actually the same (except for the natural fluctuation due to order of FP ops)\n-            torch.testing.assert_close(all_logits[:, -1:, :], last_token_logits, rtol=1e-5, atol=1e-5)\n-\n     @slow\n     @require_torch_greater_or_equal(\"2.5\")\n     @pytest.mark.torch_export_test"
        }
    ],
    "stats": {
        "total": 859,
        "additions": 363,
        "deletions": 496
    }
}