{
    "author": "remi-or",
    "message": "Fix sliding window attn mask (#41228)\n\n* Fix sliding window attn mask\n\n* Clearer test\n\n* Apply style fixes\n\n* If Picasso made ascii drawings he would have made this\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "5bdb70450d40f511c0959e632fb3db5ee933a1f1",
    "files": [
        {
            "sha": "0d1801fa163e137f69e128cebbaa36877eaaa28a",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 52,
            "deletions": 3,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bdb70450d40f511c0959e632fb3db5ee933a1f1/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bdb70450d40f511c0959e632fb3db5ee933a1f1/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=5bdb70450d40f511c0959e632fb3db5ee933a1f1",
            "patch": "@@ -42,7 +42,56 @@ def build_attention_mask(\n ) -> None:\n     \"\"\"Builds an attention mask inplace using the cumulative seqlens of the query and key. If given a sliding window, it\n     will also apply a sliding window mask on top. The attention mask is not boolean, it uses zeroes and -inf (or its\n-    equivalent) so it's more of an attention score bias tensor.\"\"\"\n+    equivalent) so it's more of an attention score bias tensor.\n+    The attention mask is a block-diagonal matrix, with each block an attention mask for a single query-key pair.\n+    Each of those block is built from a causal mask and, if there is a sliding window, a sliding window mask.\n+\n+    An example is represented below, with seqlen_k = 8, seqlen_q = 4 and sliding_window = 6:\n+\n+    CAUSAL MASK:\n+\n+           █ █ █ █ █ ░ ░ ░\n+           █ █ █ █ █ █ ░ ░\n+           █ █ █ █ █ █ █ ░\n+           █ █ █ █ █ █ █ █\n+\n+    SLIDING WINDOW MASK:\n+         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 8 - 4 - 6 = -2 offset to the right\n+       <─┴─>\n+     ░ █ | █ █ █ █ █ █ █ █\n+     ░ ░ | █ █ █ █ █ █ █ █\n+     ░ ░ | ░ █ █ █ █ █ █ █\n+     ░ ░ | ░ ░ █ █ █ █ █ █\n+\n+    ATTENTION MASK (sum of causal and sliding window masks):\n+\n+           █ █ █ █ █ ░ ░ ░\n+           █ █ █ █ █ █ ░ ░\n+           ░ █ █ █ █ █ █ ░\n+           ░ ░ █ █ █ █ █ █\n+\n+    Another example with seqlen_k = 5, seqlen_q = 3 and sliding_window = 2:\n+\n+    CAUSAL MASK:\n+\n+           █ █ █ ░ ░\n+           █ █ █ █ ░\n+           █ █ █ █ █\n+\n+    SLIDING WINDOW MASK:\n+         ┌──────────────────────── seqlen_k - seqlen_q - sliding_window = 5 - 3 - 2 = 0 offset to the right\n+        <┴>\n+         | ░ █ █ █ █\n+         | ░ ░ █ █ █\n+         | ░ ░ ░ █ █\n+\n+    ATTENTION MASK (sum of causal and sliding window masks):\n+\n+           ░ █ █ ░ ░\n+           ░ ░ █ █ ░\n+           ░ ░ ░ █ █\n+\n+    \"\"\"\n     min_value = torch.finfo(attention_mask.dtype).min\n     for i in range(len(cumulative_seqlens_q) - 1):\n         seqlen_q = cumulative_seqlens_q[i + 1] - cumulative_seqlens_q[i]\n@@ -63,8 +112,8 @@ def build_attention_mask(\n         masked = torch.triu(minus_inf, diagonal=causal_diagonal)\n         # Apply sliding window mask if needed\n         if sliding_window > 1:\n-            sliding_diagonal = seqlen_k - seqlen_q + sliding_window\n-            masked = torch.tril(masked, diagonal=sliding_diagonal)\n+            sliding_diagonal = seqlen_k - seqlen_q - sliding_window\n+            masked += torch.tril(minus_inf, diagonal=sliding_diagonal)\n         # Replace in attention mask\n         attention_mask[..., query_range, key_range] = masked\n "
        },
        {
            "sha": "943320bfe00bb8522f153f9c550fb9ca9a9cd238",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 43,
            "deletions": 0,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5bdb70450d40f511c0959e632fb3db5ee933a1f1/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5bdb70450d40f511c0959e632fb3db5ee933a1f1/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=5bdb70450d40f511c0959e632fb3db5ee933a1f1",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n+from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import Expectations, require_kernels, require_torch_gpu, slow\n \n \n@@ -88,6 +89,48 @@ def test_group_layers(\n                     f\"Test failed for: {layer_types_str = }, {sliding_window = }, {group_types = }\",\n                 )\n \n+    @parameterized.expand(\n+        [\n+            ([0, 4], [0, 4], 1, [\"1000\", \"1100\", \"1110\", \"1111\"]),\n+            ([0, 4], [0, 4], 2, [\"1000\", \"1100\", \"0110\", \"0011\"]),\n+            ([0, 3], [0, 5], 1, [\"11100\", \"11110\", \"11111\"]),\n+            ([0, 3], [0, 5], 3, [\"11100\", \"01110\", \"00111\"]),\n+            ([0, 3, 6], [0, 3, 6], 1, [\"100000\", \"110000\", \"111000\", \"000100\", \"000110\", \"000111\"]),\n+            ([0, 3, 6], [0, 3, 6], 2, [\"100000\", \"110000\", \"011000\", \"000100\", \"000110\", \"000011\"]),\n+        ]\n+    )\n+    def test_attention_mask(\n+        self,\n+        cumulative_seqlens_q: list[int],\n+        cumulative_seqlens_k: list[int],\n+        sliding_window: int,  # the sliding window size, 1 means no sliding window\n+        str_expected_mask: list[str],  # the attention mask, broken down by line as a string of 0s and 1s\n+    ) -> None:\n+        # Build expected mask\n+        minus_inf = torch.finfo(torch.float32).min\n+        expected_mask = torch.empty((cumulative_seqlens_q[-1], cumulative_seqlens_k[-1]), dtype=torch.float32)\n+        for i, line in enumerate(str_expected_mask):\n+            expected_mask[i, :] = torch.tensor([minus_inf if c == \"0\" else 0 for c in line])\n+        # Build actual mask\n+        actual_mask = torch.full_like(expected_mask, minus_inf)  # function modifies in place\n+        build_attention_mask(\n+            actual_mask, torch.tensor(cumulative_seqlens_q), torch.tensor(cumulative_seqlens_k), sliding_window\n+        )\n+        # Check that the actual mask matches the expected mask\n+        matches = (expected_mask == actual_mask).all()\n+        # If it doesn't match, print the masks in a readable form and fail the test\n+        if not matches:\n+            str_mask = [\n+                \"\".join(\"1\" if x == 0 else \"0\" for x in token_attn_vector) for token_attn_vector in actual_mask\n+            ]\n+            str_mask = \"\\n\".join(str_mask)\n+            str_expected_mask = \"\\n\".join(str_expected_mask)\n+            self.fail(\n+                f\"Test failed for: {cumulative_seqlens_q = }, {cumulative_seqlens_k = }, {sliding_window = }\\n\"\n+                f\"Expected mask:\\n{str_expected_mask}\\n\"\n+                f\"Actual mask:\\n{str_mask}\"\n+            )\n+\n     def _continuous_batching_parity(\n         self, model_id: str, attn_implementation: str, expected_outputs: dict[str, str]\n     ) -> None:"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 95,
        "deletions": 3
    }
}