{
    "author": "cypherpepe",
    "message": "Fixed broken links (#37466)\n\n* Update broken link\n\n* Update broken link",
    "sha": "7bff4bdcf661eb0595110acf536e83c53b99deb6",
    "files": [
        {
            "sha": "8f3d94d45f29ed821bb2e44e1be25adb36d9bf6f",
            "filename": "docs/source/ar/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bff4bdcf661eb0595110acf536e83c53b99deb6/docs%2Fsource%2Far%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bff4bdcf661eb0595110acf536e83c53b99deb6/docs%2Fsource%2Far%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fgguf.md?ref=7bff4bdcf661eb0595110acf536e83c53b99deb6",
            "patch": "@@ -77,7 +77,7 @@ model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n \n الآن لديك إمكانية الوصول إلى النسخة الكامل غير المكممة للنموذج في بيئة PyTorch، حيث يمكنك دمجه مع مجموعة كبيرة من الأدوات الأخرى.\n \n-لإعادة التحويل إلى ملف `gguf`، نوصي باستخدام ملف [`convert-hf-to-gguf.py`](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py) من llama.cpp.\n+لإعادة التحويل إلى ملف `gguf`، نوصي باستخدام ملف [`convert-hf-to-gguf.py`](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py) من llama.cpp.\n \n فيما يلي كيفية إكمال البرنامج النصي أعلاه لحفظ النموذج وإعادة تصديره مرة أخرى إلى `gguf`:\n "
        },
        {
            "sha": "9d9b02d708a09fe50895a3105752309c62a9ab54",
            "filename": "docs/source/ko/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7bff4bdcf661eb0595110acf536e83c53b99deb6/docs%2Fsource%2Fko%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7bff4bdcf661eb0595110acf536e83c53b99deb6/docs%2Fsource%2Fko%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fgguf.md?ref=7bff4bdcf661eb0595110acf536e83c53b99deb6",
            "patch": "@@ -88,7 +88,7 @@ model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)\n \n 이제 PyTorch 생태계에서 모델의 양자화되지 않은 전체 버전에 접근할 수 있으며, 다른 여러 도구들과 결합하여 사용할 수 있습니다.\n \n-`gguf` 파일로 다시 변환하려면 llama.cpp의 [`convert-hf-to-gguf.py`](https://github.com/ggerganov/llama.cpp/blob/master/convert-hf-to-gguf.py)를 사용하는 것을 권장합니다.\n+`gguf` 파일로 다시 변환하려면 llama.cpp의 [`convert-hf-to-gguf.py`](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)를 사용하는 것을 권장합니다.\n \n 위의 스크립트를 완료하여 모델을 저장하고 다시 `gguf`로 내보내는 방법은 다음과 같습니다:\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}