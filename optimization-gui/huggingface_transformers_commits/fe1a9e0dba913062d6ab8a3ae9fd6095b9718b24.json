{
    "author": "Rocketknight1",
    "message": "Remove TF/Flax examples (#40654)\n\n* Remove TF/Flax examples\n\n* Remove check_full_copies\n\n* Trigger CI",
    "sha": "fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24",
    "files": [
        {
            "sha": "40b33c6533164c10cea1f8378ad7e3b600c9ed5d",
            "filename": "examples/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24/examples%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24/examples%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2FREADME.md?ref=fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24",
            "patch": "@@ -15,9 +15,7 @@ limitations under the License.\n \n # Examples\n \n-We host a wide range of example scripts for multiple learning frameworks. Simply choose your favorite: [TensorFlow](https://github.com/huggingface/transformers/tree/main/examples/tensorflow), [PyTorch](https://github.com/huggingface/transformers/tree/main/examples/pytorch) or [JAX/Flax](https://github.com/huggingface/transformers/tree/main/examples/flax).\n-\n-We also have some [research projects](https://github.com/huggingface/transformers-research-projects/), as well as some [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy). Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run.\n+We host a wide range of example scripts, in addition to [research projects](https://github.com/huggingface/transformers-research-projects/), as well as some [legacy examples](https://github.com/huggingface/transformers/tree/main/examples/legacy). Note that unlike the main examples these are not actively maintained, and may require specific older versions of dependencies in order to run.\n \n While we strive to present as many use cases as possible, the example scripts are just that - examples. It is expected that they won't work out-of-the-box on your specific problem and that you will be required to change a few lines of code to adapt them to your needs. To help you with that, most of the examples fully expose the preprocessing of the data, allowing you to tweak and edit them as required.\n "
        },
        {
            "sha": "3b1aa9d494e42a9c7381aa977d0bbf251aa6e380",
            "filename": "examples/flax/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 83,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,83 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# JAX/Flax Examples\n-\n-This folder contains actively maintained examples of ðŸ¤— Transformers using the JAX/Flax backend. Porting models and examples to JAX/Flax is an ongoing effort, and more will be added in the coming months. In particular, these examples are all designed to run fast on Cloud TPUs, and we include step-by-step guides to getting started with Cloud TPU.\n-\n-*NOTE*: Currently, there is no \"Trainer\" abstraction for JAX/Flax -- all examples contain an explicit training loop.\n-\n-The following table lists all of our examples on how to use ðŸ¤— Transformers with the JAX/Flax backend:\n-- with information about the model and dataset used,\n-- whether or not they leverage the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library,\n-- links to **Colab notebooks** to walk through the scripts and run them easily.\n-\n-| Task | Example model | Example dataset | ðŸ¤— Datasets | Colab\n-|---|---|---|:---:|:---:|\n-| [**`causal-language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) | GPT2 | OSCAR | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb)\n-| [**`masked-language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling) | RoBERTa | OSCAR | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb)\n-| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) | BERT | GLUE | âœ… | [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)\n-\n-## Intro: JAX and Flax\n-\n-[JAX](https://github.com/google/jax) is a numerical computation library that exposes a NumPy-like API with tracing capabilities. With JAX's `jit`, you can\n-trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU. JAX\n-supports additional transformations such as `grad` (for arbitrary gradients), `pmap` (for parallelizing computation on multiple devices), `remat` (for gradient checkpointing), `vmap` (automatic\n-efficient vectorization), and `pjit` (for automatically sharded model parallelism). All JAX transformations compose arbitrarily with each other -- e.g., efficiently\n-computing per-example gradients is simply `vmap(grad(f))`.\n-\n-[Flax](https://github.com/google/flax) builds on top of JAX with an ergonomic\n-module abstraction using Python dataclasses that leads to concise and explicit code. Flax's \"lifted\" JAX transformations (e.g. `vmap`, `remat`) allow you to nest JAX transformation and modules in any way you wish. Flax is the most widely used JAX library, with [129 dependent projects](https://github.com/google/flax/network/dependents?package_id=UGFja2FnZS01MjEyMjA2MA%3D%3D) as of May 2021. It is also the library underlying all of the official Cloud TPU JAX examples.\n-\n-## Running on Cloud TPU\n-\n-All of our JAX/Flax models are designed to run efficiently on Google\n-Cloud TPUs. Here is [a guide for running JAX on Google Cloud TPU](https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm).\n-\n-Consider applying for the [Google TPU Research Cloud project](https://sites.research.google/trc/) for free TPU compute.\n-\n-Each example README contains more details on the specific model and training\n-procedure.\n-\n-\n-## Running on single or multiple GPUs\n-\n-All of our JAX/Flax examples also run efficiently on single and multiple GPUs. You can use the same instructions in the README to launch training on GPU.\n-Distributed training is supported out-of-the box and scripts will use all the GPUs that are detected.\n-\n-You should follow this [guide for installing JAX on GPUs](https://github.com/google/jax/#pip-installation-gpu-cuda) since the installation depends on\n-your CUDA and CuDNN version.\n-\n-## Supported models\n-\n-Porting models from PyTorch to JAX/Flax is an ongoing effort. \n-Feel free to reach out if you are interested in contributing a model in JAX/Flax -- we'll \n-be adding a guide for porting models from PyTorch in the upcoming few weeks.\n-\n-For a complete overview of models that are supported in JAX/Flax, please have a look at [this](https://huggingface.co/transformers/main/index.html#supported-frameworks) table.\n-\n-Over 3000 pretrained checkpoints are supported in JAX/Flax as of May 2021.\n-Click [here](https://huggingface.co/models?filter=jax) to see the full list on the ðŸ¤— hub.\n-\n-## Upload the trained/fine-tuned model to the Hub\n-\n-All the example scripts support automatic upload of your final model to the [Model Hub](https://huggingface.co/models) by adding a `--push_to_hub` argument. It will then create a repository with your username slash the name of the folder you are using as `output_dir`. For instance, `\"sgugger/test-mrpc\"` if your username is `sgugger` and you are working in the folder `~/tmp/test-mrpc`.\n-\n-To specify a given repository name, use the `--hub_model_id` argument. You will need to specify the whole repository name (including your username), for instance `--hub_model_id sgugger/finetuned-bert-mrpc`. To upload to an organization you are a member of, just use the name of that organization instead of your username: `--hub_model_id huggingface/finetuned-bert-mrpc`.\n-\n-A few notes on this integration:\n-\n-- you will need to be logged in to the Hugging Face website locally for it to work, the easiest way to achieve this is to run `hf auth login` and then type your username and password when prompted. You can also pass along your authentication token with the `--hub_token` argument.\n-- the `output_dir` you pick will either need to be a new folder or a local clone of the distant repository you are using."
        },
        {
            "sha": "2e93a1f2c549fffb24f6fa404b67b70323dea051",
            "filename": "examples/flax/_tests_requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2F_tests_requirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2F_tests_requirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2F_tests_requirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,10 +0,0 @@\n-datasets >= 1.13.3\n-pytest<8.0.1\n-conllu\n-nltk\n-rouge-score\n-seqeval\n-tensorboard\n-evaluate >= 0.2.0\n-torch\n-accelerate"
        },
        {
            "sha": "4cf2e46ef07393af6c9ad4f637ee76e442ba8707",
            "filename": "examples/flax/conftest.py",
            "status": "removed",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fconftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fconftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fconftest.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,45 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-# tests directory-specific settings - this file is run automatically\n-# by pytest before any tests are run\n-\n-import sys\n-import warnings\n-from os.path import abspath, dirname, join\n-\n-\n-# allow having multiple repository checkouts and not needing to remember to rerun\n-# `pip install -e '.[dev]'` when switching between checkouts and running tests.\n-git_repo_path = abspath(join(dirname(dirname(dirname(__file__))), \"src\"))\n-sys.path.insert(1, git_repo_path)\n-\n-\n-# silence FutureWarning warnings in tests since often we can't act on them until\n-# they become normal warnings - i.e. the tests still need to test the current functionality\n-warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n-\n-\n-def pytest_addoption(parser):\n-    from transformers.testing_utils import pytest_addoption_shared\n-\n-    pytest_addoption_shared(parser)\n-\n-\n-def pytest_terminal_summary(terminalreporter):\n-    from transformers.testing_utils import pytest_terminal_summary_main\n-\n-    make_reports = terminalreporter.config.getoption(\"--make-reports\")\n-    if make_reports:\n-        pytest_terminal_summary_main(terminalreporter, id=make_reports)"
        },
        {
            "sha": "dd2b420639258fbfc31a610d3d1d7609d07a691b",
            "filename": "examples/flax/image-captioning/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,68 +0,0 @@\n-# Image Captioning (vision-encoder-text-decoder model) training example\n-\n-The following example showcases how to finetune a vision-encoder-text-decoder model for image captioning\n-using the JAX/Flax backend, leveraging ðŸ¤— Transformers library's [FlaxVisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/vision-encoder-decoder#transformers.FlaxVisionEncoderDecoderModel).\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-`run_image_captioning_flax.py` is a lightweight example of how to download and preprocess a dataset from the ðŸ¤— Datasets\n-library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n-\n-For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files and you also will find examples of these below.\n-\n-### Download COCO dataset (2017)\n-This example uses COCO dataset (2017) through a custom dataset script, which requires users to manually download the\n-COCO dataset before training.\n-\n-```bash\n-mkdir data\n-cd data\n-wget http://images.cocodataset.org/zips/train2017.zip\n-wget http://images.cocodataset.org/zips/val2017.zip\n-wget http://images.cocodataset.org/zips/test2017.zip\n-wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n-wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n-cd ..\n-```\n-\n-### Create a model from a vision encoder model and a text decoder model\n-Next, we create a [FlaxVisionEncoderDecoderModel](https://huggingface.co/docs/transformers/model_doc/visionencoderdecoder#transformers.FlaxVisionEncoderDecoderModel) instance from a pre-trained vision encoder ([ViT](https://huggingface.co/docs/transformers/model_doc/vit#transformers.FlaxViTModel)) and a pre-trained text decoder ([GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2#transformers.FlaxGPT2Model)):\n-\n-```bash\n-python3 create_model_from_encoder_decoder_models.py \\\n-    --output_dir model \\\n-    --encoder_model_name_or_path google/vit-base-patch16-224-in21k \\\n-    --decoder_model_name_or_path openai-community/gpt2\n-```\n-\n-### Train the model\n-Finally, we can run the example script to train the model:\n-\n-```bash\n-python3 run_image_captioning_flax.py \\\n-\t--output_dir ./image-captioning-training-results \\\n-\t--model_name_or_path model \\\n-\t--dataset_name ydshieh/coco_dataset_script \\\n-\t--dataset_config_name=2017 \\\n-\t--data_dir $PWD/data \\\n-\t--image_column image_path \\\n-\t--caption_column caption \\\n-\t--do_train --do_eval --predict_with_generate \\\n-\t--num_train_epochs 1 \\\n-\t--eval_steps 500 \\\n-\t--learning_rate 3e-5 --warmup_steps 0 \\\n-\t--per_device_train_batch_size 32 \\\n-\t--per_device_eval_batch_size 32 \\\n-\t--overwrite_output_dir \\\n-\t--max_target_length 32 \\\n-\t--num_beams 8 \\\n-\t--preprocessing_num_workers 16 \\\n-\t--logging_steps 10 \\\n-\t--block_size 16384 \\\n-\t--push_to_hub\n-```\n-\n-This should finish in about 1h30 on Cloud TPU, with validation loss and ROUGE2 score of 2.0153 and 14.64 respectively\n-after 1 epoch. Training statistics can be accessed on [Models](https://huggingface.co/ydshieh/image-captioning-training-results/tensorboard)."
        },
        {
            "sha": "557bfcd27a3308b93480028848017dcb987ad0a5",
            "filename": "examples/flax/image-captioning/create_model_from_encoder_decoder_models.py",
            "status": "removed",
            "additions": 0,
            "deletions": 115,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2Fcreate_model_from_encoder_decoder_models.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,115 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2022 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Create a VisionEncoderDecoderModel instance from pretrained encoder/decoder models.\n-\n-The cross-attention will be randomly initialized.\n-\"\"\"\n-\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-from transformers import AutoConfig, AutoImageProcessor, AutoTokenizer, FlaxVisionEncoderDecoderModel, HfArgumentParser\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model will be written.\"},\n-    )\n-    encoder_model_name_or_path: str = field(\n-        metadata={\n-            \"help\": (\n-                \"The encoder model checkpoint for weights initialization. \"\n-                \"Don't set if you want to train an encoder model from scratch.\"\n-            )\n-        },\n-    )\n-    decoder_model_name_or_path: str = field(\n-        metadata={\n-            \"help\": (\n-                \"The decoder model checkpoint for weights initialization. \"\n-                \"Don't set if you want to train a decoder model from scratch.\"\n-            )\n-        },\n-    )\n-    encoder_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained encoder config name or path if not the same as encoder_model_name\"}\n-    )\n-    decoder_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained decoder config name or path if not the same as decoder_model_name\"}\n-    )\n-\n-\n-def main():\n-    parser = HfArgumentParser((ModelArguments,))\n-    (model_args,) = parser.parse_args_into_dataclasses()\n-\n-    # Load pretrained model and tokenizer\n-\n-    # Use explicit specified encoder config\n-    if model_args.encoder_config_name:\n-        encoder_config = AutoConfig.from_pretrained(model_args.encoder_config_name)\n-    # Use pretrained encoder model's config\n-    else:\n-        encoder_config = AutoConfig.from_pretrained(model_args.encoder_model_name_or_path)\n-\n-    # Use explicit specified decoder config\n-    if model_args.decoder_config_name:\n-        decoder_config = AutoConfig.from_pretrained(model_args.decoder_config_name)\n-    # Use pretrained decoder model's config\n-    else:\n-        decoder_config = AutoConfig.from_pretrained(model_args.decoder_model_name_or_path)\n-\n-    # necessary for `from_encoder_decoder_pretrained` when `decoder_config` is passed\n-    decoder_config.is_decoder = True\n-    decoder_config.add_cross_attention = True\n-\n-    model = FlaxVisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n-        encoder_pretrained_model_name_or_path=model_args.encoder_model_name_or_path,\n-        decoder_pretrained_model_name_or_path=model_args.decoder_model_name_or_path,\n-        encoder_config=encoder_config,\n-        decoder_config=decoder_config,\n-    )\n-\n-    # GPT2 only has bos/eos tokens but not decoder_start/pad tokens\n-    decoder_start_token_id = decoder_config.decoder_start_token_id\n-    pad_token_id = decoder_config.pad_token_id\n-    if decoder_start_token_id is None:\n-        decoder_start_token_id = decoder_config.bos_token_id\n-    if pad_token_id is None:\n-        pad_token_id = decoder_config.eos_token_id\n-\n-    # This is necessary to make Flax's generate() work\n-    model.config.eos_token_id = decoder_config.eos_token_id\n-    model.config.decoder_start_token_id = decoder_start_token_id\n-    model.config.pad_token_id = pad_token_id\n-\n-    image_processor = AutoImageProcessor.from_pretrained(model_args.encoder_model_name_or_path)\n-\n-    tokenizer = AutoTokenizer.from_pretrained(model_args.decoder_model_name_or_path)\n-    tokenizer.pad_token = tokenizer.convert_ids_to_tokens(model.config.pad_token_id)\n-\n-    model.save_pretrained(model_args.output_dir)\n-    image_processor.save_pretrained(model_args.output_dir)\n-    tokenizer.save_pretrained(model_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "414e1c62e305e7824b2ac333b775c4cd97339db2",
            "filename": "examples/flax/image-captioning/run_image_captioning_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1283,
            "changes": 1283,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,1283 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2022 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library vision-encoder-decoder models for image captioning.\n-\"\"\"\n-\n-import json\n-import logging\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from functools import partial\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import datasets\n-import evaluate\n-import jax\n-import jax.numpy as jnp\n-import nltk  # Here to have a nice missing dependency error message early on\n-import numpy as np\n-import optax\n-from datasets import Dataset, load_dataset\n-from filelock import FileLock\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n-from huggingface_hub import HfApi\n-from PIL import Image\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    AutoImageProcessor,\n-    AutoTokenizer,\n-    FlaxVisionEncoderDecoderModel,\n-    HfArgumentParser,\n-    is_tensorboard_available,\n-)\n-from transformers.utils import is_offline_mode, send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-try:\n-    nltk.data.find(\"tokenizers/punkt\")\n-except (LookupError, OSError):\n-    if is_offline_mode():\n-        raise LookupError(\n-            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n-        )\n-    with FileLock(\".lock\") as lock:\n-        nltk.download(\"punkt\", quiet=True)\n-\n-\n-# Copied from transformers.models.bart.modeling_flax_bart.shift_tokens_right\n-def shift_tokens_right(input_ids: np.ndarray, pad_token_id: int, decoder_start_token_id: int) -> np.ndarray:\n-    \"\"\"\n-    Shift input ids one token to the right.\n-    \"\"\"\n-    shifted_input_ids = np.zeros_like(input_ids)\n-    shifted_input_ids[:, 1:] = input_ids[:, :-1]\n-    shifted_input_ids[:, 0] = decoder_start_token_id\n-\n-    shifted_input_ids = np.where(shifted_input_ids == -100, pad_token_id, shifted_input_ids)\n-    return shifted_input_ids\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    _block_size_doc = \"\"\"\n-        The default value `0` will preprocess (tokenization + image processing) the whole dataset before training and\n-        cache the results. This uses more disk space, but avoids (repeated) processing time during training. This is a\n-        good option if your disk space is large enough to store the whole processed dataset.\n-        If a positive value is given, the captions in the dataset will be tokenized before training and the results are\n-        cached. During training, it iterates the dataset in chunks of size `block_size`. On each block, images are\n-        transformed by the image processor with the results being kept in memory (no cache), and batches of size\n-        `batch_size` are yielded before processing the next block. This could avoid the heavy disk usage when the\n-        dataset is large.\n-        \"\"\"\n-    block_size: int = field(default=0, metadata={\"help\": _block_size_doc})\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    label_smoothing_factor: float = field(\n-        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n-    )\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"The model checkpoint for weights initialization.\"},\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    data_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The data directory of the dataset to use (via the datasets library).\"}\n-    )\n-    image_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n-    )\n-    caption_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n-    )\n-    max_target_length: Optional[int] = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    val_max_target_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. \"\n-                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n-                \"during evaluation.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    predict_with_generate: bool = field(\n-        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n-    )\n-    num_beams: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n-                \"which is used during evaluation.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\"]:\n-                    raise ValueError(f\"`train_file` should be a csv or a json file, got {extension}.\")\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\"]:\n-                    raise ValueError(f\"`validation_file` should be a csv or a json file, got {extension}.\")\n-        if self.val_max_target_length is None:\n-            self.val_max_target_length = self.max_target_length\n-\n-\n-image_captioning_name_mapping = {\n-    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n-}\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False):\n-    \"\"\"\n-    Returns batches of size `batch_size` from truncated `dataset`, sharded over all local devices.\n-    Shuffle batches if `shuffle` is `True`.\n-    \"\"\"\n-    steps = len(dataset) // batch_size  # Skip incomplete batch.\n-\n-    # We use `numpy.ndarray` to interact with `datasets.Dataset`, since using `jax.numpy.array` to index into a\n-    # dataset is significantly slow. Using JAX array at the 1st place is only to keep JAX's PRNGs generation\n-    # mechanism, which works differently from NumPy/SciPy.\n-    if shuffle:\n-        batch_idx = jax.random.permutation(rng, len(dataset))\n-        batch_idx = np.asarray(batch_idx)\n-    else:\n-        batch_idx = np.arange(len(dataset))\n-\n-    for idx in range(steps):\n-        start_idx = batch_size * idx\n-        end_idx = batch_size * (idx + 1)\n-\n-        selected_indices = batch_idx[start_idx:end_idx]\n-        batch = dataset[selected_indices]\n-        batch = shard(batch)\n-\n-        yield batch\n-\n-\n-def write_metric(summary_writer, metrics, train_time, step, metric_key_prefix=\"train\"):\n-    if train_time:\n-        summary_writer.scalar(\"train_time\", train_time, step)\n-\n-        metrics = get_metrics(metrics)\n-        for key, vals in metrics.items():\n-            tag = f\"{metric_key_prefix}_{key}\"\n-            for i, val in enumerate(vals):\n-                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    else:\n-        for metric_name, value in metrics.items():\n-            summary_writer.scalar(f\"{metric_key_prefix}_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_image_captioning\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files this script will use the first column for the full image path and the second column for the\n-    # captions (unless you specify column names for this with the `image_column` and `caption_column` arguments).\n-    #\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            keep_in_memory=False,\n-            data_dir=data_args.data_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        dataset = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-    model = FlaxVisionEncoderDecoderModel.from_pretrained(\n-        model_args.model_name_or_path,\n-        seed=training_args.seed,\n-        dtype=getattr(jnp, model_args.dtype),\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    image_processor = AutoImageProcessor.from_pretrained(\n-        model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast_tokenizer,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer.pad_token = tokenizer.convert_ids_to_tokens(model.config.pad_token_id)\n-\n-    # Preprocessing the datasets.\n-    # We need to tokenize inputs and targets.\n-    if training_args.do_train:\n-        column_names = dataset[\"train\"].column_names\n-    elif training_args.do_eval:\n-        column_names = dataset[\"validation\"].column_names\n-    elif training_args.do_predict:\n-        column_names = dataset[\"test\"].column_names\n-    else:\n-        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n-        return\n-\n-    # Get the column names for input/target.\n-    dataset_columns = image_captioning_name_mapping.get(data_args.dataset_name, None)\n-    if data_args.image_column is None:\n-        if dataset_columns is None:\n-            raise ValueError(\n-                f\"`--dataset_name` {data_args.dataset_name} not found in dataset '{data_args.dataset_name}'. Make sure\"\n-                \" to set `--dataset_name` to the correct dataset name, one of\"\n-                f\" {', '.join(image_captioning_name_mapping.keys())}.\"\n-            )\n-        image_column = dataset_columns[0]\n-    else:\n-        image_column = data_args.image_column\n-        if image_column not in column_names:\n-            raise ValueError(\n-                f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-    if data_args.caption_column is None:\n-        if dataset_columns is None:\n-            raise ValueError(\n-                f\"`--dataset_name` {data_args.dataset_name} not found in dataset '{data_args.dataset_name}'. Make sure\"\n-                \" to set `--dataset_name` to the correct dataset name, one of\"\n-                f\" {', '.join(image_captioning_name_mapping.keys())}.\"\n-            )\n-        caption_column = dataset_columns[1]\n-    else:\n-        caption_column = data_args.caption_column\n-        if caption_column not in column_names:\n-            raise ValueError(\n-                f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-\n-    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n-    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n-    # for that dynamically import the `shift_tokens_right` function from the model file\n-    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_right\"])\n-    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\", shift_tokens_right)\n-\n-    def filter_fn(examples):\n-        \"\"\"remove problematic images\"\"\"\n-\n-        bools = []\n-        for image_file in examples[image_column]:\n-            try:\n-                image = Image.open(image_file)\n-                image_processor(images=image, return_tensors=\"np\")\n-                bools.append(True)\n-            except Exception:\n-                bools.append(False)\n-\n-        return bools\n-\n-    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n-    def tokenization_fn(examples, max_target_length):\n-        \"\"\"Run tokenization on captions.\"\"\"\n-\n-        captions = []\n-        for caption in examples[caption_column]:\n-            captions.append(caption.lower() + \" \" + tokenizer.eos_token)\n-        targets = captions\n-\n-        model_inputs = {}\n-\n-        labels = tokenizer(\n-            text_target=targets,\n-            max_length=max_target_length,\n-            padding=\"max_length\",\n-            truncation=True,\n-            return_tensors=\"np\",\n-        )\n-        model_inputs[\"labels\"] = labels[\"input_ids\"]\n-        decoder_input_ids = shift_tokens_right_fn(\n-            labels[\"input_ids\"], model.config.pad_token_id, model.config.decoder_start_token_id\n-        )\n-        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n-        # We need decoder_attention_mask so we can ignore pad tokens from loss\n-        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n-        model_inputs[image_column] = examples[image_column]\n-\n-        return model_inputs\n-\n-    def image_processing_fn(examples, check_image=True):\n-        \"\"\"\n-        Run preprocessing on images\n-\n-        If `check_image` is `True`, the examples that fails during `Image.open()` will be caught and discarded.\n-        Otherwise, an exception will be thrown.\n-        \"\"\"\n-\n-        model_inputs = {}\n-\n-        if check_image:\n-            images = []\n-            to_keep = []\n-            for image_file in examples[image_column]:\n-                try:\n-                    img = Image.open(image_file)\n-                    images.append(img)\n-                    to_keep.append(True)\n-                except Exception:\n-                    to_keep.append(False)\n-\n-            for k, v in examples.items():\n-                if k != image_column:\n-                    model_inputs[k] = v[to_keep]\n-        else:\n-            images = [Image.open(image_file) for image_file in examples[image_column]]\n-\n-        encoder_inputs = image_processor(images=images, return_tensors=\"np\")\n-        model_inputs[\"pixel_values\"] = encoder_inputs.pixel_values\n-\n-        return model_inputs\n-\n-    def preprocess_fn(examples, max_target_length, check_image=True):\n-        \"\"\"Run tokenization + image processing\"\"\"\n-\n-        model_inputs = {}\n-        # This contains image path column\n-        model_inputs.update(tokenization_fn(examples, max_target_length))\n-        model_inputs.update(image_processing_fn(model_inputs, check_image=check_image))\n-        # Remove image path column\n-        model_inputs.pop(image_column)\n-\n-        return model_inputs\n-\n-    features = datasets.Features(\n-        {\n-            \"pixel_values\": datasets.Array3D(\n-                shape=(\n-                    getattr(model.config.encoder, \"num_channels\", 3),\n-                    model.config.encoder.image_size,\n-                    model.config.encoder.image_size,\n-                ),\n-                dtype=\"float32\",\n-            ),\n-            \"labels\": datasets.Sequence(feature=datasets.Value(dtype=\"int32\", id=None), length=-1, id=None),\n-            \"decoder_input_ids\": datasets.Sequence(feature=datasets.Value(dtype=\"int32\", id=None), length=-1, id=None),\n-            \"decoder_attention_mask\": datasets.Sequence(\n-                feature=datasets.Value(dtype=\"int32\", id=None), length=-1, id=None\n-            ),\n-        }\n-    )\n-\n-    # If `block_size` is `0`, tokenization & image processing is done at the beginning\n-    run_img_proc_at_beginning = training_args.block_size == 0\n-    # Used in .map() below\n-    function_kwarg = preprocess_fn if run_img_proc_at_beginning else tokenization_fn\n-    # `features` is used only for the final preprocessed dataset (for the performance purpose).\n-    features_kwarg = features if run_img_proc_at_beginning else None\n-    # Keep `image_column` if the image processing is done during training\n-    remove_columns_kwarg = [x for x in column_names if x != image_column or run_img_proc_at_beginning]\n-    processor_names = \"tokenizer and image processor\" if run_img_proc_at_beginning else \"tokenizer\"\n-\n-    # Store some constant\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    eval_batch_size = int(training_args.per_device_eval_batch_size) * jax.device_count()\n-    if training_args.block_size % train_batch_size > 0 or training_args.block_size % eval_batch_size > 0:\n-        raise ValueError(\n-            \"`training_args.block_size` needs to be a multiple of the global train/eval batch size. \"\n-            f\"Got {training_args.block_size}, {train_batch_size} and {eval_batch_size} respectively instead.\"\n-        )\n-\n-    if training_args.do_train:\n-        if \"train\" not in dataset:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = dataset[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        # remove problematic examples\n-        # (if image processing is performed at the beginning, the filtering is done during preprocessing below\n-        # instead here.)\n-        if not run_img_proc_at_beginning:\n-            train_dataset = train_dataset.filter(filter_fn, batched=True, num_proc=data_args.preprocessing_num_workers)\n-        train_dataset = train_dataset.map(\n-            function=function_kwarg,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            # kept image paths\n-            remove_columns=remove_columns_kwarg,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=f\"Running {processor_names} on train dataset\",\n-            fn_kwargs={\"max_target_length\": data_args.max_target_length},\n-            features=features_kwarg,\n-        )\n-        if run_img_proc_at_beginning:\n-            # set format (for performance) since the dataset is ready to be used\n-            train_dataset = train_dataset.with_format(\"numpy\")\n-\n-        steps_per_epoch = len(train_dataset) // train_batch_size\n-        num_train_examples_per_epoch = steps_per_epoch * train_batch_size\n-        num_epochs = int(training_args.num_train_epochs)\n-        total_train_steps = steps_per_epoch * num_epochs\n-    else:\n-        num_train_examples_per_epoch = 0\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in dataset:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = dataset[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        # remove problematic examples\n-        # (if image processing is performed at the beginning, the filtering is done during preprocessing below\n-        # instead here.)\n-        if not run_img_proc_at_beginning:\n-            eval_dataset = eval_dataset.filter(filter_fn, batched=True, num_proc=data_args.preprocessing_num_workers)\n-        eval_dataset = eval_dataset.map(\n-            function=function_kwarg,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            # kept image paths\n-            remove_columns=remove_columns_kwarg,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=f\"Running {processor_names} on validation dataset\",\n-            fn_kwargs={\"max_target_length\": data_args.val_max_target_length},\n-            features=features_kwarg,\n-        )\n-        if run_img_proc_at_beginning:\n-            # set format (for performance) since the dataset is ready to be used\n-            eval_dataset = eval_dataset.with_format(\"numpy\")\n-\n-        num_eval_examples = len(eval_dataset)\n-        eval_steps = num_eval_examples // eval_batch_size\n-\n-    if training_args.do_predict:\n-        if \"test\" not in dataset:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        predict_dataset = dataset[\"test\"]\n-        if data_args.max_predict_samples is not None:\n-            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n-            predict_dataset = predict_dataset.select(range(max_predict_samples))\n-        # remove problematic examples\n-        # (if image processing is performed at the beginning, the filtering is done during preprocessing below\n-        # instead here.)\n-        if not run_img_proc_at_beginning:\n-            predict_dataset = predict_dataset.filter(\n-                filter_fn, batched=True, num_proc=data_args.preprocessing_num_workers\n-            )\n-        predict_dataset = predict_dataset.map(\n-            function=function_kwarg,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            # kept image paths\n-            remove_columns=remove_columns_kwarg,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=f\"Running {processor_names} on prediction dataset\",\n-            fn_kwargs={\"max_target_length\": data_args.val_max_target_length},\n-            features=features_kwarg,\n-        )\n-        if run_img_proc_at_beginning:\n-            # set format (for performance) since the dataset is ready to be used\n-            predict_dataset = predict_dataset.with_format(\"numpy\")\n-\n-        num_test_examples = len(predict_dataset)\n-        test_steps = num_test_examples // eval_batch_size\n-\n-    def blockwise_data_loader(\n-        rng: jax.random.PRNGKey,\n-        ds: Dataset,\n-        block_size: int,\n-        batch_size: int,\n-        shuffle: bool = False,\n-        keep_in_memory: bool = False,\n-        split: str = \"\",\n-    ):\n-        \"\"\"\n-        Wrap the simple `data_loader` in a block-wise way if `block_size` > 0, else it's the same as `data_loader`.\n-\n-        If `block_size` > 0, it requires `ds` to have a column that gives image paths in order to perform image\n-        processing (with the column name being specified by `image_column`). The tokenization should be done before\n-        training in this case.\n-        \"\"\"\n-\n-        # We use `numpy.ndarray` to interact with `datasets.Dataset`, since using `jax.numpy.array` to index into a\n-        # dataset is significantly slow. Using JAX array at the 1st place is only to keep JAX's PRNGs generation\n-        # mechanism, which works differently from NumPy/SciPy.\n-        if shuffle:\n-            indices = jax.random.permutation(rng, len(ds))\n-            indices = np.asarray(indices)\n-        else:\n-            indices = np.arange(len(ds))\n-\n-        _block_size = len(ds) if not block_size else block_size\n-\n-        steps_per_block = _block_size // batch_size\n-        num_examples = len(ds)\n-        steps = num_examples // batch_size\n-        num_splits = steps // steps_per_block + int(steps % steps_per_block > 0)\n-\n-        for idx in range(num_splits):\n-            if not block_size:\n-                _ds = ds\n-            else:\n-                start_idx = block_size * idx\n-                end_idx = block_size * (idx + 1)\n-\n-                selected_indices = indices[start_idx:end_idx]\n-\n-                _ds = ds.select(selected_indices)\n-\n-                _ds = _ds.map(\n-                    image_processing_fn,\n-                    batched=True,\n-                    num_proc=data_args.preprocessing_num_workers,\n-                    remove_columns=[image_column],\n-                    load_from_cache_file=not data_args.overwrite_cache,\n-                    features=features,\n-                    keep_in_memory=keep_in_memory,\n-                    # The images are already checked either in `.filter()` or in `preprocess_fn()`\n-                    fn_kwargs={\"check_image\": False},\n-                    desc=f\"Running image processing on {split} dataset\".replace(\"  \", \" \"),\n-                )\n-                _ds = _ds.with_format(\"numpy\")\n-\n-            # No need to shuffle here\n-            loader = data_loader(rng, _ds, batch_size=batch_size, shuffle=False)\n-\n-            yield from loader\n-\n-    # Metric\n-    metric = evaluate.load(\"rouge\", cache_dir=model_args.cache_dir)\n-\n-    def postprocess_text(preds, labels):\n-        preds = [pred.strip() for pred in preds]\n-        labels = [label.strip() for label in labels]\n-\n-        # rougeLSum expects newline after each sentence\n-        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n-        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n-\n-        return preds, labels\n-\n-    def compute_metrics(preds, labels):\n-        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n-        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-\n-        # Some simple post-processing\n-        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n-\n-        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n-        # Extract a few results from ROUGE\n-        result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n-\n-        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n-        result[\"gen_len\"] = np.mean(prediction_lens)\n-        result = {k: round(v, 6) for k, v in result.items()}\n-\n-        return result, decoded_preds, decoded_labels\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        num_train_examples_per_epoch,\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n-\n-    # label smoothed cross entropy\n-    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n-        \"\"\"\n-        The label smoothing implementation is adapted from Flax's official example:\n-        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n-        \"\"\"\n-        vocab_size = logits.shape[-1]\n-        confidence = 1.0 - label_smoothing_factor\n-        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n-        normalizing_constant = -(\n-            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n-        )\n-        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n-\n-        loss = optax.softmax_cross_entropy(logits, soft_labels)\n-        loss = loss - normalizing_constant\n-\n-        # ignore padded tokens from loss\n-        loss = loss * padding_mask\n-        loss = loss.sum()\n-        num_labels = padding_mask.sum()\n-        return loss, num_labels\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, label_smoothing_factor=0.0):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n-            return loss, num_labels\n-\n-        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n-        (loss, num_labels), grad = grad_fn(state.params)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        # true grad = total grad / total samples\n-        grad = jax.lax.psum(grad, \"batch\")\n-        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch, label_smoothing_factor=0.0):\n-        labels = batch.pop(\"labels\")\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        metrics = {\"loss\": loss}\n-        return metrics\n-\n-    # Define generation function\n-    max_length = (\n-        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n-    )\n-    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n-    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n-\n-    def generate_step(params, batch):\n-        model.params = params\n-        output_ids = model.generate(batch[\"pixel_values\"], **gen_kwargs)\n-        return output_ids.sequences\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(\n-        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n-    )\n-    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n-    p_generate_step = jax.pmap(generate_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    if training_args.do_train:\n-        logger.info(\"***** Running training *****\")\n-        logger.info(f\"  Num train examples = {num_train_examples_per_epoch}\")\n-        logger.info(f\"  Num Epochs = {num_epochs}\")\n-        logger.info(f\"  Instantaneous train batch size per device = {training_args.per_device_train_batch_size}\")\n-        logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-        logger.info(f\"  Optimization steps per epoch = {steps_per_epoch}\")\n-        logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-    if training_args.do_eval:\n-        logger.info(f\"  Num evaluation examples = {num_eval_examples}\")\n-        logger.info(f\"  Instantaneous evaluation batch size per device = {training_args.per_device_eval_batch_size}\")\n-        logger.info(f\"  Total evaluation batch size (w. parallel & distributed) = {eval_batch_size}\")\n-        logger.info(f\"  Evaluation steps = {eval_steps}\")\n-    if training_args.do_predict:\n-        logger.info(f\"  Num test examples = {num_test_examples}\")\n-        logger.info(f\"  Instantaneous test batch size per device = {training_args.per_device_eval_batch_size}\")\n-        logger.info(f\"  Total test batch size (w. parallel & distributed) = {eval_batch_size}\")\n-        logger.info(f\"  Test steps = {test_steps}\")\n-\n-    # create output directory\n-    if not os.path.isdir(os.path.join(training_args.output_dir)):\n-        os.makedirs(os.path.join(training_args.output_dir), exist_ok=True)\n-\n-    def save_ckpt(ckpt_dir: str, commit_msg: str = \"\"):\n-        \"\"\"save checkpoints and push to Hugging Face Hub if specified\"\"\"\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-            model.save_pretrained(os.path.join(training_args.output_dir, ckpt_dir), params=params)\n-            tokenizer.save_pretrained(os.path.join(training_args.output_dir, ckpt_dir))\n-            if training_args.push_to_hub:\n-                api.upload_folder(\n-                    commit_message=commit_msg,\n-                    folder_path=training_args.output_dir,\n-                    repo_id=repo_id,\n-                    repo_type=\"model\",\n-                    token=training_args.hub_token,\n-                )\n-\n-    def evaluation_loop(\n-        rng: jax.random.PRNGKey,\n-        dataset: Dataset,\n-        metric_key_prefix: str = \"eval\",\n-        ckpt_dir: str = \"\",\n-        is_prediction=False,\n-    ):\n-        logger.info(f\"*** {'Predict' if is_prediction else 'Evaluate'} ***\")\n-\n-        metrics = []\n-        preds = []\n-        labels = []\n-\n-        batches = blockwise_data_loader(\n-            rng,\n-            dataset,\n-            block_size=training_args.block_size,\n-            batch_size=eval_batch_size,\n-            keep_in_memory=False,\n-            shuffle=False,\n-            split=\"prediction\" if is_prediction else \"validation\",\n-        )\n-        steps = len(dataset) // eval_batch_size\n-        for _ in tqdm(\n-            range(steps), desc=f\"{'Predicting' if is_prediction else 'Evaluating'}...\", position=2, leave=False\n-        ):\n-            # Model forward\n-            batch = next(batches)\n-            _labels = batch.get(\"labels\", None)\n-            if not is_prediction and _labels is None:\n-                raise ValueError(\"Evaluation requires the validation dataset to have `labels`\")\n-\n-            if _labels is not None:\n-                _metrics = p_eval_step(state.params, batch)\n-                metrics.append(_metrics)\n-\n-            # generation\n-            if data_args.predict_with_generate:\n-                generated_ids = p_generate_step(state.params, batch)\n-                preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n-                if _labels is not None:\n-                    labels.extend(jax.device_get(_labels.reshape(-1, _labels.shape[-1])))\n-\n-        if metrics:\n-            # normalize metrics\n-            metrics = get_metrics(metrics)\n-            metrics = jax.tree_util.tree_map(jnp.mean, metrics)\n-\n-        # compute ROUGE metrics\n-        generations = []\n-        rouge_desc = \"\"\n-        if data_args.predict_with_generate:\n-            if labels:\n-                rouge_metrics, decoded_preds, decoded_labels = compute_metrics(preds, labels)\n-                metrics.update(rouge_metrics)\n-                rouge_desc = \" \".join(\n-                    [\n-                        f\"{'Predict' if is_prediction else 'Eval'} {key}: {value} |\"\n-                        for key, value in rouge_metrics.items()\n-                    ]\n-                )\n-                for pred, label in zip(decoded_preds, decoded_labels):\n-                    pred = pred.replace(\"\\n\", \" \")\n-                    label = label.replace(\"\\n\", \" \")\n-                    generations.append({\"label\": label, \"pred\": pred})\n-            else:\n-                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n-                # Some simple post-processing\n-                decoded_preds = [pred.strip() for pred in decoded_preds]\n-                # rougeLSum expects newline after each sentence\n-                decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in decoded_preds]\n-                for pred in decoded_preds:\n-                    pred = pred.replace(\"\\n\", \" \")\n-                    generations.append({\"pred\": pred})\n-\n-        if metrics:\n-            # Print metrics and update progress bar\n-            desc = f\"{'Predict' if is_prediction else 'Eval'} Loss: {metrics['loss']} | {rouge_desc})\"\n-            if training_args.do_train and not is_prediction:\n-                desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Step: {cur_step} | \" + desc\n-                epochs.write(desc)\n-                epochs.desc = desc\n-            logger.info(desc)\n-\n-        if jax.process_index() == 0:\n-            if not os.path.isdir(os.path.join(training_args.output_dir, ckpt_dir)):\n-                os.makedirs(os.path.join(training_args.output_dir, ckpt_dir), exist_ok=True)\n-\n-            if metrics:\n-                # Save metrics (only for the evaluation/prediction being done along with training)\n-                if has_tensorboard and training_args.do_train:\n-                    write_metric(\n-                        summary_writer, metrics, train_time=None, step=cur_step, metric_key_prefix=metric_key_prefix\n-                    )\n-\n-                # save final metrics in json\n-                metrics = {\n-                    f\"{metric_key_prefix}_{metric_name}\": round(value.item(), 6)\n-                    for metric_name, value in metrics.items()\n-                }\n-                _path = os.path.join(training_args.output_dir, ckpt_dir, f\"{metric_key_prefix}_results.json\")\n-                with open(_path, \"w\") as f:\n-                    json.dump(metrics, f, indent=4, sort_keys=True)\n-\n-                # Update report\n-                with open(os.path.join(training_args.output_dir, \"log\"), \"a\", encoding=\"UTF-8\") as fp:\n-                    fp.write(desc + \"\\n\")\n-\n-            # Save generations\n-            if generations:\n-                output_file = os.path.join(training_args.output_dir, ckpt_dir, f\"{metric_key_prefix}_generation.json\")\n-                with open(output_file, \"w\", encoding=\"UTF-8\") as fp:\n-                    json.dump(generations, fp, ensure_ascii=False, indent=4)\n-\n-    def evaluate(rng: jax.random.PRNGKey, dataset: Dataset, ckpt_dir: str = \"\"):\n-        evaluation_loop(rng, dataset, metric_key_prefix=\"eval\", ckpt_dir=ckpt_dir)\n-\n-    def predict(rng: jax.random.PRNGKey, dataset: Dataset):\n-        evaluation_loop(rng, dataset, metric_key_prefix=\"test\", is_prediction=True)\n-\n-    input_rng = None\n-\n-    if training_args.do_train:\n-        cur_step = 0\n-        train_time = 0\n-        epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-\n-        for epoch in epochs:\n-            # ======================== Training ================================\n-            # Create sampling rng\n-            rng, input_rng = jax.random.split(rng)\n-\n-            train_metrics = []\n-            train_batches = blockwise_data_loader(\n-                input_rng,\n-                train_dataset,\n-                block_size=training_args.block_size,\n-                batch_size=train_batch_size,\n-                keep_in_memory=True,\n-                shuffle=True,\n-                split=\"train\",\n-            )\n-\n-            # train\n-            for batch_idx, _ in enumerate(tqdm(range(steps_per_epoch), desc=\"Training...\", position=1, leave=False)):\n-                cur_step += 1\n-                batch = next(train_batches)\n-                batch_start = time.time()\n-                state, train_metric = p_train_step(state, batch)\n-                train_metrics.append(train_metric)\n-                train_time += time.time() - batch_start\n-                time_per_step = train_time / cur_step\n-\n-                # log and save info\n-                if training_args.logging_steps > 0 and cur_step % training_args.logging_steps == 0:\n-                    _train_metric = unreplicate(train_metric)\n-                    desc = (\n-                        f\"Epoch... ({epoch + 1}/{num_epochs} | Step: {cur_step} | Loss: {_train_metric['loss']} |\"\n-                        f\" Learning Rate: {_train_metric['learning_rate']} | Time per step: {time_per_step})\"\n-                    )\n-                    epochs.desc = desc\n-                    epochs.write(desc)\n-\n-                    logger.info(desc)\n-\n-                    with open(os.path.join(training_args.output_dir, \"log\"), \"a\", encoding=\"UTF-8\") as fp:\n-                        fp.write(desc + \"\\n\")\n-\n-                    # Save metrics\n-                    if has_tensorboard and jax.process_index() == 0:\n-                        write_metric(\n-                            summary_writer,\n-                            train_metrics,\n-                            train_time=train_time,\n-                            step=cur_step,\n-                            metric_key_prefix=\"train\",\n-                        )\n-\n-                # ======================== Evaluating (inside an epoch) ==============================\n-\n-                if (\n-                    training_args.do_eval\n-                    and (training_args.eval_steps is not None and training_args.eval_steps > 0)\n-                    and cur_step % training_args.eval_steps == 0\n-                ):\n-                    ckpt_dir = f\"ckpt_epoch_{epoch + 1}_step_{cur_step}\"\n-                    commit_msg = f\"Saving weights and logs of epoch {epoch + 1} - step {cur_step}\"\n-                    evaluate(input_rng, eval_dataset, ckpt_dir)\n-                    save_ckpt(ckpt_dir=ckpt_dir, commit_msg=commit_msg)\n-\n-            # ======================== Epoch End ==============================\n-\n-            # log and save info\n-            if training_args.logging_steps <= 0:\n-                logger.info(desc)\n-\n-                with open(os.path.join(training_args.output_dir, \"log\"), \"a\", encoding=\"UTF-8\") as fp:\n-                    fp.write(desc + \"\\n\")\n-\n-                # Save metrics\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_metric(\n-                        summary_writer, train_metrics, train_time=train_time, step=cur_step, metric_key_prefix=\"train\"\n-                    )\n-\n-            # ======================== Evaluating (after each epoch) ==============================\n-\n-            if training_args.do_eval and (training_args.eval_steps is None or training_args.eval_steps <= 0):\n-                ckpt_dir = f\"ckpt_epoch_{epoch + 1}_step_{cur_step}\"\n-                commit_msg = f\"Saving weights and logs of epoch {epoch + 1} - step {cur_step}\"\n-                evaluate(input_rng, eval_dataset, ckpt_dir)\n-                save_ckpt(ckpt_dir=ckpt_dir, commit_msg=commit_msg)\n-\n-    # ======================== Evaluating | Predicting ==============================\n-\n-    # Create sampling rng\n-    if input_rng is None:\n-        rng, input_rng = jax.random.split(rng)\n-\n-    # run evaluation without training\n-    if training_args.do_eval and not training_args.do_train:\n-        evaluate(input_rng, eval_dataset)\n-\n-    # run prediction after (or without) training\n-    if training_args.do_predict:\n-        predict(input_rng, predict_dataset)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "441661795ee54478792b0b7f4562d911d102cb96",
            "filename": "examples/flax/language-modeling/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 568,
            "changes": 568,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,568 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Language model training and inference examples\n-\n-The following example showcases how to train a language model from scratch\n-using the JAX/Flax backend.\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-## Masked language modeling\n-\n-In the following, we demonstrate how to train a bi-directional transformer model\n-using masked language modeling objective as introduced in [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://huggingface.co/papers/1810.04805).\n-More specifically, we demonstrate how JAX/Flax can be leveraged\n-to pre-train [**`FacebookAI/roberta-base`**](https://huggingface.co/FacebookAI/roberta-base)\n-in Norwegian on a single TPUv3-8 pod.\n-\n-The example script uses the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n-\n-To setup all relevant files for training, let's create a directory.\n-\n-```bash\n-mkdir ./norwegian-roberta-base\n-```\n-\n-### Train tokenizer\n-\n-In the first step, we train a tokenizer to efficiently process the text input for the model. Similar to how it is shown in [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train), we use a **`ByteLevelBPETokenizer`**.\n-The tokenizer is trained on the complete Norwegian dataset of OSCAR\n-and consequently saved in the cloned model directory.\n-This can take up to 10 minutes depending on your hardware â˜•.\n-\n-```python\n-from datasets import load_dataset\n-from tokenizers import trainers, Tokenizer, normalizers, ByteLevelBPETokenizer\n-\n-# load dataset\n-dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_no\", split=\"train\")\n-\n-# Instantiate tokenizer\n-tokenizer = ByteLevelBPETokenizer()\n-\n-def batch_iterator(batch_size=1000):\n-    for i in range(0, len(dataset), batch_size):\n-        yield dataset[i: i + batch_size][\"text\"]\n-\n-# Customized training\n-tokenizer.train_from_iterator(batch_iterator(), vocab_size=50265, min_frequency=2, special_tokens=[\n-    \"<s>\",\n-    \"<pad>\",\n-    \"</s>\",\n-    \"<unk>\",\n-    \"<mask>\",\n-])\n-\n-# Save files to disk\n-tokenizer.save(\"./norwegian-roberta-base/tokenizer.json\")\n-```\n-\n-### Create configuration\n-\n-Next, we create the model's configuration file. This is as simple\n-as loading and storing [`**FacebookAI/roberta-base**`](https://huggingface.co/FacebookAI/roberta-base)\n-in the local model folder:\n-\n-```python\n-from transformers import RobertaConfig\n-\n-config = RobertaConfig.from_pretrained(\"FacebookAI/roberta-base\", vocab_size=50265)\n-config.save_pretrained(\"./norwegian-roberta-base\")\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-### Train model\n-\n-Next we can run the example script to pretrain the model:\n-\n-```bash\n-python run_mlm_flax.py \\\n-    --output_dir=\"./norwegian-roberta-base\" \\\n-    --model_type=\"roberta\" \\\n-    --config_name=\"./norwegian-roberta-base\" \\\n-    --tokenizer_name=\"./norwegian-roberta-base\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-    --max_seq_length=\"128\" \\\n-    --weight_decay=\"0.01\" \\\n-    --per_device_train_batch_size=\"128\" \\\n-    --per_device_eval_batch_size=\"128\" \\\n-    --learning_rate=\"3e-4\" \\\n-    --warmup_steps=\"1000\" \\\n-    --overwrite_output_dir \\\n-    --num_train_epochs=\"18\" \\\n-    --adam_beta1=\"0.9\" \\\n-    --adam_beta2=\"0.98\" \\\n-    --logging_steps=\"500\" \\\n-    --save_steps=\"2500\" \\\n-    --eval_steps=\"2500\" \\\n-    --push_to_hub\n-```\n-\n-Training should converge at a loss and accuracy\n-of 1.78 and 0.64 respectively after 18 epochs on a single TPUv3-8.\n-This should take less than 18 hours.\n-Training statistics can be accessed on [tfhub.dev](https://tensorboard.dev/experiment/GdYmdak2TWeVz0DDRYOrrg).\n-\n-For a step-by-step walkthrough of how to do masked language modeling in Flax, please have a\n-look at [this](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/masked_language_modeling_flax.ipynb) google colab.\n-\n-## Causal language modeling\n-\n-In the following, we demonstrate how to train an auto-regressive causal transformer model\n-in JAX/Flax.\n-More specifically, we pretrain a randomly initialized [**`openai-community/gpt2`**](https://huggingface.co/openai-community/gpt2) model in Norwegian on a single TPUv3-8.\n-to pre-train 124M [**`openai-community/gpt2`**](https://huggingface.co/openai-community/gpt2)\n-in Norwegian on a single TPUv3-8 pod.\n-\n-The example script uses the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n-\n-\n-To setup all relevant files for training, let's create a directory.\n-\n-```bash\n-mkdir ./norwegian-gpt2\n-```\n-\n-### Train tokenizer\n-\n-In the first step, we train a tokenizer to efficiently process the text input for the model. Similar to how it is shown in [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train), we use a **`ByteLevelBPETokenizer`**.\n-The tokenizer is trained on the complete Norwegian dataset of OSCAR\n-and consequently saved in the cloned model directory.\n-This can take up to 10 minutes depending on your hardware â˜•.\n-\n-```python\n-from datasets import load_dataset\n-from tokenizers import trainers, Tokenizer, normalizers, ByteLevelBPETokenizer\n-\n-# load dataset\n-dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_no\", split=\"train\")\n-\n-# Instantiate tokenizer\n-tokenizer = ByteLevelBPETokenizer()\n-\n-def batch_iterator(batch_size=1000):\n-    for i in range(0, len(dataset), batch_size):\n-        yield dataset[i: i + batch_size][\"text\"]\n-\n-# Customized training\n-tokenizer.train_from_iterator(batch_iterator(), vocab_size=50257, min_frequency=2, special_tokens=[\n-    \"<s>\",\n-    \"<pad>\",\n-    \"</s>\",\n-    \"<unk>\",\n-    \"<mask>\",\n-])\n-\n-# Save files to disk\n-tokenizer.save(\"./norwegian-gpt2/tokenizer.json\")\n-```\n-\n-### Create configuration\n-\n-Next, we create the model's configuration file. This is as simple\n-as loading and storing [`**openai-community/gpt2**`](https://huggingface.co/openai-community/gpt2)\n-in the local model folder:\n-\n-```python\n-from transformers import GPT2Config\n-\n-config = GPT2Config.from_pretrained(\"openai-community/gpt2\", resid_pdrop=0.0, embd_pdrop=0.0, attn_pdrop=0.0, vocab_size=50257)\n-config.save_pretrained(\"./norwegian-gpt2\")\n-```\n-\n-Great, we have set up our model repository. During training, we will now automatically\n-push the training logs and model weights to the repo.\n-\n-### Train model\n-\n-Finally, we can run the example script to pretrain the model:\n-\n-```bash\n-python run_clm_flax.py \\\n-    --output_dir=\"./norwegian-gpt2\" \\\n-    --model_type=\"gpt2\" \\\n-    --config_name=\"./norwegian-gpt2\" \\\n-    --tokenizer_name=\"./norwegian-gpt2\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-    --do_train --do_eval \\\n-    --block_size=\"512\" \\\n-    --per_device_train_batch_size=\"64\" \\\n-    --per_device_eval_batch_size=\"64\" \\\n-    --learning_rate=\"5e-3\" --warmup_steps=\"1000\" \\\n-    --adam_beta1=\"0.9\" --adam_beta2=\"0.98\" --weight_decay=\"0.01\" \\\n-    --overwrite_output_dir \\\n-    --num_train_epochs=\"20\" \\\n-    --logging_steps=\"500\" \\\n-    --save_steps=\"2500\" \\\n-    --eval_steps=\"2500\" \\\n-    --push_to_hub\n-```\n-\n-Training should converge at a loss and perplexity\n-of 3.24 and 25.72 respectively after 20 epochs on a single TPUv3-8.\n-This should take less than ~21 hours.\n-Training statistics can be accessed on [tfhub.dev](https://tensorboard.dev/experiment/2zEhLwJ0Qp2FAkI3WVH9qA).\n-\n-For a step-by-step walkthrough of how to do causal language modeling in Flax, please have a\n-look at [this](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/causal_language_modeling_flax.ipynb) google colab.\n-\n-## T5-like span-masked language modeling\n-\n-In the following, we demonstrate how to train a T5 model using the span-masked language model\n-objective as proposed in the [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://huggingface.co/papers/1910.10683).\n-More specifically, we demonstrate how JAX/Flax can be leveraged\n-to pre-train [**`google/t5-v1_1-base`**](https://huggingface.co/google/t5-v1_1-base)\n-in Norwegian on a single TPUv3-8 pod.\n-\n-The example script uses the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n-\n-Let's start by creating a model repository to save the trained model and logs.\n-Here we call the model `\"norwegian-t5-base\"`, but you can change the model name as you like.\n-\n-To setup all relevant files for training, let's create a directory.\n-\n-```bash\n-cd ./norwegian-t5-base\n-```\n-\n-### Train tokenizer\n-\n-In the first step, we train a tokenizer to efficiently process the text input for the model.\n-We make use of the [tokenizers](https://github.com/huggingface/tokenizers) library to train\n-a sentencepiece unigram tokenizer as shown in [t5_tokenizer_model.py](https://github.com/huggingface/transformers/tree/main/examples/flax/language-modeling/t5_tokenizer_model.py)\n-which is heavily inspired from [yandex-research/DeDLOC's tokenizer model](https://github.com/yandex-research/DeDLOC/blob/5c994bc64e573702a9a79add3ecd68b38f14b548/sahajbert/tokenizer/tokenizer_model.py) .\n-\n-The tokenizer is trained on the complete Norwegian dataset of OSCAR\n-and consequently saved in the cloned model directory.\n-This can take up to 120 minutes depending on your hardware â˜•â˜•â˜• .\n-\n-```python\n-import datasets\n-\n-from t5_tokenizer_model import SentencePieceUnigramTokenizer\n-\n-\n-vocab_size = 32_000\n-input_sentence_size = None\n-\n-# Initialize a dataset\n-dataset = datasets.load_dataset(\"oscar\", name=\"unshuffled_deduplicated_no\", split=\"train\")\n-\n-tokenizer = SentencePieceUnigramTokenizer(unk_token=\"<unk>\", eos_token=\"</s>\", pad_token=\"<pad>\")\n-\n-\n-# Build an iterator over this dataset\n-def batch_iterator(input_sentence_size=None):\n-    if input_sentence_size is None:\n-        input_sentence_size = len(dataset)\n-    batch_length = 100\n-    for i in range(0, input_sentence_size, batch_length):\n-        yield dataset[i: i + batch_length][\"text\"]\n-\n-\n-# Train tokenizer\n-tokenizer.train_from_iterator(\n-    iterator=batch_iterator(input_sentence_size=input_sentence_size),\n-    vocab_size=vocab_size,\n-    show_progress=True,\n-)\n-\n-# Save files to disk\n-tokenizer.save(\"./norwegian-t5-base/tokenizer.json\")\n-```\n-\n-### Create configuration\n-\n-Next, we create the model's configuration file. This is as simple\n-as loading and storing [`**google/t5-v1_1-base**`](https://huggingface.co/google/t5-v1_1-base)\n-in the local model folder:\n-\n-```python\n-from transformers import T5Config\n-\n-config = T5Config.from_pretrained(\"google/t5-v1_1-base\", vocab_size=tokenizer.get_vocab_size())\n-config.save_pretrained(\"./norwegian-t5-base\")\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-### Train model\n-\n-Next we can run the example script to pretrain the model:\n-\n-```bash\n-python run_t5_mlm_flax.py \\\n-\t--output_dir=\"./norwegian-t5-base\" \\\n-\t--model_type=\"t5\" \\\n-\t--config_name=\"./norwegian-t5-base\" \\\n-\t--tokenizer_name=\"./norwegian-t5-base\" \\\n-\t--dataset_name=\"oscar\" \\\n-\t--dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-\t--max_seq_length=\"512\" \\\n-\t--per_device_train_batch_size=\"32\" \\\n-\t--per_device_eval_batch_size=\"32\" \\\n-\t--adafactor \\\n-\t--learning_rate=\"0.005\" \\\n-\t--weight_decay=\"0.001\" \\\n-\t--warmup_steps=\"2000\" \\\n-\t--overwrite_output_dir \\\n-\t--logging_steps=\"500\" \\\n-\t--save_steps=\"10000\" \\\n-\t--eval_steps=\"2500\" \\\n-\t--push_to_hub\n-```\n-\n-Training should converge at a loss and accuracy\n-of 2.36 and 57.0 respectively after 3 epochs on a single TPUv3-8.\n-This should take around 4.5 hours.\n-Training statistics can be accessed on directly on the ðŸ¤— [hub](https://huggingface.co/patrickvonplaten/t5-base-norwegian/tensorboard)\n-\n-## BART: Denoising language modeling\n-\n-In the following, we demonstrate how to train a BART model\n-using denoising language modeling objective as introduced in [BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension](https://huggingface.co/papers/1910.13461).\n-More specifically, we demonstrate how JAX/Flax can be leveraged\n-to pre-train [**`bart-base`**](https://huggingface.co/facebook/bart-base)\n-in Norwegian on a single TPUv3-8 pod.\n-\n-The example script uses the ðŸ¤— Datasets library. You can easily customize them to your needs if you need extra processing on your datasets.\n-\n-To setup all relevant files for training, let's create a directory.\n-\n-```bash\n-mkdir ./norwegian-bart-base\n-```\n-\n-### Train tokenizer\n-In the first step, we train a tokenizer to efficiently process the text input for the model. Similar to how it is shown in [How to train a new language model from scratch using Transformers and Tokenizers](https://huggingface.co/blog/how-to-train), we use a **`ByteLevelBPETokenizer`**.\n-The tokenizer is trained on the complete Norwegian dataset of OSCAR\n-and consequently saved in the cloned model directory.\n-This can take up to 10 minutes depending on your hardware â˜•.\n-\n-```python\n-from datasets import load_dataset\n-from tokenizers import trainers, Tokenizer, normalizers, ByteLevelBPETokenizer\n-\n-# load dataset\n-dataset = load_dataset(\"oscar\", \"unshuffled_deduplicated_no\", split=\"train\")\n-\n-# Instantiate tokenizer\n-tokenizer = ByteLevelBPETokenizer()\n-\n-def batch_iterator(batch_size=1000):\n-    for i in range(0, len(dataset), batch_size):\n-        yield dataset[i: i + batch_size][\"text\"]\n-\n-# Customized training\n-tokenizer.train_from_iterator(batch_iterator(), vocab_size=50265, min_frequency=2, special_tokens=[\n-    \"<s>\",\n-    \"<pad>\",\n-    \"</s>\",\n-    \"<unk>\",\n-    \"<mask>\",\n-])\n-\n-# Save files to disk\n-tokenizer.save(\"./norwegian-bart-base/tokenizer.json\")\n-```\n-\n-### Create configuration\n-\n-Next, we create the model's configuration file. This is as simple\n-as loading and storing [`**facebook/bart-base**`](https://huggingface.co/facebook/bart-base)\n-in the local model folder:\n-\n-```python\n-from transformers import BartConfig\n-config = BartConfig.from_pretrained(\"facebook/bart-base\", vocab_size=50265)\n-config.save_pretrained(\"./norwegian-bart-base\")\n-```\n-\n-Great, we have set up our model repository. During training, we will automatically\n-push the training logs and model weights to the repo.\n-\n-### Train model\n-\n-Next we can run the example script to pretrain the model:\n-\n-```bash\n-python run_bart_dlm_flax.py \\\n-    --output_dir=\"./norwegian-bart-base\" \\\n-    --config_name=\"./norwegian-bart-base\" \\\n-    --tokenizer_name=\"./norwegian-bart-base\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-    --max_seq_length=\"1024\" \\\n-    --per_device_train_batch_size=\"32\" \\\n-    --per_device_eval_batch_size=\"32\" \\\n-    --learning_rate=\"1e-4\" \\\n-    --warmup_steps=\"2000\" \\\n-    --overwrite_output_dir \\\n-    --logging_steps=\"500\" \\\n-    --save_steps=\"2000\" \\\n-    --eval_steps=\"2000\" \\\n-    --push_to_hub\n-```\n-\n-Training should converge at a loss and accuracy\n-of 1.36 and 0.77 respectively after 3 epochs on a single TPUv3-8.\n-This should take less than 6 hours.\n-Training statistics can be accessed on [tfhub.dev](https://tensorboard.dev/experiment/Maw62QlaSXWS0MOf2V2lbg/).\n-\n-## Runtime evaluation\n-\n-We also ran masked language modeling using PyTorch/XLA on a TPUv3-8, and PyTorch on 8 V100 GPUs. We report the\n-overall training time below.\n-For reproducibility, we state the training commands used for PyTorch/XLA and PyTorch further below.\n-\n-| Task  | [TPU v3-8 (Flax)](https://tensorboard.dev/experiment/GdYmdak2TWeVz0DDRYOrrg/)  | [TPU v3-8 (Pytorch/XLA)](https://tensorboard.dev/experiment/7Jq1kcQQRAmy12KOdXek7A/)| [8 GPU (PyTorch)](https://tensorboard.dev/experiment/PJneV8FQRxa2unPw1QnVHA)  |\n-|-------|-----------|------------|------------|\n-| MLM   |  15h32m   |  23h46m    | 44h14m     |\n-\n-*All experiments are ran on Google Cloud Platform.\n-GPU experiments are ran without further optimizations besides JAX\n-transformations. GPU experiments are ran with full precision (fp32). \"TPU v3-8\"\n-are 8 TPU cores on 4 chips (each chips has 2 cores), while \"8 GPU\" are 8 GPU chips.\n-\n-### Script to run MLM with PyTorch/XLA on TPUv3-8\n-\n-For comparison one can run the same pre-training with PyTorch/XLA on TPU. To set up PyTorch/XLA on Cloud TPU VMs, please\n-refer to [this](https://cloud.google.com/tpu/docs/pytorch-xla-ug-tpu-vm) guide.\n-Having created the tokenizer and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n-\n-```bash\n-ln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n-ln -s ~/transformers/examples/pytorch/xla_spawn.py ./\n-```\n-\n-, set the following environment variables:\n-\n-```bash\n-export XRT_TPU_CONFIG=\"localservice;0;localhost:51011\"\n-unset LD_PRELOAD\n-\n-export NUM_TPUS=8\n-export TOKENIZERS_PARALLELISM=0\n-export MODEL_DIR=\"./norwegian-roberta-base\"\n-mkdir -p ${MODEL_DIR}\n-```\n-\n-, and start training as follows:\n-\n-```bash\n-python3 xla_spawn.py --num_cores ${NUM_TPUS} run_mlm.py --output_dir=\"./runs\" \\\n-    --model_type=\"roberta\" \\\n-    --config_name=\"${MODEL_DIR}\" \\\n-    --tokenizer_name=\"${MODEL_DIR}\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-    --max_seq_length=\"128\" \\\n-    --weight_decay=\"0.01\" \\\n-    --per_device_train_batch_size=\"128\" \\\n-    --per_device_eval_batch_size=\"128\" \\\n-    --learning_rate=\"3e-4\" \\\n-    --warmup_steps=\"1000\" \\\n-    --overwrite_output_dir \\\n-    --num_train_epochs=\"18\" \\\n-    --adam_beta1=\"0.9\" \\\n-    --adam_beta2=\"0.98\" \\\n-    --do_train \\\n-    --do_eval \\\n-    --logging_steps=\"500\" \\\n-    --eval_strategy=\"epoch\" \\\n-    --report_to=\"tensorboard\" \\\n-    --save_strategy=\"no\"\n-```\n-\n-### Script to compare pre-training with PyTorch on 8 GPU V100's\n-\n-For comparison you can run the same pre-training with PyTorch on GPU. Note that we have to make use of `gradient_accumulation`\n-because the maximum batch size that fits on a single V100 GPU is 32 instead of 128.\n-Having created the tokenizer and configuration in `norwegian-roberta-base`, we create the following symbolic links:\n-\n-```bash\n-ln -s ~/transformers/examples/pytorch/language-modeling/run_mlm.py ./\n-```\n-\n-, set some environment variables:\n-\n-```bash\n-export NUM_GPUS=8\n-export TOKENIZERS_PARALLELISM=0\n-export MODEL_DIR=\"./norwegian-roberta-base\"\n-mkdir -p ${MODEL_DIR}\n-```\n-\n-, and can start training as follows:\n-\n-```bash\n-python3 -m torch.distributed.launch --nproc_per_node ${NUM_GPUS} run_mlm.py \\\n-    --output_dir=\"${MODEL_DIR}\" \\\n-    --model_type=\"roberta\" \\\n-    --config_name=\"${MODEL_DIR}\" \\\n-    --tokenizer_name=\"${MODEL_DIR}\" \\\n-    --dataset_name=\"oscar\" \\\n-    --dataset_config_name=\"unshuffled_deduplicated_no\" \\\n-    --max_seq_length=\"128\" \\\n-    --weight_decay=\"0.01\" \\\n-    --per_device_train_batch_size=\"32\" \\\n-    --per_device_eval_batch_size=\"32\" \\\n-    --gradient_accumulation=\"4\" \\\n-    --learning_rate=\"3e-4\" \\\n-    --warmup_steps=\"1000\" \\\n-    --overwrite_output_dir \\\n-    --num_train_epochs=\"18\" \\\n-    --adam_beta1=\"0.9\" \\\n-    --adam_beta2=\"0.98\" \\\n-    --do_train \\\n-    --do_eval \\\n-    --logging_steps=\"500\" \\\n-    --eval_strategy=\"steps\" \\\n-    --report_to=\"tensorboard\" \\\n-    --save_strategy=\"no\"\n-```\n-\n-## Language model inference with bfloat16\n-\n-The following example demonstrates performing inference with a language model using the JAX/Flax backend.\n-\n-The example script run_bert_flax.py uses bert-base-uncased, and the model is loaded into `FlaxBertModel`.\n-The input data are randomly generated tokens, and the model is also jitted with JAX.\n-By default, it uses float32 precision for inference. To enable bfloat16, add the flag shown in the command below.\n-\n-```bash\n-python3 run_bert_flax.py --precision bfloat16\n-> NOTE: For JAX Versions after v0.4.33 or later, users will need to set the below environment variables as a \\\n-> temporary workaround to use Bfloat16 datatype. \\\n-> This restriction is expected to be removed in future version\n-```bash\n-export XLA_FLAGS=--xla_cpu_use_thunk_runtime=false\n-```\n-bfloat16 gives better performance on GPUs and also Intel CPUs (Sapphire Rapids or later) with Advanced Matrix Extension (Intel AMX).  \n-By changing the dtype for `FlaxBertModel `to `jax.numpy.bfloat16`, you get the performance benefits of the underlying hardware.\n-```python\n-import jax\n-model = FlaxBertModel.from_pretrained(\"bert-base-uncased\", config=config, dtype=jax.numpy.bfloat16)\n-```\n-Switching from float32 to bfloat16 can increase the speed of an AWS c7i.4xlarge with Intel Sapphire Rapids by more than 2x."
        },
        {
            "sha": "f7263bf6385b907928687640ad70d2687720aab0",
            "filename": "examples/flax/language-modeling/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,5 +0,0 @@\n-datasets >= 1.1.3\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.9"
        },
        {
            "sha": "2ddadcdf99a5b09f1372a05048d9d06e5b1e9603",
            "filename": "examples/flax/language-modeling/run_bart_dlm_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 993,
            "changes": 993,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,993 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Pretraining the library models for denoising language modeling on a text file or a dataset.\n-Here is the full list of checkpoints on the hub that can be pretrained by this script:\n-https://huggingface.co/models?filter=bart\n-\"\"\"\n-# You can also adapt this script on your own denoising language modeling task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from itertools import chain\n-from pathlib import Path\n-from typing import Optional\n-\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import nltk\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_MASKED_LM_MAPPING,\n-    AutoTokenizer,\n-    BartConfig,\n-    BatchEncoding,\n-    FlaxBartForConditionalGeneration,\n-    HfArgumentParser,\n-    PreTrainedTokenizerBase,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-from transformers.models.bart.modeling_flax_bart import shift_tokens_right\n-from transformers.utils import send_example_telemetry\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_MASKED_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    train_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input train ref data file for whole word masking in Chinese.\"},\n-    )\n-    validation_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input validation ref data file for whole word masking in Chinese.\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization and masking. Sequences longer than this\"\n-                \" will be truncated. Default to the max input length of the model.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    mlm_probability: float = field(\n-        default=0.3, metadata={\"help\": \"Ratio of tokens to mask for span masked language modeling loss\"}\n-    )\n-    permute_sentence_ratio: float = field(\n-        default=1.0, metadata={\"help\": \"Ratio of sentences to be permuted in each document\"}\n-    )\n-    poisson_lambda: float = field(\n-        default=3.0, metadata={\"help\": \"Mean of Poisson distribution used to generate span-lengths to be masked\"}\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\", \"txt\"]:\n-                    raise ValueError(\"train_file` should be a csv, json or text file.\")\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\", \"txt\"]:\n-                    raise ValueError(\"`validation_file` should be a csv, json or text file.\")\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorForBartDenoisingLM:\n-    \"\"\"\n-    Data collator used for BART denoising language modeling. The code is largely copied from\n-    `<https://github.com/morganmcg1/rotobart/blob/main/data_collator.py#L223>`__.\n-    For more information on how BART denoising language modeling works, one can take a look\n-    at the `official paper <https://huggingface.co/papers/1910.13461>`__\n-    or the `official code for preprocessing <https://github.com/facebookresearch/fairseq/blob/main/fairseq/data/denoising_dataset.py>`__ .\n-    Args:\n-        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n-            The tokenizer used for encoding the data\n-        mask_ratio (:obj:`float`):\n-            The probability with which to (randomly) mask tokens in the input\n-        poisson_lambda (:obj:`float`):\n-            Mean parameter of Poisson distribution used to generate span-lengths to be masked\n-        permute_sentence_ratio (:obj:`float`):\n-            Ratio of sentences to be permuted in each document\n-        decoder_start_token_id: (:obj:`int):\n-            The decoder start token id of the model\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    decoder_start_token_id: int\n-    mask_ratio: float = 0.3\n-    poisson_lambda: float = 3.0\n-    permute_sentence_ratio: float = 1.0\n-\n-    def __post_init__(self):\n-        if self.tokenizer.mask_token is None or self.tokenizer.eos_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token or eos token which is necessary for denoising\"\n-                \" language modeling. \"\n-            )\n-\n-    def __call__(self, examples: list[dict[str, list[int]]]) -> BatchEncoding:\n-        # convert list to dict and tensorize input\n-        batch = BatchEncoding(\n-            {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n-        )\n-        batch[\"labels\"] = batch[\"input_ids\"].copy()\n-        batch[\"decoder_input_ids\"] = shift_tokens_right(\n-            batch[\"labels\"], self.tokenizer.pad_token_id, self.decoder_start_token_id\n-        )\n-        # permuting sentences\n-        do_permute = False\n-        if self.permute_sentence_ratio > 0.0:\n-            batch[\"input_ids\"] = self.permute_sentences(batch[\"input_ids\"])\n-            do_permute = True\n-\n-        # masking span of tokens (text infilling in the paper)\n-        if self.mask_ratio:\n-            batch[\"input_ids\"], batch[\"labels\"] = self.span_mask_tokens(\n-                batch[\"input_ids\"], batch[\"labels\"], do_permute\n-            )\n-\n-        # ignore pad tokens\n-        batch[\"attention_mask\"] = (batch[\"input_ids\"] != self.tokenizer.pad_token_id).astype(int)\n-        batch[\"decoder_attention_mask\"] = (batch[\"decoder_input_ids\"] != self.tokenizer.pad_token_id).astype(int)\n-        return batch\n-\n-    def permute_sentences(self, input_ids):\n-        \"\"\"\n-        Shuffle sentences in each document.\n-        \"\"\"\n-        results = input_ids.copy()\n-\n-        # find end locations of sentences\n-        end_sentence_mask = input_ids == self.tokenizer.pad_token_id\n-        sentence_ends = np.argwhere(end_sentence_mask)\n-        sentence_ends[:, 1] += 1\n-        example_has_multiple_sentences, num_sentences = np.unique(sentence_ends[:, 0], return_counts=True)\n-        num_sentences_map = dict(zip(example_has_multiple_sentences, num_sentences))\n-\n-        num_to_permute = np.ceil(num_sentences * self.permute_sentence_ratio).astype(int)\n-        num_to_permute_map = dict(zip(example_has_multiple_sentences, num_to_permute))\n-\n-        sentence_ends = np.split(sentence_ends[:, 1], np.unique(sentence_ends[:, 0], return_index=True)[1][1:])\n-        sentence_ends_map = dict(zip(example_has_multiple_sentences, sentence_ends))\n-\n-        for i in range(input_ids.shape[0]):\n-            if i not in example_has_multiple_sentences:\n-                continue\n-            substitutions = np.random.permutation(num_sentences_map[i])[: num_to_permute_map[i]]\n-            ordering = np.arange(0, num_sentences_map[i])\n-            ordering[substitutions] = substitutions[np.random.permutation(num_to_permute_map[i])]\n-\n-            # write shuffled sentences into results\n-            index = 0\n-            for j in ordering:\n-                sentence = input_ids[i, (sentence_ends_map[i][j - 1] if j > 0 else 0) : sentence_ends_map[i][j]]\n-                results[i, index : index + sentence.shape[0]] = sentence\n-                index += sentence.shape[0]\n-        return results\n-\n-    def span_mask_tokens(self, input_ids, labels, do_permute):\n-        \"\"\"\n-        Sampling text spans with span lengths drawn from a Poisson distribution and masking them.\n-        \"\"\"\n-        special_tokens_mask_labels = [\n-            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n-        ]\n-        special_tokens_mask_inputs = [\n-            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in input_ids.tolist()\n-        ]\n-        special_tokens_mask_labels = np.array(special_tokens_mask_labels, dtype=bool)\n-        special_tokens_mask_inputs = np.array(special_tokens_mask_inputs, dtype=bool)\n-\n-        # determine how many tokens we need to mask in total\n-        is_token_mask = ~(input_ids == self.tokenizer.pad_token_id) & ~special_tokens_mask_inputs\n-        num_tokens_to_mask = int(math.ceil(is_token_mask.astype(float).sum() * self.mask_ratio))\n-        if num_tokens_to_mask == 0:\n-            return input_ids, labels\n-\n-        # generate a sufficient number of span lengths\n-        span_lengths = np.random.poisson(lam=self.poisson_lambda, size=(num_tokens_to_mask,))\n-        while np.cumsum(span_lengths, 0)[-1] < num_tokens_to_mask:\n-            span_lengths = np.concatenate(\n-                [span_lengths, np.random.poisson(lam=self.poisson_lambda, size=(num_tokens_to_mask,))]\n-            )\n-\n-        # remove all spans of length 0\n-        # note that BART inserts additional mask tokens where length == 0,\n-        # which we do not implement for now as it adds additional complexity\n-        span_lengths = span_lengths[span_lengths > 0]\n-\n-        # trim to about num_tokens_to_mask tokens\n-        cutoff_idx = np.argmin(np.abs(np.cumsum(span_lengths, 0) - num_tokens_to_mask)) + 1\n-        span_lengths = span_lengths[:cutoff_idx]\n-\n-        # randomly choose starting positions for masking\n-        token_indices = np.argwhere(is_token_mask == 1)\n-        span_starts = np.random.permutation(token_indices.shape[0])[: span_lengths.shape[0]]\n-        # prepare mask\n-        masked_indices = np.array(token_indices[span_starts])\n-        mask = np.full_like(input_ids, fill_value=False)\n-\n-        # mask starting positions\n-        for mi in masked_indices:\n-            mask[tuple(mi)] = True\n-        span_lengths -= 1\n-\n-        # fill up spans\n-        max_index = input_ids.shape[1] - 1\n-        remaining = (span_lengths > 0) & (masked_indices[:, 1] < max_index)\n-        while np.any(remaining):\n-            masked_indices[remaining, 1] += 1\n-            for mi in masked_indices:\n-                mask[tuple(mi)] = True\n-            span_lengths -= 1\n-            remaining = (span_lengths > 0) & (masked_indices[:, 1] < max_index)\n-\n-        # place the mask tokens\n-        mask[np.where(special_tokens_mask_inputs)] = False\n-        input_ids[np.where(mask)] = self.tokenizer.mask_token_id\n-        if not do_permute:\n-            labels[np.where(mask == 0)] = -100\n-        else:\n-            labels[np.where(special_tokens_mask_labels)] = -100\n-\n-        # remove mask tokens that are not starts of spans\n-        to_remove = (mask == 1) & np.roll((mask == 1), 1, 1)\n-        new_input_ids = np.full_like(input_ids, fill_value=self.tokenizer.pad_token_id)\n-        for i, example in enumerate(input_ids):\n-            new_example = example[~to_remove[i]]\n-            new_input_ids[i, : new_example.shape[0]] = new_example\n-\n-        return new_input_ids, labels\n-\n-\n-def generate_batch_splits(samples_idx: np.ndarray, batch_size: int, drop_last=True) -> np.ndarray:\n-    \"\"\"Generate batches of data for a specified batch size from sample indices. If the dataset size is not divisible by\n-    the batch size and `drop_last` is `True`, the last incomplete batch is dropped. Else, it is returned.\"\"\"\n-    num_samples = len(samples_idx)\n-    if drop_last:\n-        samples_to_remove = num_samples % batch_size\n-        if samples_to_remove != 0:\n-            samples_idx = samples_idx[:-samples_to_remove]\n-        sections_split = num_samples // batch_size\n-        samples_idx = samples_idx.reshape((sections_split, batch_size))\n-    else:\n-        sections_split = math.ceil(num_samples / batch_size)\n-        samples_idx = np.array_split(samples_idx, sections_split)\n-    return samples_idx\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_bart_dlm\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        level=logging.INFO,\n-        datefmt=\"[%X]\",\n-    )\n-\n-    # Log on each process the small summary:\n-    logger = logging.getLogger(__name__)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-            trust_remote_code=data_args.trust_remote_code,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=data_args.trust_remote_code,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=data_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-        datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if model_args.config_name:\n-        config = BartConfig.from_pretrained(\n-            model_args.config_name,\n-            cache_dir=model_args.cache_dir,\n-            vocab_size=len(tokenizer),\n-            token=model_args.token,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = BartConfig.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    # Preprocessing the datasets.\n-    # First we tokenize all the texts.\n-    if training_args.do_train:\n-        column_names = datasets[\"train\"].column_names\n-    else:\n-        column_names = datasets[\"validation\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    # Use Punkt Sentence Tokenizer to divide a document into a list of sentences\n-    nltk.download(\"punkt\")\n-    sentence_tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n-\n-    def sentence_split_function(example):\n-        sents = sentence_tokenizer.tokenize(example[\"text\"])\n-        # use pad token as end of sentence indicator\n-        new_text = tokenizer.bos_token + f\"{tokenizer.pad_token}\".join(sents) + tokenizer.eos_token\n-        return {\"text\": new_text}\n-\n-    split_datasets = datasets.map(\n-        sentence_split_function,\n-        batched=False,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=column_names,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    # Tokenize every text, then concatenate them together before splitting them in smaller parts.\n-    # Since we make sure that all sequences are of the same length, no attention_mask is needed.\n-    def tokenize_function(examples):\n-        return tokenizer(examples[text_column_name], add_special_tokens=False, return_attention_mask=False)\n-\n-    tokenized_datasets = split_datasets.map(\n-        tokenize_function,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=text_column_name,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n-    # max_seq_length.\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-        # customize this part to your needs.\n-        if total_length >= max_seq_length:\n-            total_length = (total_length // max_seq_length) * max_seq_length\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        return result\n-\n-    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n-    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n-    # might be slower to preprocess.\n-    #\n-    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-    # https://huggingface.co/docs/datasets/process#map\n-    tokenized_datasets = tokenized_datasets.map(\n-        group_texts,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxBartForConditionalGeneration.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-        )\n-    else:\n-        config.vocab_size = len(tokenizer)\n-        model = FlaxBartForConditionalGeneration(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-        )\n-\n-    # Data collator\n-    # This one will take care of randomly masking the tokens and permuting the sentences.\n-    data_collator = FlaxDataCollatorForBartDenoisingLM(\n-        tokenizer=tokenizer,\n-        decoder_start_token_id=model.config.decoder_start_token_id,\n-        mask_ratio=data_args.mlm_probability,\n-        poisson_lambda=data_args.poisson_lambda,\n-        permute_sentence_ratio=data_args.permute_sentence_ratio,\n-    )\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-\n-    num_train_steps = len(tokenized_datasets[\"train\"]) // train_batch_size * num_epochs\n-\n-    # Create learning rate schedule\n-    warmup_fn = optax.linear_schedule(\n-        init_value=0.0, end_value=training_args.learning_rate, transition_steps=training_args.warmup_steps\n-    )\n-    decay_fn = optax.linear_schedule(\n-        init_value=training_args.learning_rate,\n-        end_value=0,\n-        transition_steps=num_train_steps - training_args.warmup_steps,\n-    )\n-    linear_decay_lr_schedule_fn = optax.join_schedules(\n-        schedules=[warmup_fn, decay_fn], boundaries=[training_args.warmup_steps]\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    if training_args.adafactor:\n-        # We use the default parameters here to initialize adafactor,\n-        # For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74\n-        optimizer = optax.adafactor(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-        )\n-    else:\n-        optimizer = optax.adamw(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-            b1=training_args.adam_beta1,\n-            b2=training_args.adam_beta2,\n-            weight_decay=training_args.weight_decay,\n-            mask=decay_mask_fn,\n-        )\n-\n-    # Setup train state\n-    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer)\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, dropout_rng):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-\n-        def loss_fn(params):\n-            labels = batch.pop(\"labels\")\n-\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-\n-            # compute loss, ignore padded input tokens and special tokens\n-            label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-            loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-            # take average\n-            loss = loss.sum()\n-            num_labels = label_mask.sum()\n-\n-            return loss, num_labels\n-\n-        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n-        (loss, num_labels), grad = grad_fn(state.params)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        # true grad = total grad / total samples\n-        grad = jax.lax.psum(grad, \"batch\")\n-        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n-        new_state = state.apply_gradients(grads=grad)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        return new_state, metrics, new_dropout_rng\n-\n-    # Create parallel version of the train step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        # compute loss, ignore padded input tokens and special tokens\n-        label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-        # compute accuracy\n-        accuracy = jnp.equal(jnp.argmax(logits, axis=-1), labels) * label_mask\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss.sum(), \"accuracy\": accuracy.sum(), \"normalizer\": label_mask.sum()}\n-        metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n-\n-        return metrics\n-\n-    p_eval_step = jax.pmap(eval_step, \"batch\", donate_argnums=(0,))\n-\n-    # Replicate the train state on each device\n-    state = jax_utils.replicate(state)\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        num_train_samples = len(tokenized_datasets[\"train\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        train_samples_idx = np.random.permutation(np.arange(num_train_samples))\n-        train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n-\n-        # Gather the indexes for creating the batch and do a training step\n-        for step, batch_idx in enumerate(tqdm(train_batch_idx, desc=\"Training...\", position=1)):\n-            samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-\n-            # Model forward\n-            model_inputs = shard(model_inputs.data)\n-            state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * (num_train_samples // train_batch_size) + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = jax_utils.unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step} | Loss: {train_metric['loss']}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate']})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                # ======================== Evaluating ==============================\n-                num_eval_samples = len(tokenized_datasets[\"validation\"])\n-                # Avoid using jax.numpy here in case of TPU training\n-                eval_samples_idx = np.arange(num_eval_samples)\n-                eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size)\n-\n-                eval_metrics = []\n-                for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-                    samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-                    model_inputs = data_collator(samples)\n-\n-                    # Model forward\n-                    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                        state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    eval_metrics.append(metrics)\n-\n-                # normalize eval metrics\n-                eval_metrics = get_metrics(eval_metrics)\n-                eval_metrics = jax.tree_util.tree_map(jnp.sum, eval_metrics)\n-                eval_normalizer = eval_metrics.pop(\"normalizer\")\n-                eval_metrics = jax.tree_util.tree_map(lambda x: x / eval_normalizer, eval_metrics)\n-\n-                # Update progress bar\n-                epochs.desc = f\"Step... ({cur_step} | Loss: {eval_metrics['loss']}, Acc: {eval_metrics['accuracy']})\"\n-\n-                # Save metrics\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if cur_step % training_args.save_steps == 0 and cur_step > 0:\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-\n-    # Eval after training\n-    if training_args.do_eval:\n-        num_eval_samples = len(tokenized_datasets[\"validation\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        eval_samples_idx = np.arange(num_eval_samples)\n-        eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size)\n-\n-        eval_metrics = []\n-        for _, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-            samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-\n-            # Model forward\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(lambda metric: jnp.sum(metric).item(), eval_metrics)\n-        eval_normalizer = eval_metrics.pop(\"normalizer\")\n-        eval_metrics = jax.tree_util.tree_map(lambda x: x / eval_normalizer, eval_metrics)\n-\n-        try:\n-            perplexity = math.exp(eval_metrics[\"loss\"])\n-        except OverflowError:\n-            perplexity = float(\"inf\")\n-        eval_metrics[\"perplexity\"] = perplexity\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "2faea6b5c56e6a102dd2bf2bbefb279f93de880c",
            "filename": "examples/flax/language-modeling/run_bert_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_bert_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,56 +0,0 @@\n-#!/usr/bin/env python3\n-import time\n-from argparse import ArgumentParser\n-\n-import jax\n-import numpy as np\n-\n-from transformers import BertConfig, FlaxBertModel\n-\n-\n-parser = ArgumentParser()\n-parser.add_argument(\"--precision\", type=str, choices=[\"float32\", \"bfloat16\"], default=\"float32\")\n-args = parser.parse_args()\n-\n-dtype = jax.numpy.float32\n-if args.precision == \"bfloat16\":\n-    dtype = jax.numpy.bfloat16\n-\n-VOCAB_SIZE = 30522\n-BS = 32\n-SEQ_LEN = 128\n-\n-\n-def get_input_data(batch_size=1, seq_length=384):\n-    shape = (batch_size, seq_length)\n-    input_ids = np.random.randint(1, VOCAB_SIZE, size=shape).astype(np.int32)\n-    token_type_ids = np.ones(shape).astype(np.int32)\n-    attention_mask = np.ones(shape).astype(np.int32)\n-    return {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n-\n-\n-inputs = get_input_data(BS, SEQ_LEN)\n-config = BertConfig.from_pretrained(\"bert-base-uncased\", hidden_act=\"gelu_new\")\n-model = FlaxBertModel.from_pretrained(\"bert-base-uncased\", config=config, dtype=dtype)\n-\n-\n-@jax.jit\n-def func():\n-    outputs = model(**inputs)\n-    return outputs\n-\n-\n-(nwarmup, nbenchmark) = (5, 100)\n-\n-# warmpup\n-for _ in range(nwarmup):\n-    func()\n-\n-# benchmark\n-\n-start = time.time()\n-for _ in range(nbenchmark):\n-    func()\n-end = time.time()\n-print(end - start)\n-print(f\"Throughput: {((nbenchmark * BS) / (end - start)):.3f} examples/sec\")"
        },
        {
            "sha": "fc1367fb6c16b72106b1b73d30fa216ec180acba",
            "filename": "examples/flax/language-modeling/run_clm_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 869,
            "changes": 869,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,869 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Pre-training/Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=text-generation\n-\"\"\"\n-# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from itertools import chain\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import datasets\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import Dataset, load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_CAUSAL_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForCausalLM,\n-    HfArgumentParser,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-from transformers.testing_utils import CaptureLogger\n-from transformers.utils import send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    block_size: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Optional input sequence length after tokenization. \"\n-                \"The training dataset will be truncated in block of this size for training. \"\n-                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    keep_linebreaks: bool = field(\n-        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\", \"txt\"]:\n-                    raise ValueError(\"train_file` should be a csv, json or text file.\")\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                if extension not in [\"csv\", \"json\", \"txt\"]:\n-                    raise ValueError(\"`validation_file` should be a csv, json or text file.\")\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n-    \"\"\"\n-    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n-    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n-    \"\"\"\n-    if shuffle:\n-        batch_idx = jax.random.permutation(rng, len(dataset))\n-        batch_idx = np.asarray(batch_idx)\n-    else:\n-        batch_idx = np.arange(len(dataset))\n-\n-    if drop_last:\n-        steps_per_epoch = len(dataset) // batch_size\n-        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n-    else:\n-        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n-        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-\n-        yield batch\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_clm\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    #  Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            keep_in_memory=False,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-        if \"validation\" not in dataset:\n-            dataset[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-            dataset[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        dataset_args = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-            dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n-        dataset = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            **dataset_args,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-        )\n-\n-        if \"validation\" not in dataset:\n-            dataset[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                **dataset_args,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-            dataset[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                **dataset_args,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxAutoModelForCausalLM.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        model = FlaxAutoModelForCausalLM.from_config(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-    # Preprocessing the datasets.\n-    # First we tokenize all the texts.\n-    if training_args.do_train:\n-        column_names = dataset[\"train\"].column_names\n-    else:\n-        column_names = dataset[\"validation\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    # since this will be pickled to avoid _LazyModule error in Hasher force logger loading before tokenize_function\n-    tok_logger = transformers.utils.logging.get_logger(\"transformers.tokenization_utils_base\")\n-\n-    def tokenize_function(examples):\n-        with CaptureLogger(tok_logger) as cl:\n-            output = tokenizer(examples[text_column_name])\n-        # clm input could be much much longer than block_size\n-        if \"Token indices sequence length is longer than the\" in cl.out:\n-            tok_logger.warning(\n-                \"^^^^^^^^^^^^^^^^ Please ignore the warning above - this long input will be chunked into smaller bits\"\n-                \" before being passed to the model.\"\n-            )\n-        return output\n-\n-    tokenized_datasets = dataset.map(\n-        tokenize_function,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=column_names,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    if data_args.block_size is None:\n-        block_size = tokenizer.model_max_length\n-        if block_size > config.max_position_embeddings:\n-            logger.warning(\n-                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n-                f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n-            )\n-            block_size = min(1024, config.max_position_embeddings)\n-    else:\n-        if data_args.block_size > tokenizer.model_max_length:\n-            logger.warning(\n-                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model \"\n-                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n-            )\n-        block_size = min(data_args.block_size, tokenizer.model_max_length)\n-\n-    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-        # customize this part to your needs.\n-        if total_length >= block_size:\n-            total_length = (total_length // block_size) * block_size\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        result[\"labels\"] = result[\"input_ids\"].copy()\n-        return result\n-\n-    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n-    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n-    # to preprocess.\n-    #\n-    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-    # https://huggingface.co/docs/datasets/process#map\n-\n-    lm_datasets = tokenized_datasets.map(\n-        group_texts,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    if training_args.do_train:\n-        if \"train\" not in tokenized_datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = lm_datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in tokenized_datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = lm_datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    if training_args.adafactor:\n-        # We use the default parameters here to initialize adafactor,\n-        # For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74\n-        optimizer = optax.adafactor(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-        )\n-    else:\n-        optimizer = optax.adamw(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-            b1=training_args.adam_beta1,\n-            b2=training_args.adam_beta2,\n-            eps=training_args.adam_epsilon,\n-            weight_decay=training_args.weight_decay,\n-            mask=decay_mask_fn,\n-        )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer, dropout_rng=dropout_rng)\n-\n-    def loss_fn(logits, labels):\n-        shift_logits = logits[..., :-1, :]\n-        shift_labels = labels[..., 1:]\n-        loss = optax.softmax_cross_entropy(shift_logits, onehot(shift_labels, shift_logits.shape[-1]))\n-        return loss.mean()\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = loss_fn(logits, labels)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(compute_loss)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-        logits = model(**batch, params=params, train=False)[0]\n-        loss = loss_fn(logits, labels)\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-        return metrics\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-    p_eval_step = jax.pmap(eval_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(train_dataset)}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    train_metrics = []\n-    epochs = tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n-        steps_per_epoch = len(train_dataset) // train_batch_size\n-        # train\n-        for step in tqdm(range(steps_per_epoch), desc=\"Training...\", position=1, leave=False):\n-            batch = next(train_loader)\n-            batch = shard(batch)\n-            state, train_metric = p_train_step(state, batch)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * (len(train_dataset) // train_batch_size) + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step} | Loss: {train_metric['loss'].mean()}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate'].mean()})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                # ======================== Evaluating ==============================\n-                eval_metrics = []\n-                eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n-                eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n-                for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=2, leave=False):\n-                    # Model forward\n-                    batch = next(eval_loader)\n-                    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                        state.params, batch, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    eval_metrics.append(metrics)\n-\n-                # normalize eval metrics\n-                eval_metrics = get_metrics(eval_metrics)\n-                eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-                try:\n-                    eval_metrics[\"perplexity\"] = math.exp(eval_metrics[\"loss\"])\n-                except OverflowError:\n-                    eval_metrics[\"perplexity\"] = float(\"inf\")\n-\n-                # Print metrics and update progress bar\n-                desc = (\n-                    f\"Step... ({cur_step} | Eval Loss: {eval_metrics['loss']} | Eval Perplexity:\"\n-                    f\" {eval_metrics['perplexity']})\"\n-                )\n-                epochs.write(desc)\n-                epochs.desc = desc\n-\n-                # Save metrics\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if cur_step % training_args.save_steps == 0 and cur_step > 0:\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(unreplicate(state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-    # Eval after training\n-    if training_args.do_eval:\n-        eval_metrics = []\n-        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n-        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n-        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=2, leave=False):\n-            # Model forward\n-            batch = next(eval_loader)\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, batch, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(lambda x: jnp.mean(x).item(), eval_metrics)\n-\n-        try:\n-            eval_metrics[\"perplexity\"] = math.exp(eval_metrics[\"loss\"])\n-        except OverflowError:\n-            eval_metrics[\"perplexity\"] = float(\"inf\")\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "0d80b7b0bf1568ca98866d7c8f9dd47e9dd5d80e",
            "filename": "examples/flax/language-modeling/run_mlm_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 924,
            "changes": 924,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,924 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...) with whole word masking on a\n-text file or a dataset.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=fill-mask\n-\"\"\"\n-\n-import json\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from itertools import chain\n-\n-# You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.\n-from pathlib import Path\n-from typing import Optional\n-\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_MASKED_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForMaskedLM,\n-    HfArgumentParser,\n-    PreTrainedTokenizerBase,\n-    TensorType,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-from transformers.utils import send_example_telemetry\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_MASKED_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-    gradient_checkpointing: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    train_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input train ref data file for whole word masking in Chinese.\"},\n-    )\n-    validation_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input validation ref data file for whole word masking in Chinese.\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated. Default to the max input length of the model.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    mlm_probability: float = field(\n-        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n-            )\n-        },\n-    )\n-    line_by_line: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorForLanguageModeling:\n-    \"\"\"\n-    Data collator used for language modeling. Inputs are dynamically padded to the maximum length of a batch if they\n-    are not all of the same length.\n-\n-    Args:\n-        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n-            The tokenizer used for encoding the data.\n-        mlm_probability (:obj:`float`, `optional`, defaults to 0.15):\n-            The probability with which to (randomly) mask tokens in the input.\n-\n-    .. note::\n-\n-        For best performance, this data collator should be used with a dataset having items that are dictionaries or\n-        BatchEncoding, with the :obj:`\"special_tokens_mask\"` key, as returned by a\n-        :class:`~transformers.PreTrainedTokenizer` or a :class:`~transformers.PreTrainedTokenizerFast` with the\n-        argument :obj:`return_special_tokens_mask=True`.\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    mlm_probability: float = 0.15\n-\n-    def __post_init__(self):\n-        if self.tokenizer.mask_token is None:\n-            raise ValueError(\n-                \"This tokenizer does not have a mask token which is necessary for masked language modeling. \"\n-                \"You should pass `mlm=False` to train on causal language modeling instead.\"\n-            )\n-\n-    def __call__(self, examples: list[dict[str, np.ndarray]], pad_to_multiple_of: int) -> dict[str, np.ndarray]:\n-        # Handle dict or lists with proper padding and conversion to tensor.\n-        batch = self.tokenizer.pad(examples, pad_to_multiple_of=pad_to_multiple_of, return_tensors=TensorType.NUMPY)\n-\n-        # If special token mask has been preprocessed, pop it from the dict.\n-        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n-\n-        batch[\"input_ids\"], batch[\"labels\"] = self.mask_tokens(\n-            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n-        )\n-        return batch\n-\n-    def mask_tokens(\n-        self, inputs: np.ndarray, special_tokens_mask: Optional[np.ndarray]\n-    ) -> tuple[np.ndarray, np.ndarray]:\n-        \"\"\"\n-        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n-        \"\"\"\n-        labels = inputs.copy()\n-        # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n-        probability_matrix = np.full(labels.shape, self.mlm_probability)\n-        special_tokens_mask = special_tokens_mask.astype(\"bool\")\n-\n-        probability_matrix[special_tokens_mask] = 0.0\n-        masked_indices = np.random.binomial(1, probability_matrix).astype(\"bool\")\n-        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n-\n-        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n-        indices_replaced = np.random.binomial(1, np.full(labels.shape, 0.8)).astype(\"bool\") & masked_indices\n-        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n-\n-        # 10% of the time, we replace masked input tokens with random word\n-        indices_random = np.random.binomial(1, np.full(labels.shape, 0.5)).astype(\"bool\")\n-        indices_random &= masked_indices & ~indices_replaced\n-\n-        random_words = np.random.randint(self.tokenizer.vocab_size, size=labels.shape, dtype=\"i4\")\n-        inputs[indices_random] = random_words[indices_random]\n-\n-        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n-        return inputs, labels\n-\n-\n-def generate_batch_splits(samples_idx: np.ndarray, batch_size: int, drop_last=True) -> np.ndarray:\n-    \"\"\"Generate batches of data for a specified batch size from sample indices. If the dataset size is not divisible by\n-    the batch size and `drop_last` is `True`, the last incomplete batch is dropped. Else, it is returned.\"\"\"\n-    num_samples = len(samples_idx)\n-    if drop_last:\n-        samples_to_remove = num_samples % batch_size\n-        if samples_to_remove != 0:\n-            samples_idx = samples_idx[:-samples_to_remove]\n-        sections_split = num_samples // batch_size\n-        samples_idx = samples_idx.reshape((sections_split, batch_size))\n-    else:\n-        sections_split = math.ceil(num_samples / batch_size)\n-        samples_idx = np.array_split(samples_idx, sections_split)\n-    return samples_idx\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_mlm\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        level=logging.INFO,\n-        datefmt=\"[%X]\",\n-    )\n-\n-    # Log on each process the small summary:\n-    logger = logging.getLogger(__name__)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantees that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-        datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    # Preprocessing the datasets.\n-    # First we tokenize all the texts.\n-    if training_args.do_train:\n-        column_names = datasets[\"train\"].column_names\n-    else:\n-        column_names = datasets[\"validation\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    if data_args.line_by_line:\n-        # When using line_by_line, we just tokenize each nonempty line.\n-        padding = \"max_length\" if data_args.pad_to_max_length else False\n-\n-        def tokenize_function(examples):\n-            # Remove empty lines\n-            examples = [line for line in examples if len(line) > 0 and not line.isspace()]\n-            return tokenizer(\n-                examples,\n-                return_special_tokens_mask=True,\n-                padding=padding,\n-                truncation=True,\n-                max_length=max_seq_length,\n-            )\n-\n-        tokenized_datasets = datasets.map(\n-            tokenize_function,\n-            input_columns=[text_column_name],\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    else:\n-        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n-        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n-        # efficient when it receives the `special_tokens_mask`.\n-        def tokenize_function(examples):\n-            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n-\n-        tokenized_datasets = datasets.map(\n-            tokenize_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n-        # max_seq_length.\n-        def group_texts(examples):\n-            # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-            total_length = len(concatenated_examples[list(examples.keys())[0]])\n-            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-            # customize this part to your needs.\n-            if total_length >= max_seq_length:\n-                total_length = (total_length // max_seq_length) * max_seq_length\n-            # Split by chunks of max_len.\n-            result = {\n-                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n-                for k, t in concatenated_examples.items()\n-            }\n-            return result\n-\n-        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n-        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n-        # might be slower to preprocess.\n-        #\n-        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-        # https://huggingface.co/docs/datasets/process#map\n-        tokenized_datasets = tokenized_datasets.map(\n-            group_texts,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Data collator\n-    # This one will take care of randomly masking the tokens.\n-    data_collator = FlaxDataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=data_args.mlm_probability)\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxAutoModelForMaskedLM.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        model = FlaxAutoModelForMaskedLM.from_config(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-    if training_args.gradient_checkpointing:\n-        model.enable_gradient_checkpointing()\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-\n-    num_train_steps = len(tokenized_datasets[\"train\"]) // train_batch_size * num_epochs\n-\n-    # Create learning rate schedule\n-    warmup_fn = optax.linear_schedule(\n-        init_value=0.0, end_value=training_args.learning_rate, transition_steps=training_args.warmup_steps\n-    )\n-    decay_fn = optax.linear_schedule(\n-        init_value=training_args.learning_rate,\n-        end_value=0,\n-        transition_steps=num_train_steps - training_args.warmup_steps,\n-    )\n-    linear_decay_lr_schedule_fn = optax.join_schedules(\n-        schedules=[warmup_fn, decay_fn], boundaries=[training_args.warmup_steps]\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    if training_args.adafactor:\n-        # We use the default parameters here to initialize adafactor,\n-        # For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74\n-        optimizer = optax.adafactor(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-        )\n-    else:\n-        optimizer = optax.adamw(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-            b1=training_args.adam_beta1,\n-            b2=training_args.adam_beta2,\n-            eps=training_args.adam_epsilon,\n-            weight_decay=training_args.weight_decay,\n-            mask=decay_mask_fn,\n-        )\n-\n-    # Setup train state\n-    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer)\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, dropout_rng):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-\n-        def loss_fn(params):\n-            labels = batch.pop(\"labels\")\n-\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-\n-            # compute loss, ignore padded input tokens\n-            label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-            loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-            # take average\n-            loss = loss.sum()\n-            num_labels = label_mask.sum()\n-\n-            return loss, num_labels\n-\n-        grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n-        (loss, num_labels), grad = grad_fn(state.params)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        # true grad = total grad / total samples\n-        grad = jax.lax.psum(grad, \"batch\")\n-        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n-        new_state = state.apply_gradients(grads=grad)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-\n-        return new_state, metrics, new_dropout_rng\n-\n-    # Create parallel version of the train step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        # compute loss, ignore padded input tokens\n-        label_mask = jnp.where(labels > 0, 1.0, 0.0)\n-        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])) * label_mask\n-\n-        # compute accuracy\n-        accuracy = jnp.equal(jnp.argmax(logits, axis=-1), labels) * label_mask\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss.sum(), \"accuracy\": accuracy.sum(), \"normalizer\": label_mask.sum()}\n-        metrics = jax.lax.psum(metrics, axis_name=\"batch\")\n-\n-        return metrics\n-\n-    p_eval_step = jax.pmap(eval_step, \"batch\", donate_argnums=(0,))\n-\n-    # Replicate the train state on each device\n-    state = jax_utils.replicate(state)\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        num_train_samples = len(tokenized_datasets[\"train\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        train_samples_idx = np.random.permutation(np.arange(num_train_samples))\n-        train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n-\n-        # Gather the indexes for creating the batch and do a training step\n-        for step, batch_idx in enumerate(tqdm(train_batch_idx, desc=\"Training...\", position=1)):\n-            samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples, pad_to_multiple_of=16)\n-\n-            # Model forward\n-            model_inputs = shard(model_inputs.data)\n-            state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * (num_train_samples // train_batch_size) + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = jax_utils.unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step} | Loss: {train_metric['loss']}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate']})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                # ======================== Evaluating ==============================\n-                num_eval_samples = len(tokenized_datasets[\"validation\"])\n-                # Avoid using jax.numpy here in case of TPU training\n-                eval_samples_idx = np.arange(num_eval_samples)\n-                eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size, drop_last=False)\n-\n-                eval_metrics = []\n-                for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-                    samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-                    model_inputs = data_collator(samples, pad_to_multiple_of=16)\n-\n-                    # Model forward\n-                    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                        state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    eval_metrics.append(metrics)\n-\n-                # normalize eval metrics\n-                eval_metrics = get_metrics(eval_metrics)\n-                eval_metrics = jax.tree_util.tree_map(jnp.sum, eval_metrics)\n-                eval_normalizer = eval_metrics.pop(\"normalizer\")\n-                eval_metrics = jax.tree_util.tree_map(lambda x: x / eval_normalizer, eval_metrics)\n-\n-                # Update progress bar\n-                epochs.desc = f\"Step... ({cur_step} | Loss: {eval_metrics['loss']}, Acc: {eval_metrics['accuracy']})\"\n-\n-                # Save metrics\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if cur_step % training_args.save_steps == 0 and cur_step > 0:\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-    # Eval after training\n-    if training_args.do_eval:\n-        num_eval_samples = len(tokenized_datasets[\"validation\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        eval_samples_idx = np.arange(num_eval_samples)\n-        eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size, drop_last=False)\n-\n-        eval_metrics = []\n-        for _, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-            samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples, pad_to_multiple_of=16)\n-\n-            # Model forward\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(lambda metric: jnp.sum(metric).item(), eval_metrics)\n-        eval_normalizer = eval_metrics.pop(\"normalizer\")\n-        eval_metrics = jax.tree_util.tree_map(lambda x: x / eval_normalizer, eval_metrics)\n-\n-        try:\n-            perplexity = math.exp(eval_metrics[\"loss\"])\n-        except OverflowError:\n-            perplexity = float(\"inf\")\n-        eval_metrics[\"perplexity\"] = perplexity\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "af3394cccbff633baeb7eafb698e24e3cfbae13b",
            "filename": "examples/flax/language-modeling/run_t5_mlm_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1013,
            "changes": 1013,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,1013 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Pretraining the library models for T5-like span-masked language modeling on a text file or a dataset.\n-\n-Here is the full list of checkpoints on the hub that can be pretrained by this script:\n-https://huggingface.co/models?filter=t5\n-\"\"\"\n-\n-import json\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-\n-# You can also adapt this script on your own masked language modeling task. Pointers for this are left as comments.\n-from enum import Enum\n-from itertools import chain\n-from pathlib import Path\n-from typing import Optional\n-\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_MASKED_LM_MAPPING,\n-    AutoTokenizer,\n-    BatchEncoding,\n-    FlaxT5ForConditionalGeneration,\n-    HfArgumentParser,\n-    PreTrainedTokenizerBase,\n-    T5Config,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-from transformers.models.t5.modeling_flax_t5 import shift_tokens_right\n-from transformers.utils import send_example_telemetry\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_MASKED_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    train_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input train ref data file for whole word masking in Chinese.\"},\n-    )\n-    validation_ref_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input validation ref data file for whole word masking in Chinese.\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization and masking. Sequences longer than this\"\n-                \" will be truncated. Default to the max input length of the model.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    mlm_probability: float = field(\n-        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for span masked language modeling loss\"}\n-    )\n-    mean_noise_span_length: float = field(\n-        default=3.0,\n-        metadata={\"help\": \"Mean span length of masked tokens\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-def compute_input_and_target_lengths(inputs_length, noise_density, mean_noise_span_length):\n-    \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2466>`__ .\n-\n-    Training parameters to avoid padding with random_spans_noise_mask.\n-    When training a model with random_spans_noise_mask, we would like to set the other\n-    training hyperparmeters in a way that avoids padding.\n-    This function helps us compute these hyperparameters.\n-    We assume that each noise span in the input is replaced by extra_tokens_per_span_inputs sentinel tokens,\n-    and each non-noise span in the targets is replaced by extra_tokens_per_span_targets sentinel tokens.\n-    This function tells us the required number of tokens in the raw example (for split_tokens())\n-    as well as the length of the encoded targets. Note that this function assumes\n-    the inputs and targets will have EOS appended and includes that in the reported length.\n-\n-    Args:\n-        inputs_length: an integer - desired length of the tokenized inputs sequence\n-        noise_density: a float\n-        mean_noise_span_length: a float\n-    Returns:\n-        tokens_length: length of original text in tokens\n-        targets_length: an integer - length in tokens of encoded targets sequence\n-    \"\"\"\n-\n-    def _tokens_length_to_inputs_length_targets_length(tokens_length):\n-        num_noise_tokens = int(round(tokens_length * noise_density))\n-        num_nonnoise_tokens = tokens_length - num_noise_tokens\n-        num_noise_spans = int(round(num_noise_tokens / mean_noise_span_length))\n-        # inputs contain all nonnoise tokens, sentinels for all noise spans\n-        # and one EOS token.\n-        _input_length = num_nonnoise_tokens + num_noise_spans + 1\n-        _output_length = num_noise_tokens + num_noise_spans + 1\n-        return _input_length, _output_length\n-\n-    tokens_length = inputs_length\n-\n-    while _tokens_length_to_inputs_length_targets_length(tokens_length + 1)[0] <= inputs_length:\n-        tokens_length += 1\n-\n-    inputs_length, targets_length = _tokens_length_to_inputs_length_targets_length(tokens_length)\n-\n-    # minor hack to get the targets length to be equal to inputs length\n-    # which is more likely to have been set to a nice round number.\n-    if noise_density == 0.5 and targets_length > inputs_length:\n-        tokens_length -= 1\n-        targets_length -= 1\n-    return tokens_length, targets_length\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorForT5MLM:\n-    \"\"\"\n-    Data collator used for T5 span-masked language modeling.\n-    It is made sure that after masking the inputs are of length `data_args.max_seq_length` and targets are also of fixed length.\n-    For more information on how T5 span-masked language modeling works, one can take a look\n-    at the `official paper <https://huggingface.co/papers/1910.10683>`__\n-    or the `official code for preprocessing <https://github.com/google-research/text-to-text-transfer-transformer/blob/master/t5/data/preprocessors.py>`__ .\n-\n-    Args:\n-        tokenizer (:class:`~transformers.PreTrainedTokenizer` or :class:`~transformers.PreTrainedTokenizerFast`):\n-            The tokenizer used for encoding the data.\n-        noise_density (:obj:`float`):\n-            The probability with which to (randomly) mask tokens in the input.\n-        mean_noise_span_length (:obj:`float`):\n-            The average span length of the masked tokens.\n-        input_length (:obj:`int`):\n-            The expected input length after masking.\n-        target_length (:obj:`int`):\n-            The expected target length after masking.\n-        pad_token_id: (:obj:`int`):\n-            The pad token id of the model\n-        decoder_start_token_id: (:obj:`int):\n-            The decoder start token id of the model\n-    \"\"\"\n-\n-    tokenizer: PreTrainedTokenizerBase\n-    noise_density: float\n-    mean_noise_span_length: float\n-    input_length: int\n-    target_length: int\n-    pad_token_id: int\n-    decoder_start_token_id: int\n-\n-    def __call__(self, examples: list[dict[str, np.ndarray]]) -> BatchEncoding:\n-        # convert list to dict and tensorize input\n-        batch = BatchEncoding(\n-            {k: np.array([examples[i][k] for i in range(len(examples))]) for k, v in examples[0].items()}\n-        )\n-\n-        input_ids = batch[\"input_ids\"]\n-        batch_size, expandend_input_length = input_ids.shape\n-\n-        mask_indices = np.asarray([self.random_spans_noise_mask(expandend_input_length) for i in range(batch_size)])\n-        labels_mask = ~mask_indices\n-\n-        input_ids_sentinel = self.create_sentinel_ids(mask_indices.astype(np.int8))\n-        labels_sentinel = self.create_sentinel_ids(labels_mask.astype(np.int8))\n-\n-        batch[\"input_ids\"] = self.filter_input_ids(input_ids, input_ids_sentinel)\n-        batch[\"labels\"] = self.filter_input_ids(input_ids, labels_sentinel)\n-\n-        if batch[\"input_ids\"].shape[-1] != self.input_length:\n-            raise ValueError(\n-                f\"`input_ids` are incorrectly preprocessed. `input_ids` length is {batch['input_ids'].shape[-1]}, but\"\n-                f\" should be {self.input_length}.\"\n-            )\n-\n-        if batch[\"labels\"].shape[-1] != self.target_length:\n-            raise ValueError(\n-                f\"`labels` are incorrectly preprocessed. `labels` length is {batch['labels'].shape[-1]}, but should be\"\n-                f\" {self.target_length}.\"\n-            )\n-\n-        # to check that tokens are correctly preprocessed, one can run `self.tokenizer.batch_decode(input_ids)` and `self.tokenizer.batch_decode(labels)` here...\n-        batch[\"decoder_input_ids\"] = shift_tokens_right(\n-            batch[\"labels\"], self.pad_token_id, self.decoder_start_token_id\n-        )\n-\n-        return batch\n-\n-    def create_sentinel_ids(self, mask_indices):\n-        \"\"\"\n-        Sentinel ids creation given the indices that should be masked.\n-        The start indices of each mask are replaced by the sentinel ids in increasing\n-        order. Consecutive mask indices to be deleted are replaced with `-1`.\n-        \"\"\"\n-        start_indices = mask_indices - np.roll(mask_indices, 1, axis=-1) * mask_indices\n-        start_indices[:, 0] = mask_indices[:, 0]\n-\n-        sentinel_ids = np.where(start_indices != 0, np.cumsum(start_indices, axis=-1), start_indices)\n-        sentinel_ids = np.where(sentinel_ids != 0, (len(self.tokenizer) - sentinel_ids), 0)\n-        sentinel_ids -= mask_indices - start_indices\n-\n-        return sentinel_ids\n-\n-    def filter_input_ids(self, input_ids, sentinel_ids):\n-        \"\"\"\n-        Puts sentinel mask on `input_ids` and fuse consecutive mask tokens into a single mask token by deleting.\n-        This will reduce the sequence length from `expanded_inputs_length` to `input_length`.\n-        \"\"\"\n-        batch_size = input_ids.shape[0]\n-\n-        input_ids_full = np.where(sentinel_ids != 0, sentinel_ids, input_ids)\n-        # input_ids tokens and sentinel tokens are >= 0, tokens < 0 are\n-        # masked tokens coming after sentinel tokens and should be removed\n-        input_ids = input_ids_full[input_ids_full >= 0].reshape((batch_size, -1))\n-        input_ids = np.concatenate(\n-            [input_ids, np.full((batch_size, 1), self.tokenizer.eos_token_id, dtype=np.int32)], axis=-1\n-        )\n-        return input_ids\n-\n-    def random_spans_noise_mask(self, length):\n-        \"\"\"This function is copy of `random_spans_helper <https://github.com/google-research/text-to-text-transfer-transformer/blob/84f8bcc14b5f2c03de51bd3587609ba8f6bbd1cd/t5/data/preprocessors.py#L2682>`__ .\n-\n-        Noise mask consisting of random spans of noise tokens.\n-        The number of noise tokens and the number of noise spans and non-noise spans\n-        are determined deterministically as follows:\n-        num_noise_tokens = round(length * noise_density)\n-        num_nonnoise_spans = num_noise_spans = round(num_noise_tokens / mean_noise_span_length)\n-        Spans alternate between non-noise and noise, beginning with non-noise.\n-        Subject to the above restrictions, all masks are equally likely.\n-\n-        Args:\n-            length: an int32 scalar (length of the incoming token sequence)\n-            noise_density: a float - approximate density of output mask\n-            mean_noise_span_length: a number\n-\n-        Returns:\n-            a boolean tensor with shape [length]\n-        \"\"\"\n-\n-        orig_length = length\n-\n-        num_noise_tokens = int(np.round(length * self.noise_density))\n-        num_nonnoise_tokens = length - num_noise_tokens\n-        # avoid degeneracy by ensuring positive numbers of noise and nonnoise tokens.\n-        num_noise_tokens = min(max(num_noise_tokens, 1), length - 1)\n-        # num_noise_tokens should be less than num_noise_tokens and num_nonnoise_tokens\n-        num_noise_spans = int(np.round(min(num_noise_tokens, num_nonnoise_tokens) / self.mean_noise_span_length))\n-\n-        # avoid degeneracy by ensuring positive number of noise spans\n-        num_noise_spans = max(num_noise_spans, 1)\n-\n-        # pick the lengths of the noise spans and the non-noise spans\n-        def _random_segmentation(num_items, num_segments):\n-            \"\"\"Partition a sequence of items randomly into non-empty segments.\n-            Args:\n-                num_items: an integer scalar > 0\n-                num_segments: an integer scalar in [1, num_items]\n-            Returns:\n-                a Tensor with shape [num_segments] containing positive integers that add\n-                up to num_items\n-            \"\"\"\n-            mask_indices = np.arange(num_items - 1) < (num_segments - 1)\n-            np.random.shuffle(mask_indices)\n-            first_in_segment = np.pad(mask_indices, [[1, 0]])\n-            segment_id = np.cumsum(first_in_segment)\n-            # count length of sub segments assuming that list is sorted\n-            _, segment_length = np.unique(segment_id, return_counts=True)\n-            return segment_length\n-\n-        noise_span_lengths = _random_segmentation(num_noise_tokens, num_noise_spans)\n-        nonnoise_span_lengths = _random_segmentation(num_nonnoise_tokens, num_noise_spans)\n-\n-        interleaved_span_lengths = np.reshape(\n-            np.stack([nonnoise_span_lengths, noise_span_lengths], axis=1), [num_noise_spans * 2]\n-        )\n-        span_starts = np.cumsum(interleaved_span_lengths)[:-1]\n-        span_start_indicator = np.zeros((length,), dtype=np.int8)\n-        span_start_indicator[span_starts] = True\n-        span_num = np.cumsum(span_start_indicator)\n-        is_noise = np.equal(span_num % 2, 1)\n-\n-        return is_noise[:orig_length]\n-\n-\n-def generate_batch_splits(samples_idx: np.ndarray, batch_size: int, drop_last=True) -> np.ndarray:\n-    \"\"\"Generate batches of data for a specified batch size from sample indices. If the dataset size is not divisible by\n-    the batch size and `drop_last` is `True`, the last incomplete batch is dropped. Else, it is returned.\"\"\"\n-    num_samples = len(samples_idx)\n-    if drop_last:\n-        samples_to_remove = num_samples % batch_size\n-        if samples_to_remove != 0:\n-            samples_idx = samples_idx[:-samples_to_remove]\n-        sections_split = num_samples // batch_size\n-        samples_idx = samples_idx.reshape((sections_split, batch_size))\n-    else:\n-        sections_split = math.ceil(num_samples / batch_size)\n-        samples_idx = np.array_split(samples_idx, sections_split)\n-    return samples_idx\n-\n-\n-def write_train_metric(summary_writer, train_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-\n-def write_eval_metric(summary_writer, eval_metrics, step):\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_t5_mlm\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n-        level=logging.INFO,\n-        datefmt=\"[%X]\",\n-    )\n-\n-    # Log on each process the small summary:\n-    logger = logging.getLogger(__name__)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-            trust_remote_code=data_args.trust_remote_code,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=data_args.trust_remote_code,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-                trust_remote_code=data_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-        datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            num_proc=data_args.preprocessing_num_workers,\n-        )\n-\n-        if \"validation\" not in datasets:\n-            datasets[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-            datasets[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                num_proc=data_args.preprocessing_num_workers,\n-            )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if model_args.config_name:\n-        config = T5Config.from_pretrained(\n-            model_args.config_name,\n-            cache_dir=model_args.cache_dir,\n-            vocab_size=len(tokenizer),\n-            token=model_args.token,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = T5Config.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    # Preprocessing the datasets.\n-    # First we tokenize all the texts.\n-    if training_args.do_train:\n-        column_names = datasets[\"train\"].column_names\n-    else:\n-        column_names = datasets[\"validation\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n-    # Since we make sure that all sequences are of the same length, no attention_mask is needed.\n-    def tokenize_function(examples):\n-        return tokenizer(examples[text_column_name], return_attention_mask=False)\n-\n-    tokenized_datasets = datasets.map(\n-        tokenize_function,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=column_names,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    # T5-like span masked language modeling will fuse consecutively masked tokens to a single sentinel token.\n-    # To ensure that the input length is `max_seq_length`, we need to increase the maximum length\n-    # according to `mlm_probability` and `mean_noise_span_length`. We can also define the label length accordingly.\n-    expanded_inputs_length, targets_length = compute_input_and_target_lengths(\n-        inputs_length=max_seq_length,\n-        noise_density=data_args.mlm_probability,\n-        mean_noise_span_length=data_args.mean_noise_span_length,\n-    )\n-\n-    # Main data processing function that will concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-        # customize this part to your needs.\n-        if total_length >= expanded_inputs_length:\n-            total_length = (total_length // expanded_inputs_length) * expanded_inputs_length\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + expanded_inputs_length] for i in range(0, total_length, expanded_inputs_length)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        return result\n-\n-    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n-    # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n-    # might be slower to preprocess.\n-    #\n-    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-    # https://huggingface.co/docs/datasets/process#map\n-    tokenized_datasets = tokenized_datasets.map(\n-        group_texts,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxT5ForConditionalGeneration.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-        )\n-    else:\n-        config.vocab_size = len(tokenizer)\n-        model = FlaxT5ForConditionalGeneration(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-        )\n-\n-    # Data collator\n-    # This one will take care of randomly masking the tokens.\n-    data_collator = FlaxDataCollatorForT5MLM(\n-        tokenizer=tokenizer,\n-        noise_density=data_args.mlm_probability,\n-        mean_noise_span_length=data_args.mean_noise_span_length,\n-        input_length=max_seq_length,\n-        target_length=targets_length,\n-        pad_token_id=model.config.pad_token_id,\n-        decoder_start_token_id=model.config.decoder_start_token_id,\n-    )\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-\n-    num_train_steps = len(tokenized_datasets[\"train\"]) // train_batch_size * num_epochs\n-\n-    num_of_hosts = jax.process_count()\n-    current_host_idx = jax.process_index()\n-\n-    # Create learning rate schedule\n-    warmup_fn = optax.linear_schedule(\n-        init_value=0.0, end_value=training_args.learning_rate, transition_steps=training_args.warmup_steps\n-    )\n-    decay_fn = optax.linear_schedule(\n-        init_value=training_args.learning_rate,\n-        end_value=0,\n-        transition_steps=num_train_steps - training_args.warmup_steps,\n-    )\n-    linear_decay_lr_schedule_fn = optax.join_schedules(\n-        schedules=[warmup_fn, decay_fn], boundaries=[training_args.warmup_steps]\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    if training_args.adafactor:\n-        # We use the default parameters here to initialize adafactor,\n-        # For more details about the parameters please check https://github.com/deepmind/optax/blob/ed02befef9bf81cbbf236be3d2b0e032e9ed4a40/optax/_src/alias.py#L74\n-        optimizer = optax.adafactor(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-        )\n-    else:\n-        optimizer = optax.adamw(\n-            learning_rate=linear_decay_lr_schedule_fn,\n-            b1=training_args.adam_beta1,\n-            b2=training_args.adam_beta2,\n-            weight_decay=training_args.weight_decay,\n-            mask=decay_mask_fn,\n-        )\n-\n-    # Setup train state\n-    state = train_state.TrainState.create(apply_fn=model.__call__, params=model.params, tx=optimizer)\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, dropout_rng):\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-\n-        def loss_fn(params):\n-            labels = batch.pop(\"labels\")\n-\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-\n-            # compute loss\n-            loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1])).mean()\n-\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-\n-        metrics = jax.lax.pmean(\n-            {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}, axis_name=\"batch\"\n-        )\n-\n-        return new_state, metrics, new_dropout_rng\n-\n-    # Create parallel version of the train step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        # compute loss\n-        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1]))\n-\n-        # compute accuracy\n-        accuracy = jnp.equal(jnp.argmax(logits, axis=-1), labels)\n-\n-        # summarize metrics\n-        metrics = {\"loss\": loss.mean(), \"accuracy\": accuracy.mean()}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-\n-        return metrics\n-\n-    p_eval_step = jax.pmap(eval_step, \"batch\", donate_argnums=(0,))\n-\n-    # Replicate the train state on each device\n-    state = jax_utils.replicate(state)\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=\"Epoch ... \", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        num_train_samples = len(tokenized_datasets[\"train\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        train_samples_idx = np.random.permutation(np.arange(num_train_samples))\n-        train_batch_idx = generate_batch_splits(train_samples_idx, train_batch_size)\n-\n-        # Gather the indexes for creating the batch and do a training step\n-        for step, batch_idx in enumerate(tqdm(train_batch_idx, desc=\"Training...\", position=1)):\n-            samples = [tokenized_datasets[\"train\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-\n-            local_host_model_inputs = {\n-                key: np.split(model_inputs.data[key], num_of_hosts, axis=0)[current_host_idx]\n-                for key, value in model_inputs.data.items()\n-            }\n-\n-            # Model forward\n-            model_inputs = shard(local_host_model_inputs)\n-            state, train_metric, dropout_rngs = p_train_step(state, model_inputs, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * (num_train_samples // train_batch_size) + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = jax_utils.unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step} | Loss: {train_metric['loss'].mean()}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate'].mean()})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                # ======================== Evaluating ==============================\n-                num_eval_samples = len(tokenized_datasets[\"validation\"])\n-                # Avoid using jax.numpy here in case of TPU training\n-                eval_samples_idx = np.arange(num_eval_samples)\n-                eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size, drop_last=False)\n-\n-                eval_metrics = []\n-                for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-                    samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-                    model_inputs = data_collator(samples)\n-\n-                    # Model forward\n-                    metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                        state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    eval_metrics.append(metrics)\n-\n-                # get eval metrics\n-                eval_metrics = get_metrics(eval_metrics)\n-                eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-                # Update progress bar\n-                epochs.write(f\"Step... ({cur_step} | Loss: {eval_metrics['loss']}, Acc: {eval_metrics['accuracy']})\")\n-\n-                # Save metrics\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if cur_step % training_args.save_steps == 0 and cur_step > 0:\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-    # Eval after training\n-    if training_args.do_eval:\n-        num_eval_samples = len(tokenized_datasets[\"validation\"])\n-        # Avoid using jax.numpy here in case of TPU training\n-        eval_samples_idx = np.arange(num_eval_samples)\n-        eval_batch_idx = generate_batch_splits(eval_samples_idx, eval_batch_size, drop_last=False)\n-\n-        eval_metrics = []\n-        for i, batch_idx in enumerate(tqdm(eval_batch_idx, desc=\"Evaluating ...\", position=2)):\n-            samples = [tokenized_datasets[\"validation\"][int(idx)] for idx in batch_idx]\n-            model_inputs = data_collator(samples)\n-\n-            # Model forward\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, model_inputs.data, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-        # get eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(lambda metric: jnp.mean(metric).item(), eval_metrics)\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "8b9885279668093a7089614767db6cd237e64e93",
            "filename": "examples/flax/language-modeling/t5_tokenizer_model.py",
            "status": "removed",
            "additions": 0,
            "deletions": 117,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Ft5_tokenizer_model.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,117 +0,0 @@\n-#!/usr/bin/env python3\n-import json\n-from collections.abc import Iterator\n-from typing import Union\n-\n-from tokenizers import AddedToken, Regex, Tokenizer, decoders, normalizers, pre_tokenizers, trainers\n-from tokenizers.implementations.base_tokenizer import BaseTokenizer\n-from tokenizers.models import Unigram\n-from tokenizers.processors import TemplateProcessing\n-\n-\n-class SentencePieceUnigramTokenizer(BaseTokenizer):\n-    \"\"\"\n-    This class is a copy of `DeDLOC's tokenizer implementation <https://github.com/yandex-research/DeDLOC/blob/main/sahajbert/tokenizer/tokenizer_model.py>`__ .\n-\n-    Custom SentencePiece Unigram Tokenizer with NMT, NKFC, spaces and lower-casing characters normalization\n-    Represents the Unigram algorithm, with the pretokenization used by SentencePiece\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        replacement: str = \"â–\",\n-        add_prefix_space: bool = True,\n-        unk_token: Union[str, AddedToken] = \"<unk>\",\n-        eos_token: Union[str, AddedToken] = \"</s>\",\n-        pad_token: Union[str, AddedToken] = \"<pad>\",\n-    ):\n-        self.special_tokens = {\n-            \"pad\": {\"id\": 0, \"token\": pad_token},\n-            \"eos\": {\"id\": 1, \"token\": eos_token},\n-            \"unk\": {\"id\": 2, \"token\": unk_token},\n-        }\n-\n-        self.special_tokens_list = [None] * len(self.special_tokens)\n-        for token_dict in self.special_tokens.values():\n-            self.special_tokens_list[token_dict[\"id\"]] = token_dict[\"token\"]\n-\n-        tokenizer = Tokenizer(Unigram())\n-\n-        tokenizer.normalizer = normalizers.Sequence(\n-            [\n-                normalizers.Nmt(),\n-                normalizers.NFKC(),\n-                normalizers.Replace(Regex(\" {2,}\"), \" \"),\n-                normalizers.Lowercase(),\n-            ]\n-        )\n-        tokenizer.pre_tokenizer = pre_tokenizers.Sequence(\n-            [\n-                pre_tokenizers.Metaspace(\n-                    replacement=replacement, prepend_scheme=\"always\" if add_prefix_space else \"never\"\n-                ),\n-                pre_tokenizers.Digits(individual_digits=True),\n-                pre_tokenizers.Punctuation(),\n-            ]\n-        )\n-        tokenizer.decoder = decoders.Metaspace(\n-            replacement=replacement, prepend_scheme=\"always\" if add_prefix_space else \"never\"\n-        )\n-\n-        tokenizer.post_processor = TemplateProcessing(\n-            single=f\"$A {self.special_tokens['eos']['token']}\",\n-            special_tokens=[(self.special_tokens[\"eos\"][\"token\"], self.special_tokens[\"eos\"][\"id\"])],\n-        )\n-\n-        parameters = {\n-            \"model\": \"SentencePieceUnigram\",\n-            \"replacement\": replacement,\n-            \"add_prefix_space\": add_prefix_space,\n-        }\n-\n-        super().__init__(tokenizer, parameters)\n-\n-    def train(\n-        self,\n-        files: Union[str, list[str]],\n-        vocab_size: int = 8000,\n-        show_progress: bool = True,\n-    ):\n-        \"\"\"Train the model using the given files\"\"\"\n-\n-        trainer = trainers.UnigramTrainer(\n-            vocab_size=vocab_size,\n-            special_tokens=self.special_tokens_list,\n-            show_progress=show_progress,\n-        )\n-\n-        if isinstance(files, str):\n-            files = [files]\n-        self._tokenizer.train(files, trainer=trainer)\n-\n-        self.add_unk_id()\n-\n-    def train_from_iterator(\n-        self,\n-        iterator: Union[Iterator[str], Iterator[Iterator[str]]],\n-        vocab_size: int = 8000,\n-        show_progress: bool = True,\n-    ):\n-        \"\"\"Train the model using the given iterator\"\"\"\n-\n-        trainer = trainers.UnigramTrainer(\n-            vocab_size=vocab_size,\n-            special_tokens=self.special_tokens_list,\n-            show_progress=show_progress,\n-        )\n-\n-        self._tokenizer.train_from_iterator(iterator, trainer=trainer)\n-\n-        self.add_unk_id()\n-\n-    def add_unk_id(self):\n-        tokenizer_json = json.loads(self._tokenizer.to_str())\n-\n-        tokenizer_json[\"model\"][\"unk_id\"] = self.special_tokens[\"unk\"][\"id\"]\n-\n-        self._tokenizer = Tokenizer.from_str(json.dumps(tokenizer_json))"
        },
        {
            "sha": "2f6caa984d4bc19cabc32e6a23daa6a3cfacaf6f",
            "filename": "examples/flax/question-answering/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 104,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,104 +0,0 @@\n-<!---\n-Copyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Question Answering examples\n-\n-Based on the script [`run_qa.py`](https://github.com/huggingface/transformers/blob/main/examples/flax/question-answering/run_qa.py).\n-\n-**Note:** This script only works with models that have a fast tokenizer (backed by the ðŸ¤— Tokenizers library) as it\n-uses special features of those tokenizers. You can check if your favorite model has a fast tokenizer in\n-[this table](https://huggingface.co/transformers/index.html#supported-frameworks), if it doesn't you can still use the old version\n-of the script.\n-\n-\n-The following example fine-tunes BERT on SQuAD:\n-\n-\n-```bash\n-python run_qa.py \\\n-  --model_name_or_path google-bert/bert-base-uncased \\\n-  --dataset_name squad \\\n-  --do_train   \\\n-  --do_eval   \\\n-  --max_seq_length 384 \\\n-  --doc_stride 128 \\\n-  --learning_rate 3e-5 \\\n-  --num_train_epochs 2 \\\n-  --per_device_train_batch_size 12 \\\n-  --output_dir ./bert-qa-squad \\\n-  --eval_steps 1000 \\\n-  --push_to_hub\n-```\n-\n-Using the command above, the script will train for 2 epochs and run eval after each epoch. \n-Metrics and hyperparameters are stored in Tensorflow event files in `--output_dir`.\n-You can see the results by running `tensorboard` in that directory:\n-\n-```bash\n-$ tensorboard --logdir .\n-```\n-\n-or directly on the hub under *Training metrics*.\n-\n-Training with the previously defined hyper-parameters yields the following results:\n-\n-```bash\n-f1 = 88.62\n-exact_match = 81.34\n-```\n-\n-sample Metrics - [tfhub.dev](https://tensorboard.dev/experiment/6gU75Hx8TGCnc6tr4ZgI9Q)\n-\n-Here is an example training on 4 TITAN RTX GPUs and Bert Whole Word Masking uncased model to reach a F1 > 93 on SQuAD1.1:\n-\n-```bash\n-export CUDA_VISIBLE_DEVICES=0,1,2,3\n-python run_qa.py   \\\n---model_name_or_path google-bert/bert-large-uncased-whole-word-masking   \\\n---dataset_name squad   \\\n---do_train   \\\n---do_eval   \\\n---per_device_train_batch_size 6   \\\n---learning_rate 3e-5   \\\n---num_train_epochs 2   \\\n---max_seq_length 384   \\\n---doc_stride 128   \\\n---output_dir ./wwm_uncased_finetuned_squad/ \\\n---eval_steps 1000 \\\n---push_to_hub\n-```\n-\n-Training with the previously defined hyper-parameters yields the following results:\n-\n-```bash\n-f1 = 93.31\n-exact_match = 87.04\n-```\n-\n-\n-### Usage notes\n-\n-Note that when contexts are long they may be split into multiple training cases, not all of which may contain\n-the answer span. \n-\n-As-is, the example script will train on SQuAD or any other question-answering dataset formatted the same way, and can handle user\n-inputs as well.\n-\n-### Memory usage and data loading\n-\n-One thing to note is that all data is loaded into memory in this script. Most question answering datasets are small\n-enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\n-data streaming."
        },
        {
            "sha": "e7bf43910c3c8aa6ee05dec90748834304430e0f",
            "filename": "examples/flax/question-answering/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,5 +0,0 @@\n-datasets >= 1.8.0\n-jax>=0.2.17\n-jaxlib>=0.1.68\n-flax>=0.3.5\n-optax>=0.0.8\n\\ No newline at end of file"
        },
        {
            "sha": "1d83b54a4e3f783b9e0382b05649c66bc1b035b6",
            "filename": "examples/flax/question-answering/run_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1085,
            "changes": 1085,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frun_qa.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,1085 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for question answering.\n-\"\"\"\n-# You can also adapt this script on your own question answering task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import math\n-import os\n-import random\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from pathlib import Path\n-from typing import Any, Callable, Optional\n-\n-import datasets\n-import evaluate\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import struct, traverse_util\n-from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-from utils_qa import postprocess_qa_predictions\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    EvalPrediction,\n-    FlaxAutoModelForQuestionAnswering,\n-    HfArgumentParser,\n-    PreTrainedTokenizerFast,\n-    is_tensorboard_available,\n-)\n-from transformers.utils import check_min_version, send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-Array = Any\n-Dataset = datasets.arrow_dataset.Dataset\n-PRNGKey = Any\n-\n-\n-# region Arguments\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_seq_length: int = field(\n-        default=384,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when\"\n-                \" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU).\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    version_2_with_negative: bool = field(\n-        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n-    )\n-    null_score_diff_threshold: float = field(\n-        default=0.0,\n-        metadata={\n-            \"help\": (\n-                \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n-                \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n-                \"Only useful when `version_2_with_negative=True`.\"\n-            )\n-        },\n-    )\n-    doc_stride: int = field(\n-        default=128,\n-        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\n-    )\n-    n_best_size: int = field(\n-        default=20,\n-        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n-    )\n-    max_answer_length: int = field(\n-        default=30,\n-        metadata={\n-            \"help\": (\n-                \"The maximum length of an answer that can be generated. This is needed because the start \"\n-                \"and end predictions are not conditioned on one another.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if (\n-            self.dataset_name is None\n-            and self.train_file is None\n-            and self.validation_file is None\n-            and self.test_file is None\n-        ):\n-            raise ValueError(\"Need either a dataset name or a training/validation file/test_file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-            if self.test_file is not None:\n-                extension = self.test_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n-\n-\n-# endregion\n-\n-\n-# region Create a train state\n-def create_train_state(\n-    model: FlaxAutoModelForQuestionAnswering,\n-    learning_rate_fn: Callable[[int], float],\n-    num_labels: int,\n-    training_args: TrainingArguments,\n-) -> train_state.TrainState:\n-    \"\"\"Create initial training state.\"\"\"\n-\n-    class TrainState(train_state.TrainState):\n-        \"\"\"Train state with an Optax optimizer.\n-\n-        The two functions below differ depending on whether the task is classification\n-        or regression.\n-\n-        Args:\n-          logits_fn: Applied to last layer to obtain the logits.\n-          loss_fn: Function to compute the loss.\n-        \"\"\"\n-\n-        logits_fn: Callable = struct.field(pytree_node=False)\n-        loss_fn: Callable = struct.field(pytree_node=False)\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    tx = optax.adamw(\n-        learning_rate=learning_rate_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    def cross_entropy_loss(logits, labels):\n-        start_loss = optax.softmax_cross_entropy(logits[0], onehot(labels[0], num_classes=num_labels))\n-        end_loss = optax.softmax_cross_entropy(logits[1], onehot(labels[1], num_classes=num_labels))\n-        xentropy = (start_loss + end_loss) / 2.0\n-        return jnp.mean(xentropy)\n-\n-    return TrainState.create(\n-        apply_fn=model.__call__,\n-        params=model.params,\n-        tx=tx,\n-        logits_fn=lambda logits: logits,\n-        loss_fn=cross_entropy_loss,\n-    )\n-\n-\n-# endregion\n-\n-\n-# region Create learning rate function\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-# endregion\n-\n-\n-# region train data iterator\n-def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n-    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n-    steps_per_epoch = len(dataset) // batch_size\n-    perms = jax.random.permutation(rng, len(dataset))\n-    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-    perms = perms.reshape((steps_per_epoch, batch_size))\n-\n-    for perm in perms:\n-        batch = dataset[perm]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-        batch = shard(batch)\n-\n-        yield batch\n-\n-\n-# endregion\n-\n-\n-# region eval data iterator\n-def eval_data_collator(dataset: Dataset, batch_size: int):\n-    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n-    batch_idx = np.arange(len(dataset))\n-\n-    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n-    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        # Ignore `offset_mapping` to avoid numpy/JAX array conversion issue.\n-        batch = {k: np.array(v) for k, v in batch.items() if k != \"offset_mapping\"}\n-\n-        yield batch\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument parsing\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_qa\", model_args, data_args, framework=\"flax\")\n-    # endregion\n-\n-    # region Logging\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-    # endregion\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # region Load Data\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        # Loading the dataset from local csv or json file.\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            field=\"data\",\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-    # endregion\n-\n-    # region Load pretrained model and tokenizer\n-    #\n-    # Load pretrained model and tokenizer\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=True,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    # endregion\n-\n-    # region Tokenizer check: this script requires a fast tokenizer.\n-    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise TypeError(\n-            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n-            \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n-            \" this requirement\"\n-        )\n-    # endregion\n-\n-    # region Preprocessing the datasets\n-    # Preprocessing is slightly different for training and evaluation.\n-    if training_args.do_train:\n-        column_names = raw_datasets[\"train\"].column_names\n-    elif training_args.do_eval:\n-        column_names = raw_datasets[\"validation\"].column_names\n-    else:\n-        column_names = raw_datasets[\"test\"].column_names\n-    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n-    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n-    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n-\n-    # Padding side determines if we do (question|context) or (context|question).\n-    pad_on_right = tokenizer.padding_side == \"right\"\n-\n-    if data_args.max_seq_length > tokenizer.model_max_length:\n-        logger.warning(\n-            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n-            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n-        )\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    # Training preprocessing\n-    def prepare_train_features(examples):\n-        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n-        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n-        # left whitespace\n-        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n-\n-        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n-        # in one example possible giving several features when a context is long, each of those features having a\n-        # context that overlaps a bit the context of the previous feature.\n-        tokenized_examples = tokenizer(\n-            examples[question_column_name if pad_on_right else context_column_name],\n-            examples[context_column_name if pad_on_right else question_column_name],\n-            truncation=\"only_second\" if pad_on_right else \"only_first\",\n-            max_length=max_seq_length,\n-            stride=data_args.doc_stride,\n-            return_overflowing_tokens=True,\n-            return_offsets_mapping=True,\n-            padding=\"max_length\",\n-        )\n-\n-        # Since one example might give us several features if it has a long context, we need a map from a feature to\n-        # its corresponding example. This key gives us just that.\n-        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n-        # The offset mappings will give us a map from token to character position in the original context. This will\n-        # help us compute the start_positions and end_positions.\n-        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n-\n-        # Let's label those examples!\n-        tokenized_examples[\"start_positions\"] = []\n-        tokenized_examples[\"end_positions\"] = []\n-\n-        for i, offsets in enumerate(offset_mapping):\n-            # We will label impossible answers with the index of the CLS token.\n-            input_ids = tokenized_examples[\"input_ids\"][i]\n-            cls_index = input_ids.index(tokenizer.cls_token_id)\n-\n-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n-            sequence_ids = tokenized_examples.sequence_ids(i)\n-\n-            # One example can give several spans, this is the index of the example containing this span of text.\n-            sample_index = sample_mapping[i]\n-            answers = examples[answer_column_name][sample_index]\n-            # If no answers are given, set the cls_index as answer.\n-            if len(answers[\"answer_start\"]) == 0:\n-                tokenized_examples[\"start_positions\"].append(cls_index)\n-                tokenized_examples[\"end_positions\"].append(cls_index)\n-            else:\n-                # Start/end character index of the answer in the text.\n-                start_char = answers[\"answer_start\"][0]\n-                end_char = start_char + len(answers[\"text\"][0])\n-\n-                # Start token index of the current span in the text.\n-                token_start_index = 0\n-                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n-                    token_start_index += 1\n-\n-                # End token index of the current span in the text.\n-                token_end_index = len(input_ids) - 1\n-                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n-                    token_end_index -= 1\n-\n-                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n-                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n-                    tokenized_examples[\"start_positions\"].append(cls_index)\n-                    tokenized_examples[\"end_positions\"].append(cls_index)\n-                else:\n-                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n-                    # Note: we could go after the last offset if the answer is the last word (edge case).\n-                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n-                        token_start_index += 1\n-                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n-                    while offsets[token_end_index][1] >= end_char:\n-                        token_end_index -= 1\n-                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n-\n-        return tokenized_examples\n-\n-    processed_raw_datasets = {}\n-    if training_args.do_train:\n-        if \"train\" not in raw_datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = raw_datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            # We will select sample from whole data if argument is specified\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        # Create train feature from dataset\n-        train_dataset = train_dataset.map(\n-            prepare_train_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_train_samples is not None:\n-            # Number of samples might increase during Feature Creation, We select only specified max samples\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        processed_raw_datasets[\"train\"] = train_dataset\n-\n-    # Validation preprocessing\n-    def prepare_validation_features(examples):\n-        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n-        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n-        # left whitespace\n-        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n-\n-        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n-        # in one example possible giving several features when a context is long, each of those features having a\n-        # context that overlaps a bit the context of the previous feature.\n-        tokenized_examples = tokenizer(\n-            examples[question_column_name if pad_on_right else context_column_name],\n-            examples[context_column_name if pad_on_right else question_column_name],\n-            truncation=\"only_second\" if pad_on_right else \"only_first\",\n-            max_length=max_seq_length,\n-            stride=data_args.doc_stride,\n-            return_overflowing_tokens=True,\n-            return_offsets_mapping=True,\n-            padding=\"max_length\",\n-        )\n-\n-        # Since one example might give us several features if it has a long context, we need a map from a feature to\n-        # its corresponding example. This key gives us just that.\n-        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n-\n-        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n-        # corresponding example_id and we will store the offset mappings.\n-        tokenized_examples[\"example_id\"] = []\n-\n-        for i in range(len(tokenized_examples[\"input_ids\"])):\n-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n-            sequence_ids = tokenized_examples.sequence_ids(i)\n-            context_index = 1 if pad_on_right else 0\n-\n-            # One example can give several spans, this is the index of the example containing this span of text.\n-            sample_index = sample_mapping[i]\n-            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n-\n-            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n-            # position is part of the context or not.\n-            tokenized_examples[\"offset_mapping\"][i] = [\n-                (o if sequence_ids[k] == context_index else None)\n-                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n-            ]\n-\n-        return tokenized_examples\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in raw_datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_examples = raw_datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            # We will select sample from whole data\n-            max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)\n-            eval_examples = eval_examples.select(range(max_eval_samples))\n-        # Validation Feature Creation\n-        eval_dataset = eval_examples.map(\n-            prepare_validation_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_eval_samples is not None:\n-            # During Feature creation dataset samples might increase, we will select required samples again\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        processed_raw_datasets[\"validation\"] = eval_dataset\n-\n-    if training_args.do_predict:\n-        if \"test\" not in raw_datasets:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        predict_examples = raw_datasets[\"test\"]\n-        if data_args.max_predict_samples is not None:\n-            # We will select sample from whole data\n-            predict_examples = predict_examples.select(range(data_args.max_predict_samples))\n-        # Predict Feature Creation\n-        predict_dataset = predict_examples.map(\n-            prepare_validation_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_predict_samples is not None:\n-            # During Feature creation dataset samples might increase, we will select required samples again\n-            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n-            predict_dataset = predict_dataset.select(range(max_predict_samples))\n-        processed_raw_datasets[\"test\"] = predict_dataset\n-    # endregion\n-\n-    # region Metrics and Post-processing:\n-    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n-        # Post-processing: we match the start logits and end logits to answers in the original context.\n-        predictions = postprocess_qa_predictions(\n-            examples=examples,\n-            features=features,\n-            predictions=predictions,\n-            version_2_with_negative=data_args.version_2_with_negative,\n-            n_best_size=data_args.n_best_size,\n-            max_answer_length=data_args.max_answer_length,\n-            null_score_diff_threshold=data_args.null_score_diff_threshold,\n-            output_dir=training_args.output_dir,\n-            prefix=stage,\n-        )\n-        # Format the result to the format the metric expects.\n-        if data_args.version_2_with_negative:\n-            formatted_predictions = [\n-                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n-            ]\n-        else:\n-            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n-\n-        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n-        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n-\n-    metric = evaluate.load(\n-        \"squad_v2\" if data_args.version_2_with_negative else \"squad\", cache_dir=model_args.cache_dir\n-    )\n-\n-    def compute_metrics(p: EvalPrediction):\n-        return metric.compute(predictions=p.predictions, references=p.label_ids)\n-\n-    # Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n-    def create_and_fill_np_array(start_or_end_logits, dataset, max_len):\n-        \"\"\"\n-        Create and fill numpy array of size len_of_validation_data * max_length_of_output_tensor\n-\n-        Args:\n-            start_or_end_logits(:obj:`tensor`):\n-                This is the output predictions of the model. We can only enter either start or end logits.\n-            eval_dataset: Evaluation dataset\n-            max_len(:obj:`int`):\n-                The maximum length of the output tensor. ( See the model.eval() part for more details )\n-        \"\"\"\n-\n-        step = 0\n-        # create a numpy array and fill it with -100.\n-        logits_concat = np.full((len(dataset), max_len), -100, dtype=np.float64)\n-        # Now since we have create an array now we will populate it with the outputs of the model.\n-        for i, output_logit in enumerate(start_or_end_logits):  # populate columns\n-            # We have to fill it such that we have to take the whole tensor and replace it on the newly created array\n-            # And after every iteration we have to change the step\n-\n-            batch_size = output_logit.shape[0]\n-            cols = output_logit.shape[1]\n-\n-            if step + batch_size < len(dataset):\n-                logits_concat[step : step + batch_size, :cols] = output_logit\n-            else:\n-                logits_concat[step:, :cols] = output_logit[: len(dataset) - step]\n-\n-            step += batch_size\n-\n-        return logits_concat\n-\n-    # endregion\n-\n-    # region Training steps and logging init\n-    train_dataset = processed_raw_datasets[\"train\"]\n-    eval_dataset = processed_raw_datasets[\"validation\"]\n-\n-    # Log a few random samples from the training set:\n-    for index in random.sample(range(len(train_dataset)), 3):\n-        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n-\n-    # Define a summary writer\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(training_args.output_dir)\n-            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    def write_train_metric(summary_writer, train_metrics, train_time, step):\n-        summary_writer.scalar(\"train_time\", train_time, step)\n-\n-        train_metrics = get_metrics(train_metrics)\n-        for key, vals in train_metrics.items():\n-            tag = f\"train_{key}\"\n-            for i, val in enumerate(vals):\n-                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    def write_eval_metric(summary_writer, eval_metrics, step):\n-        for metric_name, value in eval_metrics.items():\n-            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-    num_epochs = int(training_args.num_train_epochs)\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.local_device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.local_device_count()\n-    # endregion\n-\n-    # region Load model\n-    model = FlaxAutoModelForQuestionAnswering.from_pretrained(\n-        model_args.model_name_or_path,\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-        seed=training_args.seed,\n-        dtype=getattr(jnp, model_args.dtype),\n-    )\n-\n-    learning_rate_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    state = create_train_state(model, learning_rate_fn, num_labels=max_seq_length, training_args=training_args)\n-    # endregion\n-\n-    # region Define train step functions\n-    def train_step(\n-        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n-    ) -> tuple[train_state.TrainState, float]:\n-        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-        start_positions = batch.pop(\"start_positions\")\n-        end_positions = batch.pop(\"end_positions\")\n-        targets = (start_positions, end_positions)\n-\n-        def loss_fn(params):\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)\n-            loss = state.loss_fn(logits, targets)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n-        return new_state, metrics, new_dropout_rng\n-\n-    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n-    # endregion\n-\n-    # region Define eval step functions\n-    def eval_step(state, batch):\n-        logits = state.apply_fn(**batch, params=state.params, train=False)\n-        return state.logits_fn(logits)\n-\n-    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n-    # endregion\n-\n-    # region Define train and eval loop\n-    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n-    train_time = 0\n-\n-    # make sure weights are replicated on each device\n-    state = replicate(state)\n-\n-    train_time = 0\n-    step_per_epoch = len(train_dataset) // train_batch_size\n-    total_steps = step_per_epoch * num_epochs\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # train\n-        for step, batch in enumerate(\n-            tqdm(\n-                train_data_collator(input_rng, train_dataset, train_batch_size),\n-                total=step_per_epoch,\n-                desc=\"Training...\",\n-                position=1,\n-            ),\n-            1,\n-        ):\n-            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = epoch * step_per_epoch + step\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate']})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if (\n-                training_args.do_eval\n-                and (cur_step % training_args.eval_steps == 0 or cur_step % step_per_epoch == 0)\n-                and cur_step > 0\n-            ):\n-                eval_metrics = {}\n-                all_start_logits = []\n-                all_end_logits = []\n-                # evaluate\n-                for batch in tqdm(\n-                    eval_data_collator(eval_dataset, eval_batch_size),\n-                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n-                    desc=\"Evaluating ...\",\n-                    position=2,\n-                ):\n-                    _ = batch.pop(\"example_id\")\n-                    predictions = pad_shard_unpad(p_eval_step)(\n-                        state, batch, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    start_logits = np.array(predictions[0])\n-                    end_logits = np.array(predictions[1])\n-                    all_start_logits.append(start_logits)\n-                    all_end_logits.append(end_logits)\n-\n-                max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n-\n-                # concatenate the numpy array\n-                start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n-                end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n-\n-                # delete the list of numpy arrays\n-                del all_start_logits\n-                del all_end_logits\n-                outputs_numpy = (start_logits_concat, end_logits_concat)\n-                prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n-                eval_metrics = compute_metrics(prediction)\n-\n-                logger.info(f\"Step... ({cur_step}/{total_steps} | Evaluation metrics: {eval_metrics})\")\n-\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(unreplicate(state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-        epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n-    # endregion\n-\n-    # Eval after training\n-    if training_args.do_eval:\n-        eval_metrics = {}\n-        all_start_logits = []\n-        all_end_logits = []\n-\n-        eval_loader = eval_data_collator(eval_dataset, eval_batch_size)\n-        for batch in tqdm(\n-            eval_loader, total=math.ceil(len(eval_dataset) / eval_batch_size), desc=\"Evaluating ...\", position=2\n-        ):\n-            _ = batch.pop(\"example_id\")\n-            predictions = pad_shard_unpad(p_eval_step)(state, batch, min_device_batch=per_device_eval_batch_size)\n-            start_logits = np.array(predictions[0])\n-            end_logits = np.array(predictions[1])\n-            all_start_logits.append(start_logits)\n-            all_end_logits.append(end_logits)\n-\n-        max_len = max([x.shape[1] for x in all_start_logits])  # Get the max_length of the tensor\n-\n-        # concatenate the numpy array\n-        start_logits_concat = create_and_fill_np_array(all_start_logits, eval_dataset, max_len)\n-        end_logits_concat = create_and_fill_np_array(all_end_logits, eval_dataset, max_len)\n-\n-        # delete the list of numpy arrays\n-        del all_start_logits\n-        del all_end_logits\n-        outputs_numpy = (start_logits_concat, end_logits_concat)\n-        prediction = post_processing_function(eval_examples, eval_dataset, outputs_numpy)\n-        eval_metrics = compute_metrics(prediction)\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "b30322b0071fff2f919bb0b2c48a77513430b6b4",
            "filename": "examples/flax/question-answering/utils_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 443,
            "changes": 443,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Futils_qa.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,443 +0,0 @@\n-# Copyright 2020 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Post-processing utilities for question answering.\n-\"\"\"\n-\n-import collections\n-import json\n-import logging\n-import os\n-from typing import Optional\n-\n-import numpy as np\n-from tqdm.auto import tqdm\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def postprocess_qa_predictions(\n-    examples,\n-    features,\n-    predictions: tuple[np.ndarray, np.ndarray],\n-    version_2_with_negative: bool = False,\n-    n_best_size: int = 20,\n-    max_answer_length: int = 30,\n-    null_score_diff_threshold: float = 0.0,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n-):\n-    \"\"\"\n-    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n-    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n-\n-    Args:\n-        examples: The non-preprocessed dataset (see the main script for more information).\n-        features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n-            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n-            first dimension must match the number of elements of :obj:`features`.\n-        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether or not the underlying dataset contains examples with no answers.\n-        n_best_size (:obj:`int`, `optional`, defaults to 20):\n-            The total number of n-best predictions to generate when looking for an answer.\n-        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n-            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n-            are not conditioned on one another.\n-        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n-            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n-            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n-            the null answer for an example giving several features is the minimum of the scores for the null answer on\n-            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n-\n-            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n-        output_dir (:obj:`str`, `optional`):\n-            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n-            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n-            answers, are saved in `output_dir`.\n-        prefix (:obj:`str`, `optional`):\n-            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n-        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n-            ``logging`` log level (e.g., ``logging.WARNING``)\n-    \"\"\"\n-    if len(predictions) != 2:\n-        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n-    all_start_logits, all_end_logits = predictions\n-\n-    if len(predictions[0]) != len(features):\n-        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n-\n-    # Build a map example to its corresponding features.\n-    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n-    features_per_example = collections.defaultdict(list)\n-    for i, feature in enumerate(features):\n-        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n-\n-    # The dictionaries we have to fill.\n-    all_predictions = collections.OrderedDict()\n-    all_nbest_json = collections.OrderedDict()\n-    if version_2_with_negative:\n-        scores_diff_json = collections.OrderedDict()\n-\n-    # Logging.\n-    logger.setLevel(log_level)\n-    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n-\n-    # Let's loop over all the examples!\n-    for example_index, example in enumerate(tqdm(examples)):\n-        # Those are the indices of the features associated to the current example.\n-        feature_indices = features_per_example[example_index]\n-\n-        min_null_prediction = None\n-        prelim_predictions = []\n-\n-        # Looping through all the features associated to the current example.\n-        for feature_index in feature_indices:\n-            # We grab the predictions of the model for this feature.\n-            start_logits = all_start_logits[feature_index]\n-            end_logits = all_end_logits[feature_index]\n-            # This is what will allow us to map some the positions in our logits to span of texts in the original\n-            # context.\n-            offset_mapping = features[feature_index][\"offset_mapping\"]\n-            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n-            # available in the current feature.\n-            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n-\n-            # Update minimum null prediction.\n-            feature_null_score = start_logits[0] + end_logits[0]\n-            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n-                min_null_prediction = {\n-                    \"offsets\": (0, 0),\n-                    \"score\": feature_null_score,\n-                    \"start_logit\": start_logits[0],\n-                    \"end_logit\": end_logits[0],\n-                }\n-\n-            # Go through all possibilities for the `n_best_size` greater start and end logits.\n-            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n-            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n-            for start_index in start_indexes:\n-                for end_index in end_indexes:\n-                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n-                    # to part of the input_ids that are not in the context.\n-                    if (\n-                        start_index >= len(offset_mapping)\n-                        or end_index >= len(offset_mapping)\n-                        or offset_mapping[start_index] is None\n-                        or len(offset_mapping[start_index]) < 2\n-                        or offset_mapping[end_index] is None\n-                        or len(offset_mapping[end_index]) < 2\n-                    ):\n-                        continue\n-                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n-                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n-                        continue\n-                    # Don't consider answer that don't have the maximum context available (if such information is\n-                    # provided).\n-                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n-                        continue\n-\n-                    prelim_predictions.append(\n-                        {\n-                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n-                            \"score\": start_logits[start_index] + end_logits[end_index],\n-                            \"start_logit\": start_logits[start_index],\n-                            \"end_logit\": end_logits[end_index],\n-                        }\n-                    )\n-        if version_2_with_negative and min_null_prediction is not None:\n-            # Add the minimum null prediction\n-            prelim_predictions.append(min_null_prediction)\n-            null_score = min_null_prediction[\"score\"]\n-\n-        # Only keep the best `n_best_size` predictions.\n-        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n-\n-        # Add back the minimum null prediction if it was removed because of its low score.\n-        if (\n-            version_2_with_negative\n-            and min_null_prediction is not None\n-            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n-        ):\n-            predictions.append(min_null_prediction)\n-\n-        # Use the offsets to gather the answer text in the original context.\n-        context = example[\"context\"]\n-        for pred in predictions:\n-            offsets = pred.pop(\"offsets\")\n-            pred[\"text\"] = context[offsets[0] : offsets[1]]\n-\n-        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n-        # failure.\n-        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n-            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n-\n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n-        # the LogSumExp trick).\n-        scores = np.array([pred.pop(\"score\") for pred in predictions])\n-        exp_scores = np.exp(scores - np.max(scores))\n-        probs = exp_scores / exp_scores.sum()\n-\n-        # Include the probabilities in our predictions.\n-        for prob, pred in zip(probs, predictions):\n-            pred[\"probability\"] = prob\n-\n-        # Pick the best prediction. If the null answer is not possible, this is easy.\n-        if not version_2_with_negative:\n-            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n-        else:\n-            # Otherwise we first need to find the best non-empty prediction.\n-            i = 0\n-            while predictions[i][\"text\"] == \"\":\n-                i += 1\n-            best_non_null_pred = predictions[i]\n-\n-            # Then we compare to the null prediction using the threshold.\n-            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n-            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n-            if score_diff > null_score_diff_threshold:\n-                all_predictions[example[\"id\"]] = \"\"\n-            else:\n-                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n-\n-        # Make `predictions` JSON-serializable by casting np.float back to float.\n-        all_nbest_json[example[\"id\"]] = [\n-            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n-            for pred in predictions\n-        ]\n-\n-    # If we have an output_dir, let's save all those dicts.\n-    if output_dir is not None:\n-        if not os.path.isdir(output_dir):\n-            raise OSError(f\"{output_dir} is not a directory.\")\n-\n-        prediction_file = os.path.join(\n-            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n-        )\n-        nbest_file = os.path.join(\n-            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n-        )\n-        if version_2_with_negative:\n-            null_odds_file = os.path.join(\n-                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n-            )\n-\n-        logger.info(f\"Saving predictions to {prediction_file}.\")\n-        with open(prediction_file, \"w\") as writer:\n-            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n-        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n-        with open(nbest_file, \"w\") as writer:\n-            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n-        if version_2_with_negative:\n-            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n-            with open(null_odds_file, \"w\") as writer:\n-                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n-\n-    return all_predictions\n-\n-\n-def postprocess_qa_predictions_with_beam_search(\n-    examples,\n-    features,\n-    predictions: tuple[np.ndarray, np.ndarray],\n-    version_2_with_negative: bool = False,\n-    n_best_size: int = 20,\n-    max_answer_length: int = 30,\n-    start_n_top: int = 5,\n-    end_n_top: int = 5,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n-):\n-    \"\"\"\n-    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\n-    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\n-    cls token predictions.\n-\n-    Args:\n-        examples: The non-preprocessed dataset (see the main script for more information).\n-        features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n-            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n-            first dimension must match the number of elements of :obj:`features`.\n-        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether or not the underlying dataset contains examples with no answers.\n-        n_best_size (:obj:`int`, `optional`, defaults to 20):\n-            The total number of n-best predictions to generate when looking for an answer.\n-        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n-            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n-            are not conditioned on one another.\n-        start_n_top (:obj:`int`, `optional`, defaults to 5):\n-            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\n-        end_n_top (:obj:`int`, `optional`, defaults to 5):\n-            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\n-        output_dir (:obj:`str`, `optional`):\n-            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n-            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n-            answers, are saved in `output_dir`.\n-        prefix (:obj:`str`, `optional`):\n-            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n-        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n-            ``logging`` log level (e.g., ``logging.WARNING``)\n-    \"\"\"\n-    if len(predictions) != 5:\n-        raise ValueError(\"`predictions` should be a tuple with five elements.\")\n-    start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits = predictions\n-\n-    if len(predictions[0]) != len(features):\n-        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n-\n-    # Build a map example to its corresponding features.\n-    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n-    features_per_example = collections.defaultdict(list)\n-    for i, feature in enumerate(features):\n-        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n-\n-    # The dictionaries we have to fill.\n-    all_predictions = collections.OrderedDict()\n-    all_nbest_json = collections.OrderedDict()\n-    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n-\n-    # Logging.\n-    logger.setLevel(log_level)\n-    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n-\n-    # Let's loop over all the examples!\n-    for example_index, example in enumerate(tqdm(examples)):\n-        # Those are the indices of the features associated to the current example.\n-        feature_indices = features_per_example[example_index]\n-\n-        min_null_score = None\n-        prelim_predictions = []\n-\n-        # Looping through all the features associated to the current example.\n-        for feature_index in feature_indices:\n-            # We grab the predictions of the model for this feature.\n-            start_log_prob = start_top_log_probs[feature_index]\n-            start_indexes = start_top_index[feature_index]\n-            end_log_prob = end_top_log_probs[feature_index]\n-            end_indexes = end_top_index[feature_index]\n-            feature_null_score = cls_logits[feature_index]\n-            # This is what will allow us to map some the positions in our logits to span of texts in the original\n-            # context.\n-            offset_mapping = features[feature_index][\"offset_mapping\"]\n-            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n-            # available in the current feature.\n-            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n-\n-            # Update minimum null prediction\n-            if min_null_score is None or feature_null_score < min_null_score:\n-                min_null_score = feature_null_score\n-\n-            # Go through all possibilities for the `n_start_top`/`n_end_top` greater start and end logits.\n-            for i in range(start_n_top):\n-                for j in range(end_n_top):\n-                    start_index = int(start_indexes[i])\n-                    j_index = i * end_n_top + j\n-                    end_index = int(end_indexes[j_index])\n-                    # Don't consider out-of-scope answers (last part of the test should be unnecessary because of the\n-                    # p_mask but let's not take any risk)\n-                    if (\n-                        start_index >= len(offset_mapping)\n-                        or end_index >= len(offset_mapping)\n-                        or offset_mapping[start_index] is None\n-                        or len(offset_mapping[start_index]) < 2\n-                        or offset_mapping[end_index] is None\n-                        or len(offset_mapping[end_index]) < 2\n-                    ):\n-                        continue\n-\n-                    # Don't consider answers with a length negative or > max_answer_length.\n-                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n-                        continue\n-                    # Don't consider answer that don't have the maximum context available (if such information is\n-                    # provided).\n-                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n-                        continue\n-                    prelim_predictions.append(\n-                        {\n-                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n-                            \"score\": start_log_prob[i] + end_log_prob[j_index],\n-                            \"start_log_prob\": start_log_prob[i],\n-                            \"end_log_prob\": end_log_prob[j_index],\n-                        }\n-                    )\n-\n-        # Only keep the best `n_best_size` predictions.\n-        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n-\n-        # Use the offsets to gather the answer text in the original context.\n-        context = example[\"context\"]\n-        for pred in predictions:\n-            offsets = pred.pop(\"offsets\")\n-            pred[\"text\"] = context[offsets[0] : offsets[1]]\n-\n-        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n-        # failure.\n-        if len(predictions) == 0:\n-            # Without predictions min_null_score is going to be None and None will cause an exception later\n-            min_null_score = -2e-6\n-            predictions.insert(0, {\"text\": \"\", \"start_logit\": -1e-6, \"end_logit\": -1e-6, \"score\": min_null_score})\n-\n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n-        # the LogSumExp trick).\n-        scores = np.array([pred.pop(\"score\") for pred in predictions])\n-        exp_scores = np.exp(scores - np.max(scores))\n-        probs = exp_scores / exp_scores.sum()\n-\n-        # Include the probabilities in our predictions.\n-        for prob, pred in zip(probs, predictions):\n-            pred[\"probability\"] = prob\n-\n-        # Pick the best prediction and set the probability for the null answer.\n-        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n-        if version_2_with_negative:\n-            scores_diff_json[example[\"id\"]] = float(min_null_score)\n-\n-        # Make `predictions` JSON-serializable by casting np.float back to float.\n-        all_nbest_json[example[\"id\"]] = [\n-            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n-            for pred in predictions\n-        ]\n-\n-    # If we have an output_dir, let's save all those dicts.\n-    if output_dir is not None:\n-        if not os.path.isdir(output_dir):\n-            raise OSError(f\"{output_dir} is not a directory.\")\n-\n-        prediction_file = os.path.join(\n-            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n-        )\n-        nbest_file = os.path.join(\n-            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n-        )\n-        if version_2_with_negative:\n-            null_odds_file = os.path.join(\n-                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n-            )\n-\n-        logger.info(f\"Saving predictions to {prediction_file}.\")\n-        with open(prediction_file, \"w\") as writer:\n-            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n-        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n-        with open(nbest_file, \"w\") as writer:\n-            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n-        if version_2_with_negative:\n-            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n-            with open(null_odds_file, \"w\") as writer:\n-                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n-\n-    return all_predictions, scores_diff_json"
        },
        {
            "sha": "943c98761aa660c8d03e8f3d8fdd462ff612a8db",
            "filename": "examples/flax/speech-recognition/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 68,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,68 +0,0 @@\n-<!---\n-Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Automatic Speech Recognition - Flax Examples\n-\n-## Sequence to Sequence\n-\n-The script [`run_flax_speech_recognition_seq2seq.py`](https://github.com/huggingface/transformers/blob/main/examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py) \n-can be used to fine-tune any [Flax Speech Sequence-to-Sequence Model](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.FlaxAutoModelForSpeechSeq2Seq) \n-for automatic speech recognition on one of the [official speech recognition datasets](https://huggingface.co/datasets?task_ids=task_ids:automatic-speech-recognition) \n-or a custom dataset. This includes the Whisper model from OpenAI, or a warm-started Speech-Encoder-Decoder Model, \n-an example for which is included below.\n-\n-### Whisper Model\n-\n-We can load all components of the Whisper model directly from the pretrained checkpoint, including the pretrained model \n-weights, feature extractor and tokenizer. We simply have to specify the id of fine-tuning dataset and the necessary\n-training hyperparameters.\n-\n-The following example shows how to fine-tune the [Whisper small](https://huggingface.co/openai/whisper-small) checkpoint \n-on the Hindi subset of the [Common Voice 13](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0) dataset.\n-Note that before running this script you must accept the dataset's [terms of use](https://huggingface.co/datasets/mozilla-foundation/common_voice_13_0) \n-and register your Hugging Face Hub token on your device by running `huggingface-hub login`.\n-\n-```bash\n-python run_flax_speech_recognition_seq2seq.py \\\n-\t--model_name_or_path=\"openai/whisper-small\" \\\n-\t--dataset_name=\"mozilla-foundation/common_voice_13_0\" \\\n-\t--dataset_config_name=\"hi\" \\\n-\t--language=\"hindi\" \\\n-\t--train_split_name=\"train+validation\" \\\n-\t--eval_split_name=\"test\" \\\n-\t--output_dir=\"./whisper-small-hi-flax\" \\\n-\t--per_device_train_batch_size=\"16\" \\\n-\t--per_device_eval_batch_size=\"16\" \\\n-\t--num_train_epochs=\"10\" \\\n-\t--learning_rate=\"1e-4\" \\\n-\t--warmup_steps=\"500\" \\\n-\t--logging_steps=\"25\" \\\n-\t--generation_max_length=\"40\" \\\n-\t--preprocessing_num_workers=\"32\" \\\n-\t--dataloader_num_workers=\"32\" \\\n-\t--max_duration_in_seconds=\"30\" \\\n-\t--text_column_name=\"sentence\" \\\n-\t--overwrite_output_dir \\\n-\t--do_train \\\n-\t--do_eval \\\n-\t--predict_with_generate \\\n-\t--push_to_hub \\\n-\t--use_auth_token\n-```\n-\n-On a TPU v4-8, training should take approximately 25 minutes, with a final cross-entropy loss of 0.02 and word error \n-rate of **34%**. See the checkpoint [sanchit-gandhi/whisper-small-hi-flax](https://huggingface.co/sanchit-gandhi/whisper-small-hi-flax)\n-for an example training run."
        },
        {
            "sha": "b68b236ad76c2b06e7e7e7711d90c5f2914a7bcb",
            "filename": "examples/flax/speech-recognition/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,8 +0,0 @@\n-datasets[audio]>=2.14.0\n-jax>=0.3.6\n-jaxlib>=0.3.6\n-flax>=0.4.1\n-optax>=0.0.8\n-torch>=1.9.0\n-jiwer\n-evaluate"
        },
        {
            "sha": "5ef8216d25ff197e4e0f1ea54f4c5ccfd621152a",
            "filename": "examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py",
            "status": "removed",
            "additions": 0,
            "deletions": 877,
            "changes": 877,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,877 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the Flax library models for sequence to sequence speech recognition.\n-\"\"\"\n-# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n-\n-import logging\n-import os\n-import sys\n-import time\n-from dataclasses import field\n-from functools import partial\n-from pathlib import Path\n-from typing import Any, Callable, Optional, Union\n-\n-import datasets\n-import evaluate\n-import flax\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import DatasetDict, load_dataset\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n-from huggingface_hub import HfApi\n-from torch.utils.data import DataLoader\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoFeatureExtractor,\n-    AutoProcessor,\n-    AutoTokenizer,\n-    FlaxAutoModelForSpeechSeq2Seq,\n-    HfArgumentParser,\n-    Seq2SeqTrainingArguments,\n-    is_tensorboard_available,\n-)\n-from transformers.file_utils import get_full_repo_name\n-from transformers.utils import check_min_version, send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risk.\n-check_min_version(\"4.57.0.dev0\")\n-\n-require_version(\"datasets>=2.14.0\", \"To fix: pip install -r examples/flax/speech-recognition/requirements.txt\")\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-@flax.struct.dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    feature_extractor_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"feature extractor name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    use_auth_token: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"Will use the token generated when running `transformers login` (necessary to use this script \"\n-            \"with private models).\"\n-        },\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    num_beams: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n-                \"which is used during evaluation.\"\n-            )\n-        },\n-    )\n-\n-\n-@flax.struct.dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: str = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    text_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n-    )\n-    dataset_cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Path to cache directory for saving and loading datasets\"}\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-            \"value if set.\"\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-            \"value if set.\"\n-        },\n-    )\n-    audio_column_name: str = field(\n-        default=\"audio\",\n-        metadata={\"help\": \"The name of the dataset column containing the audio data. Defaults to 'audio'\"},\n-    )\n-    text_column_name: str = field(\n-        default=\"text\",\n-        metadata={\"help\": \"The name of the dataset column containing the text data. Defaults to 'text'\"},\n-    )\n-    max_duration_in_seconds: float = field(\n-        default=20.0,\n-        metadata={\"help\": \"Filter audio files that are longer than `max_duration_in_seconds` seconds\"},\n-    )\n-    min_duration_in_seconds: float = field(\n-        default=0.0,\n-        metadata={\"help\": \"Filter audio files that are shorter than `min_duration_in_seconds` seconds\"},\n-    )\n-    max_label_length: float = field(\n-        default=128,\n-        metadata={\"help\": \"Truncate transcriptions that are longer `max_eval_length` tokens.\"},\n-    )\n-    pad_input_to_multiple_of: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"If set will pad the input sequence to a multiple of the provided value. \"\n-            \"This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the inputs to max length.\"\n-        },\n-    )\n-    pad_target_to_multiple_of: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"If set will pad the target sequence to a multiple of the provided value. \"\n-            \"This is important to avoid triggering recompilations on TPU. If unspecified, will default to padding the targets to max length.\"\n-        },\n-    )\n-    preprocessing_only: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"Whether to only do data preprocessing and skip training. \"\n-            \"This is especially useful when data preprocessing errors out in distributed training due to timeout. \"\n-            \"In this case, one should run the preprocessing in a non-distributed setup with `preprocessing_only=True` \"\n-            \"so that the cached datasets can consequently be loaded in distributed training\"\n-        },\n-    )\n-    train_split_name: str = field(\n-        default=\"train\",\n-        metadata={\n-            \"help\": \"The name of the training data set split to use (via the datasets library). Defaults to 'train'\"\n-        },\n-    )\n-    eval_split_name: str = field(\n-        default=\"validation\",\n-        metadata={\n-            \"help\": \"The name of the evaluation data set split to use (via the datasets library). Defaults to 'validation'\"\n-        },\n-    )\n-    do_lower_case: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether the target text should be lower cased.\"},\n-    )\n-    language: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Language for multilingual fine-tuning. This argument should be set for multilingual fine-tuning \"\n-                \"only. For English speech recognition, it should be set to `None`.\"\n-            )\n-        },\n-    )\n-    task: str = field(\n-        default=\"transcribe\",\n-        metadata={\"help\": \"Task, either `transcribe` for speech recognition or `translate` for speech translation.\"},\n-    )\n-\n-\n-def shift_tokens_right(label_ids: np.array, decoder_start_token_id: int) -> np.ndarray:\n-    \"\"\"\n-    Shift label ids one token to the right.\n-    \"\"\"\n-    shifted_label_ids = np.zeros_like(label_ids)\n-    shifted_label_ids[:, 1:] = label_ids[:, :-1]\n-    shifted_label_ids[:, 0] = decoder_start_token_id\n-\n-    return shifted_label_ids\n-\n-\n-@flax.struct.dataclass\n-class FlaxDataCollatorSpeechSeq2SeqWithPadding:\n-    \"\"\"\n-    Data collator that will dynamically pad the inputs received.\n-    Args:\n-        processor ([`Wav2Vec2Processor`])\n-            The processor used for processing the data.\n-        decoder_start_token_id (:obj: `int`)\n-            The begin-of-sentence of the decoder.\n-        input_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n-            Select a strategy to pad the returned input sequences (according to the model's padding side and padding index)\n-            among:\n-            * :obj:`True` or :obj:`'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-              sequence if provided).\n-            * :obj:`'max_length'`: Pad to a maximum length specified with the argument :obj:`max_length` or to the\n-              maximum acceptable input length for the model if that argument is not provided.\n-            * :obj:`False` or :obj:`'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of\n-              different lengths).\n-        target_padding (:obj:`bool`, :obj:`str` or :class:`~transformers.tokenization_utils_base.PaddingStrategy`, `optional`, defaults to :obj:`True`):\n-            Select a strategy to pad the returned target sequences (according to the model's padding side and padding index).\n-            See above for details.\n-        max_input_length (:obj:`float`, `optional`):\n-            Maximum length of the ``input_values`` of the returned list and optionally padding length (see above).\n-        max_target_length (:obj:`int`, `optional`):\n-            Maximum length of the ``labels`` of the returned list and optionally padding length (see above).\n-        pad_input_to_multiple_of (:obj:`int`, `optional`):\n-            If set will pad the input sequence to a multiple of the provided value.\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-        pad_target_to_multiple_of (:obj:`int`, `optional`):\n-            If set will pad the target sequence to a multiple of the provided value.\n-            This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability >=\n-            7.5 (Volta).\n-    \"\"\"\n-\n-    processor: Any\n-    decoder_start_token_id: int\n-    input_padding: Union[bool, str] = \"longest\"\n-    target_padding: Union[bool, str] = \"max_length\"\n-    max_input_length: Optional[float] = None\n-    max_target_length: Optional[int] = None\n-    pad_input_to_multiple_of: Optional[int] = None\n-    pad_target_to_multiple_of: Optional[int] = None\n-\n-    def __call__(self, features: list[dict[str, Union[list[int], np.ndarray]]]) -> dict[str, np.ndarray]:\n-        # split inputs and labels since they have to be of different lengths and need\n-        # different padding methods\n-        model_input_name = self.processor.model_input_names[0]\n-\n-        # dataloader returns a list of features which we convert to a dict\n-        input_features = {model_input_name: [feature[model_input_name] for feature in features]}\n-        label_features = {\"input_ids\": [feature[\"labels\"] for feature in features]}\n-\n-        # reformat list to dict and set to pytorch format\n-        batch = self.processor.feature_extractor.pad(\n-            input_features,\n-            max_length=self.max_input_length,\n-            padding=self.input_padding,\n-            pad_to_multiple_of=self.pad_input_to_multiple_of,\n-            return_tensors=\"np\",\n-        )\n-\n-        labels_batch = self.processor.tokenizer.pad(\n-            label_features,\n-            max_length=self.max_target_length,\n-            padding=self.target_padding,\n-            pad_to_multiple_of=self.pad_target_to_multiple_of,\n-            return_tensors=\"np\",\n-        )\n-\n-        # if bos token is appended in previous tokenization step,\n-        # cut bos token here as it's append later anyways\n-        labels = labels_batch[\"input_ids\"]\n-        if (labels[:, 0] == self.decoder_start_token_id).all().item():\n-            labels = labels[:, 1:]\n-            labels_batch.attention_mask = labels_batch.attention_mask[:, 1:]\n-\n-        decoder_input_ids = shift_tokens_right(labels, self.decoder_start_token_id)\n-\n-        # replace padding with -100 to ignore correctly when computing the loss\n-        labels = np.ma.array(labels, mask=np.not_equal(labels_batch.attention_mask, 1))\n-        labels = labels.filled(fill_value=-100)\n-\n-        batch[\"labels\"] = labels\n-        batch[\"decoder_input_ids\"] = decoder_input_ids\n-\n-        return batch\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    num_train_steps: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # 1. Parse input arguments\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, Seq2SeqTrainingArguments))\n-\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your JAX/Flax versions.\n-    send_example_telemetry(\"run_speech_recognition_seq2seq\", model_args, data_args, framework=\"flax\")\n-\n-    # 2. Setup logging\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    # Set the verbosity to info of the Transformers logger.\n-    # We only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    logger.info(\"Training/evaluation parameters %s\", training_args)\n-\n-    # Check the output dir is valid\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use `--overwrite_output_dir` to overcome.\"\n-        )\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        if training_args.hub_model_id is None:\n-            repo_name = get_full_repo_name(\n-                Path(training_args.output_dir).absolute().name, token=training_args.hub_token\n-            )\n-        else:\n-            repo_name = training_args.hub_model_id\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # 3. Load dataset\n-    raw_datasets = DatasetDict()\n-\n-    if training_args.do_train:\n-        raw_datasets[\"train\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=data_args.train_split_name,\n-            cache_dir=data_args.dataset_cache_dir,\n-            num_proc=data_args.preprocessing_num_workers,\n-            token=True if model_args.use_auth_token else None,\n-            trust_remote_code=data_args.trust_remote_code,\n-        )\n-\n-    if training_args.do_eval:\n-        raw_datasets[\"eval\"] = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            split=data_args.eval_split_name,\n-            cache_dir=data_args.dataset_cache_dir,\n-            num_proc=data_args.preprocessing_num_workers,\n-            token=True if model_args.use_auth_token else None,\n-            trust_remote_code=data_args.trust_remote_code,\n-        )\n-\n-    if not training_args.do_train and not training_args.do_eval:\n-        raise ValueError(\n-            \"Cannot not train and not do evaluation. At least one of training or evaluation has to be performed.\"\n-        )\n-\n-    if data_args.audio_column_name not in next(iter(raw_datasets.values())).column_names:\n-        raise ValueError(\n-            f\"--audio_column_name '{data_args.audio_column_name}' not found in dataset '{data_args.dataset_name}'. \"\n-            \"Make sure to set `--audio_column_name` to the correct audio column - one of \"\n-            f\"{', '.join(next(iter(raw_datasets.values())).column_names)}.\"\n-        )\n-\n-    if data_args.text_column_name not in next(iter(raw_datasets.values())).column_names:\n-        raise ValueError(\n-            f\"--text_column_name {data_args.text_column_name} not found in dataset '{data_args.dataset_name}'. \"\n-            \"Make sure to set `--text_column_name` to the correct text column - one of \"\n-            f\"{', '.join(next(iter(raw_datasets.values())).column_names)}.\"\n-        )\n-\n-    # 5. Load pretrained model, tokenizer, and feature extractor\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n-    )\n-    feature_extractor = AutoFeatureExtractor.from_pretrained(\n-        model_args.feature_extractor_name if model_args.feature_extractor_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast_tokenizer,\n-        revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n-    )\n-\n-    model = FlaxAutoModelForSpeechSeq2Seq.from_pretrained(\n-        model_args.model_name_or_path,\n-        config=config,\n-        dtype=getattr(jnp, model_args.dtype),\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=True if model_args.use_auth_token else None,\n-    )\n-\n-    if model.config.decoder_start_token_id is None:\n-        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n-\n-    # 6. Resample speech dataset: `datasets` takes care of automatically loading and resampling the audio,\n-    # so we just need to set the correct target sampling rate.\n-    raw_datasets = raw_datasets.cast_column(\n-        data_args.audio_column_name, datasets.features.Audio(sampling_rate=feature_extractor.sampling_rate)\n-    )\n-\n-    # 7. Preprocessing the datasets.\n-    # We need to read the audio files as arrays and tokenize the targets.\n-    max_input_length = int(data_args.max_duration_in_seconds * feature_extractor.sampling_rate)\n-    min_input_length = int(data_args.min_duration_in_seconds * feature_extractor.sampling_rate)\n-    max_label_length = (\n-        data_args.max_label_length if data_args.max_label_length is not None else model.config.max_length\n-    )\n-    pad_input_to_multiple_of = data_args.pad_input_to_multiple_of\n-    pad_target_to_multiple_of = data_args.pad_target_to_multiple_of\n-    audio_column_name = data_args.audio_column_name\n-    num_workers = data_args.preprocessing_num_workers\n-    text_column_name = data_args.text_column_name\n-    model_input_name = feature_extractor.model_input_names[0]\n-    do_lower_case = data_args.do_lower_case\n-\n-    if training_args.do_train and data_args.max_train_samples is not None:\n-        raw_datasets[\"train\"] = raw_datasets[\"train\"].select(range(data_args.max_train_samples))\n-\n-    if training_args.do_eval and data_args.max_eval_samples is not None:\n-        raw_datasets[\"eval\"] = raw_datasets[\"eval\"].select(range(data_args.max_eval_samples))\n-\n-    if data_args.language is not None:\n-        # We only need to set the task id when the language is specified (i.e. in a multilingual setting)\n-        tokenizer.set_prefix_tokens(language=data_args.language, task=data_args.task)\n-\n-    def prepare_dataset(batch):\n-        # process audio\n-        sample = batch[audio_column_name]\n-        inputs = feature_extractor(sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n-        # process audio length\n-        batch[model_input_name] = inputs.get(model_input_name)[0]\n-        batch[\"input_length\"] = len(sample[\"array\"])\n-\n-        # process targets\n-        input_str = batch[text_column_name].lower() if do_lower_case else batch[text_column_name]\n-        batch[\"labels\"] = tokenizer(input_str).input_ids\n-        return batch\n-\n-    vectorized_datasets = raw_datasets.map(\n-        prepare_dataset,\n-        remove_columns=next(iter(raw_datasets.values())).column_names,\n-        num_proc=num_workers,\n-        desc=\"preprocess train and eval dataset\",\n-    )\n-\n-    # filter training data with inputs longer than max_input_length\n-    def is_audio_in_length_range(length):\n-        return min_input_length < length < max_input_length\n-\n-    vectorized_datasets = vectorized_datasets.filter(\n-        is_audio_in_length_range,\n-        num_proc=num_workers,\n-        input_columns=[\"input_length\"],\n-    )\n-\n-    # for large datasets it is advised to run the preprocessing on a\n-    # single machine first with `args.preprocessing_only` since there will mostly likely\n-    # be a timeout when running the script in distributed mode.\n-    # In a second step `args.preprocessing_only` can then be set to `False` to load the\n-    # cached dataset\n-    if data_args.preprocessing_only:\n-        cache = {k: v.cache_files for k, v in vectorized_datasets.items()}\n-        logger.info(f\"Data preprocessing finished. Files cached at {cache}.\")\n-        return\n-\n-    # 8. Load Metric\n-    metric = evaluate.load(\"wer\", cache_dir=model_args.cache_dir)\n-\n-    def compute_metrics(preds, labels):\n-        # replace padded labels by the padding token\n-        for idx in range(len(labels)):\n-            labels[idx][labels[idx] == -100] = tokenizer.pad_token_id\n-\n-        pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n-        # we do not want to group tokens when computing the metrics\n-        label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-\n-        wer = metric.compute(predictions=pred_str, references=label_str)\n-        return {\"wer\": wer}\n-\n-    # 9. Save feature extractor, tokenizer and config\n-    feature_extractor.save_pretrained(training_args.output_dir)\n-    tokenizer.save_pretrained(training_args.output_dir)\n-    config.save_pretrained(training_args.output_dir)\n-\n-    processor = AutoProcessor.from_pretrained(training_args.output_dir)\n-\n-    data_collator = FlaxDataCollatorSpeechSeq2SeqWithPadding(\n-        processor=processor,\n-        decoder_start_token_id=model.config.decoder_start_token_id,\n-        input_padding=\"longest\",\n-        target_padding=\"longest\",\n-        max_target_length=max_label_length,\n-        pad_input_to_multiple_of=pad_input_to_multiple_of,\n-        pad_target_to_multiple_of=pad_target_to_multiple_of if pad_target_to_multiple_of else max_label_length,\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-    steps_per_epoch = len(vectorized_datasets[\"train\"]) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        total_train_steps,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layer_norm\", \"self_attn_layer_norm\", \"final_layer_norm\", \"encoder_attn_layer_norm\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n-\n-    # label smoothed cross entropy\n-    def loss_fn(logits, labels, label_smoothing_factor=0.0):\n-        \"\"\"\n-        The label smoothing implementation is adapted from Flax's official example:\n-        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n-        \"\"\"\n-        vocab_size = logits.shape[-1]\n-        confidence = 1.0 - label_smoothing_factor\n-        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n-        normalizing_constant = -(\n-            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n-        )\n-        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n-\n-        loss = optax.softmax_cross_entropy(logits, soft_labels)\n-        loss = loss - normalizing_constant\n-\n-        # ignore padded tokens from loss, i.e. where labels are not set to -100\n-        padding_mask = labels >= 0\n-        loss = loss * padding_mask\n-        loss = loss.sum()\n-        num_labels = padding_mask.sum()\n-        return loss, num_labels\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, label_smoothing_factor=0.0):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)\n-            return loss, num_labels\n-\n-        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n-        (loss, num_labels), grad = grad_fn(state.params)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        # true grad = total grad / total samples\n-        grad = jax.lax.psum(grad, \"batch\")\n-        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch, label_smoothing_factor=0.0):\n-        labels = batch.pop(\"labels\")\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        loss, num_labels = loss_fn(logits, labels, label_smoothing_factor)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        metrics = {\"loss\": loss}\n-        return metrics\n-\n-    # Define generation function\n-    num_beams = model_args.num_beams if model_args.num_beams is not None else model.config.num_beams\n-    gen_kwargs = {\"max_length\": max_label_length, \"num_beams\": num_beams}\n-\n-    def generate_step(params, batch):\n-        model.params = params\n-        output_ids = model.generate(batch[model_input_name], attention_mask=batch.get(\"attention_mask\"), **gen_kwargs)\n-        return output_ids.sequences\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(\n-        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n-    )\n-    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n-    p_generate_step = jax.pmap(generate_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(vectorized_datasets['train'])}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        train_metrics = []\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset and create a data loader\n-        vectorized_datasets[\"train\"] = vectorized_datasets[\"train\"].shuffle(training_args.seed)\n-        train_loader = DataLoader(\n-            vectorized_datasets[\"train\"],\n-            batch_size=train_batch_size,\n-            drop_last=True,\n-            collate_fn=data_collator,\n-            num_workers=training_args.dataloader_num_workers,\n-        )\n-        # train\n-        for batch in tqdm(train_loader, desc=\"Training...\", position=1, leave=False):\n-            batch = shard(batch.data)\n-            state, train_metric = p_train_step(state, batch)\n-            train_metrics.append(train_metric)\n-\n-        train_time += time.time() - train_start\n-\n-        train_metric = unreplicate(train_metric)\n-\n-        epochs.write(\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n-            f\" {train_metric['learning_rate']})\"\n-        )\n-\n-        # ======================== Evaluating ==============================\n-        eval_metrics = []\n-        eval_preds = []\n-        eval_labels = []\n-\n-        eval_loader = DataLoader(\n-            vectorized_datasets[\"eval\"],\n-            batch_size=eval_batch_size,\n-            drop_last=False,\n-            collate_fn=data_collator,\n-            num_workers=training_args.dataloader_num_workers,\n-        )\n-        for batch in tqdm(eval_loader, desc=\"Evaluating...\", position=2, leave=False):\n-            # Model forward\n-            labels = batch[\"labels\"]\n-\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, batch.data, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-            # generation\n-            if training_args.predict_with_generate:\n-                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch.data)\n-                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n-                eval_labels.extend(labels)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-        # compute WER metric\n-        wer_desc = \"\"\n-        if training_args.predict_with_generate:\n-            wer_metric = compute_metrics(eval_preds, eval_labels)\n-            eval_metrics.update(wer_metric)\n-            wer_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in wer_metric.items()])\n-\n-        # Print metrics and update progress bar\n-        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {eval_metrics['loss']} | {wer_desc})\"\n-        epochs.write(desc)\n-        epochs.desc = desc\n-\n-        # Save metrics\n-        if has_tensorboard and jax.process_index() == 0:\n-            cur_step = epoch * (len(vectorized_datasets[\"train\"]) // train_batch_size)\n-            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-            model.save_pretrained(training_args.output_dir, params=params)\n-            tokenizer.save_pretrained(training_args.output_dir)\n-            if training_args.push_to_hub:\n-                api.upload_folder(\n-                    commit_message=f\"Saving weights and logs of epoch {epoch}\",\n-                    folder_path=training_args.output_dir,\n-                    repo_id=repo_id,\n-                    repo_type=\"model\",\n-                    token=training_args.hub_token,\n-                )\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "2eb21f49b65fe282f0e7768554555a94d2962ed2",
            "filename": "examples/flax/summarization/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 35,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fsummarization%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,35 +0,0 @@\n-# Summarization (Seq2Seq model) training examples\n-\n-The following example showcases how to finetune a sequence-to-sequence model for summarization\n-using the JAX/Flax backend.\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-`run_summarization_flax.py` is a lightweight example of how to download and preprocess a dataset from the ðŸ¤— Datasets library or use your own files (jsonlines or csv), then fine-tune one of the architectures above on it.\n-\n-For custom datasets in `jsonlines` format please see: https://huggingface.co/docs/datasets/loading_datasets#json-files and you also will find examples of these below.\n-\n-### Train the model\n-Next we can run the example script to train the model:\n-\n-```bash\n-python run_summarization_flax.py \\\n-\t--output_dir ./bart-base-xsum \\\n-\t--model_name_or_path facebook/bart-base \\\n-\t--tokenizer_name facebook/bart-base \\\n-\t--dataset_name=\"xsum\" \\\n-\t--do_train --do_eval --do_predict --predict_with_generate \\\n-\t--num_train_epochs 6 \\\n-\t--learning_rate 5e-5 --warmup_steps 0 \\\n-\t--per_device_train_batch_size 64 \\\n-\t--per_device_eval_batch_size 64 \\\n-\t--overwrite_output_dir \\\n-\t--max_source_length 512 --max_target_length 64 \\\n-\t--push_to_hub\n-```\n-\n-This should finish in 37min, with validation loss and ROUGE2 score of 1.7785 and 17.01 respectively after 6 epochs. training statistics can be accessed on [tfhub.dev](https://tensorboard.dev/experiment/OcPfOIgXRMSJqYB4RdK2tA/#scalars).\n-\n-> Note that here we used default `generate` arguments, using arguments specific for `xsum` dataset should give better ROUGE scores.  "
        },
        {
            "sha": "58c7c26af78a065713b0413428a4adeefa365aa4",
            "filename": "examples/flax/summarization/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fsummarization%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,6 +0,0 @@\n-datasets >= 1.1.3\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.8\n-evaluate>=0.2.0"
        },
        {
            "sha": "5240db323f1cad675fd034e5c4b8f42a74339b46",
            "filename": "examples/flax/summarization/run_summarization_flax.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1020,
            "changes": 1020,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,1020 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for summarization.\n-\"\"\"\n-# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import math\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from functools import partial\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import datasets\n-import evaluate\n-import jax\n-import jax.numpy as jnp\n-import nltk  # Here to have a nice missing dependency error message early on\n-import numpy as np\n-import optax\n-from datasets import Dataset, load_dataset\n-from filelock import FileLock\n-from flax import jax_utils, traverse_util\n-from flax.jax_utils import pad_shard_unpad, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForSeq2SeqLM,\n-    HfArgumentParser,\n-    is_tensorboard_available,\n-)\n-from transformers.utils import is_offline_mode, send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-try:\n-    nltk.data.find(\"tokenizers/punkt\")\n-except (LookupError, OSError):\n-    if is_offline_mode():\n-        raise LookupError(\n-            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n-        )\n-    with FileLock(\".lock\") as lock:\n-        nltk.download(\"punkt\", quiet=True)\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    label_smoothing_factor: float = field(\n-        default=0.0, metadata={\"help\": \"The label smoothing epsilon to apply (zero means no label smoothing).\"}\n-    )\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-    gradient_checkpointing: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    text_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n-    )\n-    summary_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input predict data file to do prediction on (a text file).\"},\n-    )\n-    max_source_length: Optional[int] = field(\n-        default=1024,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_target_length: Optional[int] = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    val_max_target_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. \"\n-                \"This argument is also used to override the `max_length` param of `model.generate`, which is used \"\n-                \"during evaluation.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    source_prefix: Optional[str] = field(\n-        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n-    )\n-    predict_with_generate: bool = field(\n-        default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n-    )\n-    num_beams: Optional[int] = field(\n-        default=1,\n-        metadata={\n-            \"help\": (\n-                \"Number of beams to use for evaluation. This argument will be passed to `model.generate`, \"\n-                \"which is used during evaluation.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-\n-    def __post_init__(self):\n-        if (\n-            self.dataset_name is None\n-            and self.train_file is None\n-            and self.validation_file is None\n-            and self.test_file is None\n-        ):\n-            raise ValueError(\"Need either a dataset name or a training, validation, or test file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-            if self.test_file is not None:\n-                extension = self.test_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n-        if self.val_max_target_length is None:\n-            self.val_max_target_length = self.max_target_length\n-\n-\n-summarization_name_mapping = {\n-    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n-    \"big_patent\": (\"description\", \"abstract\"),\n-    \"cnn_dailymail\": (\"article\", \"highlights\"),\n-    \"orange_sum\": (\"text\", \"summary\"),\n-    \"pn_summary\": (\"article\", \"summary\"),\n-    \"psc\": (\"extract_text\", \"summary_text\"),\n-    \"samsum\": (\"dialogue\", \"summary\"),\n-    \"thaisum\": (\"body\", \"summary\"),\n-    \"xglue\": (\"news_body\", \"news_title\"),\n-    \"xsum\": (\"document\", \"summary\"),\n-    \"wiki_summary\": (\"article\", \"highlights\"),\n-}\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def data_loader(rng: jax.random.PRNGKey, dataset: Dataset, batch_size: int, shuffle: bool = False, drop_last=True):\n-    \"\"\"\n-    Returns batches of size `batch_size` from `dataset`. If `drop_last` is set to `False`, the final batch may be incomplete,\n-    and range in size from 1 to `batch_size`. Shuffle batches if `shuffle` is `True`.\n-    \"\"\"\n-    if shuffle:\n-        batch_idx = jax.random.permutation(rng, len(dataset))\n-        batch_idx = np.asarray(batch_idx)\n-    else:\n-        batch_idx = np.arange(len(dataset))\n-\n-    if drop_last:\n-        steps_per_epoch = len(dataset) // batch_size\n-        batch_idx = batch_idx[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-        batch_idx = batch_idx.reshape((steps_per_epoch, batch_size))\n-    else:\n-        steps_per_epoch = math.ceil(len(dataset) / batch_size)\n-        batch_idx = np.array_split(batch_idx, steps_per_epoch)\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-\n-        yield batch\n-\n-\n-def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n-    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n-    #\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            keep_in_memory=False,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        dataset = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Load pretrained model and tokenizer\n-\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxAutoModelForSeq2SeqLM.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        model = FlaxAutoModelForSeq2SeqLM.from_config(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-    if training_args.gradient_checkpointing:\n-        model.enable_gradient_checkpointing()\n-\n-    if model.config.decoder_start_token_id is None:\n-        raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n-\n-    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n-\n-    # Preprocessing the datasets.\n-    # We need to tokenize inputs and targets.\n-    if training_args.do_train:\n-        if \"train\" not in dataset:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        column_names = dataset[\"train\"].column_names\n-    elif training_args.do_eval:\n-        if \"validation\" not in dataset:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        column_names = dataset[\"validation\"].column_names\n-    elif training_args.do_predict:\n-        if \"test\" not in dataset:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        column_names = dataset[\"test\"].column_names\n-    else:\n-        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n-        return\n-\n-    # Get the column names for input/target.\n-    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n-    if data_args.text_column is None:\n-        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n-    else:\n-        text_column = data_args.text_column\n-        if text_column not in column_names:\n-            raise ValueError(\n-                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-    if data_args.summary_column is None:\n-        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n-    else:\n-        summary_column = data_args.summary_column\n-        if summary_column not in column_names:\n-            raise ValueError(\n-                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-\n-    # Temporarily set max_target_length for training.\n-    max_target_length = data_args.max_target_length\n-\n-    # In Flax, for seq2seq models we need to pass `decoder_input_ids`\n-    # as the Flax models don't accept `labels`, we need to prepare the decoder_input_ids here\n-    # for that dynamically import the `shift_tokens_right` function from the model file\n-    model_module = __import__(model.__module__, fromlist=[\"shift_tokens_tight\"])\n-    shift_tokens_right_fn = getattr(model_module, \"shift_tokens_right\")\n-\n-    # Setting padding=\"max_length\" as we need fixed length inputs for jitted functions\n-    def preprocess_function(examples):\n-        inputs = examples[text_column]\n-        targets = examples[summary_column]\n-        inputs = [prefix + inp for inp in inputs]\n-        model_inputs = tokenizer(\n-            inputs, max_length=data_args.max_source_length, padding=\"max_length\", truncation=True, return_tensors=\"np\"\n-        )\n-\n-        # Setup the tokenizer for targets\n-        labels = tokenizer(\n-            text_target=targets,\n-            max_length=max_target_length,\n-            padding=\"max_length\",\n-            truncation=True,\n-            return_tensors=\"np\",\n-        )\n-\n-        model_inputs[\"labels\"] = labels[\"input_ids\"]\n-        decoder_input_ids = shift_tokens_right_fn(\n-            labels[\"input_ids\"], config.pad_token_id, config.decoder_start_token_id\n-        )\n-        model_inputs[\"decoder_input_ids\"] = np.asarray(decoder_input_ids)\n-\n-        # We need decoder_attention_mask so we can ignore pad tokens from loss\n-        model_inputs[\"decoder_attention_mask\"] = labels[\"attention_mask\"]\n-\n-        return model_inputs\n-\n-    if training_args.do_train:\n-        train_dataset = dataset[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        train_dataset = train_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on train dataset\",\n-        )\n-\n-    if training_args.do_eval:\n-        max_target_length = data_args.val_max_target_length\n-        eval_dataset = dataset[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        eval_dataset = eval_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on validation dataset\",\n-        )\n-\n-    if training_args.do_predict:\n-        max_target_length = data_args.val_max_target_length\n-        predict_dataset = dataset[\"test\"]\n-        if data_args.max_predict_samples is not None:\n-            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n-            predict_dataset = predict_dataset.select(range(max_predict_samples))\n-        predict_dataset = predict_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on prediction dataset\",\n-        )\n-\n-    # Metric\n-    metric = evaluate.load(\"rouge\", cache_dir=model_args.cache_dir)\n-\n-    def postprocess_text(preds, labels):\n-        preds = [pred.strip() for pred in preds]\n-        labels = [label.strip() for label in labels]\n-\n-        # rougeLSum expects newline after each sentence\n-        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n-        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n-\n-        return preds, labels\n-\n-    def compute_metrics(preds, labels):\n-        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n-        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-\n-        # Some simple post-processing\n-        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n-\n-        result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n-        result = {k: round(v * 100, 4) for k, v in result.items()}\n-        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n-        result[\"gen_len\"] = np.mean(prediction_lens)\n-        return result\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n-\n-    # label smoothed cross entropy\n-    def loss_fn(logits, labels, padding_mask, label_smoothing_factor=0.0):\n-        \"\"\"\n-        The label smoothing implementation is adapted from Flax's official example:\n-        https://github.com/google/flax/blob/87a211135c6a377c8f29048a1cac3840e38b9da4/examples/wmt/train.py#L104\n-        \"\"\"\n-        vocab_size = logits.shape[-1]\n-        confidence = 1.0 - label_smoothing_factor\n-        low_confidence = (1.0 - confidence) / (vocab_size - 1)\n-        normalizing_constant = -(\n-            confidence * jnp.log(confidence) + (vocab_size - 1) * low_confidence * jnp.log(low_confidence + 1e-20)\n-        )\n-        soft_labels = onehot(labels, vocab_size, on_value=confidence, off_value=low_confidence)\n-\n-        loss = optax.softmax_cross_entropy(logits, soft_labels)\n-        loss = loss - normalizing_constant\n-\n-        # ignore padded tokens from loss\n-        loss = loss * padding_mask\n-        loss = loss.sum()\n-        num_labels = padding_mask.sum()\n-        return loss, num_labels\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch, label_smoothing_factor=0.0):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n-            return loss, num_labels\n-\n-        grad_fn = jax.value_and_grad(compute_loss, has_aux=True)\n-        (loss, num_labels), grad = grad_fn(state.params)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        # true grad = total grad / total samples\n-        grad = jax.lax.psum(grad, \"batch\")\n-        grad = jax.tree_util.tree_map(lambda x: x / num_labels, grad)\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch, label_smoothing_factor=0.0):\n-        labels = batch.pop(\"labels\")\n-        logits = model(**batch, params=params, train=False)[0]\n-\n-        loss, num_labels = loss_fn(logits, labels, batch[\"decoder_attention_mask\"], label_smoothing_factor)\n-        num_labels = jax.lax.psum(num_labels, \"batch\")\n-\n-        # true loss = total loss / total samples\n-        loss = jax.lax.psum(loss, \"batch\")\n-        loss = jax.tree_util.tree_map(lambda x: x / num_labels, loss)\n-\n-        metrics = {\"loss\": loss}\n-        return metrics\n-\n-    # Define generation function\n-    max_length = (\n-        data_args.val_max_target_length if data_args.val_max_target_length is not None else model.config.max_length\n-    )\n-    num_beams = data_args.num_beams if data_args.num_beams is not None else model.config.num_beams\n-    gen_kwargs = {\"max_length\": max_length, \"num_beams\": num_beams}\n-\n-    def generate_step(params, batch):\n-        model.params = params\n-        output_ids = model.generate(batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"], **gen_kwargs)\n-        return output_ids.sequences\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(\n-        partial(train_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\", donate_argnums=(0,)\n-    )\n-    p_eval_step = jax.pmap(partial(eval_step, label_smoothing_factor=training_args.label_smoothing_factor), \"batch\")\n-    p_generate_step = jax.pmap(generate_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(train_dataset)}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-        train_metrics = []\n-\n-        # Generate an epoch by shuffling sampling indices from the train dataset\n-        train_loader = data_loader(input_rng, train_dataset, train_batch_size, shuffle=True)\n-        steps_per_epoch = len(train_dataset) // train_batch_size\n-        # train\n-        for _ in tqdm(range(steps_per_epoch), desc=\"Training...\", position=1, leave=False):\n-            batch = next(train_loader)\n-            batch = shard(batch)\n-            state, train_metric = p_train_step(state, batch)\n-            train_metrics.append(train_metric)\n-\n-        train_time += time.time() - train_start\n-\n-        train_metric = unreplicate(train_metric)\n-\n-        epochs.write(\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n-            f\" {train_metric['learning_rate']})\"\n-        )\n-\n-        # ======================== Evaluating ==============================\n-        eval_metrics = []\n-        eval_preds = []\n-        eval_labels = []\n-\n-        eval_loader = data_loader(input_rng, eval_dataset, eval_batch_size, drop_last=False)\n-        eval_steps = math.ceil(len(eval_dataset) / eval_batch_size)\n-        for _ in tqdm(range(eval_steps), desc=\"Evaluating...\", position=2, leave=False):\n-            # Model forward\n-            batch = next(eval_loader)\n-            labels = batch[\"labels\"]\n-\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, batch, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-            # generation\n-            if data_args.predict_with_generate:\n-                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n-                eval_preds.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n-                eval_labels.extend(labels)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-        # compute ROUGE metrics\n-        rouge_desc = \"\"\n-        if data_args.predict_with_generate:\n-            rouge_metrics = compute_metrics(eval_preds, eval_labels)\n-            eval_metrics.update(rouge_metrics)\n-            rouge_desc = \" \".join([f\"Eval {key}: {value} |\" for key, value in rouge_metrics.items()])\n-\n-        # Print metrics and update progress bar\n-        desc = f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {eval_metrics['loss']} | {rouge_desc})\"\n-        epochs.write(desc)\n-        epochs.desc = desc\n-\n-        # Save metrics\n-        if has_tensorboard and jax.process_index() == 0:\n-            cur_step = epoch * (len(train_dataset) // train_batch_size)\n-            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-            model.save_pretrained(training_args.output_dir, params=params)\n-            tokenizer.save_pretrained(training_args.output_dir)\n-            if training_args.push_to_hub:\n-                api.upload_folder(\n-                    commit_message=f\"Saving weights and logs of epoch {epoch}\",\n-                    folder_path=training_args.output_dir,\n-                    repo_id=repo_id,\n-                    repo_type=\"model\",\n-                    token=training_args.hub_token,\n-                )\n-\n-    # ======================== Prediction loop ==============================\n-    if training_args.do_predict:\n-        logger.info(\"*** Predict ***\")\n-\n-        pred_metrics = []\n-        pred_generations = []\n-        pred_labels = []\n-\n-        pred_loader = data_loader(input_rng, predict_dataset, eval_batch_size, drop_last=False)\n-        pred_steps = math.ceil(len(predict_dataset) / eval_batch_size)\n-        for _ in tqdm(range(pred_steps), desc=\"Predicting...\", position=2, leave=False):\n-            # Model forward\n-            batch = next(pred_loader)\n-            labels = batch[\"labels\"]\n-\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, batch, min_device_batch=per_device_eval_batch_size\n-            )\n-            pred_metrics.append(metrics)\n-\n-            # generation\n-            if data_args.predict_with_generate:\n-                generated_ids = pad_shard_unpad(p_generate_step)(state.params, batch)\n-                pred_generations.extend(jax.device_get(generated_ids.reshape(-1, gen_kwargs[\"max_length\"])))\n-                pred_labels.extend(labels)\n-\n-        # normalize prediction metrics\n-        pred_metrics = get_metrics(pred_metrics)\n-        pred_metrics = jax.tree_util.tree_map(jnp.mean, pred_metrics)\n-\n-        # compute ROUGE metrics\n-        rouge_desc = \"\"\n-        if data_args.predict_with_generate:\n-            rouge_metrics = compute_metrics(pred_generations, pred_labels)\n-            pred_metrics.update(rouge_metrics)\n-            rouge_desc = \" \".join([f\"Predict {key}: {value} |\" for key, value in rouge_metrics.items()])\n-\n-        # Print metrics\n-        desc = f\"Predict Loss: {pred_metrics['loss']} | {rouge_desc})\"\n-        logger.info(desc)\n-\n-        # save final metrics in json\n-        if jax.process_index() == 0:\n-            rouge_metrics = {f\"test_{metric_name}\": value for metric_name, value in rouge_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"test_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(rouge_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "ab1930e001c2f905bd988529855810ab366607f3",
            "filename": "examples/flax/test_flax_examples.py",
            "status": "removed",
            "additions": 0,
            "deletions": 284,
            "changes": 284,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftest_flax_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftest_flax_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftest_flax_examples.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,284 +0,0 @@\n-# Copyright 2021 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import argparse\n-import json\n-import logging\n-import os\n-import sys\n-from unittest.mock import patch\n-\n-from transformers.testing_utils import TestCasePlus, get_gpu_count, slow\n-\n-\n-SRC_DIRS = [\n-    os.path.join(os.path.dirname(__file__), dirname)\n-    for dirname in [\n-        \"text-classification\",\n-        \"language-modeling\",\n-        \"summarization\",\n-        \"token-classification\",\n-        \"question-answering\",\n-        \"speech-recognition\",\n-    ]\n-]\n-sys.path.extend(SRC_DIRS)\n-\n-\n-if SRC_DIRS is not None:\n-    import run_clm_flax\n-    import run_flax_glue\n-    import run_flax_ner\n-    import run_flax_speech_recognition_seq2seq\n-    import run_mlm_flax\n-    import run_qa\n-    import run_summarization_flax\n-    import run_t5_mlm_flax\n-\n-\n-logging.basicConfig(level=logging.DEBUG)\n-\n-logger = logging.getLogger()\n-\n-\n-def get_setup_file():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"-f\")\n-    args = parser.parse_args()\n-    return args.f\n-\n-\n-def get_results(output_dir, split=\"eval\"):\n-    path = os.path.join(output_dir, f\"{split}_results.json\")\n-    if os.path.exists(path):\n-        with open(path) as f:\n-            return json.load(f)\n-    raise ValueError(f\"can't find {path}\")\n-\n-\n-stream_handler = logging.StreamHandler(sys.stdout)\n-logger.addHandler(stream_handler)\n-\n-\n-class ExamplesTests(TestCasePlus):\n-    def test_run_glue(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_glue.py\n-            --model_name_or_path distilbert/distilbert-base-uncased\n-            --output_dir {tmp_dir}\n-            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n-            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-            --learning_rate=1e-4\n-            --eval_steps=2\n-            --warmup_steps=2\n-            --seed=42\n-            --max_seq_length=128\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_flax_glue.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n-\n-    @slow\n-    def test_run_clm(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_clm_flax.py\n-            --model_name_or_path distilbert/distilgpt2\n-            --train_file ./tests/fixtures/sample_text.txt\n-            --validation_file ./tests/fixtures/sample_text.txt\n-            --do_train\n-            --do_eval\n-            --block_size 128\n-            --per_device_train_batch_size 4\n-            --per_device_eval_batch_size 4\n-            --num_train_epochs 2\n-            --logging_steps 2 --eval_steps 2\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_clm_flax.main()\n-            result = get_results(tmp_dir)\n-            self.assertLess(result[\"eval_perplexity\"], 100)\n-\n-    @slow\n-    def test_run_summarization(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_summarization.py\n-            --model_name_or_path google-t5/t5-small\n-            --train_file tests/fixtures/tests_samples/xsum/sample.json\n-            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n-            --test_file tests/fixtures/tests_samples/xsum/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --num_train_epochs=3\n-            --warmup_steps=8\n-            --do_train\n-            --do_eval\n-            --do_predict\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-            --predict_with_generate\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_summarization_flax.main()\n-            result = get_results(tmp_dir, split=\"test\")\n-            self.assertGreaterEqual(result[\"test_rouge1\"], 10)\n-            self.assertGreaterEqual(result[\"test_rouge2\"], 2)\n-            self.assertGreaterEqual(result[\"test_rougeL\"], 7)\n-            self.assertGreaterEqual(result[\"test_rougeLsum\"], 7)\n-\n-    @slow\n-    def test_run_mlm(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_mlm.py\n-            --model_name_or_path distilbert/distilroberta-base\n-            --train_file ./tests/fixtures/sample_text.txt\n-            --validation_file ./tests/fixtures/sample_text.txt\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --max_seq_length 128\n-            --per_device_train_batch_size 4\n-            --per_device_eval_batch_size 4\n-            --logging_steps 2 --eval_steps 2\n-            --do_train\n-            --do_eval\n-            --num_train_epochs=1\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_mlm_flax.main()\n-            result = get_results(tmp_dir)\n-            self.assertLess(result[\"eval_perplexity\"], 42)\n-\n-    @slow\n-    def test_run_t5_mlm(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_t5_mlm_flax.py\n-            --model_name_or_path google-t5/t5-small\n-            --train_file ./tests/fixtures/sample_text.txt\n-            --validation_file ./tests/fixtures/sample_text.txt\n-            --do_train\n-            --do_eval\n-            --max_seq_length 128\n-            --per_device_train_batch_size 4\n-            --per_device_eval_batch_size 4\n-            --num_train_epochs 2\n-            --logging_steps 2 --eval_steps 2\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_t5_mlm_flax.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.42)\n-\n-    @slow\n-    def test_run_ner(self):\n-        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n-        epochs = 7 if get_gpu_count() > 1 else 2\n-\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_flax_ner.py\n-            --model_name_or_path google-bert/bert-base-uncased\n-            --train_file tests/fixtures/tests_samples/conll/sample.json\n-            --validation_file tests/fixtures/tests_samples/conll/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --do_train\n-            --do_eval\n-            --warmup_steps=2\n-            --learning_rate=2e-4\n-            --logging_steps 2 --eval_steps 2\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=2\n-            --num_train_epochs={epochs}\n-            --seed 7\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_flax_ner.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n-            self.assertGreaterEqual(result[\"eval_f1\"], 0.3)\n-\n-    @slow\n-    def test_run_qa(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_qa.py\n-            --model_name_or_path google-bert/bert-base-uncased\n-            --version_2_with_negative\n-            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n-            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --num_train_epochs=3\n-            --warmup_steps=2\n-            --do_train\n-            --do_eval\n-            --logging_steps 2 --eval_steps 2\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_qa.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"eval_f1\"], 30)\n-            self.assertGreaterEqual(result[\"eval_exact\"], 30)\n-\n-    @slow\n-    def test_run_flax_speech_recognition_seq2seq(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_flax_speech_recognition_seq2seq.py\n-            --model_name_or_path openai/whisper-tiny.en\n-            --dataset_name hf-internal-testing/librispeech_asr_dummy\n-            --dataset_config clean\n-            --train_split_name validation\n-            --eval_split_name validation\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --num_train_epochs=2\n-            --max_train_samples 10\n-            --max_eval_samples 10\n-            --warmup_steps=8\n-            --do_train\n-            --do_eval\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-            --predict_with_generate\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_flax_speech_recognition_seq2seq.main()\n-            result = get_results(tmp_dir, split=\"eval\")\n-            self.assertLessEqual(result[\"eval_wer\"], 0.05)"
        },
        {
            "sha": "65e50a075b78d59541093fe1fc830eb447e8d6ca",
            "filename": "examples/flax/text-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 108,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftext-classification%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,108 +0,0 @@\n-<!---\n-Copyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Text classification examples\n-\n-## GLUE tasks\n-\n-Based on the script [`run_flax_glue.py`](https://github.com/huggingface/transformers/blob/main/examples/flax/text-classification/run_flax_glue.py).\n-\n-Fine-tuning the library models for sequence classification on the GLUE benchmark: [General Language Understanding\n-Evaluation](https://gluebenchmark.com/). This script can fine-tune any of the models on the [hub](https://huggingface.co/models)  and can also be used for a \n-dataset hosted on our [hub](https://huggingface.co/datasets) or your own data in a csv or a JSON file (the script might need some tweaks in that case, \n-refer to the comments inside for help).\n-\n-GLUE is made up of a total of 9 different tasks. Here is how to run the script on one of them:\n-\n-```bash\n-export TASK_NAME=mrpc\n-\n-python run_flax_glue.py \\\n-  --model_name_or_path google-bert/bert-base-cased \\\n-  --task_name ${TASK_NAME} \\\n-  --max_seq_length 128 \\\n-  --learning_rate 2e-5 \\\n-  --num_train_epochs 3 \\\n-  --per_device_train_batch_size 4 \\\n-  --eval_steps 100 \\\n-  --output_dir ./$TASK_NAME/ \\\n-  --push_to_hub\n-```\n-\n-where task name can be one of cola, mnli, mnli_mismatched, mnli_matched, mrpc, qnli, qqp, rte, sst2, stsb, wnli.\n-\n-Using the command above, the script will train for 3 epochs and run eval after each epoch. \n-Metrics and hyperparameters are stored in Tensorflow event files in `--output_dir`.\n-You can see the results by running `tensorboard` in that directory:\n-\n-```bash\n-$ tensorboard --logdir .\n-```\n-\n-or directly on the hub under *Training metrics*.\n-\n-### Accuracy Evaluation\n-\n-We train five replicas and report mean accuracy and stdev on the dev set below.\n-We use the settings as in the command above (with an exception for MRPC and\n-WNLI which are tiny and where we used 5 epochs instead of 3), and we use a total\n-train batch size of 32 (we train on 8 Cloud v3 TPUs, so a per-device batch size of 4),\n-\n-On the task other than MRPC and WNLI we train for 3 these epochs because this is the standard,\n-but looking at the training curves of some of them (e.g., SST-2, STS-b), it appears the models\n-are undertrained and we could get better results when training longer.\n-\n-In the Tensorboard results linked below, the random seed of each model is equal to the ID of the run. So in order to reproduce run 1, run the command above with `--seed=1`. The best run used random seed 3, which is the default in the script. The results of all runs are in [this Google Sheet](https://docs.google.com/spreadsheets/d/1p3XzReMO75m_XdEJvPue-PIq_PN-96J2IJpJW1yS-10/edit?usp=sharing).\n-\n-| Task  | Metric                       | Acc (best run) | Acc (avg/5runs) | Stdev     | Metrics                                                                  |\n-|-------|------------------------------|----------------|-----------------|-----------|--------------------------------------------------------------------------|\n-| CoLA  | Matthews corr                | 60.57          | 59.04           | 1.06      | [tfhub.dev](https://tensorboard.dev/experiment/lfr2adVpRtmLDALKrElkzg/)  |\n-| SST-2 | Accuracy                     | 92.66          | 92.23           | 0.57      | [tfhub.dev](https://tensorboard.dev/experiment/jYvfv2trRHKMjoWnXVwrZA/)  |\n-| MRPC  | F1/Accuracy                  | 89.90/85.78    | 88.97/84.36     | 0.72/1.09 | [tfhub.dev](https://tensorboard.dev/experiment/bo3W3DEoRw2Q7YXjWrJkfg/)  |\n-| STS-B | Pearson/Spearman corr.       | 89.04/88.70    | 88.94/88.63     | 0.07/0.07 | [tfhub.dev](https://tensorboard.dev/experiment/fxVwbLD7QpKhbot0r9rn2w/)  |\n-| QQP   | Accuracy/F1                  | 90.81/87.58    | 90.76/87.51     | 0.05/0.06 | [tfhub.dev](https://tensorboard.dev/experiment/di089Rc9TZmsnKRMrYNLsA/)  |\n-| MNLI  | Matched acc.                 | 84.10          | 83.80           | 0.16      | [tfhub.dev](https://tensorboard.dev/experiment/JgNCGHDJSRaW6HBx6YQFYQ/)  |\n-| QNLI  | Accuracy                     | 91.01          | 90.82           | 0.17      | [tfhub.dev](https://tensorboard.dev/experiment/Bq7cMGJnQMSggYgL8qNGeQ/)  |\n-| RTE   | Accuracy                     | 66.06          | 64.76           | 1.04      | [tfhub.dev](https://tensorboard.dev/experiment/66Eq24bhRjqN6CEhgDSGqQ/)  |\n-| WNLI  | Accuracy                     | 46.48          | 37.01           | 6.83      | [tfhub.dev](https://tensorboard.dev/experiment/TAqcnddqTkWvVEeGaWwIdQ/)  |\n-\n-Some of these results are significantly different from the ones reported on the test set of GLUE benchmark on the\n-website. For QQP and WNLI, please refer to [FAQ #12](https://gluebenchmark.com/faq) on the website.\n-\n-### Runtime evaluation\n-\n-We also ran each task once on a single V100 GPU, 8 V100 GPUs, and 8 Cloud v3 TPUs and report the\n-overall training time below. For comparison we ran Pytorch's [run_glue.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py) on a single GPU (last column).\n-\n-\n-| Task  | TPU v3-8  | 8 GPU      | [1 GPU](https://tensorboard.dev/experiment/mkPS4Zh8TnGe1HB6Yzwj4Q)  | 1 GPU (Pytorch) |\n-|-------|-----------|------------|------------|-----------------|\n-| CoLA  |  1m 42s   |  1m 26s    | 3m 9s      | 4m 6s           |\n-| SST-2 |  5m 12s   |  6m 28s    | 22m 33s    | 34m 37s         |\n-| MRPC  |  1m 29s   |  1m 14s    | 2m 20s     | 2m 56s          |\n-| STS-B |  1m 30s   |  1m 12s    | 2m 16s     | 2m 48s          |\n-| QQP   | 22m 50s   | 31m 48s    | 1h 59m 41s | 2h 54m          |\n-| MNLI  | 25m 03s   | 33m 55s    | 2h 9m 37s  | 3h 7m 6s        |\n-| QNLI  |  7m30s    |  9m 40s    | 34m 40s    | 49m 8s          |\n-| RTE   |  1m 20s   |     55s    | 1m 10s     | 1m 16s          |\n-| WNLI  |  1m 11s   |     48s    | 39s        | 36s             |\n-|-------|\n-| **TOTAL** | 1h 03m | 1h 28m | 5h 16m | 6h 37m      |\n-\n-*All experiments are ran on Google Cloud Platform. \n-GPU experiments are ran without further optimizations besides JAX\n-transformations. GPU experiments are ran with full precision (fp32). \"TPU v3-8\"\n-are 8 TPU cores on 4 chips (each chips has 2 cores), while \"8 GPU\" are 8 GPU chips."
        },
        {
            "sha": "7507ae1b69c9aa60e4ad35f0b2ec2771b0ac8e39",
            "filename": "examples/flax/text-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftext-classification%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,5 +0,0 @@\n-datasets >= 1.1.3\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.8"
        },
        {
            "sha": "d447adfe104781bd577e2760588cd1bc8844796f",
            "filename": "examples/flax/text-classification/run_flax_glue.py",
            "status": "removed",
            "additions": 0,
            "deletions": 697,
            "changes": 697,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,697 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Finetuning a ðŸ¤— Flax Transformers model for sequence classification on GLUE.\"\"\"\n-\n-import json\n-import logging\n-import math\n-import os\n-import random\n-import sys\n-import time\n-import warnings\n-from dataclasses import dataclass, field\n-from pathlib import Path\n-from typing import Any, Callable, Optional\n-\n-import datasets\n-import evaluate\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import load_dataset\n-from flax import struct, traverse_util\n-from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForSequenceClassification,\n-    HfArgumentParser,\n-    PretrainedConfig,\n-    TrainingArguments,\n-    is_tensorboard_available,\n-)\n-from transformers.utils import check_min_version, send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-Array = Any\n-Dataset = datasets.arrow_dataset.Dataset\n-PRNGKey = Any\n-\n-\n-task_to_keys = {\n-    \"cola\": (\"sentence\", None),\n-    \"mnli\": (\"premise\", \"hypothesis\"),\n-    \"mrpc\": (\"sentence1\", \"sentence2\"),\n-    \"qnli\": (\"question\", \"sentence\"),\n-    \"qqp\": (\"question1\", \"question2\"),\n-    \"rte\": (\"sentence1\", \"sentence2\"),\n-    \"sst2\": (\"sentence\", None),\n-    \"stsb\": (\"sentence1\", \"sentence2\"),\n-    \"wnli\": (\"sentence1\", \"sentence2\"),\n-}\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    use_slow_tokenizer: Optional[bool] = field(\n-        default=False,\n-        metadata={\"help\": \"If passed, will use a slow tokenizer (not backed by the ðŸ¤— Tokenizers library).\"},\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    use_auth_token: bool = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n-                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n-                \"execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    task_name: Optional[str] = field(\n-        default=None, metadata={\"help\": f\"The name of the glue task to train on. choices {list(task_to_keys.keys())}\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n-    )\n-    text_column_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n-    )\n-    label_column_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_seq_length: int = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.task_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-        self.task_name = self.task_name.lower() if isinstance(self.task_name, str) else self.task_name\n-\n-\n-def create_train_state(\n-    model: FlaxAutoModelForSequenceClassification,\n-    learning_rate_fn: Callable[[int], float],\n-    is_regression: bool,\n-    num_labels: int,\n-    weight_decay: float,\n-) -> train_state.TrainState:\n-    \"\"\"Create initial training state.\"\"\"\n-\n-    class TrainState(train_state.TrainState):\n-        \"\"\"Train state with an Optax optimizer.\n-\n-        The two functions below differ depending on whether the task is classification\n-        or regression.\n-\n-        Args:\n-          logits_fn: Applied to last layer to obtain the logits.\n-          loss_fn: Function to compute the loss.\n-        \"\"\"\n-\n-        logits_fn: Callable = struct.field(pytree_node=False)\n-        loss_fn: Callable = struct.field(pytree_node=False)\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    tx = optax.adamw(\n-        learning_rate=learning_rate_fn, b1=0.9, b2=0.999, eps=1e-6, weight_decay=weight_decay, mask=decay_mask_fn\n-    )\n-\n-    if is_regression:\n-\n-        def mse_loss(logits, labels):\n-            return jnp.mean((logits[..., 0] - labels) ** 2)\n-\n-        return TrainState.create(\n-            apply_fn=model.__call__,\n-            params=model.params,\n-            tx=tx,\n-            logits_fn=lambda logits: logits[..., 0],\n-            loss_fn=mse_loss,\n-        )\n-    else:  # Classification.\n-\n-        def cross_entropy_loss(logits, labels):\n-            xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n-            return jnp.mean(xentropy)\n-\n-        return TrainState.create(\n-            apply_fn=model.__call__,\n-            params=model.params,\n-            tx=tx,\n-            logits_fn=lambda logits: logits.argmax(-1),\n-            loss_fn=cross_entropy_loss,\n-        )\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def glue_train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n-    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n-    steps_per_epoch = len(dataset) // batch_size\n-    perms = jax.random.permutation(rng, len(dataset))\n-    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-    perms = perms.reshape((steps_per_epoch, batch_size))\n-\n-    for perm in perms:\n-        batch = dataset[perm]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-        batch = shard(batch)\n-\n-        yield batch\n-\n-\n-def glue_eval_data_collator(dataset: Dataset, batch_size: int):\n-    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n-    batch_idx = np.arange(len(dataset))\n-\n-    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n-    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-\n-        yield batch\n-\n-\n-def main():\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if model_args.use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if model_args.token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        model_args.token = model_args.use_auth_token\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_glue\", model_args, data_args, framework=\"flax\")\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n-    # or specify a GLUE benchmark task (the dataset will be downloaded automatically from the datasets Hub).\n-\n-    # For CSV/JSON files, this script will use as labels the column called 'label' and as pair of sentences the\n-    # sentences in columns called 'sentence1' and 'sentence2' if such column exists or the first two columns not named\n-    # label if at least two columns are provided.\n-\n-    # If the CSVs/JSONs contain only one non-label column, the script does single sentence classification on this\n-    # single column. You can easily tweak this behavior (see below)\n-\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.task_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            \"glue\",\n-            data_args.task_name,\n-            token=model_args.token,\n-        )\n-    else:\n-        # Loading the dataset from local csv or json file.\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-        extension = (data_args.train_file if data_args.train_file is not None else data_args.valid_file).split(\".\")[-1]\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Labels\n-    if data_args.task_name is not None:\n-        is_regression = data_args.task_name == \"stsb\"\n-        if not is_regression:\n-            label_list = raw_datasets[\"train\"].features[\"label\"].names\n-            num_labels = len(label_list)\n-        else:\n-            num_labels = 1\n-    else:\n-        # Trying to have good defaults here, don't hesitate to tweak to your needs.\n-        is_regression = raw_datasets[\"train\"].features[\"label\"].dtype in [\"float32\", \"float64\"]\n-        if is_regression:\n-            num_labels = 1\n-        else:\n-            # A useful fast method:\n-            # https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.unique\n-            label_list = raw_datasets[\"train\"].unique(\"label\")\n-            label_list.sort()  # Let's sort it for determinism\n-            num_labels = len(label_list)\n-\n-    # Load pretrained model and tokenizer\n-    config = AutoConfig.from_pretrained(\n-        model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=data_args.task_name,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.model_name_or_path,\n-        use_fast=not model_args.use_slow_tokenizer,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    model = FlaxAutoModelForSequenceClassification.from_pretrained(\n-        model_args.model_name_or_path,\n-        config=config,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-\n-    # Preprocessing the datasets\n-    if data_args.task_name is not None:\n-        sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n-    else:\n-        # Again, we try to have some nice defaults but don't hesitate to tweak to your use case.\n-        non_label_column_names = [name for name in raw_datasets[\"train\"].column_names if name != \"label\"]\n-        if \"sentence1\" in non_label_column_names and \"sentence2\" in non_label_column_names:\n-            sentence1_key, sentence2_key = \"sentence1\", \"sentence2\"\n-        else:\n-            if len(non_label_column_names) >= 2:\n-                sentence1_key, sentence2_key = non_label_column_names[:2]\n-            else:\n-                sentence1_key, sentence2_key = non_label_column_names[0], None\n-\n-    # Some models have set the order of the labels to use, so let's make sure we do use it.\n-    label_to_id = None\n-    if (\n-        model.config.label2id != PretrainedConfig(num_labels=num_labels).label2id\n-        and data_args.task_name is not None\n-        and not is_regression\n-    ):\n-        # Some have all caps in their config, some don't.\n-        label_name_to_id = {k.lower(): v for k, v in model.config.label2id.items()}\n-        if sorted(label_name_to_id.keys()) == sorted(label_list):\n-            logger.info(\n-                f\"The configuration of the model provided the following label correspondence: {label_name_to_id}. \"\n-                \"Using it!\"\n-            )\n-            label_to_id = {i: label_name_to_id[label_list[i]] for i in range(num_labels)}\n-        else:\n-            logger.warning(\n-                \"Your model seems to have been trained with labels, but they don't match the dataset: \"\n-                f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n-                \"\\nIgnoring the model labels as a result.\",\n-            )\n-    elif data_args.task_name is None:\n-        label_to_id = {v: i for i, v in enumerate(label_list)}\n-\n-    def preprocess_function(examples):\n-        # Tokenize the texts\n-        texts = (\n-            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n-        )\n-        result = tokenizer(*texts, padding=\"max_length\", max_length=data_args.max_seq_length, truncation=True)\n-\n-        if \"label\" in examples:\n-            if label_to_id is not None:\n-                # Map labels to IDs (not necessary for GLUE tasks)\n-                result[\"labels\"] = [label_to_id[l] for l in examples[\"label\"]]\n-            else:\n-                # In all cases, rename the column to labels because the model will expect that.\n-                result[\"labels\"] = examples[\"label\"]\n-        return result\n-\n-    processed_datasets = raw_datasets.map(\n-        preprocess_function, batched=True, remove_columns=raw_datasets[\"train\"].column_names\n-    )\n-\n-    train_dataset = processed_datasets[\"train\"]\n-    eval_dataset = processed_datasets[\"validation_matched\" if data_args.task_name == \"mnli\" else \"validation\"]\n-\n-    # Log a few random samples from the training set:\n-    for index in random.sample(range(len(train_dataset)), 3):\n-        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n-\n-    # Define a summary writer\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(training_args.output_dir)\n-            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    def write_train_metric(summary_writer, train_metrics, train_time, step):\n-        summary_writer.scalar(\"train_time\", train_time, step)\n-\n-        train_metrics = get_metrics(train_metrics)\n-        for key, vals in train_metrics.items():\n-            tag = f\"train_{key}\"\n-            for i, val in enumerate(vals):\n-                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    def write_eval_metric(summary_writer, eval_metrics, step):\n-        for metric_name, value in eval_metrics.items():\n-            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-    num_epochs = int(training_args.num_train_epochs)\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.local_device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-\n-    learning_rate_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    state = create_train_state(\n-        model, learning_rate_fn, is_regression, num_labels=num_labels, weight_decay=training_args.weight_decay\n-    )\n-\n-    # define step functions\n-    def train_step(\n-        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n-    ) -> tuple[train_state.TrainState, float]:\n-        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-        targets = batch.pop(\"labels\")\n-\n-        def loss_fn(params):\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = state.loss_fn(logits, targets)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n-        return new_state, metrics, new_dropout_rng\n-\n-    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n-\n-    def eval_step(state, batch):\n-        logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n-        return state.logits_fn(logits)\n-\n-    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n-\n-    if data_args.task_name is not None:\n-        metric = evaluate.load(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)\n-    else:\n-        metric = evaluate.load(\"accuracy\", cache_dir=model_args.cache_dir)\n-\n-    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n-    train_time = 0\n-\n-    # make sure weights are replicated on each device\n-    state = replicate(state)\n-\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_steps = steps_per_epoch * num_epochs\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (0/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # train\n-        train_loader = glue_train_data_collator(input_rng, train_dataset, train_batch_size)\n-        for step, batch in enumerate(\n-            tqdm(\n-                train_loader,\n-                total=steps_per_epoch,\n-                desc=\"Training...\",\n-                position=1,\n-            ),\n-        ):\n-            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = (epoch * steps_per_epoch) + (step + 1)\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate']})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if (cur_step % training_args.eval_steps == 0 or cur_step % steps_per_epoch == 0) and cur_step > 0:\n-                # evaluate\n-                eval_loader = glue_eval_data_collator(eval_dataset, eval_batch_size)\n-                for batch in tqdm(\n-                    eval_loader,\n-                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n-                    desc=\"Evaluating ...\",\n-                    position=2,\n-                ):\n-                    labels = batch.pop(\"labels\")\n-                    predictions = pad_shard_unpad(p_eval_step)(\n-                        state, batch, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    metric.add_batch(predictions=np.array(predictions), references=labels)\n-\n-                eval_metric = metric.compute()\n-\n-                logger.info(f\"Step... ({cur_step}/{total_steps} | Eval metrics: {eval_metric})\")\n-\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metric, cur_step)\n-\n-            if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(unreplicate(state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of epoch {epoch}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-            epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n-\n-    # save the eval metrics in json\n-    if jax.process_index() == 0:\n-        eval_metric = {f\"eval_{metric_name}\": value for metric_name, value in eval_metric.items()}\n-        path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-        with open(path, \"w\") as f:\n-            json.dump(eval_metric, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "1f8175072148bbbe9fc9aa884616589104e05ed5",
            "filename": "examples/flax/token-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftoken-classification%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,49 +0,0 @@\n-<!---\n-Copyright 2021 The Google Flax Team Authors and HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Token classification examples\n-\n-Fine-tuning the library models for token classification task such as Named Entity Recognition (NER), Parts-of-speech tagging (POS) or phrase extraction (CHUNKS). The main script run_flax_ner.py leverages the ðŸ¤— Datasets library. You can easily customize it to your needs if you need extra processing on your datasets.\n-\n-It will either run on a datasets hosted on our hub or with your own text files for training and validation, you might just need to add some tweaks in the data preprocessing.\n-\n-The following example fine-tunes BERT on CoNLL-2003:\n-\n-\n-```bash\n-python run_flax_ner.py \\\n-  --model_name_or_path google-bert/bert-base-cased \\\n-  --dataset_name conll2003 \\\n-  --max_seq_length 128 \\\n-  --learning_rate 2e-5 \\\n-  --num_train_epochs 3 \\\n-  --per_device_train_batch_size 4 \\\n-  --output_dir ./bert-ner-conll2003 \\\n-  --eval_steps 300 \\\n-  --push_to_hub\n-```\n-\n-Using the command above, the script will train for 3 epochs and run eval after each epoch. \n-Metrics and hyperparameters are stored in Tensorflow event files in `--output_dir`.\n-You can see the results by running `tensorboard` in that directory:\n-\n-```bash\n-$ tensorboard --logdir .\n-```\n-\n-or directly on the hub under *Training metrics*.\n-\n-sample Metrics - [tfhub.dev](https://tensorboard.dev/experiment/u52qsBIpQSKEEXEJd2LVYA)\n\\ No newline at end of file"
        },
        {
            "sha": "f5ae92023d0c41158dd73931a1dc9ba0d5b80e25",
            "filename": "examples/flax/token-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftoken-classification%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,6 +0,0 @@\n-datasets >= 1.8.0\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.8\n-seqeval\n\\ No newline at end of file"
        },
        {
            "sha": "1ef5a2c6bfed8e2a71d51dd78e6105a4a2cf2b6a",
            "filename": "examples/flax/token-classification/run_flax_ner.py",
            "status": "removed",
            "additions": 0,
            "deletions": 832,
            "changes": 832,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,832 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Fine-tuning a ðŸ¤— Flax Transformers model on token classification tasks (NER, POS, CHUNKS)\"\"\"\n-\n-import json\n-import logging\n-import math\n-import os\n-import random\n-import sys\n-import time\n-import warnings\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from itertools import chain\n-from pathlib import Path\n-from typing import Any, Callable, Optional\n-\n-import datasets\n-import evaluate\n-import jax\n-import jax.numpy as jnp\n-import numpy as np\n-import optax\n-from datasets import ClassLabel, load_dataset\n-from flax import struct, traverse_util\n-from flax.jax_utils import pad_shard_unpad, replicate, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard\n-from huggingface_hub import HfApi\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    FlaxAutoModelForTokenClassification,\n-    HfArgumentParser,\n-    is_tensorboard_available,\n-)\n-from transformers.utils import check_min_version, send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-logger = logging.getLogger(__name__)\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/token-classification/requirements.txt\")\n-\n-Array = Any\n-Dataset = datasets.arrow_dataset.Dataset\n-PRNGKey = Any\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    use_auth_token: bool = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\"\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    task_name: Optional[str] = field(default=\"ner\", metadata={\"help\": \"The name of the task (ner, pos...).\"})\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a csv or JSON file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate on (a csv or JSON file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input test data file to predict on (a csv or JSON file).\"},\n-    )\n-    text_column_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The column name of text to input in the file (a csv or JSON file).\"}\n-    )\n-    label_column_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The column name of label to input in the file (a csv or JSON file).\"}\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_seq_length: int = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. If set, sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    label_all_tokens: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to put the label for one word on all tokens of generated by that word or just on the \"\n-                \"one (in which case the other tokens will have a padding index).\"\n-            )\n-        },\n-    )\n-    return_entity_level_metrics: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to return all the entity levels during evaluation or just the overall ones.\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-        self.task_name = self.task_name.lower()\n-\n-\n-def create_train_state(\n-    model: FlaxAutoModelForTokenClassification,\n-    learning_rate_fn: Callable[[int], float],\n-    num_labels: int,\n-    training_args: TrainingArguments,\n-) -> train_state.TrainState:\n-    \"\"\"Create initial training state.\"\"\"\n-\n-    class TrainState(train_state.TrainState):\n-        \"\"\"Train state with an Optax optimizer.\n-\n-        The two functions below differ depending on whether the task is classification\n-        or regression.\n-\n-        Args:\n-          logits_fn: Applied to last layer to obtain the logits.\n-          loss_fn: Function to compute the loss.\n-        \"\"\"\n-\n-        logits_fn: Callable = struct.field(pytree_node=False)\n-        loss_fn: Callable = struct.field(pytree_node=False)\n-\n-    # We use Optax's \"masking\" functionality to not apply weight decay\n-    # to bias and LayerNorm scale parameters. decay_mask_fn returns a\n-    # mask boolean with the same structure as the parameters.\n-    # The mask is True for parameters that should be decayed.\n-    def decay_mask_fn(params):\n-        flat_params = traverse_util.flatten_dict(params)\n-        # find out all LayerNorm parameters\n-        layer_norm_candidates = [\"layernorm\", \"layer_norm\", \"ln\"]\n-        layer_norm_named_params = {\n-            layer[-2:]\n-            for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params\n-            if layer_norm_name in \"\".join(layer).lower()\n-        }\n-        flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}\n-        return traverse_util.unflatten_dict(flat_mask)\n-\n-    tx = optax.adamw(\n-        learning_rate=learning_rate_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-        mask=decay_mask_fn,\n-    )\n-\n-    def cross_entropy_loss(logits, labels):\n-        xentropy = optax.softmax_cross_entropy(logits, onehot(labels, num_classes=num_labels))\n-        return jnp.mean(xentropy)\n-\n-    return TrainState.create(\n-        apply_fn=model.__call__,\n-        params=model.params,\n-        tx=tx,\n-        logits_fn=lambda logits: logits.argmax(-1),\n-        loss_fn=cross_entropy_loss,\n-    )\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def train_data_collator(rng: PRNGKey, dataset: Dataset, batch_size: int):\n-    \"\"\"Returns shuffled batches of size `batch_size` from truncated `train dataset`, sharded over all local devices.\"\"\"\n-    steps_per_epoch = len(dataset) // batch_size\n-    perms = jax.random.permutation(rng, len(dataset))\n-    perms = perms[: steps_per_epoch * batch_size]  # Skip incomplete batch.\n-    perms = perms.reshape((steps_per_epoch, batch_size))\n-\n-    for perm in perms:\n-        batch = dataset[perm]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-        batch = shard(batch)\n-\n-        yield batch\n-\n-\n-def eval_data_collator(dataset: Dataset, batch_size: int):\n-    \"\"\"Returns batches of size `batch_size` from `eval dataset`. Sharding handled by `pad_shard_unpad` in the eval loop.\"\"\"\n-    batch_idx = np.arange(len(dataset))\n-\n-    steps_per_epoch = math.ceil(len(dataset) / batch_size)\n-    batch_idx = np.array_split(batch_idx, steps_per_epoch)\n-\n-    for idx in batch_idx:\n-        batch = dataset[idx]\n-        batch = {k: np.array(v) for k, v in batch.items()}\n-\n-        yield batch\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if model_args.use_auth_token is not None:\n-        warnings.warn(\n-            \"The `use_auth_token` argument is deprecated and will be removed in v4.34. Please use `token` instead.\",\n-            FutureWarning,\n-        )\n-        if model_args.token is not None:\n-            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n-        model_args.token = model_args.use_auth_token\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_ner\", model_args, data_args, framework=\"flax\")\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        datasets.utils.logging.set_verbosity_warning()\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        datasets.utils.logging.set_verbosity_error()\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets for token classification task available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'tokens' or the first column if no column called\n-    # 'tokens' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        # Loading the dataset from local csv or json file.\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-        extension = (data_args.train_file if data_args.train_file is not None else data_args.valid_file).split(\".\")[-1]\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    if raw_datasets[\"train\"] is not None:\n-        column_names = raw_datasets[\"train\"].column_names\n-        features = raw_datasets[\"train\"].features\n-    else:\n-        column_names = raw_datasets[\"validation\"].column_names\n-        features = raw_datasets[\"validation\"].features\n-\n-    if data_args.text_column_name is not None:\n-        text_column_name = data_args.text_column_name\n-    elif \"tokens\" in column_names:\n-        text_column_name = \"tokens\"\n-    else:\n-        text_column_name = column_names[0]\n-\n-    if data_args.label_column_name is not None:\n-        label_column_name = data_args.label_column_name\n-    elif f\"{data_args.task_name}_tags\" in column_names:\n-        label_column_name = f\"{data_args.task_name}_tags\"\n-    else:\n-        label_column_name = column_names[1]\n-\n-    # In the event the labels are not a `Sequence[ClassLabel]`, we will need to go through the dataset to get the\n-    # unique labels.\n-    def get_label_list(labels):\n-        unique_labels = set()\n-        for label in labels:\n-            unique_labels = unique_labels | set(label)\n-        label_list = list(unique_labels)\n-        label_list.sort()\n-        return label_list\n-\n-    if isinstance(features[label_column_name].feature, ClassLabel):\n-        label_list = features[label_column_name].feature.names\n-        # No need to convert the labels since they are already ints.\n-        label_to_id = {i: i for i in range(len(label_list))}\n-    else:\n-        label_list = get_label_list(raw_datasets[\"train\"][label_column_name])\n-        label_to_id = {l: i for i, l in enumerate(label_list)}\n-    num_labels = len(label_list)\n-\n-    # Load pretrained model and tokenizer\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        label2id=label_to_id,\n-        id2label={i: l for l, i in label_to_id.items()},\n-        finetuning_task=data_args.task_name,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer_name_or_path = model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path\n-    if config.model_type in {\"gpt2\", \"roberta\"}:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            tokenizer_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-            add_prefix_space=True,\n-        )\n-    else:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            tokenizer_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    model = FlaxAutoModelForTokenClassification.from_pretrained(\n-        model_args.model_name_or_path,\n-        config=config,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-\n-    # Preprocessing the datasets\n-    # Tokenize all texts and align the labels with them.\n-    def tokenize_and_align_labels(examples):\n-        tokenized_inputs = tokenizer(\n-            examples[text_column_name],\n-            max_length=data_args.max_seq_length,\n-            padding=\"max_length\",\n-            truncation=True,\n-            # We use this argument because the texts in our dataset are lists of words (with a label for each word).\n-            is_split_into_words=True,\n-        )\n-\n-        labels = []\n-\n-        for i, label in enumerate(examples[label_column_name]):\n-            word_ids = tokenized_inputs.word_ids(batch_index=i)\n-            previous_word_idx = None\n-            label_ids = []\n-            for word_idx in word_ids:\n-                # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n-                # ignored in the loss function.\n-                if word_idx is None:\n-                    label_ids.append(-100)\n-                # We set the label for the first token of each word.\n-                elif word_idx != previous_word_idx:\n-                    label_ids.append(label_to_id[label[word_idx]])\n-                # For the other tokens in a word, we set the label to either the current label or -100, depending on\n-                # the label_all_tokens flag.\n-                else:\n-                    label_ids.append(label_to_id[label[word_idx]] if data_args.label_all_tokens else -100)\n-                previous_word_idx = word_idx\n-\n-            labels.append(label_ids)\n-        tokenized_inputs[\"labels\"] = labels\n-        return tokenized_inputs\n-\n-    processed_raw_datasets = raw_datasets.map(\n-        tokenize_and_align_labels,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-        remove_columns=raw_datasets[\"train\"].column_names,\n-        desc=\"Running tokenizer on dataset\",\n-    )\n-\n-    train_dataset = processed_raw_datasets[\"train\"]\n-    eval_dataset = processed_raw_datasets[\"validation\"]\n-\n-    # Log a few random samples from the training set:\n-    for index in random.sample(range(len(train_dataset)), 3):\n-        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n-\n-    # Define a summary writer\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(training_args.output_dir)\n-            summary_writer.hparams({**training_args.to_dict(), **vars(model_args), **vars(data_args)})\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    def write_train_metric(summary_writer, train_metrics, train_time, step):\n-        summary_writer.scalar(\"train_time\", train_time, step)\n-\n-        train_metrics = get_metrics(train_metrics)\n-        for key, vals in train_metrics.items():\n-            tag = f\"train_{key}\"\n-            for i, val in enumerate(vals):\n-                summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    def write_eval_metric(summary_writer, eval_metrics, step):\n-        for metric_name, value in eval_metrics.items():\n-            summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-    num_epochs = int(training_args.num_train_epochs)\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    dropout_rngs = jax.random.split(rng, jax.local_device_count())\n-\n-    train_batch_size = training_args.per_device_train_batch_size * jax.local_device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = training_args.per_device_eval_batch_size * jax.local_device_count()\n-\n-    learning_rate_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    state = create_train_state(model, learning_rate_fn, num_labels=num_labels, training_args=training_args)\n-\n-    # define step functions\n-    def train_step(\n-        state: train_state.TrainState, batch: dict[str, Array], dropout_rng: PRNGKey\n-    ) -> tuple[train_state.TrainState, float]:\n-        \"\"\"Trains model with an optimizer (both in `state`) on `batch`, returning a pair `(new_state, loss)`.\"\"\"\n-        dropout_rng, new_dropout_rng = jax.random.split(dropout_rng)\n-        targets = batch.pop(\"labels\")\n-\n-        def loss_fn(params):\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = state.loss_fn(logits, targets)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(loss_fn)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-        new_state = state.apply_gradients(grads=grad)\n-        metrics = jax.lax.pmean({\"loss\": loss, \"learning_rate\": learning_rate_fn(state.step)}, axis_name=\"batch\")\n-        return new_state, metrics, new_dropout_rng\n-\n-    p_train_step = jax.pmap(train_step, axis_name=\"batch\", donate_argnums=(0,))\n-\n-    def eval_step(state, batch):\n-        logits = state.apply_fn(**batch, params=state.params, train=False)[0]\n-        return state.logits_fn(logits)\n-\n-    p_eval_step = jax.pmap(eval_step, axis_name=\"batch\")\n-\n-    metric = evaluate.load(\"seqeval\", cache_dir=model_args.cache_dir)\n-\n-    def get_labels(y_pred, y_true):\n-        # Transform predictions and references tensos to numpy arrays\n-\n-        # Remove ignored index (special tokens)\n-        true_predictions = [\n-            [label_list[p] for (p, l) in zip(pred, gold_label) if l != -100]\n-            for pred, gold_label in zip(y_pred, y_true)\n-        ]\n-        true_labels = [\n-            [label_list[l] for (p, l) in zip(pred, gold_label) if l != -100]\n-            for pred, gold_label in zip(y_pred, y_true)\n-        ]\n-        return true_predictions, true_labels\n-\n-    def compute_metrics():\n-        results = metric.compute()\n-        if data_args.return_entity_level_metrics:\n-            # Unpack nested dictionaries\n-            final_results = {}\n-            for key, value in results.items():\n-                if isinstance(value, dict):\n-                    for n, v in value.items():\n-                        final_results[f\"{key}_{n}\"] = v\n-                else:\n-                    final_results[key] = value\n-            return final_results\n-        else:\n-            return {\n-                \"precision\": results[\"overall_precision\"],\n-                \"recall\": results[\"overall_recall\"],\n-                \"f1\": results[\"overall_f1\"],\n-                \"accuracy\": results[\"overall_accuracy\"],\n-            }\n-\n-    logger.info(f\"===== Starting training ({num_epochs} epochs) =====\")\n-    train_time = 0\n-\n-    # make sure weights are replicated on each device\n-    state = replicate(state)\n-\n-    train_time = 0\n-    step_per_epoch = len(train_dataset) // train_batch_size\n-    total_steps = step_per_epoch * num_epochs\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        train_start = time.time()\n-        train_metrics = []\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-\n-        # train\n-        for step, batch in enumerate(\n-            tqdm(\n-                train_data_collator(input_rng, train_dataset, train_batch_size),\n-                total=step_per_epoch,\n-                desc=\"Training...\",\n-                position=1,\n-            )\n-        ):\n-            state, train_metric, dropout_rngs = p_train_step(state, batch, dropout_rngs)\n-            train_metrics.append(train_metric)\n-\n-            cur_step = (epoch * step_per_epoch) + (step + 1)\n-\n-            if cur_step % training_args.logging_steps == 0 and cur_step > 0:\n-                # Save metrics\n-                train_metric = unreplicate(train_metric)\n-                train_time += time.time() - train_start\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_train_metric(summary_writer, train_metrics, train_time, cur_step)\n-\n-                epochs.write(\n-                    f\"Step... ({cur_step}/{total_steps} | Training Loss: {train_metric['loss']}, Learning Rate:\"\n-                    f\" {train_metric['learning_rate']})\"\n-                )\n-\n-                train_metrics = []\n-\n-            if cur_step % training_args.eval_steps == 0 and cur_step > 0:\n-                eval_metrics = {}\n-                # evaluate\n-                for batch in tqdm(\n-                    eval_data_collator(eval_dataset, eval_batch_size),\n-                    total=math.ceil(len(eval_dataset) / eval_batch_size),\n-                    desc=\"Evaluating ...\",\n-                    position=2,\n-                ):\n-                    labels = batch.pop(\"labels\")\n-                    predictions = pad_shard_unpad(p_eval_step)(\n-                        state, batch, min_device_batch=per_device_eval_batch_size\n-                    )\n-                    predictions = np.array(predictions)\n-                    labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n-                    preds, refs = get_labels(predictions, labels)\n-                    metric.add_batch(\n-                        predictions=preds,\n-                        references=refs,\n-                    )\n-\n-                eval_metrics = compute_metrics()\n-\n-                if data_args.return_entity_level_metrics:\n-                    logger.info(f\"Step... ({cur_step}/{total_steps} | Validation metrics: {eval_metrics}\")\n-                else:\n-                    logger.info(\n-                        f\"Step... ({cur_step}/{total_steps} | Validation f1: {eval_metrics['f1']}, Validation Acc:\"\n-                        f\" {eval_metrics['accuracy']})\"\n-                    )\n-\n-                if has_tensorboard and jax.process_index() == 0:\n-                    write_eval_metric(summary_writer, eval_metrics, cur_step)\n-\n-            if (cur_step % training_args.save_steps == 0 and cur_step > 0) or (cur_step == total_steps):\n-                # save checkpoint after each epoch and push checkpoint to the hub\n-                if jax.process_index() == 0:\n-                    params = jax.device_get(unreplicate(state.params))\n-                    model.save_pretrained(training_args.output_dir, params=params)\n-                    tokenizer.save_pretrained(training_args.output_dir)\n-                    if training_args.push_to_hub:\n-                        api.upload_folder(\n-                            commit_message=f\"Saving weights and logs of step {cur_step}\",\n-                            folder_path=training_args.output_dir,\n-                            repo_id=repo_id,\n-                            repo_type=\"model\",\n-                            token=training_args.hub_token,\n-                        )\n-        epochs.desc = f\"Epoch ... {epoch + 1}/{num_epochs}\"\n-\n-    # Eval after training\n-    if training_args.do_eval:\n-        eval_metrics = {}\n-        eval_loader = eval_data_collator(eval_dataset, eval_batch_size)\n-        for batch in tqdm(eval_loader, total=len(eval_dataset) // eval_batch_size, desc=\"Evaluating ...\", position=2):\n-            labels = batch.pop(\"labels\")\n-            predictions = pad_shard_unpad(p_eval_step)(state, batch, min_device_batch=per_device_eval_batch_size)\n-            predictions = np.array(predictions)\n-            labels[np.array(chain(*batch[\"attention_mask\"])) == 0] = -100\n-            preds, refs = get_labels(predictions, labels)\n-            metric.add_batch(predictions=preds, references=refs)\n-\n-        eval_metrics = compute_metrics()\n-\n-        if jax.process_index() == 0:\n-            eval_metrics = {f\"eval_{metric_name}\": value for metric_name, value in eval_metrics.items()}\n-            path = os.path.join(training_args.output_dir, \"eval_results.json\")\n-            with open(path, \"w\") as f:\n-                json.dump(eval_metrics, f, indent=4, sort_keys=True)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "d865b8a30ce5e0ce4236262cef00e61822f7794f",
            "filename": "examples/flax/vision/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fvision%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,70 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Image Classification training examples\n-\n-The following example showcases how to train/fine-tune `ViT` for image-classification using the JAX/Flax backend.\n-\n-JAX/Flax allows you to trace pure functions and compile them into efficient, fused accelerator code on both GPU and TPU.\n-Models written in JAX/Flax are **immutable** and updated in a purely functional\n-way which enables simple and efficient model parallelism.\n-\n-\n-In this example we will train/fine-tune the model on the [imagenette](https://github.com/fastai/imagenette) dataset.\n-\n-## Prepare the dataset\n-\n-We will use the [imagenette](https://github.com/fastai/imagenette) dataset to train/fine-tune our model. Imagenette is a subset of 10 easily classified classes from Imagenet (tench, English springer, cassette player, chain saw, church, French horn, garbage truck, gas pump, golf ball, parachute).\n-\n-\n-### Download and extract the data.\n-\n-```bash\n-wget https://s3.amazonaws.com/fast-ai-imageclas/imagenette2.tgz\n-tar -xvzf imagenette2.tgz\n-```\n-\n-This will create a `imagenette2` dir with two subdirectories `train` and `val` each with multiple subdirectories per class. The training script expects the following directory structure\n-\n-```bash\n-root/dog/xxx.png\n-root/dog/xxy.png\n-root/dog/[...]/xxz.png\n-\n-root/cat/123.png\n-root/cat/nsdf3.png\n-root/cat/[...]/asd932_.png\n-```\n-\n-## Train the model\n-\n-Next we can run the example script to fine-tune the model:\n-\n-```bash\n-python run_image_classification.py \\\n-    --output_dir ./vit-base-patch16-imagenette \\\n-    --model_name_or_path google/vit-base-patch16-224-in21k \\\n-    --train_dir=\"imagenette2/train\" \\\n-    --validation_dir=\"imagenette2/val\" \\\n-    --num_train_epochs 5 \\\n-    --learning_rate 1e-3 \\\n-    --per_device_train_batch_size 128 --per_device_eval_batch_size 128 \\\n-    --overwrite_output_dir \\\n-    --preprocessing_num_workers 32 \\\n-    --push_to_hub\n-```\n-\n-This should finish in ~7mins with 99% validation accuracy.\n\\ No newline at end of file"
        },
        {
            "sha": "7567b430b13d2c766299bfca01372271f0cb897e",
            "filename": "examples/flax/vision/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fvision%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,8 +0,0 @@\n-jax>=0.2.8\n-jaxlib>=0.1.59\n-flax>=0.3.5\n-optax>=0.0.8\n--f https://download.pytorch.org/whl/torch_stable.html\n-torch==2.7.1\n--f https://download.pytorch.org/whl/torch_stable.html\n-torchvision==0.12.0+cpu"
        },
        {
            "sha": "4c6550e6b8033e0f76b9ed054340e0c96a108a6b",
            "filename": "examples/flax/vision/run_image_classification.py",
            "status": "removed",
            "additions": 0,
            "deletions": 590,
            "changes": 590,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Fflax%2Fvision%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fvision%2Frun_image_classification.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,590 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Pre-training/Fine-tuning ViT for image classification .\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=vit\n-\"\"\"\n-\n-import logging\n-import os\n-import sys\n-import time\n-from dataclasses import asdict, dataclass, field\n-from enum import Enum\n-from pathlib import Path\n-from typing import Callable, Optional\n-\n-import jax\n-import jax.numpy as jnp\n-import optax\n-\n-# for dataset and preprocessing\n-import torch\n-import torchvision\n-from flax import jax_utils\n-from flax.jax_utils import pad_shard_unpad, unreplicate\n-from flax.training import train_state\n-from flax.training.common_utils import get_metrics, onehot, shard, shard_prng_key\n-from huggingface_hub import HfApi\n-from torchvision import transforms\n-from tqdm import tqdm\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n-    AutoConfig,\n-    FlaxAutoModelForImageClassification,\n-    HfArgumentParser,\n-    is_tensorboard_available,\n-    set_seed,\n-)\n-from transformers.utils import send_example_telemetry\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-MODEL_CONFIG_CLASSES = list(FLAX_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-@dataclass\n-class TrainingArguments:\n-    output_dir: str = field(\n-        metadata={\"help\": \"The output directory where the model predictions and checkpoints will be written.\"},\n-    )\n-    overwrite_output_dir: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Overwrite the content of the output directory. \"\n-                \"Use this to continue training if output_dir points to a checkpoint directory.\"\n-            )\n-        },\n-    )\n-    do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n-    do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n-    per_device_train_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for training.\"}\n-    )\n-    per_device_eval_batch_size: int = field(\n-        default=8, metadata={\"help\": \"Batch size per GPU/TPU core/CPU for evaluation.\"}\n-    )\n-    learning_rate: float = field(default=5e-5, metadata={\"help\": \"The initial learning rate for AdamW.\"})\n-    weight_decay: float = field(default=0.0, metadata={\"help\": \"Weight decay for AdamW if we apply some.\"})\n-    adam_beta1: float = field(default=0.9, metadata={\"help\": \"Beta1 for AdamW optimizer\"})\n-    adam_beta2: float = field(default=0.999, metadata={\"help\": \"Beta2 for AdamW optimizer\"})\n-    adam_epsilon: float = field(default=1e-8, metadata={\"help\": \"Epsilon for AdamW optimizer.\"})\n-    adafactor: bool = field(default=False, metadata={\"help\": \"Whether or not to replace AdamW by Adafactor.\"})\n-    num_train_epochs: float = field(default=3.0, metadata={\"help\": \"Total number of training epochs to perform.\"})\n-    warmup_steps: int = field(default=0, metadata={\"help\": \"Linear warmup over warmup_steps.\"})\n-    logging_steps: int = field(default=500, metadata={\"help\": \"Log every X updates steps.\"})\n-    save_steps: int = field(default=500, metadata={\"help\": \"Save checkpoint every X updates steps.\"})\n-    eval_steps: int = field(default=None, metadata={\"help\": \"Run an evaluation every X steps.\"})\n-    seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    push_to_hub: bool = field(\n-        default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n-    )\n-    hub_model_id: str = field(\n-        default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n-    )\n-    hub_token: str = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-\n-    def __post_init__(self):\n-        if self.output_dir is not None:\n-            self.output_dir = os.path.expanduser(self.output_dir)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance while replace `Enum` by their values (for JSON serialization support). It obfuscates\n-        the token values by removing their value.\n-        \"\"\"\n-        d = asdict(self)\n-        for k, v in d.items():\n-            if isinstance(v, Enum):\n-                d[k] = v.value\n-            if isinstance(v, list) and len(v) > 0 and isinstance(v[0], Enum):\n-                d[k] = [x.value for x in v]\n-            if k.endswith(\"_token\"):\n-                d[k] = f\"<{k.upper()}>\"\n-        return d\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"float32\",\n-        metadata={\n-            \"help\": (\n-                \"Floating-point format in which the model weights should be initialized and trained. Choose one of\"\n-                \" `[float32, float16, bfloat16]`.\"\n-            )\n-        },\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n-                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n-                \"execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    train_dir: str = field(\n-        metadata={\"help\": \"Path to the root training directory which contains one subdirectory per class.\"}\n-    )\n-    validation_dir: str = field(\n-        metadata={\"help\": \"Path to the root validation directory which contains one subdirectory per class.\"},\n-    )\n-    image_size: Optional[int] = field(default=224, metadata={\"help\": \" The size (resolution) of each image.\"})\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-\n-\n-class TrainState(train_state.TrainState):\n-    dropout_rng: jnp.ndarray\n-\n-    def replicate(self):\n-        return jax_utils.replicate(self).replace(dropout_rng=shard_prng_key(self.dropout_rng))\n-\n-\n-def write_metric(summary_writer, train_metrics, eval_metrics, train_time, step):\n-    summary_writer.scalar(\"train_time\", train_time, step)\n-\n-    train_metrics = get_metrics(train_metrics)\n-    for key, vals in train_metrics.items():\n-        tag = f\"train_{key}\"\n-        for i, val in enumerate(vals):\n-            summary_writer.scalar(tag, val, step - len(vals) + i + 1)\n-\n-    for metric_name, value in eval_metrics.items():\n-        summary_writer.scalar(f\"eval_{metric_name}\", value, step)\n-\n-\n-def create_learning_rate_fn(\n-    train_ds_size: int, train_batch_size: int, num_train_epochs: int, num_warmup_steps: int, learning_rate: float\n-) -> Callable[[int], jnp.ndarray]:\n-    \"\"\"Returns a linear warmup, linear_decay learning rate function.\"\"\"\n-    steps_per_epoch = train_ds_size // train_batch_size\n-    num_train_steps = steps_per_epoch * num_train_epochs\n-    warmup_fn = optax.linear_schedule(init_value=0.0, end_value=learning_rate, transition_steps=num_warmup_steps)\n-    decay_fn = optax.linear_schedule(\n-        init_value=learning_rate, end_value=0, transition_steps=num_train_steps - num_warmup_steps\n-    )\n-    schedule_fn = optax.join_schedules(schedules=[warmup_fn, decay_fn], boundaries=[num_warmup_steps])\n-    return schedule_fn\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_image_classification\", model_args, data_args, framework=\"flax\")\n-\n-    if (\n-        os.path.exists(training_args.output_dir)\n-        and os.listdir(training_args.output_dir)\n-        and training_args.do_train\n-        and not training_args.overwrite_output_dir\n-    ):\n-        raise ValueError(\n-            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-            \"Use --overwrite_output_dir to overcome.\"\n-        )\n-\n-    # Make one log on every process with the configuration for debugging.\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        level=logging.INFO,\n-    )\n-    # Setup logging, we only want one process per machine to log things on the screen.\n-    logger.setLevel(logging.INFO if jax.process_index() == 0 else logging.ERROR)\n-    if jax.process_index() == 0:\n-        transformers.utils.logging.set_verbosity_info()\n-    else:\n-        transformers.utils.logging.set_verbosity_error()\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # set seed for random transforms and torch dataloaders\n-    set_seed(training_args.seed)\n-\n-    # Handle the repository creation\n-    if training_args.push_to_hub:\n-        # Retrieve of infer repo_name\n-        repo_name = training_args.hub_model_id\n-        if repo_name is None:\n-            repo_name = Path(training_args.output_dir).absolute().name\n-        # Create repo and retrieve repo_id\n-        api = HfApi()\n-        repo_id = api.create_repo(repo_name, exist_ok=True, token=training_args.hub_token).repo_id\n-\n-    # Initialize datasets and pre-processing transforms\n-    # We use torchvision here for faster pre-processing\n-    # Note that here we are using some default pre-processing, for maximum accuracy\n-    # one should tune this part and carefully select what transformations to use.\n-    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n-    train_dataset = torchvision.datasets.ImageFolder(\n-        data_args.train_dir,\n-        transforms.Compose(\n-            [\n-                transforms.RandomResizedCrop(data_args.image_size),\n-                transforms.RandomHorizontalFlip(),\n-                transforms.ToTensor(),\n-                normalize,\n-            ]\n-        ),\n-    )\n-\n-    eval_dataset = torchvision.datasets.ImageFolder(\n-        data_args.validation_dir,\n-        transforms.Compose(\n-            [\n-                transforms.Resize(data_args.image_size),\n-                transforms.CenterCrop(data_args.image_size),\n-                transforms.ToTensor(),\n-                normalize,\n-            ]\n-        ),\n-    )\n-\n-    # Load pretrained model and tokenizer\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name,\n-            num_labels=len(train_dataset.classes),\n-            image_size=data_args.image_size,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path,\n-            num_labels=len(train_dataset.classes),\n-            image_size=data_args.image_size,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.model_name_or_path:\n-        model = FlaxAutoModelForImageClassification.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        model = FlaxAutoModelForImageClassification.from_config(\n-            config,\n-            seed=training_args.seed,\n-            dtype=getattr(jnp, model_args.dtype),\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-    # Store some constant\n-    num_epochs = int(training_args.num_train_epochs)\n-    train_batch_size = int(training_args.per_device_train_batch_size) * jax.device_count()\n-    per_device_eval_batch_size = int(training_args.per_device_eval_batch_size)\n-    eval_batch_size = per_device_eval_batch_size * jax.device_count()\n-    steps_per_epoch = len(train_dataset) // train_batch_size\n-    total_train_steps = steps_per_epoch * num_epochs\n-\n-    def collate_fn(examples):\n-        pixel_values = torch.stack([example[0] for example in examples])\n-        labels = torch.tensor([example[1] for example in examples])\n-\n-        batch = {\"pixel_values\": pixel_values, \"labels\": labels}\n-        batch = {k: v.numpy() for k, v in batch.items()}\n-\n-        return batch\n-\n-    # Create data loaders\n-    train_loader = torch.utils.data.DataLoader(\n-        train_dataset,\n-        batch_size=train_batch_size,\n-        shuffle=True,\n-        num_workers=data_args.preprocessing_num_workers,\n-        persistent_workers=True,\n-        drop_last=True,\n-        collate_fn=collate_fn,\n-    )\n-\n-    eval_loader = torch.utils.data.DataLoader(\n-        eval_dataset,\n-        batch_size=eval_batch_size,\n-        shuffle=False,\n-        num_workers=data_args.preprocessing_num_workers,\n-        persistent_workers=True,\n-        drop_last=False,\n-        collate_fn=collate_fn,\n-    )\n-\n-    # Enable tensorboard only on the master node\n-    has_tensorboard = is_tensorboard_available()\n-    if has_tensorboard and jax.process_index() == 0:\n-        try:\n-            from flax.metrics.tensorboard import SummaryWriter\n-\n-            summary_writer = SummaryWriter(log_dir=Path(training_args.output_dir))\n-        except ImportError as ie:\n-            has_tensorboard = False\n-            logger.warning(\n-                f\"Unable to display metrics through TensorBoard because some package are not installed: {ie}\"\n-            )\n-    else:\n-        logger.warning(\n-            \"Unable to display metrics through TensorBoard because the package is not installed: \"\n-            \"Please run pip install tensorboard to enable.\"\n-        )\n-\n-    # Initialize our training\n-    rng = jax.random.PRNGKey(training_args.seed)\n-    rng, dropout_rng = jax.random.split(rng)\n-\n-    # Create learning rate schedule\n-    linear_decay_lr_schedule_fn = create_learning_rate_fn(\n-        len(train_dataset),\n-        train_batch_size,\n-        training_args.num_train_epochs,\n-        training_args.warmup_steps,\n-        training_args.learning_rate,\n-    )\n-\n-    # create adam optimizer\n-    adamw = optax.adamw(\n-        learning_rate=linear_decay_lr_schedule_fn,\n-        b1=training_args.adam_beta1,\n-        b2=training_args.adam_beta2,\n-        eps=training_args.adam_epsilon,\n-        weight_decay=training_args.weight_decay,\n-    )\n-\n-    # Setup train state\n-    state = TrainState.create(apply_fn=model.__call__, params=model.params, tx=adamw, dropout_rng=dropout_rng)\n-\n-    def loss_fn(logits, labels):\n-        loss = optax.softmax_cross_entropy(logits, onehot(labels, logits.shape[-1]))\n-        return loss.mean()\n-\n-    # Define gradient update step fn\n-    def train_step(state, batch):\n-        dropout_rng, new_dropout_rng = jax.random.split(state.dropout_rng)\n-\n-        def compute_loss(params):\n-            labels = batch.pop(\"labels\")\n-            logits = state.apply_fn(**batch, params=params, dropout_rng=dropout_rng, train=True)[0]\n-            loss = loss_fn(logits, labels)\n-            return loss\n-\n-        grad_fn = jax.value_and_grad(compute_loss)\n-        loss, grad = grad_fn(state.params)\n-        grad = jax.lax.pmean(grad, \"batch\")\n-\n-        new_state = state.apply_gradients(grads=grad, dropout_rng=new_dropout_rng)\n-\n-        metrics = {\"loss\": loss, \"learning_rate\": linear_decay_lr_schedule_fn(state.step)}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-\n-        return new_state, metrics\n-\n-    # Define eval fn\n-    def eval_step(params, batch):\n-        labels = batch.pop(\"labels\")\n-        logits = model(**batch, params=params, train=False)[0]\n-        loss = loss_fn(logits, labels)\n-\n-        # summarize metrics\n-        accuracy = (jnp.argmax(logits, axis=-1) == labels).mean()\n-        metrics = {\"loss\": loss, \"accuracy\": accuracy}\n-        metrics = jax.lax.pmean(metrics, axis_name=\"batch\")\n-        return metrics\n-\n-    # Create parallel version of the train and eval step\n-    p_train_step = jax.pmap(train_step, \"batch\", donate_argnums=(0,))\n-    p_eval_step = jax.pmap(eval_step, \"batch\")\n-\n-    # Replicate the train state on each device\n-    state = state.replicate()\n-\n-    logger.info(\"***** Running training *****\")\n-    logger.info(f\"  Num examples = {len(train_dataset)}\")\n-    logger.info(f\"  Num Epochs = {num_epochs}\")\n-    logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-    logger.info(f\"  Total train batch size (w. parallel & distributed) = {train_batch_size}\")\n-    logger.info(f\"  Total optimization steps = {total_train_steps}\")\n-\n-    train_time = 0\n-    epochs = tqdm(range(num_epochs), desc=f\"Epoch ... (1/{num_epochs})\", position=0)\n-    for epoch in epochs:\n-        # ======================== Training ================================\n-        train_start = time.time()\n-\n-        # Create sampling rng\n-        rng, input_rng = jax.random.split(rng)\n-        train_metrics = []\n-\n-        steps_per_epoch = len(train_dataset) // train_batch_size\n-        train_step_progress_bar = tqdm(total=steps_per_epoch, desc=\"Training...\", position=1, leave=False)\n-        # train\n-        for batch in train_loader:\n-            batch = shard(batch)\n-            state, train_metric = p_train_step(state, batch)\n-            train_metrics.append(train_metric)\n-\n-            train_step_progress_bar.update(1)\n-\n-        train_time += time.time() - train_start\n-\n-        train_metric = unreplicate(train_metric)\n-\n-        train_step_progress_bar.close()\n-        epochs.write(\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Loss: {train_metric['loss']}, Learning Rate:\"\n-            f\" {train_metric['learning_rate']})\"\n-        )\n-\n-        # ======================== Evaluating ==============================\n-        eval_metrics = []\n-        eval_steps = len(eval_dataset) // eval_batch_size\n-        eval_step_progress_bar = tqdm(total=eval_steps, desc=\"Evaluating...\", position=2, leave=False)\n-        for batch in eval_loader:\n-            # Model forward\n-            metrics = pad_shard_unpad(p_eval_step, static_return=True)(\n-                state.params, batch, min_device_batch=per_device_eval_batch_size\n-            )\n-            eval_metrics.append(metrics)\n-\n-            eval_step_progress_bar.update(1)\n-\n-        # normalize eval metrics\n-        eval_metrics = get_metrics(eval_metrics)\n-        eval_metrics = jax.tree_util.tree_map(jnp.mean, eval_metrics)\n-\n-        # Print metrics and update progress bar\n-        eval_step_progress_bar.close()\n-        desc = (\n-            f\"Epoch... ({epoch + 1}/{num_epochs} | Eval Loss: {round(eval_metrics['loss'].item(), 4)} | \"\n-            f\"Eval Accuracy: {round(eval_metrics['accuracy'].item(), 4)})\"\n-        )\n-        epochs.write(desc)\n-        epochs.desc = desc\n-\n-        # Save metrics\n-        if has_tensorboard and jax.process_index() == 0:\n-            cur_step = epoch * (len(train_dataset) // train_batch_size)\n-            write_metric(summary_writer, train_metrics, eval_metrics, train_time, cur_step)\n-\n-        # save checkpoint after each epoch and push checkpoint to the hub\n-        if jax.process_index() == 0:\n-            params = jax.device_get(jax.tree_util.tree_map(lambda x: x[0], state.params))\n-            model.save_pretrained(training_args.output_dir, params=params)\n-            if training_args.push_to_hub:\n-                api.upload_folder(\n-                    commit_message=f\"Saving weights and logs of epoch {epoch}\",\n-                    folder_path=training_args.output_dir,\n-                    repo_id=repo_id,\n-                    repo_type=\"model\",\n-                    token=training_args.hub_token,\n-                )\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "2c4115b369f75ac1ffa6c32e3a99092c1cb07ac5",
            "filename": "examples/tensorflow/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,44 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Examples\n-\n-This folder contains actively maintained examples of the use of ðŸ¤— Transformers organized into different ML tasks. All examples in this folder are **TensorFlow** examples and are written using native Keras. If you've previously only used ðŸ¤— Transformers via `TFTrainer`, we highly recommend taking a look at the new style - we think it's a big improvement!\n-\n-In addition, all scripts here now support the [ðŸ¤— Datasets](https://github.com/huggingface/datasets) library - you can grab entire datasets just by changing one command-line argument!\n-\n-## A note on code folding\n-\n-Most of these examples have been formatted with #region blocks. In IDEs such as PyCharm and VSCode, these blocks mark\n-named regions of code that can be folded for easier viewing. If you find any of these scripts overwhelming or difficult\n-to follow, we highly recommend beginning with all regions folded and then examining regions one at a time!\n-\n-## The Big Table of Tasks\n-\n-Here is the list of all our examples:\n-\n-| Task | Example datasets |\n-|---|---|\n-| [**`language-modeling`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling) | WikiText-2\n-| [**`multiple-choice`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) | SWAG\n-| [**`question-answering`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering) | SQuAD\n-| [**`summarization`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/summarization) | XSum\n-| [**`text-classification`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) | GLUE\n-| [**`token-classification`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) | CoNLL NER\n-| [**`translation`**](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/translation) | WMT\n-\n-## Coming soon\n-\n-- **Colab notebooks** to easily run through these scripts!"
        },
        {
            "sha": "6971795ce4ea198b8207bcee50f3ee14a21fdc6e",
            "filename": "examples/tensorflow/_tests_requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2F_tests_requirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2F_tests_requirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2F_tests_requirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,25 +0,0 @@\n-tensorflow<2.16\n-keras<2.16\n-tensorboard\n-scikit-learn\n-seqeval\n-psutil\n-sacrebleu >= 1.4.12\n-rouge-score\n-tensorflow_datasets\n-matplotlib\n-git-python==1.0.3\n-faiss-cpu\n-streamlit\n-elasticsearch\n-nltk\n-pandas\n-datasets >= 1.13.3\n-fire\n-pytest<8.0.1\n-conllu\n-sentencepiece != 0.1.92\n-protobuf\n-jiwer\n-librosa\n-evaluate >= 0.2.0"
        },
        {
            "sha": "29d9b897734cb27e211129d32eb50e60121f12fa",
            "filename": "examples/tensorflow/contrastive-image-text/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 81,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fcontrastive-image-text%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,81 +0,0 @@\n-<!---\n-Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# TFVisionTextDualEncoder and CLIP model training examples\n-\n-The following example showcases how to train a CLIP-like vision-text dual encoder model\n-using a pre-trained vision and text encoder.\n-\n-Such a model can be used for natural language image search and potentially zero-shot image classification.\n-The model is inspired by [CLIP](https://openai.com/blog/clip/), introduced by Alec Radford et al.\n-The idea is to train a vision encoder and a text encoder jointly to project the representation of images and their\n-captions into the same embedding space, such that the caption embeddings are located near the embeddings\n-of the images they describe.\n-\n-### Download COCO dataset (2017)\n-This example uses COCO dataset (2017) through a custom dataset script, which requires users to manually download the\n-COCO dataset before training.\n-\n-```bash\n-mkdir data\n-cd data\n-wget http://images.cocodataset.org/zips/train2017.zip\n-wget http://images.cocodataset.org/zips/val2017.zip\n-wget http://images.cocodataset.org/zips/test2017.zip\n-wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n-wget http://images.cocodataset.org/annotations/image_info_test2017.zip\n-cd ..\n-```\n-\n-Having downloaded COCO dataset manually you should be able to load with the `ydshieh/coc_dataset_script` dataset loading script:\n-\n-```py\n-import os\n-import datasets\n-\n-COCO_DIR = os.path.join(os.getcwd(), \"data\")\n-ds = datasets.load_dataset(\"ydshieh/coco_dataset_script\", \"2017\", data_dir=COCO_DIR)\n-```\n-\n-### Create a model from a vision encoder model and a text encoder model\n-We can either load a CLIP-like vision-text dual encoder model from an existing dual encoder model, or\n-by using a pre-trained vision encoder model and a pre-trained text encoder model.\n-\n-If you wish to load an existing dual encoder model, please use the `--model_name_or_path` argument. If\n-you want to use separate pre-trained vision and text models, please use the\n-`--vision_model_name_or_path` and `--text_model_name_or_path` arguments instead.\n-\n-### Train the model\n-Finally, we can run the example script to train the model:\n-\n-```bash\n-python examples/tensorflow/contrastive-image-text/run_clip.py \\\n-    --output_dir ./clip-roberta-finetuned \\\n-    --vision_model_name_or_path openai/clip-vit-base-patch32 \\\n-    --text_model_name_or_path FacebookAI/roberta-base \\\n-    --data_dir $PWD/data \\\n-    --dataset_name ydshieh/coco_dataset_script \\\n-    --dataset_config_name=2017 \\\n-    --image_column image_path \\\n-    --caption_column caption \\\n-    --remove_unused_columns=False \\\n-    --do_train  --do_eval \\\n-    --per_device_train_batch_size=\"64\" \\\n-    --per_device_eval_batch_size=\"64\" \\\n-    --learning_rate=\"5e-5\" --warmup_steps=\"0\" --weight_decay 0.1 \\\n-    --overwrite_output_dir \\\n-    --push_to_hub\n-```"
        },
        {
            "sha": "ef4bf188bff20310196af7d37f99e9bd1581dddd",
            "filename": "examples/tensorflow/contrastive-image-text/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fcontrastive-image-text%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,2 +0,0 @@\n-tensorflow>=2.6.0\n-datasets>=1.8.0\n\\ No newline at end of file"
        },
        {
            "sha": "e4a98f4ae0f9db9119feaaccc4d2d97d84f1f553",
            "filename": "examples/tensorflow/contrastive-image-text/run_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 610,
            "changes": 610,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fcontrastive-image-text%2Frun_clip.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,610 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2023 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Training a CLIP like dual encoder models using text and vision encoders in the library.\n-\n-The script can be used to train CLIP like models for languages other than English by using\n-a text encoder pre-trained in the desired language. Currently this script supports the following vision\n-and text models:\n-Vision models: ViT(https://huggingface.co/models?filter=vit), CLIP (https://huggingface.co/models?filter=clip)\n-Text models: BERT, ROBERTa (https://huggingface.co/models?filter=fill-mask)\n-\"\"\"\n-\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import tensorflow as tf\n-from datasets import load_dataset\n-from PIL import Image\n-\n-import transformers\n-from transformers import (\n-    AutoImageProcessor,\n-    AutoTokenizer,\n-    HfArgumentParser,\n-    PushToHubCallback,\n-    TFAutoModel,\n-    TFTrainingArguments,\n-    TFVisionTextDualEncoderModel,\n-    create_optimizer,\n-)\n-from transformers.utils import check_min_version, send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-require_version(\n-    \"datasets>=1.8.0\", \"To fix: pip install -r examples/tensorflow/contrastive-image-text/requirements.txt\"\n-)\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}, default=None\n-    )\n-    vision_model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained image model or model identifier from huggingface.co/models\"},\n-        default=None,\n-    )\n-    text_model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained text model or model identifier from huggingface.co/models\"}, default=None\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    freeze_vision_model: bool = field(\n-        default=False, metadata={\"help\": \"Whether to freeze the vision model parameters or not.\"}\n-    )\n-    freeze_text_model: bool = field(\n-        default=False, metadata={\"help\": \"Whether to freeze the text model parameters or not.\"}\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    data_dir: Optional[str] = field(default=None, metadata={\"help\": \"The data directory containing input files.\"})\n-    image_column: Optional[str] = field(\n-        default=\"image_path\",\n-        metadata={\"help\": \"The name of the column in the datasets containing the full image file paths.\"},\n-    )\n-    caption_column: Optional[str] = field(\n-        default=\"caption\",\n-        metadata={\"help\": \"The name of the column in the datasets containing the image captions.\"},\n-    )\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a jsonlines file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file (a jsonlines file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input testing data file (a jsonlines file).\"},\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-            if self.test_file is not None:\n-                extension = self.test_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n-\n-\n-dataset_name_mapping = {\n-    \"image_caption_dataset.py\": (\"image_path\", \"caption\"),\n-}\n-\n-\n-def crop_to_square(image):\n-    height, width = tf.shape(image)[0], tf.shape(image)[1]\n-    if height > width:\n-        image = tf.image.crop_to_bounding_box(image, (height - width) // 2, 0, width, width)\n-    elif width > height:\n-        image = tf.image.crop_to_bounding_box(image, 0, (width - height) // 2, height, height)\n-    return image\n-\n-\n-def load_as_tf_dataset(dataset, image_column, image_size, mean, std, batch_size, shuffle):\n-    dataset = dataset.with_format(\"tensorflow\")[:]  # Load the dataset as tensor slices, but not the images yet!\n-    tf_dataset = tf.data.Dataset.from_tensor_slices(dataset)\n-\n-    def load_image(sample):\n-        image_path = sample[image_column]\n-        image = tf.io.read_file(image_path)\n-        image = tf.image.decode_image(image, channels=3, expand_animations=False)\n-        image = crop_to_square(image)\n-        image = tf.image.resize(image, [image_size, image_size], method=\"bicubic\", antialias=True)\n-        image = image / 255.0\n-        image = (image - mean) / std\n-        image = tf.transpose(image, perm=[2, 0, 1])  # Convert to channels-first\n-        sample[\"pixel_values\"] = image\n-        del sample[image_column]\n-        return sample\n-\n-    if shuffle:\n-        tf_dataset = tf_dataset.shuffle(len(tf_dataset))\n-    tf_dataset = tf_dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n-    tf_dataset = tf_dataset.batch(batch_size, drop_remainder=shuffle)\n-    tf_dataset = tf_dataset.prefetch(tf.data.experimental.AUTOTUNE)\n-\n-    return tf_dataset\n-\n-\n-def main():\n-    # 1. Parse input arguments\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    if model_args.model_name_or_path is not None:\n-        if model_args.vision_model_name_or_path is not None or model_args.text_model_name_or_path is not None:\n-            raise ValueError(\n-                \"If using model_name_or_path, you cannot specify separate image/text model paths as well!\"\n-            )\n-\n-    if model_args.vision_model_name_or_path is not None or model_args.text_model_name_or_path is not None:\n-        if model_args.model_name_or_path is not None:\n-            raise ValueError(\n-                \"If using separate image/text model paths, you cannot specify model_name_or_path as well!\"\n-            )\n-        if not (model_args.vision_model_name_or_path is not None and model_args.text_model_name_or_path is not None):\n-            raise ValueError(\n-                \"If using separate image/text model paths, you must specify both vision_model_name_or_path \"\n-                \"and text_model_name_or_path!\"\n-            )\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/TensorFlow versions.\n-    send_example_telemetry(\"run_clip\", model_args, data_args, framework=\"tensorflow\")\n-\n-    # 2. Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-\n-    # The default of training_args.log_level is passive, so we set log level at info here to have that default.\n-    transformers.utils.logging.set_verbosity_info()\n-\n-    log_level = training_args.get_process_log_level()\n-    logger.setLevel(log_level)\n-    transformers.utils.logging.set_verbosity(log_level)\n-    transformers.utils.logging.enable_default_handler()\n-    transformers.utils.logging.enable_explicit_format()\n-\n-    # Log on each process the small summary:\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # 3. Detecting last checkpoint and eventually continue from last checkpoint\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n-    # 4. Load dataset\n-    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files this script will use the first column for the full image path and the second column for the\n-    # captions (unless you specify column names for this with the `image_column` and `caption_column` arguments).\n-    #\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            keep_in_memory=False,\n-            data_dir=data_args.data_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        dataset = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # 5. Load pretrained model, tokenizer, and image processor\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.text_model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.text_model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            use_fast=model_args.use_fast_tokenizer,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-\n-    if model_args.model_name_or_path:\n-        # Load image_processor, in this script we only use this to get the mean and std for normalization.\n-        image_processor = AutoImageProcessor.from_pretrained(\n-            model_args.image_processor_name or model_args.model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        with training_args.strategy.scope():\n-            model = TFAutoModel.from_pretrained(\n-                model_args.model_name_or_path,\n-                cache_dir=model_args.cache_dir,\n-                revision=model_args.model_revision,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    else:\n-        # Load image_processor, in this script we only use this to get the mean and std for normalization.\n-        image_processor = AutoImageProcessor.from_pretrained(\n-            model_args.image_processor_name or model_args.vision_model_name_or_path,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        with training_args.strategy.scope():\n-            model = TFVisionTextDualEncoderModel.from_vision_text_pretrained(\n-                vision_model_name_or_path=model_args.vision_model_name_or_path,\n-                text_model_name_or_path=model_args.text_model_name_or_path,\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    config = model.config\n-\n-    if model_args.freeze_vision_model:\n-        model.vision_model.trainable = False\n-\n-    if model_args.freeze_text_model:\n-        model.text_model.trainable = False\n-\n-    # Preprocessing the datasets.\n-    # We need to tokenize inputs and targets.\n-    if training_args.do_train:\n-        column_names = dataset[\"train\"].column_names\n-    elif training_args.do_eval:\n-        column_names = dataset[\"validation\"].column_names\n-    elif training_args.do_predict:\n-        column_names = dataset[\"test\"].column_names\n-    else:\n-        logger.info(\"There is nothing to do. Please pass `do_train`, `do_eval` and/or `do_predict`.\")\n-        return\n-\n-    # 6. Get the column names for input/target.\n-    dataset_columns = dataset_name_mapping.get(data_args.dataset_name, None)\n-    if data_args.image_column is None:\n-        image_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n-    else:\n-        image_column = data_args.image_column\n-        if image_column not in column_names:\n-            raise ValueError(\n-                f\"--image_column' value '{data_args.image_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-    if data_args.caption_column is None:\n-        caption_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n-    else:\n-        caption_column = data_args.caption_column\n-        if caption_column not in column_names:\n-            raise ValueError(\n-                f\"--caption_column' value '{data_args.caption_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-\n-    # # 7. Preprocessing the datasets.\n-\n-    # We need to tokenize input captions and transform the images.\n-    def tokenize_captions(examples):\n-        captions = list(examples[caption_column])\n-        text_inputs = tokenizer(captions, max_length=data_args.max_seq_length, padding=\"max_length\", truncation=True)\n-        examples[\"input_ids\"] = text_inputs.input_ids\n-        examples[\"attention_mask\"] = text_inputs.attention_mask\n-        return examples\n-\n-    def filter_corrupt_images(examples):\n-        \"\"\"remove problematic images\"\"\"\n-        valid_images = []\n-        for image_file in examples[image_column]:\n-            try:\n-                Image.open(image_file)\n-                valid_images.append(True)\n-            except Exception:\n-                valid_images.append(False)\n-        return valid_images\n-\n-    if training_args.do_train:\n-        if \"train\" not in dataset:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = dataset[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-\n-        train_dataset = train_dataset.filter(\n-            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n-        )\n-        train_dataset = train_dataset.map(\n-            function=tokenize_captions,\n-            batched=True,\n-            remove_columns=[col for col in column_names if col != image_column],\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on train dataset\",\n-        )\n-\n-        tf_train_dataset = load_as_tf_dataset(\n-            dataset=train_dataset,\n-            batch_size=training_args.per_device_train_batch_size,\n-            image_column=image_column,\n-            image_size=config.vision_config.image_size,\n-            mean=image_processor.image_mean,\n-            std=image_processor.image_std,\n-            shuffle=True,\n-        )\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in dataset:\n-            raise ValueError(\"--do_eval requires a train validation\")\n-        eval_dataset = dataset[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-\n-        eval_dataset = eval_dataset.filter(\n-            filter_corrupt_images, batched=True, num_proc=data_args.preprocessing_num_workers\n-        )\n-        eval_dataset = eval_dataset.map(\n-            function=tokenize_captions,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=[col for col in column_names if col != image_column],\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on validation dataset\",\n-        )\n-\n-        tf_eval_dataset = load_as_tf_dataset(\n-            dataset=eval_dataset,\n-            batch_size=training_args.per_device_eval_batch_size,\n-            image_column=image_column,\n-            image_size=config.vision_config.image_size,\n-            mean=image_processor.image_mean,\n-            std=image_processor.image_std,\n-            shuffle=False,\n-        )\n-\n-    # 8. Preparing push_to_hub and model card\n-    push_to_hub_model_id = training_args.push_to_hub_model_id\n-    if model_args.model_name_or_path is not None:\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-    else:\n-        vision_name = model_args.vision_model_name_or_path.split(\"/\")[-1]\n-        text_name = model_args.text_model_name_or_path.split(\"/\")[-1]\n-        model_name = f\"{vision_name}-{text_name}\"\n-    if not push_to_hub_model_id:\n-        if data_args.dataset_name is not None:\n-            push_to_hub_model_id = f\"{model_name}-finetuned-{data_args.dataset_name}\"\n-        else:\n-            push_to_hub_model_id = f\"{model_name}-finetuned-contrastive-image-text-modeling\"\n-\n-    model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"contrastive-image-text-modeling\"}\n-    if data_args.dataset_name is not None:\n-        model_card_kwargs[\"dataset_tags\"] = data_args.dataset_name\n-        if data_args.dataset_config_name is not None:\n-            model_card_kwargs[\"dataset_args\"] = data_args.dataset_config_name\n-            model_card_kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n-        else:\n-            model_card_kwargs[\"dataset\"] = data_args.dataset_name\n-\n-    if training_args.push_to_hub:\n-        callbacks = [\n-            PushToHubCallback(\n-                output_dir=training_args.output_dir,\n-                hub_model_id=push_to_hub_model_id,\n-                hub_token=training_args.push_to_hub_token,\n-                tokenizer=tokenizer,\n-                **model_card_kwargs,\n-            )\n-        ]\n-    else:\n-        callbacks = []\n-\n-    # # 9. Training\n-    if training_args.do_train:\n-        num_train_steps = int(len(tf_train_dataset) * int(training_args.num_train_epochs))\n-        if training_args.warmup_steps > 0:\n-            num_warmup_steps = training_args.warmup_steps\n-        elif training_args.warmup_ratio > 0:\n-            num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-        else:\n-            num_warmup_steps = 0\n-        optimizer, lr_schedule = create_optimizer(\n-            init_lr=training_args.learning_rate,\n-            num_train_steps=num_train_steps,\n-            num_warmup_steps=num_warmup_steps,\n-            adam_beta1=training_args.adam_beta1,\n-            adam_beta2=training_args.adam_beta2,\n-            adam_epsilon=training_args.adam_epsilon,\n-            weight_decay_rate=training_args.weight_decay,\n-            adam_global_clipnorm=training_args.max_grad_norm,\n-        )\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, jit_compile=training_args.xla)\n-\n-        if not training_args.do_eval:\n-            tf_eval_dataset = None\n-        model.fit(\n-            tf_train_dataset,\n-            validation_data=tf_eval_dataset,\n-            epochs=int(training_args.num_train_epochs),\n-            callbacks=callbacks,\n-        )\n-\n-    # # 10. Evaluation\n-\n-    if training_args.do_eval and not training_args.do_train:\n-        model.evaluate(tf_eval_dataset)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "e779a29c1b9799ac16a32732c0440a6e23a6f008",
            "filename": "examples/tensorflow/image-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 162,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fimage-classification%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,162 +0,0 @@\n-<!---\n-Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Image classification examples\n-\n-This directory contains 2 scripts that showcase how to fine-tune any model supported by the [`TFAutoModelForImageClassification` API](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForImageClassification) (such as [ViT](https://huggingface.co/docs/transformers/main/en/model_doc/vit), [ConvNeXT](https://huggingface.co/docs/transformers/main/en/model_doc/convnext), [ResNet](https://huggingface.co/docs/transformers/main/en/model_doc/resnet), [Swin Transformer](https://huggingface.co/docs/transformers/main/en/model_doc/swin)...) using TensorFlow. They can be used to fine-tune models on both [datasets from the hub](#using-datasets-from-hub) as well as on [your own custom data](#using-your-own-data).\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/image_classification_inference_widget.png\" height=\"400\" />\n-\n-Try out the inference widget here: https://huggingface.co/google/vit-base-patch16-224\n-\n-## TensorFlow\n-\n-Based on the script [`run_image_classification.py`](https://github.com/huggingface/transformers/blob/main/examples/tensorflow/image-classification/run_image_classification.py).\n-\n-### Using datasets from Hub\n-\n-Here we show how to fine-tune a Vision Transformer (`ViT`) on the [beans](https://huggingface.co/datasets/beans) dataset, to classify the disease type of bean leaves. The following will train a model and push it to the `amyeroberts/vit-base-beans` repo.\n-\n-```bash\n-python run_image_classification.py \\\n-    --dataset_name beans \\\n-    --output_dir ./beans_outputs/ \\\n-    --remove_unused_columns False \\\n-    --do_train \\\n-    --do_eval \\\n-    --push_to_hub \\\n-    --hub_model_id amyeroberts/vit-base-beans \\\n-    --learning_rate 2e-5 \\\n-    --num_train_epochs 5 \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 8 \\\n-    --logging_strategy steps \\\n-    --logging_steps 10 \\\n-    --eval_strategy epoch \\\n-    --save_strategy epoch \\\n-    --load_best_model_at_end True \\\n-    --save_total_limit 3 \\\n-    --seed 1337\n-```\n-\n-ðŸ‘€ See the results here: [amyeroberts/vit-base-beans](https://huggingface.co/amyeroberts/vit-base-beans).\n-\n-Note that you can replace the model and dataset by simply setting the `model_name_or_path` and `dataset_name` arguments respectively, with any model or dataset from the [hub](https://huggingface.co/). For an overview of all possible arguments, we refer to the [docs](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) of the `TrainingArguments`, which can be passed as flags.\n-\n-> If your model classification head dimensions do not fit the number of labels in the dataset, you can specify `--ignore_mismatched_sizes` to adapt it.\n-\n-### Using your own data\n-\n-To use your own dataset, there are 2 ways:\n-- you can either provide your own folders as `--train_dir` and/or `--validation_dir` arguments\n-- you can upload your dataset to the hub (possibly as a private repo, if you prefer so), and simply pass the `--dataset_name` argument.\n-\n-Below, we explain both in more detail.\n-\n-#### Provide them as folders\n-\n-If you provide your own folders with images, the script expects the following directory structure:\n-\n-```bash\n-root/dog/xxx.png\n-root/dog/xxy.png\n-root/dog/[...]/xxz.png\n-\n-root/cat/123.png\n-root/cat/nsdf3.png\n-root/cat/[...]/asd932_.png\n-```\n-\n-In other words, you need to organize your images in subfolders, based on their class. You can then run the script like this:\n-\n-```bash\n-python run_image_classification.py \\\n-    --train_dir <path-to-train-root> \\\n-    --output_dir ./outputs/ \\\n-    --remove_unused_columns False \\\n-    --do_train \\\n-    --do_eval\n-```\n-\n-Internally, the script will use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) feature which will automatically turn the folders into ðŸ¤— Dataset objects.\n-\n-##### ðŸ’¡ The above will split the train dir into training and evaluation sets\n-  - To control the split amount, use the `--train_val_split` flag.\n-  - To provide your own validation split in its own directory, you can pass the `--validation_dir <path-to-val-root>` flag.\n-\n-#### Upload your data to the hub, as a (possibly private) repo\n-\n-To upload your image dataset to the hub you can use the [`ImageFolder`](https://huggingface.co/docs/datasets/v2.0.0/en/image_process#imagefolder) feature available in ðŸ¤— Datasets. Simply do the following:\n-\n-```python\n-from datasets import load_dataset\n-\n-# example 1: local folder\n-dataset = load_dataset(\"imagefolder\", data_dir=\"path_to_your_folder\")\n-\n-# example 2: local files (supported formats are tar, gzip, zip, xz, rar, zstd)\n-dataset = load_dataset(\"imagefolder\", data_files=\"path_to_zip_file\")\n-\n-# example 3: remote files (supported formats are tar, gzip, zip, xz, rar, zstd)\n-dataset = load_dataset(\"imagefolder\", data_files=\"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\")\n-\n-# example 4: providing several splits\n-dataset = load_dataset(\"imagefolder\", data_files={\"train\": [\"path/to/file1\", \"path/to/file2\"], \"test\": [\"path/to/file3\", \"path/to/file4\"]})\n-```\n-\n-`ImageFolder` will create a `label` column, and the label name is based on the directory name.\n-\n-Next, push it to the hub!\n-\n-```python\n-# assuming you have ran the hf auth login command in a terminal\n-dataset.push_to_hub(\"name_of_your_dataset\")\n-\n-# if you want to push to a private repo, simply pass private=True:\n-dataset.push_to_hub(\"name_of_your_dataset\", private=True)\n-```\n-\n-and that's it! You can now train your model by simply setting the `--dataset_name` argument to the name of your dataset on the hub (as explained in [Using datasets from the ðŸ¤— hub](#using-datasets-from-hub)).\n-\n-More on this can also be found in [this blog post](https://huggingface.co/blog/image-search-datasets).\n-\n-### Sharing your model on ðŸ¤— Hub\n-\n-0. If you haven't already, [sign up](https://huggingface.co/join) for a ðŸ¤— account\n-\n-1. Make sure you have `git-lfs` installed and git set up.\n-\n-```bash\n-$ apt install git-lfs\n-$ git config --global user.email \"you@example.com\"\n-$ git config --global user.name \"Your Name\"\n-```\n-\n-2. Log in with your HuggingFace account credentials using `hf`:\n-\n-```bash\n-$ hf auth login\n-# ...follow the prompts\n-```\n-\n-3. When running the script, pass the following arguments:\n-\n-```bash\n-python run_image_classification.py \\\n-    --push_to_hub \\\n-    --push_to_hub_model_id <name-your-model> \\\n-    ...\n-```"
        },
        {
            "sha": "ccdff7ba7884c3a43e65f000a57409ea63dfaacb",
            "filename": "examples/tensorflow/image-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fimage-classification%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,3 +0,0 @@\n-datasets>=1.17.0\n-evaluate\n-tensorflow>=2.4"
        },
        {
            "sha": "0cd49ee88f9c99a241504d0c6e47f4014f06be9d",
            "filename": "examples/tensorflow/image-classification/run_image_classification.py",
            "status": "removed",
            "additions": 0,
            "deletions": 576,
            "changes": 576,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,576 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-\"\"\"\n-Fine-tuning a ðŸ¤— Transformers model for image classification.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=image-classification\n-\"\"\"\n-\n-import json\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import evaluate\n-import numpy as np\n-import tensorflow as tf\n-from datasets import load_dataset\n-from PIL import Image\n-\n-import transformers\n-from transformers import (\n-    TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n-    AutoConfig,\n-    AutoImageProcessor,\n-    DefaultDataCollator,\n-    HfArgumentParser,\n-    PushToHubCallback,\n-    TFAutoModelForImageClassification,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.keras_callbacks import KerasMetricCallback\n-from transformers.modeling_tf_utils import keras\n-from transformers.trainer_utils import get_last_checkpoint, is_main_process\n-from transformers.utils import check_min_version, send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/image-classification/requirements.txt\")\n-\n-MODEL_CONFIG_CLASSES = list(TF_MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-def pil_loader(path: str):\n-    with open(path, \"rb\") as f:\n-        im = Image.open(f)\n-        return im.convert(\"RGB\")\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    Using `HfArgumentParser` we can turn this class into argparse arguments to be able to specify\n-    them on the command line.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"Name of a dataset from the hub (could be your own, possibly private dataset hosted on the hub).\"\n-        },\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the training data.\"})\n-    validation_dir: Optional[str] = field(default=None, metadata={\"help\": \"A folder containing the validation data.\"})\n-    train_val_split: Optional[float] = field(\n-        default=0.15, metadata={\"help\": \"Percent to split off of train for validation.\"}\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and (self.train_dir is None and self.validation_dir is None):\n-            raise ValueError(\n-                \"You must specify either a dataset name from the hub or a train and/or validation directory.\"\n-            )\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        default=\"google/vit-base-patch16-224-in21k\",\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"},\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Where do you want to store the pretrained models downloaded from s3\"}\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    image_processor_name: str = field(default=None, metadata={\"help\": \"Name or path of preprocessor config.\"})\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-    ignore_mismatched_sizes: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Will enable to load a pretrained model whose head dimensions are different.\"},\n-    )\n-\n-\n-def center_crop(image, size):\n-    size = (size, size) if isinstance(size, int) else size\n-    orig_height, orig_width, _ = image.shape\n-    crop_height, crop_width = size\n-    top = (orig_height - orig_width) // 2\n-    left = (orig_width - crop_width) // 2\n-    image = tf.image.crop_to_bounding_box(image, top, left, crop_height, crop_width)\n-    return image\n-\n-\n-# Numpy and TensorFlow compatible version of PyTorch RandomResizedCrop. Code adapted from:\n-# https://pytorch.org/vision/main/_modules/torchvision/transforms/transforms.html#RandomResizedCrop\n-def random_crop(image, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n-    height, width, _ = image.shape\n-    area = height * width\n-    log_ratio = np.log(ratio)\n-    for _ in range(10):\n-        target_area = np.random.uniform(*scale) * area\n-        aspect_ratio = np.exp(np.random.uniform(*log_ratio))\n-        w = int(round(np.sqrt(target_area * aspect_ratio)))\n-        h = int(round(np.sqrt(target_area / aspect_ratio)))\n-        if 0 < w <= width and 0 < h <= height:\n-            i = np.random.randint(0, height - h + 1)\n-            j = np.random.randint(0, width - w + 1)\n-            return image[i : i + h, j : j + w, :]\n-\n-    # Fallback to central crop\n-    in_ratio = float(width) / float(height)\n-    w = width if in_ratio < min(ratio) else int(round(height * max(ratio)))\n-    h = height if in_ratio > max(ratio) else int(round(width / min(ratio)))\n-    i = (height - h) // 2\n-    j = (width - w) // 2\n-    return image[i : i + h, j : j + w, :]\n-\n-\n-def random_resized_crop(image, size, scale=(0.08, 1.0), ratio=(3.0 / 4.0, 4.0 / 3.0)):\n-    size = (size, size) if isinstance(size, int) else size\n-    image = random_crop(image, scale, ratio)\n-    image = tf.image.resize(image, size)\n-    return image\n-\n-\n-def main():\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/TensorFlow versions.\n-    send_example_telemetry(\"run_image_classification\", model_args, data_args, framework=\"tensorflow\")\n-\n-    # Checkpoints. Find the checkpoint the use when loading the model.\n-    checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-\n-    # Setup logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    log_level = training_args.get_process_log_level()\n-    logger.setLevel(log_level)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-\n-    # region Dataset and labels\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Initialize our dataset and prepare it for the 'image-classification' task.\n-    if data_args.dataset_name is not None:\n-        dataset = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_dir is not None:\n-            data_files[\"train\"] = os.path.join(data_args.train_dir, \"**\")\n-        if data_args.validation_dir is not None:\n-            data_files[\"validation\"] = os.path.join(data_args.validation_dir, \"**\")\n-        dataset = load_dataset(\n-            \"imagefolder\",\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # Prepare label mappings.\n-    # We'll include these in the model's config to get human readable labels in the Inference API.\n-    labels = dataset[\"train\"].features[\"labels\"].names\n-    label2id, id2label = {}, {}\n-    for i, label in enumerate(labels):\n-        label2id[label] = str(i)\n-        id2label[str(i)] = label\n-\n-    # Load model image processor and configuration\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name or model_args.model_name_or_path,\n-        num_labels=len(labels),\n-        label2id=label2id,\n-        id2label=id2label,\n-        finetuning_task=\"image-classification\",\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    image_processor = AutoImageProcessor.from_pretrained(\n-        model_args.image_processor_name or model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-\n-    # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n-    if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n-        split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n-        dataset[\"train\"] = split[\"train\"]\n-        dataset[\"validation\"] = split[\"test\"]\n-\n-    # Define our data preprocessing function. It takes an image file path as input and returns\n-    # Write a note describing the resizing behaviour.\n-    if \"shortest_edge\" in image_processor.size:\n-        # We instead set the target size as (shortest_edge, shortest_edge) to here to ensure all images are batchable.\n-        image_size = (image_processor.size[\"shortest_edge\"], image_processor.size[\"shortest_edge\"])\n-    else:\n-        image_size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-\n-    def _train_transforms(image):\n-        img_size = image_size\n-        image = keras.utils.img_to_array(image)\n-        image = random_resized_crop(image, size=img_size)\n-        image = tf.image.random_flip_left_right(image)\n-        image /= 255.0\n-        image = (image - image_processor.image_mean) / image_processor.image_std\n-        image = tf.transpose(image, perm=[2, 0, 1])\n-        return image\n-\n-    def _val_transforms(image):\n-        image = keras.utils.img_to_array(image)\n-        image = tf.image.resize(image, size=image_size)\n-        # image = np.array(image) # FIXME - use tf.image function\n-        image = center_crop(image, size=image_size)\n-        image /= 255.0\n-        image = (image - image_processor.image_mean) / image_processor.image_std\n-        image = tf.transpose(image, perm=[2, 0, 1])\n-        return image\n-\n-    def train_transforms(example_batch):\n-        \"\"\"Apply _train_transforms across a batch.\"\"\"\n-        example_batch[\"pixel_values\"] = [\n-            _train_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"]\n-        ]\n-        return example_batch\n-\n-    def val_transforms(example_batch):\n-        \"\"\"Apply _val_transforms across a batch.\"\"\"\n-        example_batch[\"pixel_values\"] = [_val_transforms(pil_img.convert(\"RGB\")) for pil_img in example_batch[\"image\"]]\n-        return example_batch\n-\n-    train_dataset = None\n-    if training_args.do_train:\n-        if \"train\" not in dataset:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = dataset[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            train_dataset = train_dataset.shuffle(seed=training_args.seed).select(range(data_args.max_train_samples))\n-        train_dataset = train_dataset.map(\n-            train_transforms,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    eval_dataset = None\n-    if training_args.do_eval:\n-        if \"validation\" not in dataset:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = dataset[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            eval_dataset = eval_dataset.select(range(data_args.max_eval_samples))\n-        # Set the validation transforms\n-        eval_dataset = eval_dataset.map(\n-            val_transforms,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    predict_dataset = None\n-    if training_args.do_predict:\n-        if \"test\" not in dataset:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        predict_dataset = dataset[\"test\"]\n-        if data_args.max_predict_samples is not None:\n-            predict_dataset = predict_dataset.select(range(data_args.max_predict_samples))\n-        # Set the test transforms\n-        predict_dataset = predict_dataset.map(\n-            val_transforms,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    collate_fn = DefaultDataCollator(return_tensors=\"np\")\n-\n-    # Load the accuracy metric from the datasets package\n-    metric = evaluate.load(\"accuracy\", cache_dir=model_args.cache_dir)\n-\n-    # Define our compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n-    # predictions and label_ids field) and has to return a dictionary string to float.\n-    def compute_metrics(p):\n-        \"\"\"Computes accuracy on a batch of predictions\"\"\"\n-        logits, label_ids = p\n-        predictions = np.argmax(logits, axis=-1)\n-        metrics = metric.compute(predictions=predictions, references=label_ids)\n-        return metrics\n-\n-    with training_args.strategy.scope():\n-        if checkpoint is None:\n-            model_path = model_args.model_name_or_path\n-        else:\n-            model_path = checkpoint\n-\n-        model = TFAutoModelForImageClassification.from_pretrained(\n-            model_path,\n-            config=config,\n-            from_pt=bool(\".bin\" in model_path),\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-            ignore_mismatched_sizes=model_args.ignore_mismatched_sizes,\n-        )\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-        total_train_batch_size = training_args.per_device_train_batch_size * num_replicas\n-        total_eval_batch_size = training_args.per_device_eval_batch_size * num_replicas\n-\n-        dataset_options = tf.data.Options()\n-        dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-\n-        if training_args.do_train:\n-            num_train_steps = int(len(train_dataset) * training_args.num_train_epochs)\n-            if training_args.warmup_steps > 0:\n-                num_warmpup_steps = int(training_args.warmup_steps)\n-            elif training_args.warmup_ratio > 0:\n-                num_warmpup_steps = int(training_args.warmup_ratio * num_train_steps)\n-            else:\n-                num_warmpup_steps = 0\n-\n-            optimizer, _ = create_optimizer(\n-                init_lr=training_args.learning_rate,\n-                num_train_steps=num_train_steps,\n-                num_warmup_steps=num_warmpup_steps,\n-                adam_beta1=training_args.adam_beta1,\n-                adam_beta2=training_args.adam_beta2,\n-                adam_epsilon=training_args.adam_epsilon,\n-                weight_decay_rate=training_args.weight_decay,\n-                adam_global_clipnorm=training_args.max_grad_norm,\n-            )\n-            # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-            # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-            # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-            # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-            # using model.prepare_tf_dataset()\n-            # For more info see the docs:\n-            # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-            # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-            train_dataset = model.prepare_tf_dataset(\n-                train_dataset,\n-                shuffle=True,\n-                batch_size=total_train_batch_size,\n-                collate_fn=collate_fn,\n-            ).with_options(dataset_options)\n-        else:\n-            optimizer = \"sgd\"  # Just write anything because we won't be using it\n-\n-        if training_args.do_eval:\n-            eval_dataset = model.prepare_tf_dataset(\n-                eval_dataset,\n-                shuffle=False,\n-                batch_size=total_eval_batch_size,\n-                collate_fn=collate_fn,\n-            ).with_options(dataset_options)\n-\n-        if training_args.do_predict:\n-            predict_dataset = model.prepare_tf_dataset(\n-                predict_dataset,\n-                shuffle=False,\n-                batch_size=total_eval_batch_size,\n-                collate_fn=collate_fn,\n-            ).with_options(dataset_options)\n-\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, jit_compile=training_args.xla, metrics=[\"accuracy\"])\n-\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        if not push_to_hub_model_id:\n-            model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-            push_to_hub_model_id = f\"{model_name}-finetuned-image-classification\"\n-\n-        model_card_kwargs = {\n-            \"finetuned_from\": model_args.model_name_or_path,\n-            \"tasks\": \"image-classification\",\n-            \"dataset\": data_args.dataset_name,\n-            \"tags\": [\"image-classification\", \"tensorflow\", \"vision\"],\n-        }\n-\n-        callbacks = []\n-        if eval_dataset is not None:\n-            callbacks.append(KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=eval_dataset))\n-        if training_args.push_to_hub:\n-            callbacks.append(\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=image_processor,\n-                    **model_card_kwargs,\n-                )\n-            )\n-\n-        if training_args.do_train:\n-            model.fit(\n-                train_dataset,\n-                validation_data=eval_dataset,\n-                epochs=int(training_args.num_train_epochs),\n-                callbacks=callbacks,\n-            )\n-\n-        if training_args.do_eval:\n-            n_eval_batches = len(eval_dataset)\n-            eval_predictions = model.predict(eval_dataset, steps=n_eval_batches)\n-            eval_labels = dataset[\"validation\"][\"labels\"][: n_eval_batches * total_eval_batch_size]\n-            eval_metrics = compute_metrics((eval_predictions.logits, eval_labels))\n-            logging.info(\"Eval metrics:\")\n-            for metric_name, value in eval_metrics.items():\n-                logging.info(f\"{metric_name}: {value:.3f}\")\n-\n-        if training_args.output_dir is not None:\n-            os.makedirs(training_args.output_dir, exist_ok=True)\n-            with open(os.path.join(training_args.output_dir, \"all_results.json\"), \"w\") as f:\n-                f.write(json.dumps(eval_metrics))\n-\n-        if training_args.do_predict:\n-            n_predict_batches = len(predict_dataset)\n-            test_predictions = model.predict(predict_dataset, steps=n_predict_batches)\n-            test_labels = dataset[\"validation\"][\"labels\"][: n_predict_batches * total_eval_batch_size]\n-            test_metrics = compute_metrics((test_predictions.logits, test_labels))\n-            logging.info(\"Test metrics:\")\n-            for metric_name, value in test_metrics.items():\n-                logging.info(f\"{metric_name}: {value:.3f}\")\n-\n-        if training_args.output_dir is not None and not training_args.push_to_hub:\n-            # If we're not pushing to hub, at least save a local copy when we're done\n-            model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "094c95cd395a0bd83178cb2d365eaa7a696a8401",
            "filename": "examples/tensorflow/language-modeling-tpu/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,110 +0,0 @@\n-# Training a masked language model end-to-end from scratch on TPUs\n-\n-In this example, we're going to demonstrate how to train a TensorFlow model from ðŸ¤— Transformers from scratch. If you're interested in some background theory on training Hugging Face models with TensorFlow on TPU, please check out our \n-[tutorial doc](https://huggingface.co/docs/transformers/main/perf_train_tpu_tf) on this topic!\n-If you're interested in smaller-scale TPU training from a pre-trained checkpoint, you can also check out the  [TPU fine-tuning example](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/tpu_training-tf.ipynb).\n-\n-This example will demonstrate pre-training language models at the 100M-1B parameter scale, similar to BERT or GPT-2. More concretely, we will show how to train a [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta) (base model) from scratch on the [WikiText dataset (v1)](https://huggingface.co/datasets/wikitext).\n-\n-We've tried to ensure that all the practices we show you here are scalable, though - with relatively few changes, the code could be scaled up to much larger models. \n-\n-Google's gargantuan [PaLM model](https://huggingface.co/papers/2204.02311), with\n-over 500B parameters, is a good example of how far you can go with pure TPU training, though gathering the dataset and the budget to train at that scale is not an easy task!\n-\n-### Table of contents \n-\n-- [Setting up a TPU-VM](#setting-up-a-tpu-vm)\n-- [Training a tokenizer](#training-a-tokenizer)\n-- [Preparing the dataset](#preparing-the-dataset)\n-- [Training the model](#training-the-model)\n-- [Inference](#inference)\n-\n-## Setting up a TPU-VM\n-\n-Since this example focuses on using TPUs, the first step is to set up access to TPU hardware. For this example, we chose to use a TPU v3-8 VM. Follow [this guide](https://cloud.google.com/tpu/docs/run-calculation-tensorflow) to quickly create a TPU VM with TensorFlow pre-installed. \n-\n-> ðŸ’¡ **Note**: You don't need a TPU-enabled hardware for tokenizer training and TFRecord shard preparation.\n-\n-## Training a tokenizer\n-\n-To train a language model from scratch, the first step is to tokenize text. In most Hugging Face examples, we begin from a pre-trained model and use its tokenizer. However, in this example, we're going to train a tokenizer from scratch as well. The script for this is `train_unigram.py`. An example command is:\n-\n-```bash \n-python train_unigram.py --batch_size 1000 --vocab_size 25000 --export_to_hub\n-```\n-\n-The script will automatically load the `train` split of the WikiText dataset and train a [Unigram tokenizer](https://huggingface.co/course/chapter6/7?fw=pt) on it.\n-\n-> ðŸ’¡ **Note**: In order for `export_to_hub` to work, you must authenticate yourself with the `hf`. Run `hf auth login` and follow the on-screen instructions.\n-\n-## Preparing the dataset\n-\n-The next step is to prepare the dataset. This consists of loading a text dataset from the Hugging Face Hub, tokenizing it and grouping it into chunks of a fixed length ready for training. The script for this is `prepare_tfrecord_shards.py`.\n-\n-The reason we create TFRecord output files from this step is that these files work well with [`tf.data` pipelines](https://www.tensorflow.org/guide/data_performance). This makes them very suitable for scalable TPU training - the dataset can easily be sharded and read in parallel just by tweaking a few parameters in the pipeline. An example command is:\n-\n-```bash\n-python prepare_tfrecord_shards.py \\\n-  --tokenizer_name_or_path tf-tpu/unigram-tokenizer-wikitext \\\n-  --shard_size 5000  \\\n-  --split test \n-  --max_length 128 \\\n-  --output_dir gs://tf-tpu-training-resources\n-```\n-\n-**Notes**:\n-\n-* While running the above script, you need to specify the `split` accordingly. The example command above will only filter the `test` split of the dataset. \n-* If you append `gs://` in your `output_dir` the TFRecord shards will be directly serialized to a Google Cloud Storage (GCS) bucket. Ensure that you have already [created the GCS bucket](https://cloud.google.com/storage/docs). \n-* If you're using a TPU node, you must stream data from a GCS bucket. Otherwise, if you're using a TPU VM,you can store the data locally. You may need to [attach](https://cloud.google.com/tpu/docs/setup-persistent-disk) a persistent storage to the VM. \n-* Additional CLI arguments are also supported. We encourage you to run `python prepare_tfrecord_shards.py -h` to know more about them.\n-\n-## Training the model\n-\n-Once that's done, the model is ready for training. By default, training takes place on TPU, but you can use the `--no_tpu` flag to train on CPU for testing purposes. An example command is:\n-\n-```bash\n-python3 run_mlm.py \\\n-  --train_dataset gs://tf-tpu-training-resources/train/ \\\n-  --eval_dataset gs://tf-tpu-training-resources/validation/ \\\n-  --tokenizer tf-tpu/unigram-tokenizer-wikitext \\\n-  --output_dir trained_model  \n-```\n-\n-If you had specified a `hub_model_id` while launching training, then your model will be pushed to a model repository on the Hugging Face Hub. You can find such an example repository here:\n-[tf-tpu/roberta-base-epochs-500-no-wd](https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd).\n-\n-## Inference\n-\n-Once the model is trained, you can use ðŸ¤— Pipelines to perform inference:\n-\n-```python\n-from transformers import pipeline\n-\n-model_id = \"tf-tpu/roberta-base-epochs-500-no-wd\"\n-unmasker = pipeline(\"fill-mask\", model=model_id, framework=\"tf\")\n-unmasker(\"Goal of my life is to [MASK].\")\n-\n-[{'score': 0.1003185287117958,\n-  'token': 52,\n-  'token_str': 'be',\n-  'sequence': 'Goal of my life is to be.'},\n- {'score': 0.032648514956235886,\n-  'token': 5,\n-  'token_str': '',\n-  'sequence': 'Goal of my life is to .'},\n- {'score': 0.02152673341333866,\n-  'token': 138,\n-  'token_str': 'work',\n-  'sequence': 'Goal of my life is to work.'},\n- {'score': 0.019547373056411743,\n-  'token': 984,\n-  'token_str': 'act',\n-  'sequence': 'Goal of my life is to act.'},\n- {'score': 0.01939118467271328,\n-  'token': 73,\n-  'token_str': 'have',\n-  'sequence': 'Goal of my life is to have.'}]\n-```\n-\n-You can also try out inference using the [Inference Widget](https://huggingface.co/tf-tpu/roberta-base-epochs-500-no-wd?text=Goal+of+my+life+is+to+%5BMASK%5D.) from the model page.\n\\ No newline at end of file"
        },
        {
            "sha": "a839c79a5c9bf0ed7984092489040373f0994f57",
            "filename": "examples/tensorflow/language-modeling-tpu/prepare_tfrecord_shards.py",
            "status": "removed",
            "additions": 0,
            "deletions": 191,
            "changes": 191,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,191 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"Script for preparing TFRecord shards for pre-tokenized examples.\"\"\"\n-\n-import argparse\n-import logging\n-import os\n-\n-import datasets\n-import tensorflow as tf\n-\n-from transformers import AutoTokenizer\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def parse_args():\n-    parser = argparse.ArgumentParser(\n-        description=\"Prepare TFRecord shards from pre-tokenized samples of the wikitext dataset.\"\n-    )\n-    parser.add_argument(\n-        \"--dataset_name\",\n-        type=str,\n-        default=\"wikitext\",\n-        help=\"Name of the training. Explore datasets at: hf.co/datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--dataset_config\", type=str, default=\"wikitext-103-raw-v1\", help=\"Configuration name of the dataset.\"\n-    )\n-    parser.add_argument(\n-        \"--trust_remote_code\",\n-        action=\"store_true\",\n-        help=(\n-            \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-            \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-            \" code, as it will execute code present on the Hub on your local machine.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--tokenizer_name_or_path\",\n-        type=str,\n-        default=\"sayakpaul/unigram-tokenizer-wikitext\",\n-        help=\"Tokenizer identifier. Can be a local filepath or a Hub identifier.\",\n-    )\n-    parser.add_argument(\n-        \"--shard_size\",\n-        type=int,\n-        default=1000,\n-        help=\"Number of entries to go in a single shard.\",\n-    )\n-    parser.add_argument(\"--split\", type=str, default=\"train\", choices=[\"train\", \"test\", \"validation\"])\n-    parser.add_argument(\n-        \"--limit\",\n-        default=None,\n-        type=int,\n-        help=\"Limit the number of shards (used for debugging).\",\n-    )\n-    parser.add_argument(\n-        \"--max_length\",\n-        type=int,\n-        default=512,\n-        help=\"Maximum sequence length. For training on TPUs, it helps to have a maximum\"\n-        \" sequence length that is a multiple of 8.\",\n-    )\n-    parser.add_argument(\n-        \"--output_dir\",\n-        default=\"tf-tpu\",\n-        type=str,\n-        help=\"Output directory where the TFRecord shards will be saved. If the\"\n-        \" path is appended with `gs://` ('gs://tf-tpu', for example) then the TFRecord\"\n-        \" shards will be directly saved to a Google Cloud Storage bucket.\",\n-    )\n-\n-    args = parser.parse_args()\n-    return args\n-\n-\n-def tokenize_function(tokenizer):\n-    def fn(examples):\n-        return tokenizer(examples[\"text\"])\n-\n-    return fn\n-\n-\n-def get_serialized_examples(tokenized_data):\n-    records = []\n-    for i in range(len(tokenized_data[\"input_ids\"])):\n-        features = {\n-            \"input_ids\": tf.train.Feature(int64_list=tf.train.Int64List(value=tokenized_data[\"input_ids\"][i])),\n-            \"attention_mask\": tf.train.Feature(\n-                int64_list=tf.train.Int64List(value=tokenized_data[\"attention_mask\"][i])\n-            ),\n-        }\n-        features = tf.train.Features(feature=features)\n-        example = tf.train.Example(features=features)\n-        record_bytes = example.SerializeToString()\n-        records.append(record_bytes)\n-    return records\n-\n-\n-def main(args):\n-    dataset = datasets.load_dataset(\n-        args.dataset_name, args.dataset_config, split=args.split, trust_remote_code=args.trust_remote_code\n-    )\n-\n-    if args.limit is not None:\n-        max_samples = min(len(dataset), args.limit)\n-        dataset = dataset.select(range(max_samples))\n-        print(f\"Limiting the dataset to {args.limit} entries.\")\n-\n-    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name_or_path)\n-\n-    # Handle output directory creation.\n-    # For serializing into a Google Cloud Storage Bucket, one needs to first\n-    # create a bucket.\n-    if \"gs\" not in args.output_dir:\n-        if not os.path.exists(args.output_dir):\n-            os.makedirs(args.output_dir)\n-        split_dir = os.path.join(args.output_dir, args.split)\n-        if not os.path.exists(split_dir):\n-            os.makedirs(split_dir)\n-    else:\n-        split_dir = os.path.join(args.output_dir, args.split)\n-\n-    # Tokenize the whole dataset at once.\n-    tokenize_fn = tokenize_function(tokenizer)\n-    dataset_tokenized = dataset.map(tokenize_fn, batched=True, num_proc=4, remove_columns=[\"text\"])\n-\n-    # We need to concatenate all our texts together, and then split the result\n-    # into chunks of a fixed size, which we will call block_size. To do this, we\n-    # will use the map method again, with the option batched=True. When we use batched=True,\n-    # the function we pass to map() will be passed multiple inputs at once, allowing us\n-    # to group them into more or fewer examples than we had in the input.\n-    # This allows us to create our new fixed-length samples. The advantage of this\n-    # method is that we don't lose a whole lot of content from the dataset compared to the\n-    # case where we simply tokenize with a pre-defined max_length.\n-\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: sum(examples[k], []) for k in examples}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, though you could add padding instead if the model supports it\n-        # In this, as in all things, we advise you to follow your heart ðŸ«€\n-        total_length = (total_length // args.max_length) * args.max_length\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + args.max_length] for i in range(0, total_length, args.max_length)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        return result\n-\n-    grouped_dataset = dataset_tokenized.map(group_texts, batched=True, batch_size=1000, num_proc=4)\n-\n-    shard_count = 0\n-    total_records = 0\n-    for shard in range(0, len(grouped_dataset), args.shard_size):\n-        dataset_snapshot = grouped_dataset[shard : shard + args.shard_size]\n-        records_containing = len(dataset_snapshot[\"input_ids\"])\n-        filename = os.path.join(split_dir, f\"dataset-{shard_count}-{records_containing}.tfrecord\")\n-        serialized_examples = get_serialized_examples(dataset_snapshot)\n-\n-        with tf.io.TFRecordWriter(filename) as out_file:\n-            for i in range(len(serialized_examples)):\n-                example = serialized_examples[i]\n-                out_file.write(example)\n-            print(f\"Wrote file {filename} containing {records_containing} records\")\n-\n-        shard_count += 1\n-        total_records += records_containing\n-\n-    with open(f\"split-{args.split}-records-count.txt\", \"w\") as f:\n-        print(f\"Total {args.split} records: {total_records}\", file=f)\n-\n-\n-if __name__ == \"__main__\":\n-    args = parse_args()\n-    main(args)"
        },
        {
            "sha": "1996f78468c9a562ca516cf15797794feb1d6507",
            "filename": "examples/tensorflow/language-modeling-tpu/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,3 +0,0 @@\n-transformers==4.53.0\n-datasets==2.9.0\n-tokenizers==0.13.2"
        },
        {
            "sha": "7b4155f26ed1c397e63aba0952c14bf2225ee5e4",
            "filename": "examples/tensorflow/language-modeling-tpu/run_mlm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 322,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Frun_mlm.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,322 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"Script for training a masked language model on TPU.\"\"\"\n-\n-import argparse\n-import logging\n-import os\n-import re\n-\n-import tensorflow as tf\n-from packaging.version import parse\n-\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    DataCollatorForLanguageModeling,\n-    PushToHubCallback,\n-    TFAutoModelForMaskedLM,\n-    create_optimizer,\n-)\n-\n-\n-try:\n-    import tf_keras as keras\n-except (ModuleNotFoundError, ImportError):\n-    import keras\n-\n-    if parse(keras.__version__).major > 2:\n-        raise ValueError(\n-            \"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\n-            \"Transformers. Please install the backwards-compatible tf-keras package with \"\n-            \"`pip install tf-keras`.\"\n-        )\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-AUTO = tf.data.AUTOTUNE\n-\n-\n-def parse_args():\n-    parser = argparse.ArgumentParser(description=\"Train a masked language model on TPU.\")\n-    parser.add_argument(\n-        \"--pretrained_model_config\",\n-        type=str,\n-        default=\"FacebookAI/roberta-base\",\n-        help=\"The model config to use. Note that we don't copy the model's weights, only the config!\",\n-    )\n-    parser.add_argument(\n-        \"--tokenizer\",\n-        type=str,\n-        default=\"unigram-tokenizer-wikitext\",\n-        help=\"The name of the tokenizer to load. We use the pretrained tokenizer to initialize the model's vocab size.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--per_replica_batch_size\",\n-        type=int,\n-        default=8,\n-        help=\"Batch size per TPU core.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--no_tpu\",\n-        action=\"store_true\",\n-        help=\"If set, run on CPU and don't try to initialize a TPU. Useful for debugging on non-TPU instances.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--tpu_name\",\n-        type=str,\n-        help=\"Name of TPU resource to initialize. Should be blank on Colab, and 'local' on TPU VMs.\",\n-        default=\"local\",\n-    )\n-\n-    parser.add_argument(\n-        \"--tpu_zone\",\n-        type=str,\n-        help=\"Google cloud zone that TPU resource is located in. Only used for non-Colab TPU nodes.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--gcp_project\", type=str, help=\"Google cloud project name. Only used for non-Colab TPU nodes.\"\n-    )\n-\n-    parser.add_argument(\n-        \"--bfloat16\",\n-        action=\"store_true\",\n-        help=\"Use mixed-precision bfloat16 for training. This is the recommended lower-precision format for TPU.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--train_dataset\",\n-        type=str,\n-        help=\"Path to training dataset to load. If the path begins with `gs://`\"\n-        \" then the dataset will be loaded from a Google Cloud Storage bucket.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--shuffle_buffer_size\",\n-        type=int,\n-        default=2**18,  # Default corresponds to a 1GB buffer for seq_len 512\n-        help=\"Size of the shuffle buffer (in samples)\",\n-    )\n-\n-    parser.add_argument(\n-        \"--eval_dataset\",\n-        type=str,\n-        help=\"Path to evaluation dataset to load. If the path begins with `gs://`\"\n-        \" then the dataset will be loaded from a Google Cloud Storage bucket.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--num_epochs\",\n-        type=int,\n-        default=1,\n-        help=\"Number of epochs to train for.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--learning_rate\",\n-        type=float,\n-        default=1e-4,\n-        help=\"Learning rate to use for training.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--weight_decay_rate\",\n-        type=float,\n-        default=1e-3,\n-        help=\"Weight decay rate to use for training.\",\n-    )\n-\n-    parser.add_argument(\n-        \"--max_length\",\n-        type=int,\n-        default=512,\n-        help=\"Maximum length of tokenized sequences. Should match the setting used in prepare_tfrecord_shards.py\",\n-    )\n-\n-    parser.add_argument(\n-        \"--mlm_probability\",\n-        type=float,\n-        default=0.15,\n-        help=\"Fraction of tokens to mask during training.\",\n-    )\n-\n-    parser.add_argument(\"--output_dir\", type=str, required=True, help=\"Path to save model checkpoints to.\")\n-    parser.add_argument(\"--hub_model_id\", type=str, help=\"Model ID to upload to on the Hugging Face Hub.\")\n-\n-    args = parser.parse_args()\n-    return args\n-\n-\n-def initialize_tpu(args):\n-    try:\n-        if args.tpu_name:\n-            tpu = tf.distribute.cluster_resolver.TPUClusterResolver(\n-                args.tpu_name, zone=args.tpu_zone, project=args.gcp_project\n-            )\n-        else:\n-            tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n-    except ValueError:\n-        raise RuntimeError(\n-            \"Couldn't connect to TPU! Most likely you need to specify --tpu_name, --tpu_zone, or \"\n-            \"--gcp_project. When running on a TPU VM, use --tpu_name local.\"\n-        )\n-\n-    tf.config.experimental_connect_to_cluster(tpu)\n-    tf.tpu.experimental.initialize_tpu_system(tpu)\n-\n-    return tpu\n-\n-\n-def count_samples(file_list):\n-    num_samples = 0\n-    for file in file_list:\n-        filename = file.split(\"/\")[-1]\n-        sample_count = re.search(r\"-\\d+-(\\d+)\\.tfrecord\", filename).group(1)\n-        sample_count = int(sample_count)\n-        num_samples += sample_count\n-\n-    return num_samples\n-\n-\n-def prepare_dataset(records, decode_fn, mask_fn, batch_size, shuffle, shuffle_buffer_size=None):\n-    num_samples = count_samples(records)\n-    dataset = tf.data.Dataset.from_tensor_slices(records)\n-    if shuffle:\n-        dataset = dataset.shuffle(len(dataset))\n-    dataset = tf.data.TFRecordDataset(dataset, num_parallel_reads=AUTO)\n-    # TF can't infer the total sample count because it doesn't read all the records yet, so we assert it here\n-    dataset = dataset.apply(tf.data.experimental.assert_cardinality(num_samples))\n-    dataset = dataset.map(decode_fn, num_parallel_calls=AUTO)\n-    if shuffle:\n-        assert shuffle_buffer_size is not None\n-        dataset = dataset.shuffle(args.shuffle_buffer_size)\n-    dataset = dataset.batch(batch_size, drop_remainder=True)\n-    dataset = dataset.map(mask_fn, num_parallel_calls=AUTO)\n-    dataset = dataset.prefetch(AUTO)\n-    return dataset\n-\n-\n-def main(args):\n-    if not args.no_tpu:\n-        tpu = initialize_tpu(args)\n-        strategy = tf.distribute.TPUStrategy(tpu)\n-    else:\n-        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n-\n-    if args.bfloat16:\n-        keras.mixed_precision.set_global_policy(\"mixed_bfloat16\")\n-\n-    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer)\n-    config = AutoConfig.from_pretrained(args.pretrained_model_config)\n-    config.vocab_size = tokenizer.vocab_size\n-\n-    training_records = tf.io.gfile.glob(os.path.join(args.train_dataset, \"*.tfrecord\"))\n-    if not training_records:\n-        raise ValueError(f\"No .tfrecord files found in {args.train_dataset}.\")\n-    eval_records = tf.io.gfile.glob(os.path.join(args.eval_dataset, \"*.tfrecord\"))\n-    if not eval_records:\n-        raise ValueError(f\"No .tfrecord files found in {args.eval_dataset}.\")\n-\n-    num_train_samples = count_samples(training_records)\n-\n-    steps_per_epoch = num_train_samples // (args.per_replica_batch_size * strategy.num_replicas_in_sync)\n-    total_train_steps = steps_per_epoch * args.num_epochs\n-\n-    with strategy.scope():\n-        model = TFAutoModelForMaskedLM.from_config(config)\n-        model(model.dummy_inputs)  # Pass some dummy inputs through the model to ensure all the weights are built\n-        optimizer, schedule = create_optimizer(\n-            num_train_steps=total_train_steps,\n-            num_warmup_steps=total_train_steps // 20,\n-            init_lr=args.learning_rate,\n-            weight_decay_rate=args.weight_decay_rate,\n-        )\n-\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, metrics=[\"accuracy\"])\n-\n-    def decode_fn(example):\n-        features = {\n-            \"input_ids\": tf.io.FixedLenFeature(dtype=tf.int64, shape=(args.max_length,)),\n-            \"attention_mask\": tf.io.FixedLenFeature(dtype=tf.int64, shape=(args.max_length,)),\n-        }\n-        return tf.io.parse_single_example(example, features)\n-\n-    # Many of the data collators in Transformers are TF-compilable when return_tensors == \"tf\", so we can\n-    # use their methods in our data pipeline.\n-    data_collator = DataCollatorForLanguageModeling(\n-        tokenizer=tokenizer, mlm_probability=args.mlm_probability, mlm=True, return_tensors=\"tf\"\n-    )\n-\n-    def mask_with_collator(batch):\n-        # TF really needs an isin() function\n-        special_tokens_mask = (\n-            ~tf.cast(batch[\"attention_mask\"], tf.bool)\n-            | (batch[\"input_ids\"] == tokenizer.cls_token_id)\n-            | (batch[\"input_ids\"] == tokenizer.sep_token_id)\n-        )\n-        batch[\"input_ids\"], batch[\"labels\"] = data_collator.tf_mask_tokens(\n-            batch[\"input_ids\"],\n-            vocab_size=len(tokenizer),\n-            mask_token_id=tokenizer.mask_token_id,\n-            special_tokens_mask=special_tokens_mask,\n-        )\n-        return batch\n-\n-    batch_size = args.per_replica_batch_size * strategy.num_replicas_in_sync\n-\n-    train_dataset = prepare_dataset(\n-        training_records,\n-        decode_fn=decode_fn,\n-        mask_fn=mask_with_collator,\n-        batch_size=batch_size,\n-        shuffle=True,\n-        shuffle_buffer_size=args.shuffle_buffer_size,\n-    )\n-\n-    eval_dataset = prepare_dataset(\n-        eval_records,\n-        decode_fn=decode_fn,\n-        mask_fn=mask_with_collator,\n-        batch_size=batch_size,\n-        shuffle=False,\n-    )\n-\n-    callbacks = []\n-    if args.hub_model_id:\n-        callbacks.append(\n-            PushToHubCallback(output_dir=args.output_dir, hub_model_id=args.hub_model_id, tokenizer=tokenizer)\n-        )\n-\n-    model.fit(\n-        train_dataset,\n-        validation_data=eval_dataset,\n-        epochs=args.num_epochs,\n-        callbacks=callbacks,\n-    )\n-\n-    model.save_pretrained(args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    args = parse_args()\n-    main(args)"
        },
        {
            "sha": "9eb9c8427b0f6d4d0e68171dc31ee112073a306a",
            "filename": "examples/tensorflow/language-modeling-tpu/train_unigram.py",
            "status": "removed",
            "additions": 0,
            "deletions": 129,
            "changes": 129,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Ftrain_unigram.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,129 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\"\"\"Script for training a Unigram tokenizer.\"\"\"\n-\n-import argparse\n-import logging\n-\n-import datasets\n-from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n-from tokenizers.models import Unigram\n-from tokenizers.trainers import UnigramTrainer\n-\n-from transformers import AlbertTokenizerFast\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def parse_args():\n-    parser = argparse.ArgumentParser(description=\"Train a unigram tokenizer on the wikitext dataset.\")\n-    parser.add_argument(\n-        \"--dataset_name\",\n-        type=str,\n-        default=\"wikitext\",\n-        help=\"Name of the training. Explore datasets at: hf.co/datasets.\",\n-    )\n-    parser.add_argument(\n-        \"--dataset_config\", type=str, default=\"wikitext-103-raw-v1\", help=\"Configuration name of the dataset.\"\n-    )\n-    parser.add_argument(\n-        \"--trust_remote_code\",\n-        action=\"store_true\",\n-        help=(\n-            \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-            \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-            \" code, as it will execute code present on the Hub on your local machine.\"\n-        ),\n-    )\n-    parser.add_argument(\n-        \"--batch_size\",\n-        type=int,\n-        default=1000,\n-        help=\"Batch size during training.\",\n-    )\n-    parser.add_argument(\n-        \"--vocab_size\",\n-        type=int,\n-        default=10048,\n-        help=\"Size of the desired vocabulary.\",\n-    )\n-    parser.add_argument(\n-        \"--limit\",\n-        default=None,\n-        type=int,\n-        help=\"Limit the number of shards (used for debugging).\",\n-    )\n-    parser.add_argument(\n-        \"--export_to_hub\",\n-        action=\"store_true\",\n-    )\n-\n-    args = parser.parse_args()\n-    return args\n-\n-\n-def main(args):\n-    dataset = datasets.load_dataset(\n-        args.dataset_name, args.dataset_config, split=\"train\", trust_remote_code=args.trust_remote_code\n-    )\n-\n-    if args.limit is not None:\n-        max_train_samples = min(len(dataset), args.limit)\n-        dataset = dataset.select(range(max_train_samples))\n-        logger.info(f\"Limiting the dataset to {args.limit} entries.\")\n-\n-    def batch_iterator():\n-        for i in range(0, len(dataset), args.batch_size):\n-            yield dataset[i : i + args.batch_size][\"text\"]\n-\n-    # Prepare the tokenizer.\n-    tokenizer = Tokenizer(Unigram())\n-    tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(\"``\", '\"'), normalizers.Replace(\"''\", '\"')])\n-    tokenizer.pre_tokenizer = pre_tokenizers.Metaspace()\n-\n-    # Prepare the trainer.\n-    trainer = UnigramTrainer(\n-        unk_token=\"<unk>\",\n-        special_tokens=[\"[CLS]\", \"[SEP]\", \"<unk>\", \"<pad>\", \"[MASK]\"],\n-        vocab_size=args.vocab_size,\n-    )\n-\n-    logger.info(\"Training the tokenizer.\")\n-    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)\n-    logger.info(\"Tokenizer training complete!\")\n-\n-    cls_token_id = tokenizer.token_to_id(\"[CLS]\")\n-    sep_token_id = tokenizer.token_to_id(\"[SEP]\")\n-    tokenizer.post_processor = processors.TemplateProcessing(\n-        single=\"[CLS]:0 $A:0 [SEP]:0\",\n-        pair=\"[CLS]:0 $A:0 [SEP]:0 $B:1 [SEP]:1\",\n-        special_tokens=[\n-            (\"[CLS]\", cls_token_id),\n-            (\"[SEP]\", sep_token_id),\n-        ],\n-    )\n-    tokenizer.decoder = decoders.Metaspace()\n-\n-    if args.export_to_hub:\n-        logger.info(\"Exporting the trained tokenizer to Hub.\")\n-        new_tokenizer = AlbertTokenizerFast(tokenizer_object=tokenizer)\n-        new_tokenizer.push_to_hub(\"unigram-tokenizer-dataset\")\n-\n-\n-if __name__ == \"__main__\":\n-    args = parse_args()\n-    main(args)"
        },
        {
            "sha": "ed4f507d4e82ce15ddf0651c7ed51ba333fe209d",
            "filename": "examples/tensorflow/language-modeling/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 80,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,80 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Language modelling examples\n-\n-This folder contains some scripts showing examples of *language model pre-training* with the ðŸ¤— Transformers library.\n-For straightforward use-cases you may be able to use these scripts without modification, although we have also\n-included comments in the code to indicate areas that you may need to adapt to your own projects. The two scripts\n-have almost identical arguments, but they differ in the type of LM they train - a causal language model (like GPT) or a \n-masked language model (like BERT). Masked language models generally train more quickly and perform better when \n-fine-tuned on new tasks with a task-specific output head, like text classification. However, their ability to generate\n-text is weaker than causal language models.\n-\n-## Pre-training versus fine-tuning\n-\n-These scripts can be used to both *pre-train* a language model completely from scratch, as well as to *fine-tune*\n-a language model on text from your domain of interest. To start with an existing pre-trained language model you\n-can use the `--model_name_or_path` argument, or to train from scratch you can use the `--model_type` argument\n-to indicate the class of model architecture to initialize.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, these scripts use a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument.\n-\n-## run_mlm.py\n-\n-This script trains a masked language model.\n-\n-### Example command\n-```bash\n-python run_mlm.py \\\n---model_name_or_path distilbert/distilbert-base-cased \\\n---output_dir output \\\n---dataset_name wikitext \\\n---dataset_config_name wikitext-103-raw-v1\n-```\n-\n-When using a custom dataset, the validation file can be separately passed as an input argument. Otherwise some split (customizable) of training data is used as validation.\n-```bash\n-python run_mlm.py \\\n---model_name_or_path distilbert/distilbert-base-cased \\\n---output_dir output \\\n---train_file train_file_path\n-```\n-\n-## run_clm.py\n-\n-This script trains a causal language model.\n-\n-### Example command\n-```bash\n-python run_clm.py \\\n---model_name_or_path distilbert/distilgpt2 \\\n---output_dir output \\\n---dataset_name wikitext \\\n---dataset_config_name wikitext-103-raw-v1\n-```\n-\n-When using a custom dataset, the validation file can be separately passed as an input argument. Otherwise some split (customizable) of training data is used as validation.\n-\n-```bash\n-python run_clm.py \\\n---model_name_or_path distilbert/distilgpt2 \\\n---output_dir output \\\n---train_file train_file_path\n-```"
        },
        {
            "sha": "c4ae4890d2e2c8591a914a80a986db83ab3cbe03",
            "filename": "examples/tensorflow/language-modeling/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,2 +0,0 @@\n-datasets >= 1.8.0\n-sentencepiece != 0.1.92\n\\ No newline at end of file"
        },
        {
            "sha": "28a955734b71d2934f0096c9a6cbfdf2f507a402",
            "filename": "examples/tensorflow/language-modeling/run_clm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 657,
            "changes": 657,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,657 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for causal language modeling (GPT-2, GPT-Neo...)\n-on a text file or a dataset without using HuggingFace Trainer.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=text-generation\n-\"\"\"\n-# You can also adapt this script on your own clm task. Pointers for this are left as comments.\n-\n-import json\n-\n-# region Imports\n-import logging\n-import math\n-import os\n-import random\n-import sys\n-from dataclasses import dataclass, field\n-from itertools import chain\n-from pathlib import Path\n-from typing import Optional\n-\n-import datasets\n-import tensorflow as tf\n-from datasets import load_dataset\n-from sklearn.model_selection import train_test_split\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    CONFIG_NAME,\n-    TF2_WEIGHTS_NAME,\n-    TF_MODEL_FOR_CAUSAL_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    HfArgumentParser,\n-    PushToHubCallback,\n-    TFAutoModelForCausalLM,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.utils import send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-logger = logging.getLogger(__name__)\n-require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/tensorflow/language-modeling/requirements.txt\")\n-MODEL_CONFIG_CLASSES = list(TF_MODEL_FOR_CAUSAL_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-# endregion\n-\n-\n-# region Command-line arguments\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_overrides: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n-                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n-            )\n-        },\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n-            raise ValueError(\n-                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n-            )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    block_size: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Optional input sequence length after tokenization. \"\n-                \"The training dataset will be truncated in block of this size for training. \"\n-                \"Default to the model max input length for single sentence inputs (take into account special tokens).\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    line_by_line: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    keep_linebreaks: bool = field(\n-        default=True, metadata={\"help\": \"Whether to keep line breaks when using TXT files or not.\"}\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument Parsing\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_clm\", model_args, data_args, framework=\"tensorflow\")\n-\n-    # Sanity checks\n-    if data_args.dataset_name is None and data_args.train_file is None and data_args.validation_file is None:\n-        raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-    else:\n-        if data_args.train_file is not None:\n-            extension = data_args.train_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\n-        if data_args.validation_file is not None:\n-            extension = data_args.validation_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\n-\n-    if training_args.output_dir is not None:\n-        training_args.output_dir = Path(training_args.output_dir)\n-        os.makedirs(training_args.output_dir, exist_ok=True)\n-    # endregion\n-\n-    # region Checkpoints\n-    # Detecting last checkpoint.\n-    checkpoint = None\n-    if len(os.listdir(training_args.output_dir)) > 0 and not training_args.overwrite_output_dir:\n-        config_path = training_args.output_dir / CONFIG_NAME\n-        weights_path = training_args.output_dir / TF2_WEIGHTS_NAME\n-        if config_path.is_file() and weights_path.is_file():\n-            checkpoint = training_args.output_dir\n-            logger.info(\n-                f\"Checkpoint detected, resuming training from checkpoint in {training_args.output_dir}. To avoid this\"\n-                \" behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-        else:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to continue regardless.\"\n-            )\n-\n-    # endregion\n-\n-    # region Setup logging\n-    # accelerator.is_local_main_process is only True for one process per machine.\n-    logger.setLevel(logging.INFO)\n-    datasets.utils.logging.set_verbosity_warning()\n-    transformers.utils.logging.set_verbosity_info()\n-    # endregion\n-\n-    # If passed along, set the training seed now.\n-    if training_args.seed is not None:\n-        set_seed(training_args.seed)\n-\n-    # region Load datasets\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        if \"validation\" not in raw_datasets:\n-            raw_datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-            raw_datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        dataset_args = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-        extension = (\n-            data_args.train_file.split(\".\")[-1]\n-            if data_args.train_file is not None\n-            else data_args.validation_file.split(\".\")[-1]\n-        )\n-        if extension == \"txt\":\n-            extension = \"text\"\n-            dataset_args[\"keep_linebreaks\"] = data_args.keep_linebreaks\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            **dataset_args,\n-        )\n-        # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets:\n-            raw_datasets[\"validation\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                **dataset_args,\n-            )\n-            raw_datasets[\"train\"] = load_dataset(\n-                extension,\n-                data_files=data_files,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                cache_dir=model_args.cache_dir,\n-                token=model_args.token,\n-                **dataset_args,\n-            )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-    # endregion\n-\n-    # region Load pretrained model and tokenizer\n-    #\n-    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    if model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-    # endregion\n-\n-    # region Dataset preprocessing\n-    # First we tokenize all the texts.\n-    column_names = raw_datasets[\"train\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    def tokenize_function(examples):\n-        return tokenizer(examples[text_column_name])\n-\n-    tokenized_datasets = raw_datasets.map(\n-        tokenize_function,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        remove_columns=column_names,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-        desc=\"Running tokenizer on dataset\",\n-    )\n-\n-    if data_args.block_size is None:\n-        block_size = tokenizer.model_max_length\n-        if block_size > config.max_position_embeddings:\n-            logger.warning(\n-                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n-                f\"Using block_size={min(1024, config.max_position_embeddings)} instead. You can change that default value by passing --block_size xxx.\"\n-            )\n-            block_size = min(1024, config.max_position_embeddings)\n-    else:\n-        if data_args.block_size > tokenizer.model_max_length:\n-            logger.warning(\n-                f\"The block_size passed ({data_args.block_size}) is larger than the maximum length for the model \"\n-                f\"({tokenizer.model_max_length}). Using block_size={tokenizer.model_max_length}.\"\n-            )\n-        block_size = min(data_args.block_size, tokenizer.model_max_length)\n-\n-    # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n-    def group_texts(examples):\n-        # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-        total_length = len(concatenated_examples[list(examples.keys())[0]])\n-        # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-        # customize this part to your needs.\n-        if total_length >= block_size:\n-            total_length = (total_length // block_size) * block_size\n-        # Split by chunks of max_len.\n-        result = {\n-            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n-            for k, t in concatenated_examples.items()\n-        }\n-        result[\"labels\"] = result[\"input_ids\"].copy()\n-        return result\n-\n-    # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n-    # for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n-    # to preprocess.\n-    #\n-    # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-    # https://huggingface.co/docs/datasets/process#map\n-\n-    lm_datasets = tokenized_datasets.map(\n-        group_texts,\n-        batched=True,\n-        num_proc=data_args.preprocessing_num_workers,\n-        load_from_cache_file=not data_args.overwrite_cache,\n-        desc=f\"Grouping texts in chunks of {block_size}\",\n-    )\n-\n-    train_dataset = lm_datasets[\"train\"]\n-    if data_args.validation_file is not None:\n-        eval_dataset = lm_datasets[\"validation\"]\n-    else:\n-        logger.info(\n-            f\"Validation file not found: using {data_args.validation_split_percentage}% of the dataset as validation\"\n-            \" as provided in data_args\"\n-        )\n-        train_indices, val_indices = train_test_split(\n-            list(range(len(train_dataset))), test_size=data_args.validation_split_percentage / 100\n-        )\n-\n-        eval_dataset = train_dataset.select(val_indices)\n-        train_dataset = train_dataset.select(train_indices)\n-\n-    if data_args.max_train_samples is not None:\n-        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-        train_dataset = train_dataset.select(range(max_train_samples))\n-    if data_args.max_eval_samples is not None:\n-        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-        eval_dataset = eval_dataset.select(range(max_eval_samples))\n-\n-    # Log a few random samples from the training set:\n-    for index in random.sample(range(len(train_dataset)), min(3, len(train_dataset))):\n-        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        # region Prepare model\n-        if checkpoint is not None:\n-            model = TFAutoModelForCausalLM.from_pretrained(\n-                checkpoint, config=config, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-            )\n-        elif model_args.model_name_or_path:\n-            model = TFAutoModelForCausalLM.from_pretrained(\n-                model_args.model_name_or_path,\n-                config=config,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-        else:\n-            logger.info(\"Training new model from scratch\")\n-            model = TFAutoModelForCausalLM.from_config(\n-                config, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-            )\n-\n-        # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n-        # on a small vocab and want a smaller embedding size, remove this test.\n-        embeddings = model.get_input_embeddings()\n-\n-        # Matt: This is a temporary workaround as we transition our models to exclusively using Keras embeddings.\n-        #       As soon as the transition is complete, all embeddings should be keras.Embeddings layers, and\n-        #       the weights will always be in embeddings.embeddings.\n-        if hasattr(embeddings, \"embeddings\"):\n-            embedding_size = embeddings.embeddings.shape[0]\n-        else:\n-            embedding_size = embeddings.weight.shape[0]\n-        if len(tokenizer) > embedding_size:\n-            model.resize_token_embeddings(len(tokenizer))\n-        # endregion\n-\n-        # region TF Dataset preparation\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-        options = tf.data.Options()\n-        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-\n-        # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-        # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-        # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-        # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-        # using model.prepare_tf_dataset()\n-        # For more info see the docs:\n-        # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-        # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-\n-        tf_train_dataset = model.prepare_tf_dataset(\n-            train_dataset,\n-            shuffle=True,\n-            batch_size=num_replicas * training_args.per_device_train_batch_size,\n-        ).with_options(options)\n-\n-        tf_eval_dataset = model.prepare_tf_dataset(\n-            eval_dataset,\n-            shuffle=False,\n-            batch_size=num_replicas * training_args.per_device_eval_batch_size,\n-            drop_remainder=True,\n-        ).with_options(options)\n-        # endregion\n-\n-        # region Optimizer and loss\n-        num_train_steps = len(tf_train_dataset) * int(training_args.num_train_epochs)\n-        if training_args.warmup_steps > 0:\n-            num_warmup_steps = training_args.warmup_steps\n-        elif training_args.warmup_ratio > 0:\n-            num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-        else:\n-            num_warmup_steps = 0\n-\n-        # Bias and layernorm weights are automatically excluded from the decay\n-        optimizer, lr_schedule = create_optimizer(\n-            init_lr=training_args.learning_rate,\n-            num_train_steps=num_train_steps,\n-            num_warmup_steps=num_warmup_steps,\n-            adam_beta1=training_args.adam_beta1,\n-            adam_beta2=training_args.adam_beta2,\n-            adam_epsilon=training_args.adam_epsilon,\n-            weight_decay_rate=training_args.weight_decay,\n-            adam_global_clipnorm=training_args.max_grad_norm,\n-        )\n-\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, jit_compile=training_args.xla)\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            if data_args.dataset_name is not None:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-{data_args.dataset_name}\"\n-            else:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-clm\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"text-generation\"}\n-        if data_args.dataset_name is not None:\n-            model_card_kwargs[\"dataset_tags\"] = data_args.dataset_name\n-            if data_args.dataset_config_name is not None:\n-                model_card_kwargs[\"dataset_args\"] = data_args.dataset_config_name\n-                model_card_kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n-            else:\n-                model_card_kwargs[\"dataset\"] = data_args.dataset_name\n-\n-        if training_args.push_to_hub:\n-            callbacks = [\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            ]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Training and validation\n-        logger.info(\"***** Running training *****\")\n-        logger.info(f\"  Num examples = {len(train_dataset)}\")\n-        logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n-        logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-        logger.info(f\"  Total train batch size = {training_args.per_device_train_batch_size * num_replicas}\")\n-\n-        # For long training runs, you may wish to use the PushToHub() callback here to save intermediate checkpoints\n-        # to the Hugging Face Hub rather than just pushing the finished model.\n-        # See https://huggingface.co/docs/transformers/main_classes/keras_callbacks#transformers.PushToHubCallback\n-\n-        history = model.fit(\n-            tf_train_dataset,\n-            validation_data=tf_eval_dataset,\n-            epochs=int(training_args.num_train_epochs),\n-            callbacks=callbacks,\n-        )\n-        train_loss = history.history[\"loss\"][-1]\n-        try:\n-            train_perplexity = math.exp(train_loss)\n-        except OverflowError:\n-            train_perplexity = math.inf\n-        logger.info(f\"  Final train loss: {train_loss:.3f}\")\n-        logger.info(f\"  Final train perplexity: {train_perplexity:.3f}\")\n-        validation_loss = history.history[\"val_loss\"][-1]\n-        try:\n-            validation_perplexity = math.exp(validation_loss)\n-        except OverflowError:\n-            validation_perplexity = math.inf\n-        logger.info(f\"  Final validation loss: {validation_loss:.3f}\")\n-        logger.info(f\"  Final validation perplexity: {validation_perplexity:.3f}\")\n-\n-        if training_args.output_dir is not None:\n-            output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-            results_dict = {}\n-            results_dict[\"train_loss\"] = train_loss\n-            results_dict[\"train_perplexity\"] = train_perplexity\n-            results_dict[\"eval_loss\"] = validation_loss\n-            results_dict[\"eval_perplexity\"] = validation_perplexity\n-            with open(output_eval_file, \"w\") as writer:\n-                writer.write(json.dumps(results_dict))\n-        # endregion\n-\n-    if training_args.output_dir is not None and not training_args.push_to_hub:\n-        # If we're not pushing to hub, at least save a local copy when we're done\n-        model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "ef2c43c69eff5403be0d3925ef88121985936206",
            "filename": "examples/tensorflow/language-modeling/run_mlm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 681,
            "changes": 681,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,681 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for masked language modeling (BERT, ALBERT, RoBERTa...)\n-on a text file or a dataset without using HuggingFace Trainer.\n-\n-Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n-https://huggingface.co/models?filter=fill-mask\n-\"\"\"\n-# You can also adapt this script on your own mlm task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import math\n-import os\n-import random\n-import sys\n-from dataclasses import dataclass, field\n-from itertools import chain\n-from pathlib import Path\n-from typing import Optional\n-\n-import datasets\n-import tensorflow as tf\n-from datasets import load_dataset\n-from sklearn.model_selection import train_test_split\n-\n-import transformers\n-from transformers import (\n-    CONFIG_MAPPING,\n-    CONFIG_NAME,\n-    TF2_WEIGHTS_NAME,\n-    TF_MODEL_FOR_MASKED_LM_MAPPING,\n-    AutoConfig,\n-    AutoTokenizer,\n-    DataCollatorForLanguageModeling,\n-    HfArgumentParser,\n-    PushToHubCallback,\n-    TFAutoModelForMaskedLM,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.utils import send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-logger = logging.getLogger(__name__)\n-require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/tensorflow/language-modeling/requirements.txt\")\n-MODEL_CONFIG_CLASSES = list(TF_MODEL_FOR_MASKED_LM_MAPPING.keys())\n-MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)\n-\n-\n-# region Command-line arguments\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune, or train from scratch.\n-    \"\"\"\n-\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The model checkpoint for weights initialization. Don't set if you want to train a model from scratch.\"\n-            )\n-        },\n-    )\n-    model_type: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"If training from scratch, pass a model type from the list: \" + \", \".join(MODEL_TYPES)},\n-    )\n-    config_overrides: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Override some existing default config settings when a model is trained from scratch. Example: \"\n-                \"n_embd=10,resid_pdrop=0.2,scale_attn_weights=false,summary_type=cls_index\"\n-            )\n-        },\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.config_overrides is not None and (self.config_name is not None or self.model_name_or_path is not None):\n-            raise ValueError(\n-                \"--config_overrides can't be used in combination with --config_name or --model_name_or_path\"\n-            )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    validation_split_percentage: Optional[int] = field(\n-        default=5,\n-        metadata={\n-            \"help\": \"The percentage of the train set used as validation set in case there's no validation split\"\n-        },\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated.\"\n-            )\n-        },\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    mlm_probability: float = field(\n-        default=0.15, metadata={\"help\": \"Ratio of tokens to mask for masked language modeling loss\"}\n-    )\n-    line_by_line: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether distinct lines of text in the dataset are to be handled as distinct sequences.\"},\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, a json or a txt file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, a json or a txt file.\"\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument Parsing\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_mlm\", model_args, data_args, framework=\"tensorflow\")\n-\n-    # Sanity checks\n-    if data_args.dataset_name is None and data_args.train_file is None and data_args.validation_file is None:\n-        raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-    else:\n-        if data_args.train_file is not None:\n-            extension = data_args.train_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\", \"txt\"], \"`train_file` should be a csv, json or txt file.\"\n-        if data_args.validation_file is not None:\n-            extension = data_args.validation_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\", \"txt\"], \"`validation_file` should be a csv, json or txt file.\"\n-\n-    if training_args.output_dir is not None:\n-        training_args.output_dir = Path(training_args.output_dir)\n-        os.makedirs(training_args.output_dir, exist_ok=True)\n-\n-    if isinstance(training_args.strategy, tf.distribute.TPUStrategy) and not data_args.pad_to_max_length:\n-        logger.warning(\"We are training on TPU - forcing pad_to_max_length\")\n-        data_args.pad_to_max_length = True\n-    # endregion\n-\n-    # region Checkpoints\n-    # Detecting last checkpoint.\n-    checkpoint = None\n-    if len(os.listdir(training_args.output_dir)) > 0 and not training_args.overwrite_output_dir:\n-        config_path = training_args.output_dir / CONFIG_NAME\n-        weights_path = training_args.output_dir / TF2_WEIGHTS_NAME\n-        if config_path.is_file() and weights_path.is_file():\n-            checkpoint = training_args.output_dir\n-            logger.warning(\n-                f\"Checkpoint detected, resuming training from checkpoint in {training_args.output_dir}. To avoid this\"\n-                \" behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-        else:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to continue regardless.\"\n-            )\n-\n-    # endregion\n-\n-    # region Setup logging\n-    # accelerator.is_local_main_process is only True for one process per machine.\n-    logger.setLevel(logging.INFO)\n-    datasets.utils.logging.set_verbosity_warning()\n-    transformers.utils.logging.set_verbosity_info()\n-    # endregion\n-\n-    # If passed along, set the training seed now.\n-    if training_args.seed is not None:\n-        set_seed(training_args.seed)\n-\n-    # region Load datasets\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        if \"validation\" not in raw_datasets:\n-            raw_datasets[\"validation\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[:{data_args.validation_split_percentage}%]\",\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-            raw_datasets[\"train\"] = load_dataset(\n-                data_args.dataset_name,\n-                data_args.dataset_config_name,\n-                split=f\"train[{data_args.validation_split_percentage}%:]\",\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if extension == \"txt\":\n-            extension = \"text\"\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            token=model_args.token,\n-        )\n-\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-    # endregion\n-\n-    # region Load pretrained model and tokenizer\n-    #\n-    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    if checkpoint is not None:\n-        config = AutoConfig.from_pretrained(\n-            checkpoint, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    elif model_args.config_name:\n-        config = AutoConfig.from_pretrained(\n-            model_args.config_name, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    elif model_args.model_name_or_path:\n-        config = AutoConfig.from_pretrained(\n-            model_args.model_name_or_path, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    else:\n-        config = CONFIG_MAPPING[model_args.model_type]()\n-        logger.warning(\"You are instantiating a new config instance from scratch.\")\n-\n-    if model_args.tokenizer_name:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.tokenizer_name, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    elif model_args.model_name_or_path:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            model_args.model_name_or_path, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-        )\n-    else:\n-        raise ValueError(\n-            \"You are instantiating a new tokenizer from scratch. This is not supported by this script. \"\n-            \"You can do it from another script, save it, and load it from here, using --tokenizer_name.\"\n-        )\n-    # endregion\n-\n-    # region Dataset preprocessing\n-    # First we tokenize all the texts.\n-    column_names = raw_datasets[\"train\"].column_names\n-    text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n-\n-    if data_args.max_seq_length is None:\n-        max_seq_length = tokenizer.model_max_length\n-        if max_seq_length > 1024:\n-            logger.warning(\n-                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n-                \"Picking 1024 instead. You can reduce that default value by passing --max_seq_length xxx.\"\n-            )\n-            max_seq_length = 1024\n-    else:\n-        if data_args.max_seq_length > tokenizer.model_max_length:\n-            logger.warning(\n-                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n-                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n-            )\n-        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    if data_args.line_by_line:\n-        # When using line_by_line, we just tokenize each nonempty line.\n-        padding = \"max_length\" if data_args.pad_to_max_length else False\n-\n-        def tokenize_function(examples):\n-            # Remove empty lines\n-            examples[text_column_name] = [\n-                line for line in examples[text_column_name] if len(line) > 0 and not line.isspace()\n-            ]\n-            return tokenizer(\n-                examples[text_column_name],\n-                padding=padding,\n-                truncation=True,\n-                max_length=max_seq_length,\n-                # We use this option because DataCollatorForLanguageModeling (see below) is more efficient when it\n-                # receives the `special_tokens_mask`.\n-                return_special_tokens_mask=True,\n-            )\n-\n-        tokenized_datasets = raw_datasets.map(\n-            tokenize_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=[text_column_name],\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on dataset line_by_line\",\n-        )\n-    else:\n-        # Otherwise, we tokenize every text, then concatenate them together before splitting them in smaller parts.\n-        # We use `return_special_tokens_mask=True` because DataCollatorForLanguageModeling (see below) is more\n-        # efficient when it receives the `special_tokens_mask`.\n-        def tokenize_function(examples):\n-            return tokenizer(examples[text_column_name], return_special_tokens_mask=True)\n-\n-        tokenized_datasets = raw_datasets.map(\n-            tokenize_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on every text in dataset\",\n-        )\n-\n-        # Main data processing function that will concatenate all texts from our dataset and generate chunks of\n-        # max_seq_length.\n-        def group_texts(examples):\n-            # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n-            total_length = len(concatenated_examples[list(examples.keys())[0]])\n-            # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n-            # customize this part to your needs.\n-            if total_length >= max_seq_length:\n-                total_length = (total_length // max_seq_length) * max_seq_length\n-            # Split by chunks of max_len.\n-            result = {\n-                k: [t[i : i + max_seq_length] for i in range(0, total_length, max_seq_length)]\n-                for k, t in concatenated_examples.items()\n-            }\n-            return result\n-\n-        # Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a\n-        # remainder for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value\n-        # might be slower to preprocess.\n-        #\n-        # To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n-        # https://huggingface.co/docs/datasets/process#map\n-\n-        tokenized_datasets = tokenized_datasets.map(\n-            group_texts,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=f\"Grouping texts in chunks of {max_seq_length}\",\n-        )\n-\n-    train_dataset = tokenized_datasets[\"train\"]\n-\n-    if data_args.validation_file is not None:\n-        eval_dataset = tokenized_datasets[\"validation\"]\n-    else:\n-        logger.info(\n-            f\"Validation file not found: using {data_args.validation_split_percentage}% of the dataset as validation\"\n-            \" as provided in data_args\"\n-        )\n-        train_indices, val_indices = train_test_split(\n-            list(range(len(train_dataset))), test_size=data_args.validation_split_percentage / 100\n-        )\n-\n-        eval_dataset = train_dataset.select(val_indices)\n-        train_dataset = train_dataset.select(train_indices)\n-\n-    if data_args.max_train_samples is not None:\n-        max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-        train_dataset = train_dataset.select(range(max_train_samples))\n-    if data_args.max_eval_samples is not None:\n-        max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-        eval_dataset = eval_dataset.select(range(max_eval_samples))\n-\n-    # Log a few random samples from the training set:\n-    for index in random.sample(range(len(train_dataset)), min(3, len(train_dataset))):\n-        logger.info(f\"Sample {index} of the training set: {train_dataset[index]}.\")\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        # region Prepare model\n-        if checkpoint is not None:\n-            model = TFAutoModelForMaskedLM.from_pretrained(\n-                checkpoint, config=config, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-            )\n-        elif model_args.model_name_or_path:\n-            model = TFAutoModelForMaskedLM.from_pretrained(\n-                model_args.model_name_or_path,\n-                config=config,\n-                token=model_args.token,\n-                trust_remote_code=model_args.trust_remote_code,\n-            )\n-        else:\n-            logger.info(\"Training new model from scratch\")\n-            model = TFAutoModelForMaskedLM.from_config(\n-                config, token=model_args.token, trust_remote_code=model_args.trust_remote_code\n-            )\n-\n-        # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n-        # on a small vocab and want a smaller embedding size, remove this test.\n-        embeddings = model.get_input_embeddings()\n-\n-        # Matt: This is a temporary workaround as we transition our models to exclusively using Keras embeddings.\n-        #       As soon as the transition is complete, all embeddings should be keras.Embeddings layers, and\n-        #       the weights will always be in embeddings.embeddings.\n-        if hasattr(embeddings, \"embeddings\"):\n-            embedding_size = embeddings.embeddings.shape[0]\n-        else:\n-            embedding_size = embeddings.weight.shape[0]\n-        if len(tokenizer) > embedding_size:\n-            model.resize_token_embeddings(len(tokenizer))\n-        # endregion\n-\n-        # region TF Dataset preparation\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-        data_collator = DataCollatorForLanguageModeling(\n-            tokenizer=tokenizer, mlm_probability=data_args.mlm_probability, return_tensors=\"np\"\n-        )\n-        options = tf.data.Options()\n-        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-\n-        # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-        # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-        # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-        # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-        # using model.prepare_tf_dataset()\n-        # For more info see the docs:\n-        # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-        # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-\n-        tf_train_dataset = model.prepare_tf_dataset(\n-            train_dataset,\n-            shuffle=True,\n-            batch_size=num_replicas * training_args.per_device_train_batch_size,\n-            collate_fn=data_collator,\n-        ).with_options(options)\n-\n-        tf_eval_dataset = model.prepare_tf_dataset(\n-            eval_dataset,\n-            # labels are passed as input, as we will use the model's internal loss\n-            shuffle=False,\n-            batch_size=num_replicas * training_args.per_device_eval_batch_size,\n-            collate_fn=data_collator,\n-            drop_remainder=True,\n-        ).with_options(options)\n-        # endregion\n-\n-        # region Optimizer and loss\n-        num_train_steps = len(tf_train_dataset) * int(training_args.num_train_epochs)\n-        if training_args.warmup_steps > 0:\n-            num_warmup_steps = training_args.warmup_steps\n-        elif training_args.warmup_ratio > 0:\n-            num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-        else:\n-            num_warmup_steps = 0\n-\n-        # Bias and layernorm weights are automatically excluded from the decay\n-        optimizer, lr_schedule = create_optimizer(\n-            init_lr=training_args.learning_rate,\n-            num_train_steps=num_train_steps,\n-            num_warmup_steps=num_warmup_steps,\n-            adam_beta1=training_args.adam_beta1,\n-            adam_beta2=training_args.adam_beta2,\n-            adam_epsilon=training_args.adam_epsilon,\n-            weight_decay_rate=training_args.weight_decay,\n-            adam_global_clipnorm=training_args.max_grad_norm,\n-        )\n-\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, jit_compile=training_args.xla)\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            if data_args.dataset_name is not None:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-{data_args.dataset_name}\"\n-            else:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-mlm\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"fill-mask\"}\n-        if data_args.dataset_name is not None:\n-            model_card_kwargs[\"dataset_tags\"] = data_args.dataset_name\n-            if data_args.dataset_config_name is not None:\n-                model_card_kwargs[\"dataset_args\"] = data_args.dataset_config_name\n-                model_card_kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n-            else:\n-                model_card_kwargs[\"dataset\"] = data_args.dataset_name\n-\n-        if training_args.push_to_hub:\n-            callbacks = [\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            ]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Training and validation\n-        logger.info(\"***** Running training *****\")\n-        logger.info(f\"  Num examples = {len(train_dataset)}\")\n-        logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n-        logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-        logger.info(f\"  Total train batch size = {training_args.per_device_train_batch_size * num_replicas}\")\n-\n-        # For long training runs, you may wish to use the PushToHub() callback here to save intermediate checkpoints\n-        # to the Hugging Face Hub rather than just pushing the finished model.\n-        # See https://huggingface.co/docs/transformers/main_classes/keras_callbacks#transformers.PushToHubCallback\n-\n-        history = model.fit(\n-            tf_train_dataset,\n-            validation_data=tf_eval_dataset,\n-            epochs=int(training_args.num_train_epochs),\n-            callbacks=callbacks,\n-        )\n-        train_loss = history.history[\"loss\"][-1]\n-        try:\n-            train_perplexity = math.exp(train_loss)\n-        except OverflowError:\n-            train_perplexity = math.inf\n-        logger.info(f\"  Final train loss: {train_loss:.3f}\")\n-        logger.info(f\"  Final train perplexity: {train_perplexity:.3f}\")\n-\n-    validation_loss = history.history[\"val_loss\"][-1]\n-    try:\n-        validation_perplexity = math.exp(validation_loss)\n-    except OverflowError:\n-        validation_perplexity = math.inf\n-    logger.info(f\"  Final validation loss: {validation_loss:.3f}\")\n-    logger.info(f\"  Final validation perplexity: {validation_perplexity:.3f}\")\n-\n-    if training_args.output_dir is not None:\n-        output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-        results_dict = {}\n-        results_dict[\"train_loss\"] = train_loss\n-        results_dict[\"train_perplexity\"] = train_perplexity\n-        results_dict[\"eval_loss\"] = validation_loss\n-        results_dict[\"eval_perplexity\"] = validation_perplexity\n-        with open(output_eval_file, \"w\") as writer:\n-            writer.write(json.dumps(results_dict))\n-        # endregion\n-\n-    if training_args.output_dir is not None and not training_args.push_to_hub:\n-        # If we're not pushing to hub, at least save a local copy when we're done\n-        model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "a7f499963ec678692d6be4a36878dee105336600",
            "filename": "examples/tensorflow/multiple-choice/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fmultiple-choice%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,43 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-# Multiple-choice training (e.g. SWAG)\n-\n-This folder contains the `run_swag.py` script, showing an examples of *multiple-choice answering* with the \n-ðŸ¤— Transformers library. For straightforward use-cases you may be able to use these scripts without modification, \n-although we have also included comments in the code to indicate areas that you may need to adapt to your own projects.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument.\n-\n-### Memory usage and data loading\n-\n-One thing to note is that all data is loaded into memory in this script. Most multiple-choice datasets are small\n-enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\n-data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\n-required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and \n-README, but for more information you can see the 'Input Datasets' section of \n-[this document](https://www.tensorflow.org/guide/tpu).\n-\n-### Example command\n-```bash\n-python run_swag.py \\\n- --model_name_or_path distilbert/distilbert-base-cased \\\n- --output_dir output \\\n- --do_eval \\\n- --do_train\n-```"
        },
        {
            "sha": "657fbc90a5b6ae5eb7f33e10b268b6b8fceedb66",
            "filename": "examples/tensorflow/multiple-choice/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fmultiple-choice%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,3 +0,0 @@\n-sentencepiece != 0.1.92\n-protobuf\n-tensorflow >= 2.3"
        },
        {
            "sha": "3172fa7de22901fd8ef4dd112ab6aedd9d6deb7b",
            "filename": "examples/tensorflow/multiple-choice/run_swag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 503,
            "changes": 503,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fmultiple-choice%2Frun_swag.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,503 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright The HuggingFace Team and The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for multiple choice.\n-\"\"\"\n-# You can also adapt this script on your own multiple choice task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from itertools import chain\n-from pathlib import Path\n-from typing import Optional\n-\n-import datasets\n-import tensorflow as tf\n-from datasets import load_dataset\n-\n-import transformers\n-from transformers import (\n-    CONFIG_NAME,\n-    TF2_WEIGHTS_NAME,\n-    AutoConfig,\n-    AutoTokenizer,\n-    DataCollatorForMultipleChoice,\n-    DefaultDataCollator,\n-    HfArgumentParser,\n-    PushToHubCallback,\n-    TFAutoModelForMultipleChoice,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.utils import check_min_version, send_example_telemetry\n-\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-# region Arguments\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n-                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n-                \"execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_seq_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. If passed, sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to the maximum sentence length. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n-                \"efficient on GPU but very bad for TPU.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if self.train_file is not None:\n-            extension = self.train_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-        if self.validation_file is not None:\n-            extension = self.validation_file.split(\".\")[-1]\n-            assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument parsing\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_swag\", model_args, data_args, framework=\"tensorflow\")\n-\n-    output_dir = Path(training_args.output_dir)\n-    output_dir.mkdir(parents=True, exist_ok=True)\n-    # endregion\n-\n-    # region Logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    log_level = training_args.get_process_log_level()\n-    logger.setLevel(log_level)\n-    datasets.utils.logging.set_verbosity(log_level)\n-    transformers.utils.logging.set_verbosity(log_level)\n-    transformers.utils.logging.enable_default_handler()\n-    transformers.utils.logging.enable_explicit_format()\n-    # endregion\n-\n-    # region Checkpoints\n-    checkpoint = None\n-    if len(os.listdir(training_args.output_dir)) > 0 and not training_args.overwrite_output_dir:\n-        if (output_dir / CONFIG_NAME).is_file() and (output_dir / TF2_WEIGHTS_NAME).is_file():\n-            checkpoint = output_dir\n-            logger.info(\n-                f\"Checkpoint detected, resuming training from checkpoint in {training_args.output_dir}. To avoid this\"\n-                \" behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-        else:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to continue regardless.\"\n-            )\n-    # endregion\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # region Load datasets\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.train_file is not None or data_args.validation_file is not None:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    else:\n-        # Downloading and loading the swag dataset from the hub.\n-        raw_datasets = load_dataset(\n-            \"swag\",\n-            \"regular\",\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    # When using your own dataset or a different dataset from swag, you will probably need to change this.\n-    ending_names = [f\"ending{i}\" for i in range(4)]\n-    context_name = \"sent1\"\n-    question_header_name = \"sent2\"\n-    # endregion\n-\n-    # region Load model config and tokenizer\n-    if checkpoint is not None:\n-        config_path = training_args.output_dir\n-    elif model_args.config_name:\n-        config_path = model_args.config_name\n-    else:\n-        config_path = model_args.model_name_or_path\n-\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    config = AutoConfig.from_pretrained(\n-        config_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast_tokenizer,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    # endregion\n-\n-    # region Dataset preprocessing\n-    if data_args.max_seq_length is None:\n-        max_seq_length = tokenizer.model_max_length\n-        if max_seq_length > 1024:\n-            logger.warning(\n-                f\"The tokenizer picked seems to have a very large `model_max_length` ({tokenizer.model_max_length}). \"\n-                \"Picking 1024 instead. You can change that default value by passing --max_seq_length xxx.\"\n-            )\n-            max_seq_length = 1024\n-    else:\n-        if data_args.max_seq_length > tokenizer.model_max_length:\n-            logger.warning(\n-                f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n-                f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n-            )\n-        max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    def preprocess_function(examples):\n-        first_sentences = [[context] * 4 for context in examples[context_name]]\n-        question_headers = examples[question_header_name]\n-        second_sentences = [\n-            [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n-        ]\n-\n-        # Flatten out\n-        first_sentences = list(chain(*first_sentences))\n-        second_sentences = list(chain(*second_sentences))\n-\n-        # Tokenize\n-        tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True, max_length=max_seq_length)\n-        # Un-flatten\n-        data = {k: [v[i : i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}\n-        return data\n-\n-    if training_args.do_train:\n-        if \"train\" not in raw_datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = raw_datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        train_dataset = train_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in raw_datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = raw_datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        eval_dataset = eval_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-\n-    if data_args.pad_to_max_length:\n-        data_collator = DefaultDataCollator(return_tensors=\"np\")\n-    else:\n-        data_collator = DataCollatorForMultipleChoice(tokenizer, return_tensors=\"tf\")\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        # region Build model\n-        if checkpoint is None:\n-            model_path = model_args.model_name_or_path\n-        else:\n-            model_path = checkpoint\n-        model = TFAutoModelForMultipleChoice.from_pretrained(\n-            model_path,\n-            config=config,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-        total_train_batch_size = training_args.per_device_train_batch_size * num_replicas\n-        total_eval_batch_size = training_args.per_device_eval_batch_size * num_replicas\n-\n-        if training_args.do_train:\n-            num_train_steps = (len(train_dataset) // total_train_batch_size) * int(training_args.num_train_epochs)\n-            if training_args.warmup_steps > 0:\n-                num_warmup_steps = training_args.warmup_steps\n-            elif training_args.warmup_ratio > 0:\n-                num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-            else:\n-                num_warmup_steps = 0\n-            optimizer, lr_schedule = create_optimizer(\n-                init_lr=training_args.learning_rate,\n-                num_train_steps=num_train_steps,\n-                num_warmup_steps=num_warmup_steps,\n-                adam_beta1=training_args.adam_beta1,\n-                adam_beta2=training_args.adam_beta2,\n-                adam_epsilon=training_args.adam_epsilon,\n-                weight_decay_rate=training_args.weight_decay,\n-                adam_global_clipnorm=training_args.max_grad_norm,\n-            )\n-        else:\n-            optimizer = \"sgd\"  # Just write anything because we won't be using it\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, metrics=[\"accuracy\"], jit_compile=training_args.xla)\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            push_to_hub_model_id = f\"{model_name}-finetuned-multiplechoice\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"multiple-choice\"}\n-\n-        if training_args.push_to_hub:\n-            callbacks = [\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            ]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Training\n-        eval_metrics = None\n-        if training_args.do_train:\n-            dataset_options = tf.data.Options()\n-            dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-\n-            # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-            # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-            # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-            # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-            # using model.prepare_tf_dataset()\n-            # For more info see the docs:\n-            # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-            # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-\n-            tf_train_dataset = model.prepare_tf_dataset(\n-                train_dataset,\n-                shuffle=True,\n-                batch_size=total_train_batch_size,\n-                collate_fn=data_collator,\n-            ).with_options(dataset_options)\n-\n-            if training_args.do_eval:\n-                validation_data = model.prepare_tf_dataset(\n-                    eval_dataset,\n-                    shuffle=False,\n-                    batch_size=total_eval_batch_size,\n-                    collate_fn=data_collator,\n-                    drop_remainder=True,\n-                ).with_options(dataset_options)\n-            else:\n-                validation_data = None\n-            history = model.fit(\n-                tf_train_dataset,\n-                validation_data=validation_data,\n-                epochs=int(training_args.num_train_epochs),\n-                callbacks=callbacks,\n-            )\n-            eval_metrics = {key: val[-1] for key, val in history.history.items()}\n-        # endregion\n-\n-        # region Evaluation\n-        if training_args.do_eval and not training_args.do_train:\n-            dataset_options = tf.data.Options()\n-            dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-            # Do a standalone evaluation pass\n-            tf_eval_dataset = model.prepare_tf_dataset(\n-                eval_dataset,\n-                shuffle=False,\n-                batch_size=total_eval_batch_size,\n-                collate_fn=data_collator,\n-                drop_remainder=True,\n-            ).with_options(dataset_options)\n-            eval_results = model.evaluate(tf_eval_dataset)\n-            eval_metrics = {\"val_loss\": eval_results[0], \"val_accuracy\": eval_results[1]}\n-        # endregion\n-\n-        if eval_metrics is not None and training_args.output_dir is not None:\n-            output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-            with open(output_eval_file, \"w\") as writer:\n-                writer.write(json.dumps(eval_metrics))\n-\n-        # region Push to hub\n-\n-        if training_args.output_dir is not None and not training_args.push_to_hub:\n-            # If we're not pushing to hub, at least save a local copy when we're done\n-            model.save_pretrained(training_args.output_dir)\n-        # endregion\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "c7e85623199fbe3a15853309c780dab1f5a2fd21",
            "filename": "examples/tensorflow/question-answering/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,57 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Question answering example\n-\n-This folder contains the `run_qa.py` script, demonstrating *question answering* with the ðŸ¤— Transformers library.\n-For straightforward use-cases you may be able to use this script without modification, although we have also\n-included comments in the code to indicate areas that you may need to adapt to your own projects.\n-\n-### Usage notes\n-\n-Note that when contexts are long they may be split into multiple training cases, not all of which may contain\n-the answer span.\n-\n-As-is, the example script will train on SQuAD or any other question-answering dataset formatted the same way, and can handle user\n-inputs as well.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument. There are some issues surrounding\n-these strategies and our models right now, which are most likely to appear in the evaluation/prediction steps. We're\n-actively working on better support for multi-GPU and TPU training in TF, but if you encounter problems a quick\n-workaround is to train in the multi-GPU or TPU context and then perform predictions outside of it.\n-\n-### Memory usage and data loading\n-\n-One thing to note is that all data is loaded into memory in this script. Most question answering datasets are small\n-enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\n-data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\n-required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and\n-README, but for more information you can see the 'Input Datasets' section of\n-[this document](https://www.tensorflow.org/guide/tpu).\n-\n-### Example command\n-\n-```bash\n-python run_qa.py \\\n---model_name_or_path distilbert/distilbert-base-cased \\\n---output_dir output \\\n---dataset_name squad \\\n---do_train \\\n---do_eval\n-```"
        },
        {
            "sha": "99aff2bb32b2bb92f7628eb9bab4c7535d4c7f92",
            "filename": "examples/tensorflow/question-answering/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,3 +0,0 @@\n-datasets >= 1.4.0\n-tensorflow >= 2.3.0\n-evaluate >= 0.2.0\n\\ No newline at end of file"
        },
        {
            "sha": "b7322f2a7fbbf0060c2b715844af594e9027b615",
            "filename": "examples/tensorflow/question-answering/run_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 830,
            "changes": 830,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Frun_qa.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,830 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for question answering.\n-\"\"\"\n-# You can also adapt this script on your own question answering task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from pathlib import Path\n-from typing import Optional\n-\n-import evaluate\n-import tensorflow as tf\n-from datasets import load_dataset\n-from packaging.version import parse\n-from utils_qa import postprocess_qa_predictions\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    EvalPrediction,\n-    HfArgumentParser,\n-    PreTrainedTokenizerFast,\n-    PushToHubCallback,\n-    TFAutoModelForQuestionAnswering,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.utils import CONFIG_NAME, TF2_WEIGHTS_NAME, check_min_version, send_example_telemetry\n-\n-\n-try:\n-    import tf_keras as keras\n-except (ModuleNotFoundError, ImportError):\n-    import keras\n-\n-    if parse(keras.__version__).major > 2:\n-        raise ValueError(\n-            \"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\n-            \"Transformers. Please install the backwards-compatible tf-keras package with \"\n-            \"`pip install tf-keras`.\"\n-        )\n-\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-# region Arguments\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Path to directory to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    train_file: Optional[str] = field(default=None, metadata={\"help\": \"The input training data file (a text file).\"})\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input evaluation data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"An optional input test data file to evaluate the perplexity on (a text file).\"},\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_seq_length: int = field(\n-        default=384,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. If False, will pad the samples dynamically when\"\n-                \" batching to the maximum length in the batch (which can be faster on GPU but will be slower on TPU).\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    version_2_with_negative: bool = field(\n-        default=False, metadata={\"help\": \"If true, some of the examples do not have an answer.\"}\n-    )\n-    null_score_diff_threshold: float = field(\n-        default=0.0,\n-        metadata={\n-            \"help\": (\n-                \"The threshold used to select the null answer: if the best answer has a score that is less than \"\n-                \"the score of the null answer minus this threshold, the null answer is selected for this example. \"\n-                \"Only useful when `version_2_with_negative=True`.\"\n-            )\n-        },\n-    )\n-    doc_stride: int = field(\n-        default=128,\n-        metadata={\"help\": \"When splitting up a long document into chunks, how much stride to take between chunks.\"},\n-    )\n-    n_best_size: int = field(\n-        default=20,\n-        metadata={\"help\": \"The total number of n-best predictions to generate when looking for an answer.\"},\n-    )\n-    max_answer_length: int = field(\n-        default=30,\n-        metadata={\n-            \"help\": (\n-                \"The maximum length of an answer that can be generated. This is needed because the start \"\n-                \"and end predictions are not conditioned on one another.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        if (\n-            self.dataset_name is None\n-            and self.train_file is None\n-            and self.validation_file is None\n-            and self.test_file is None\n-        ):\n-            raise ValueError(\"Need either a dataset name or a training/validation file/test_file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-            if self.test_file is not None:\n-                extension = self.test_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`test_file` should be a csv or a json file.\"\n-\n-\n-# endregion\n-\n-\n-# region Helper classes\n-class SavePretrainedCallback(keras.callbacks.Callback):\n-    # Hugging Face models have a save_pretrained() method that saves both the weights and the necessary\n-    # metadata to allow them to be loaded as a pretrained model in future. This is a simple Keras callback\n-    # that saves the model with this method after each epoch.\n-    def __init__(self, output_dir, **kwargs):\n-        super().__init__()\n-        self.output_dir = output_dir\n-\n-    def on_epoch_end(self, epoch, logs=None):\n-        self.model.save_pretrained(self.output_dir)\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument parsing\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_qa\", model_args, data_args, framework=\"tensorflow\")\n-\n-    output_dir = Path(training_args.output_dir)\n-    output_dir.mkdir(parents=True, exist_ok=True)\n-    # endregion\n-\n-    # region Checkpoints\n-    checkpoint = None\n-    if len(os.listdir(training_args.output_dir)) > 0 and not training_args.overwrite_output_dir:\n-        if (output_dir / CONFIG_NAME).is_file() and (output_dir / TF2_WEIGHTS_NAME).is_file():\n-            checkpoint = output_dir\n-            logger.info(\n-                f\"Checkpoint detected, resuming training from checkpoint in {training_args.output_dir}. To avoid this\"\n-                \" behavior, change the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-        else:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to continue regardless.\"\n-            )\n-    # endregion\n-\n-    # region Logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    logger.setLevel(logging.INFO if training_args.should_log else logging.WARN)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if training_args.should_log:\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-    # endregion\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # region Load Data\n-    # Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n-    # 'text' is found. You can easily tweak this behavior (see below).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            field=\"data\",\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-    # endregion\n-\n-    # region Load pretrained model and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=True,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    # endregion\n-\n-    # region Tokenizer check: this script requires a fast tokenizer.\n-    if not isinstance(tokenizer, PreTrainedTokenizerFast):\n-        raise TypeError(\n-            \"This example script only works for models that have a fast tokenizer. Check out the big table of models at\"\n-            \" https://huggingface.co/transformers/index.html#supported-frameworks to find the model types that meet\"\n-            \" this requirement\"\n-        )\n-    # endregion\n-\n-    # region Preprocessing the datasets\n-    # Preprocessing is slightly different for training and evaluation.\n-    if training_args.do_train:\n-        column_names = datasets[\"train\"].column_names\n-    elif training_args.do_eval:\n-        column_names = datasets[\"validation\"].column_names\n-    else:\n-        column_names = datasets[\"test\"].column_names\n-    question_column_name = \"question\" if \"question\" in column_names else column_names[0]\n-    context_column_name = \"context\" if \"context\" in column_names else column_names[1]\n-    answer_column_name = \"answers\" if \"answers\" in column_names else column_names[2]\n-\n-    # Padding side determines if we do (question|context) or (context|question).\n-    pad_on_right = tokenizer.padding_side == \"right\"\n-\n-    if data_args.max_seq_length > tokenizer.model_max_length:\n-        logger.warning(\n-            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n-            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n-        )\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    if data_args.pad_to_max_length or isinstance(training_args.strategy, tf.distribute.TPUStrategy):\n-        logger.info(\"Padding all batches to max length because argument was set or we're on TPU.\")\n-        padding = \"max_length\"\n-    else:\n-        padding = False\n-\n-    # Training preprocessing\n-    def prepare_train_features(examples):\n-        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n-        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n-        # left whitespace\n-        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n-\n-        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n-        # in one example possible giving several features when a context is long, each of those features having a\n-        # context that overlaps a bit the context of the previous feature.\n-        tokenized_examples = tokenizer(\n-            examples[question_column_name if pad_on_right else context_column_name],\n-            examples[context_column_name if pad_on_right else question_column_name],\n-            truncation=\"only_second\" if pad_on_right else \"only_first\",\n-            max_length=max_seq_length,\n-            stride=data_args.doc_stride,\n-            return_overflowing_tokens=True,\n-            return_offsets_mapping=True,\n-            padding=padding,\n-        )\n-\n-        # Since one example might give us several features if it has a long context, we need a map from a feature to\n-        # its corresponding example. This key gives us just that.\n-        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n-        # The offset mappings will give us a map from token to character position in the original context. This will\n-        # help us compute the start_positions and end_positions.\n-        offset_mapping = tokenized_examples.pop(\"offset_mapping\")\n-\n-        # Let's label those examples!\n-        tokenized_examples[\"start_positions\"] = []\n-        tokenized_examples[\"end_positions\"] = []\n-\n-        for i, offsets in enumerate(offset_mapping):\n-            # We will label impossible answers with the index of the CLS token.\n-            input_ids = tokenized_examples[\"input_ids\"][i]\n-            cls_index = input_ids.index(tokenizer.cls_token_id)\n-\n-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n-            sequence_ids = tokenized_examples.sequence_ids(i)\n-\n-            # One example can give several spans, this is the index of the example containing this span of text.\n-            sample_index = sample_mapping[i]\n-            answers = examples[answer_column_name][sample_index]\n-            # If no answers are given, set the cls_index as answer.\n-            if len(answers[\"answer_start\"]) == 0:\n-                tokenized_examples[\"start_positions\"].append(cls_index)\n-                tokenized_examples[\"end_positions\"].append(cls_index)\n-            else:\n-                # Start/end character index of the answer in the text.\n-                start_char = answers[\"answer_start\"][0]\n-                end_char = start_char + len(answers[\"text\"][0])\n-\n-                # Start token index of the current span in the text.\n-                token_start_index = 0\n-                while sequence_ids[token_start_index] != (1 if pad_on_right else 0):\n-                    token_start_index += 1\n-\n-                # End token index of the current span in the text.\n-                token_end_index = len(input_ids) - 1\n-                while sequence_ids[token_end_index] != (1 if pad_on_right else 0):\n-                    token_end_index -= 1\n-\n-                # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index).\n-                if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):\n-                    tokenized_examples[\"start_positions\"].append(cls_index)\n-                    tokenized_examples[\"end_positions\"].append(cls_index)\n-                else:\n-                    # Otherwise move the token_start_index and token_end_index to the two ends of the answer.\n-                    # Note: we could go after the last offset if the answer is the last word (edge case).\n-                    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n-                        token_start_index += 1\n-                    tokenized_examples[\"start_positions\"].append(token_start_index - 1)\n-                    while offsets[token_end_index][1] >= end_char:\n-                        token_end_index -= 1\n-                    tokenized_examples[\"end_positions\"].append(token_end_index + 1)\n-\n-        return tokenized_examples\n-\n-    processed_datasets = {}\n-    if training_args.do_train:\n-        if \"train\" not in datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            # We will select sample from whole data if argument is specified\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        # Create train feature from dataset\n-        train_dataset = train_dataset.map(\n-            prepare_train_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_train_samples is not None:\n-            # Number of samples might increase during Feature Creation, We select only specified max samples\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        processed_datasets[\"train\"] = train_dataset\n-\n-    # Validation preprocessing\n-    def prepare_validation_features(examples):\n-        # Some of the questions have lots of whitespace on the left, which is not useful and will make the\n-        # truncation of the context fail (the tokenized question will take a lots of space). So we remove that\n-        # left whitespace\n-        examples[question_column_name] = [q.lstrip() for q in examples[question_column_name]]\n-\n-        # Tokenize our examples with truncation and maybe padding, but keep the overflows using a stride. This results\n-        # in one example possible giving several features when a context is long, each of those features having a\n-        # context that overlaps a bit the context of the previous feature.\n-        tokenized_examples = tokenizer(\n-            examples[question_column_name if pad_on_right else context_column_name],\n-            examples[context_column_name if pad_on_right else question_column_name],\n-            truncation=\"only_second\" if pad_on_right else \"only_first\",\n-            max_length=max_seq_length,\n-            stride=data_args.doc_stride,\n-            return_overflowing_tokens=True,\n-            return_offsets_mapping=True,\n-            padding=padding,\n-        )\n-\n-        # Since one example might give us several features if it has a long context, we need a map from a feature to\n-        # its corresponding example. This key gives us just that.\n-        sample_mapping = tokenized_examples.pop(\"overflow_to_sample_mapping\")\n-\n-        # For evaluation, we will need to convert our predictions to substrings of the context, so we keep the\n-        # corresponding example_id and we will store the offset mappings.\n-        tokenized_examples[\"example_id\"] = []\n-\n-        for i in range(len(tokenized_examples[\"input_ids\"])):\n-            # Grab the sequence corresponding to that example (to know what is the context and what is the question).\n-            sequence_ids = tokenized_examples.sequence_ids(i)\n-            context_index = 1 if pad_on_right else 0\n-\n-            # One example can give several spans, this is the index of the example containing this span of text.\n-            sample_index = sample_mapping[i]\n-            tokenized_examples[\"example_id\"].append(examples[\"id\"][sample_index])\n-\n-            # Set to None the offset_mapping that are not part of the context so it's easy to determine if a token\n-            # position is part of the context or not.\n-            tokenized_examples[\"offset_mapping\"][i] = [\n-                (o if sequence_ids[k] == context_index else None)\n-                for k, o in enumerate(tokenized_examples[\"offset_mapping\"][i])\n-            ]\n-\n-        return tokenized_examples\n-\n-    if training_args.do_eval:\n-        if \"validation\" not in datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_examples = datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            # We will select sample from whole data\n-            max_eval_samples = min(len(eval_examples), data_args.max_eval_samples)\n-            eval_examples = eval_examples.select(range(max_eval_samples))\n-        # Validation Feature Creation\n-        eval_dataset = eval_examples.map(\n-            prepare_validation_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_eval_samples is not None:\n-            # During Feature creation dataset samples might increase, we will select required samples again\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        processed_datasets[\"validation\"] = eval_dataset\n-\n-    if training_args.do_predict:\n-        if \"test\" not in datasets:\n-            raise ValueError(\"--do_predict requires a test dataset\")\n-        predict_examples = datasets[\"test\"]\n-        if data_args.max_predict_samples is not None:\n-            # We will select sample from whole data\n-            predict_examples = predict_examples.select(range(data_args.max_predict_samples))\n-        # Predict Feature Creation\n-        predict_dataset = predict_examples.map(\n-            prepare_validation_features,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-        )\n-        if data_args.max_predict_samples is not None:\n-            # During Feature creation dataset samples might increase, we will select required samples again\n-            max_predict_samples = min(len(predict_dataset), data_args.max_predict_samples)\n-            predict_dataset = predict_dataset.select(range(max_predict_samples))\n-        processed_datasets[\"test\"] = predict_dataset\n-    # endregion\n-\n-    # region Metrics and Post-processing:\n-    def post_processing_function(examples, features, predictions, stage=\"eval\"):\n-        # Post-processing: we match the start logits and end logits to answers in the original context.\n-        predictions = postprocess_qa_predictions(\n-            examples=examples,\n-            features=features,\n-            predictions=predictions,\n-            version_2_with_negative=data_args.version_2_with_negative,\n-            n_best_size=data_args.n_best_size,\n-            max_answer_length=data_args.max_answer_length,\n-            null_score_diff_threshold=data_args.null_score_diff_threshold,\n-            output_dir=training_args.output_dir,\n-            prefix=stage,\n-        )\n-        # Format the result to the format the metric expects.\n-        if data_args.version_2_with_negative:\n-            formatted_predictions = [\n-                {\"id\": k, \"prediction_text\": v, \"no_answer_probability\": 0.0} for k, v in predictions.items()\n-            ]\n-        else:\n-            formatted_predictions = [{\"id\": k, \"prediction_text\": v} for k, v in predictions.items()]\n-\n-        references = [{\"id\": ex[\"id\"], \"answers\": ex[answer_column_name]} for ex in examples]\n-        return EvalPrediction(predictions=formatted_predictions, label_ids=references)\n-\n-    metric = evaluate.load(\n-        \"squad_v2\" if data_args.version_2_with_negative else \"squad\", cache_dir=model_args.cache_dir\n-    )\n-\n-    def compute_metrics(p: EvalPrediction):\n-        return metric.compute(predictions=p.predictions, references=p.label_ids)\n-\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        dataset_options = tf.data.Options()\n-        dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-\n-        # region Load model and prepare datasets\n-        if checkpoint is None:\n-            model_path = model_args.model_name_or_path\n-        else:\n-            model_path = checkpoint\n-        model = TFAutoModelForQuestionAnswering.from_pretrained(\n-            model_path,\n-            config=config,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        if training_args.do_train:\n-            training_dataset = model.prepare_tf_dataset(\n-                processed_datasets[\"train\"],\n-                shuffle=True,\n-                batch_size=training_args.per_device_train_batch_size * num_replicas,\n-                tokenizer=tokenizer,\n-            )\n-\n-            training_dataset = training_dataset.with_options(dataset_options)\n-\n-            num_train_steps = len(training_dataset) * training_args.num_train_epochs\n-            if training_args.warmup_steps > 0:\n-                num_warmup_steps = training_args.warmup_steps\n-            elif training_args.warmup_ratio > 0:\n-                num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-            else:\n-                num_warmup_steps = 0\n-\n-            optimizer, schedule = create_optimizer(\n-                init_lr=training_args.learning_rate,\n-                num_train_steps=len(training_dataset) * training_args.num_train_epochs,\n-                num_warmup_steps=num_warmup_steps,\n-                adam_beta1=training_args.adam_beta1,\n-                adam_beta2=training_args.adam_beta2,\n-                adam_epsilon=training_args.adam_epsilon,\n-                weight_decay_rate=training_args.weight_decay,\n-                adam_global_clipnorm=training_args.max_grad_norm,\n-            )\n-\n-            # Transformers models compute the right loss for their task by default when labels are passed, and will\n-            # use this for training unless you specify your own loss function in compile().\n-            model.compile(optimizer=optimizer, jit_compile=training_args.xla, metrics=[\"accuracy\"])\n-\n-        else:\n-            # Optimizer doesn't matter as it won't be used anyway\n-            model.compile(optimizer=\"sgd\", jit_compile=training_args.xla, metrics=[\"accuracy\"])\n-            training_dataset = None\n-\n-        if training_args.do_eval:\n-            eval_dataset = model.prepare_tf_dataset(\n-                processed_datasets[\"validation\"],\n-                shuffle=False,\n-                batch_size=training_args.per_device_train_batch_size * num_replicas,\n-                tokenizer=tokenizer,\n-            )\n-            eval_dataset = eval_dataset.with_options(dataset_options)\n-        else:\n-            eval_dataset = None\n-\n-        if training_args.do_predict:\n-            predict_dataset = model.prepare_tf_dataset(\n-                processed_datasets[\"test\"],\n-                shuffle=False,\n-                batch_size=training_args.per_device_eval_batch_size * num_replicas,\n-                tokenizer=tokenizer,\n-            )\n-            predict_dataset = predict_dataset.with_options(dataset_options)\n-        else:\n-            predict_dataset = None\n-\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            if data_args.dataset_name is not None:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-{data_args.dataset_name}\"\n-            else:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-question-answering\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"question-answering\"}\n-        if data_args.dataset_name is not None:\n-            model_card_kwargs[\"dataset_tags\"] = data_args.dataset_name\n-            if data_args.dataset_config_name is not None:\n-                model_card_kwargs[\"dataset_args\"] = data_args.dataset_config_name\n-                model_card_kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n-            else:\n-                model_card_kwargs[\"dataset\"] = data_args.dataset_name\n-\n-        if training_args.push_to_hub:\n-            callbacks = [\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            ]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Training and Evaluation\n-\n-        if training_args.do_train:\n-            # Note that the validation and test datasets have been processed in a different way to the\n-            # training datasets in this example, and so they don't have the same label structure.\n-            # As such, we don't pass them directly to Keras, but instead get model predictions to evaluate\n-            # after training.\n-            model.fit(training_dataset, epochs=int(training_args.num_train_epochs), callbacks=callbacks)\n-\n-        if training_args.do_eval:\n-            logger.info(\"*** Evaluation ***\")\n-\n-            # In this example, we compute advanced metrics at the end of training, but\n-            # if you'd like to compute metrics every epoch that are too complex to be written as\n-            # standard Keras metrics, you can use our KerasMetricCallback. See\n-            # https://huggingface.co/docs/transformers/main/en/main_classes/keras_callbacks\n-\n-            eval_predictions = model.predict(eval_dataset)\n-            if isinstance(eval_predictions.start_logits, tf.RaggedTensor):\n-                # If predictions are RaggedTensor, we densify them. Since they are logits, padding with 0 is a bad idea!\n-                # The reason is that a logit of 0 can often end up as quite a high probability value, sometimes even\n-                # the highest probability in a sample. Instead, we use a large negative value, which ensures that the\n-                # padding positions are correctly masked.\n-                eval_start_logits = eval_predictions.start_logits.to_tensor(default_value=-1000).numpy()\n-                eval_end_logits = eval_predictions.end_logits.to_tensor(default_value=-1000).numpy()\n-            else:\n-                eval_start_logits = eval_predictions.start_logits\n-                eval_end_logits = eval_predictions.end_logits\n-\n-            post_processed_eval = post_processing_function(\n-                datasets[\"validation\"],\n-                processed_datasets[\"validation\"],\n-                (eval_start_logits, eval_end_logits),\n-            )\n-            metrics = compute_metrics(post_processed_eval)\n-            logging.info(\"Evaluation metrics:\")\n-            for metric, value in metrics.items():\n-                logging.info(f\"{metric}: {value:.3f}\")\n-            if training_args.output_dir is not None:\n-                output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-                with open(output_eval_file, \"w\") as writer:\n-                    writer.write(json.dumps(metrics))\n-        # endregion\n-\n-        # region Prediction\n-        if training_args.do_predict:\n-            logger.info(\"*** Predict ***\")\n-\n-            test_predictions = model.predict(predict_dataset)\n-            if isinstance(test_predictions.start_logits, tf.RaggedTensor):\n-                # If predictions are RaggedTensor, we densify them. Since they are logits, padding with 0 is a bad idea!\n-                # The reason is that a logit of 0 can often end up as quite a high probability value, sometimes even\n-                # the highest probability in a sample. Instead, we use a large negative value, which ensures that the\n-                # padding positions are correctly masked.\n-                test_start_logits = test_predictions.start_logits.to_tensor(default_value=-1000).numpy()\n-                test_end_logits = test_predictions.end_logits.to_tensor(default_value=-1000).numpy()\n-            else:\n-                test_start_logits = test_predictions.start_logits\n-                test_end_logits = test_predictions.end_logits\n-            post_processed_test = post_processing_function(\n-                datasets[\"test\"],\n-                processed_datasets[\"test\"],\n-                (test_start_logits, test_end_logits),\n-            )\n-            metrics = compute_metrics(post_processed_test)\n-\n-            logging.info(\"Test metrics:\")\n-            for metric, value in metrics.items():\n-                logging.info(f\"{metric}: {value:.3f}\")\n-        # endregion\n-\n-    if training_args.output_dir is not None and not training_args.push_to_hub:\n-        # If we're not pushing to hub, at least save a local copy when we're done\n-        model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "b30322b0071fff2f919bb0b2c48a77513430b6b4",
            "filename": "examples/tensorflow/question-answering/utils_qa.py",
            "status": "removed",
            "additions": 0,
            "deletions": 443,
            "changes": 443,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fquestion-answering%2Futils_qa.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,443 +0,0 @@\n-# Copyright 2020 The HuggingFace Team All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Post-processing utilities for question answering.\n-\"\"\"\n-\n-import collections\n-import json\n-import logging\n-import os\n-from typing import Optional\n-\n-import numpy as np\n-from tqdm.auto import tqdm\n-\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-def postprocess_qa_predictions(\n-    examples,\n-    features,\n-    predictions: tuple[np.ndarray, np.ndarray],\n-    version_2_with_negative: bool = False,\n-    n_best_size: int = 20,\n-    max_answer_length: int = 30,\n-    null_score_diff_threshold: float = 0.0,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n-):\n-    \"\"\"\n-    Post-processes the predictions of a question-answering model to convert them to answers that are substrings of the\n-    original contexts. This is the base postprocessing functions for models that only return start and end logits.\n-\n-    Args:\n-        examples: The non-preprocessed dataset (see the main script for more information).\n-        features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n-            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n-            first dimension must match the number of elements of :obj:`features`.\n-        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether or not the underlying dataset contains examples with no answers.\n-        n_best_size (:obj:`int`, `optional`, defaults to 20):\n-            The total number of n-best predictions to generate when looking for an answer.\n-        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n-            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n-            are not conditioned on one another.\n-        null_score_diff_threshold (:obj:`float`, `optional`, defaults to 0):\n-            The threshold used to select the null answer: if the best answer has a score that is less than the score of\n-            the null answer minus this threshold, the null answer is selected for this example (note that the score of\n-            the null answer for an example giving several features is the minimum of the scores for the null answer on\n-            each feature: all features must be aligned on the fact they `want` to predict a null answer).\n-\n-            Only useful when :obj:`version_2_with_negative` is :obj:`True`.\n-        output_dir (:obj:`str`, `optional`):\n-            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n-            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n-            answers, are saved in `output_dir`.\n-        prefix (:obj:`str`, `optional`):\n-            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n-        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n-            ``logging`` log level (e.g., ``logging.WARNING``)\n-    \"\"\"\n-    if len(predictions) != 2:\n-        raise ValueError(\"`predictions` should be a tuple with two elements (start_logits, end_logits).\")\n-    all_start_logits, all_end_logits = predictions\n-\n-    if len(predictions[0]) != len(features):\n-        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n-\n-    # Build a map example to its corresponding features.\n-    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n-    features_per_example = collections.defaultdict(list)\n-    for i, feature in enumerate(features):\n-        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n-\n-    # The dictionaries we have to fill.\n-    all_predictions = collections.OrderedDict()\n-    all_nbest_json = collections.OrderedDict()\n-    if version_2_with_negative:\n-        scores_diff_json = collections.OrderedDict()\n-\n-    # Logging.\n-    logger.setLevel(log_level)\n-    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n-\n-    # Let's loop over all the examples!\n-    for example_index, example in enumerate(tqdm(examples)):\n-        # Those are the indices of the features associated to the current example.\n-        feature_indices = features_per_example[example_index]\n-\n-        min_null_prediction = None\n-        prelim_predictions = []\n-\n-        # Looping through all the features associated to the current example.\n-        for feature_index in feature_indices:\n-            # We grab the predictions of the model for this feature.\n-            start_logits = all_start_logits[feature_index]\n-            end_logits = all_end_logits[feature_index]\n-            # This is what will allow us to map some the positions in our logits to span of texts in the original\n-            # context.\n-            offset_mapping = features[feature_index][\"offset_mapping\"]\n-            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n-            # available in the current feature.\n-            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n-\n-            # Update minimum null prediction.\n-            feature_null_score = start_logits[0] + end_logits[0]\n-            if min_null_prediction is None or min_null_prediction[\"score\"] > feature_null_score:\n-                min_null_prediction = {\n-                    \"offsets\": (0, 0),\n-                    \"score\": feature_null_score,\n-                    \"start_logit\": start_logits[0],\n-                    \"end_logit\": end_logits[0],\n-                }\n-\n-            # Go through all possibilities for the `n_best_size` greater start and end logits.\n-            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n-            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n-            for start_index in start_indexes:\n-                for end_index in end_indexes:\n-                    # Don't consider out-of-scope answers, either because the indices are out of bounds or correspond\n-                    # to part of the input_ids that are not in the context.\n-                    if (\n-                        start_index >= len(offset_mapping)\n-                        or end_index >= len(offset_mapping)\n-                        or offset_mapping[start_index] is None\n-                        or len(offset_mapping[start_index]) < 2\n-                        or offset_mapping[end_index] is None\n-                        or len(offset_mapping[end_index]) < 2\n-                    ):\n-                        continue\n-                    # Don't consider answers with a length that is either < 0 or > max_answer_length.\n-                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n-                        continue\n-                    # Don't consider answer that don't have the maximum context available (if such information is\n-                    # provided).\n-                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n-                        continue\n-\n-                    prelim_predictions.append(\n-                        {\n-                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n-                            \"score\": start_logits[start_index] + end_logits[end_index],\n-                            \"start_logit\": start_logits[start_index],\n-                            \"end_logit\": end_logits[end_index],\n-                        }\n-                    )\n-        if version_2_with_negative and min_null_prediction is not None:\n-            # Add the minimum null prediction\n-            prelim_predictions.append(min_null_prediction)\n-            null_score = min_null_prediction[\"score\"]\n-\n-        # Only keep the best `n_best_size` predictions.\n-        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n-\n-        # Add back the minimum null prediction if it was removed because of its low score.\n-        if (\n-            version_2_with_negative\n-            and min_null_prediction is not None\n-            and not any(p[\"offsets\"] == (0, 0) for p in predictions)\n-        ):\n-            predictions.append(min_null_prediction)\n-\n-        # Use the offsets to gather the answer text in the original context.\n-        context = example[\"context\"]\n-        for pred in predictions:\n-            offsets = pred.pop(\"offsets\")\n-            pred[\"text\"] = context[offsets[0] : offsets[1]]\n-\n-        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n-        # failure.\n-        if len(predictions) == 0 or (len(predictions) == 1 and predictions[0][\"text\"] == \"\"):\n-            predictions.insert(0, {\"text\": \"empty\", \"start_logit\": 0.0, \"end_logit\": 0.0, \"score\": 0.0})\n-\n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n-        # the LogSumExp trick).\n-        scores = np.array([pred.pop(\"score\") for pred in predictions])\n-        exp_scores = np.exp(scores - np.max(scores))\n-        probs = exp_scores / exp_scores.sum()\n-\n-        # Include the probabilities in our predictions.\n-        for prob, pred in zip(probs, predictions):\n-            pred[\"probability\"] = prob\n-\n-        # Pick the best prediction. If the null answer is not possible, this is easy.\n-        if not version_2_with_negative:\n-            all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n-        else:\n-            # Otherwise we first need to find the best non-empty prediction.\n-            i = 0\n-            while predictions[i][\"text\"] == \"\":\n-                i += 1\n-            best_non_null_pred = predictions[i]\n-\n-            # Then we compare to the null prediction using the threshold.\n-            score_diff = null_score - best_non_null_pred[\"start_logit\"] - best_non_null_pred[\"end_logit\"]\n-            scores_diff_json[example[\"id\"]] = float(score_diff)  # To be JSON-serializable.\n-            if score_diff > null_score_diff_threshold:\n-                all_predictions[example[\"id\"]] = \"\"\n-            else:\n-                all_predictions[example[\"id\"]] = best_non_null_pred[\"text\"]\n-\n-        # Make `predictions` JSON-serializable by casting np.float back to float.\n-        all_nbest_json[example[\"id\"]] = [\n-            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n-            for pred in predictions\n-        ]\n-\n-    # If we have an output_dir, let's save all those dicts.\n-    if output_dir is not None:\n-        if not os.path.isdir(output_dir):\n-            raise OSError(f\"{output_dir} is not a directory.\")\n-\n-        prediction_file = os.path.join(\n-            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n-        )\n-        nbest_file = os.path.join(\n-            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n-        )\n-        if version_2_with_negative:\n-            null_odds_file = os.path.join(\n-                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n-            )\n-\n-        logger.info(f\"Saving predictions to {prediction_file}.\")\n-        with open(prediction_file, \"w\") as writer:\n-            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n-        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n-        with open(nbest_file, \"w\") as writer:\n-            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n-        if version_2_with_negative:\n-            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n-            with open(null_odds_file, \"w\") as writer:\n-                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n-\n-    return all_predictions\n-\n-\n-def postprocess_qa_predictions_with_beam_search(\n-    examples,\n-    features,\n-    predictions: tuple[np.ndarray, np.ndarray],\n-    version_2_with_negative: bool = False,\n-    n_best_size: int = 20,\n-    max_answer_length: int = 30,\n-    start_n_top: int = 5,\n-    end_n_top: int = 5,\n-    output_dir: Optional[str] = None,\n-    prefix: Optional[str] = None,\n-    log_level: Optional[int] = logging.WARNING,\n-):\n-    \"\"\"\n-    Post-processes the predictions of a question-answering model with beam search to convert them to answers that are substrings of the\n-    original contexts. This is the postprocessing functions for models that return start and end logits, indices, as well as\n-    cls token predictions.\n-\n-    Args:\n-        examples: The non-preprocessed dataset (see the main script for more information).\n-        features: The processed dataset (see the main script for more information).\n-        predictions (:obj:`tuple[np.ndarray, np.ndarray]`):\n-            The predictions of the model: two arrays containing the start logits and the end logits respectively. Its\n-            first dimension must match the number of elements of :obj:`features`.\n-        version_2_with_negative (:obj:`bool`, `optional`, defaults to :obj:`False`):\n-            Whether or not the underlying dataset contains examples with no answers.\n-        n_best_size (:obj:`int`, `optional`, defaults to 20):\n-            The total number of n-best predictions to generate when looking for an answer.\n-        max_answer_length (:obj:`int`, `optional`, defaults to 30):\n-            The maximum length of an answer that can be generated. This is needed because the start and end predictions\n-            are not conditioned on one another.\n-        start_n_top (:obj:`int`, `optional`, defaults to 5):\n-            The number of top start logits too keep when searching for the :obj:`n_best_size` predictions.\n-        end_n_top (:obj:`int`, `optional`, defaults to 5):\n-            The number of top end logits too keep when searching for the :obj:`n_best_size` predictions.\n-        output_dir (:obj:`str`, `optional`):\n-            If provided, the dictionaries of predictions, n_best predictions (with their scores and logits) and, if\n-            :obj:`version_2_with_negative=True`, the dictionary of the scores differences between best and null\n-            answers, are saved in `output_dir`.\n-        prefix (:obj:`str`, `optional`):\n-            If provided, the dictionaries mentioned above are saved with `prefix` added to their names.\n-        log_level (:obj:`int`, `optional`, defaults to ``logging.WARNING``):\n-            ``logging`` log level (e.g., ``logging.WARNING``)\n-    \"\"\"\n-    if len(predictions) != 5:\n-        raise ValueError(\"`predictions` should be a tuple with five elements.\")\n-    start_top_log_probs, start_top_index, end_top_log_probs, end_top_index, cls_logits = predictions\n-\n-    if len(predictions[0]) != len(features):\n-        raise ValueError(f\"Got {len(predictions[0])} predictions and {len(features)} features.\")\n-\n-    # Build a map example to its corresponding features.\n-    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n-    features_per_example = collections.defaultdict(list)\n-    for i, feature in enumerate(features):\n-        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n-\n-    # The dictionaries we have to fill.\n-    all_predictions = collections.OrderedDict()\n-    all_nbest_json = collections.OrderedDict()\n-    scores_diff_json = collections.OrderedDict() if version_2_with_negative else None\n-\n-    # Logging.\n-    logger.setLevel(log_level)\n-    logger.info(f\"Post-processing {len(examples)} example predictions split into {len(features)} features.\")\n-\n-    # Let's loop over all the examples!\n-    for example_index, example in enumerate(tqdm(examples)):\n-        # Those are the indices of the features associated to the current example.\n-        feature_indices = features_per_example[example_index]\n-\n-        min_null_score = None\n-        prelim_predictions = []\n-\n-        # Looping through all the features associated to the current example.\n-        for feature_index in feature_indices:\n-            # We grab the predictions of the model for this feature.\n-            start_log_prob = start_top_log_probs[feature_index]\n-            start_indexes = start_top_index[feature_index]\n-            end_log_prob = end_top_log_probs[feature_index]\n-            end_indexes = end_top_index[feature_index]\n-            feature_null_score = cls_logits[feature_index]\n-            # This is what will allow us to map some the positions in our logits to span of texts in the original\n-            # context.\n-            offset_mapping = features[feature_index][\"offset_mapping\"]\n-            # Optional `token_is_max_context`, if provided we will remove answers that do not have the maximum context\n-            # available in the current feature.\n-            token_is_max_context = features[feature_index].get(\"token_is_max_context\", None)\n-\n-            # Update minimum null prediction\n-            if min_null_score is None or feature_null_score < min_null_score:\n-                min_null_score = feature_null_score\n-\n-            # Go through all possibilities for the `n_start_top`/`n_end_top` greater start and end logits.\n-            for i in range(start_n_top):\n-                for j in range(end_n_top):\n-                    start_index = int(start_indexes[i])\n-                    j_index = i * end_n_top + j\n-                    end_index = int(end_indexes[j_index])\n-                    # Don't consider out-of-scope answers (last part of the test should be unnecessary because of the\n-                    # p_mask but let's not take any risk)\n-                    if (\n-                        start_index >= len(offset_mapping)\n-                        or end_index >= len(offset_mapping)\n-                        or offset_mapping[start_index] is None\n-                        or len(offset_mapping[start_index]) < 2\n-                        or offset_mapping[end_index] is None\n-                        or len(offset_mapping[end_index]) < 2\n-                    ):\n-                        continue\n-\n-                    # Don't consider answers with a length negative or > max_answer_length.\n-                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n-                        continue\n-                    # Don't consider answer that don't have the maximum context available (if such information is\n-                    # provided).\n-                    if token_is_max_context is not None and not token_is_max_context.get(str(start_index), False):\n-                        continue\n-                    prelim_predictions.append(\n-                        {\n-                            \"offsets\": (offset_mapping[start_index][0], offset_mapping[end_index][1]),\n-                            \"score\": start_log_prob[i] + end_log_prob[j_index],\n-                            \"start_log_prob\": start_log_prob[i],\n-                            \"end_log_prob\": end_log_prob[j_index],\n-                        }\n-                    )\n-\n-        # Only keep the best `n_best_size` predictions.\n-        predictions = sorted(prelim_predictions, key=lambda x: x[\"score\"], reverse=True)[:n_best_size]\n-\n-        # Use the offsets to gather the answer text in the original context.\n-        context = example[\"context\"]\n-        for pred in predictions:\n-            offsets = pred.pop(\"offsets\")\n-            pred[\"text\"] = context[offsets[0] : offsets[1]]\n-\n-        # In the very rare edge case we have not a single non-null prediction, we create a fake prediction to avoid\n-        # failure.\n-        if len(predictions) == 0:\n-            # Without predictions min_null_score is going to be None and None will cause an exception later\n-            min_null_score = -2e-6\n-            predictions.insert(0, {\"text\": \"\", \"start_logit\": -1e-6, \"end_logit\": -1e-6, \"score\": min_null_score})\n-\n-        # Compute the softmax of all scores (we do it with numpy to stay independent from torch/tf in this file, using\n-        # the LogSumExp trick).\n-        scores = np.array([pred.pop(\"score\") for pred in predictions])\n-        exp_scores = np.exp(scores - np.max(scores))\n-        probs = exp_scores / exp_scores.sum()\n-\n-        # Include the probabilities in our predictions.\n-        for prob, pred in zip(probs, predictions):\n-            pred[\"probability\"] = prob\n-\n-        # Pick the best prediction and set the probability for the null answer.\n-        all_predictions[example[\"id\"]] = predictions[0][\"text\"]\n-        if version_2_with_negative:\n-            scores_diff_json[example[\"id\"]] = float(min_null_score)\n-\n-        # Make `predictions` JSON-serializable by casting np.float back to float.\n-        all_nbest_json[example[\"id\"]] = [\n-            {k: (float(v) if isinstance(v, (np.float16, np.float32, np.float64)) else v) for k, v in pred.items()}\n-            for pred in predictions\n-        ]\n-\n-    # If we have an output_dir, let's save all those dicts.\n-    if output_dir is not None:\n-        if not os.path.isdir(output_dir):\n-            raise OSError(f\"{output_dir} is not a directory.\")\n-\n-        prediction_file = os.path.join(\n-            output_dir, \"predictions.json\" if prefix is None else f\"{prefix}_predictions.json\"\n-        )\n-        nbest_file = os.path.join(\n-            output_dir, \"nbest_predictions.json\" if prefix is None else f\"{prefix}_nbest_predictions.json\"\n-        )\n-        if version_2_with_negative:\n-            null_odds_file = os.path.join(\n-                output_dir, \"null_odds.json\" if prefix is None else f\"{prefix}_null_odds.json\"\n-            )\n-\n-        logger.info(f\"Saving predictions to {prediction_file}.\")\n-        with open(prediction_file, \"w\") as writer:\n-            writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n-        logger.info(f\"Saving nbest_preds to {nbest_file}.\")\n-        with open(nbest_file, \"w\") as writer:\n-            writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n-        if version_2_with_negative:\n-            logger.info(f\"Saving null_odds to {null_odds_file}.\")\n-            with open(null_odds_file, \"w\") as writer:\n-                writer.write(json.dumps(scores_diff_json, indent=4) + \"\\n\")\n-\n-    return all_predictions, scores_diff_json"
        },
        {
            "sha": "032af0241c77ae32865cc8a1c7d518b3ec8680d3",
            "filename": "examples/tensorflow/summarization/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fsummarization%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,40 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Summarization example\n-\n-This script shows an example of training a *summarization* model with the ðŸ¤— Transformers library.\n-For straightforward use-cases you may be able to use these scripts without modification, although we have also\n-included comments in the code to indicate areas that you may need to adapt to your own projects.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, these scripts use a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument.\n-\n-### Example command\n-```\n-python run_summarization.py  \\\n---model_name_or_path facebook/bart-base \\\n---dataset_name cnn_dailymail \\\n---dataset_config \"3.0.0\" \\\n---output_dir /tmp/tst-summarization  \\\n---per_device_train_batch_size 8 \\\n---per_device_eval_batch_size 16 \\\n---num_train_epochs 3 \\\n---do_train \\\n---do_eval\n-```\n\\ No newline at end of file"
        },
        {
            "sha": "99aff2bb32b2bb92f7628eb9bab4c7535d4c7f92",
            "filename": "examples/tensorflow/summarization/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fsummarization%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,3 +0,0 @@\n-datasets >= 1.4.0\n-tensorflow >= 2.3.0\n-evaluate >= 0.2.0\n\\ No newline at end of file"
        },
        {
            "sha": "bccbc8b2b6ddbd9026f7d89013c3266b219cbfb9",
            "filename": "examples/tensorflow/summarization/run_summarization.py",
            "status": "removed",
            "additions": 0,
            "deletions": 749,
            "changes": 749,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fsummarization%2Frun_summarization.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,749 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"\n-Fine-tuning the library models for summarization.\n-\"\"\"\n-# You can also adapt this script on your own sequence to sequence task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import datasets\n-import evaluate\n-import nltk  # Here to have a nice missing dependency error message early on\n-import numpy as np\n-import tensorflow as tf\n-from datasets import load_dataset\n-from filelock import FileLock\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    DataCollatorForSeq2Seq,\n-    HfArgumentParser,\n-    KerasMetricCallback,\n-    PushToHubCallback,\n-    TFAutoModelForSeq2SeqLM,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.trainer_utils import get_last_checkpoint\n-from transformers.utils import check_min_version, is_offline_mode, send_example_telemetry\n-from transformers.utils.versions import require_version\n-\n-\n-# region Checking dependencies\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/summarization/requirements.txt\")\n-\n-logger = logging.getLogger(__name__)\n-\n-try:\n-    nltk.data.find(\"tokenizers/punkt\")\n-except (LookupError, OSError):\n-    if is_offline_mode():\n-        raise LookupError(\n-            \"Offline mode: run this script without TRANSFORMERS_OFFLINE first to download nltk data files\"\n-        )\n-    with FileLock(\".lock\") as lock:\n-        nltk.download(\"punkt\", quiet=True)\n-# endregion\n-\n-\n-# region Arguments\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to trust the execution of code from datasets/models defined on the Hub.\"\n-                \" This option should only be set to `True` for repositories you trust and in which you have read the\"\n-                \" code, as it will execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-    \"\"\"\n-\n-    dataset_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The name of the dataset to use (via the datasets library).\"}\n-    )\n-    dataset_config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The configuration name of the dataset to use (via the datasets library).\"}\n-    )\n-    text_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the full texts (for summarization).\"},\n-    )\n-    summary_column: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"The name of the column in the datasets containing the summaries (for summarization).\"},\n-    )\n-    train_file: Optional[str] = field(\n-        default=None, metadata={\"help\": \"The input training data file (a jsonlines or csv file).\"}\n-    )\n-    validation_file: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"An optional input evaluation data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n-            )\n-        },\n-    )\n-    test_file: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"An optional input test data file to evaluate the metrics (rouge) on (a jsonlines or csv file).\"\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached training and evaluation sets\"}\n-    )\n-    preprocessing_num_workers: Optional[int] = field(\n-        default=None,\n-        metadata={\"help\": \"The number of processes to use for the preprocessing.\"},\n-    )\n-    max_source_length: Optional[int] = field(\n-        default=1024,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    max_target_length: Optional[int] = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    val_max_target_length: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total sequence length for validation target text after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded. Will default to `max_target_length`. \"\n-                \"This argument is also used to override the ``max_length`` param of ``model.generate``, which is used \"\n-                \"during ``evaluate`` and ``predict``.\"\n-            )\n-        },\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to model maximum sentence length. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch. More \"\n-                \"efficient on GPU but very bad for TPU.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    num_beams: Optional[int] = field(\n-        default=1,\n-        metadata={\n-            \"help\": (\n-                \"Number of beams to use for evaluation. This argument will be passed to ``model.generate``, \"\n-                \"which is used during ``evaluate`` and ``predict``.\"\n-            )\n-        },\n-    )\n-    ignore_pad_token_for_loss: bool = field(\n-        default=True,\n-        metadata={\n-            \"help\": \"Whether to ignore the tokens corresponding to padded labels in the loss computation or not.\"\n-        },\n-    )\n-    source_prefix: Optional[str] = field(\n-        default=None, metadata={\"help\": \"A prefix to add before every source text (useful for T5 models).\"}\n-    )\n-\n-    def __post_init__(self):\n-        if self.dataset_name is None and self.train_file is None and self.validation_file is None:\n-            raise ValueError(\"Need either a dataset name or a training/validation file.\")\n-        else:\n-            if self.train_file is not None:\n-                extension = self.train_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`train_file` should be a csv or a json file.\"\n-            if self.validation_file is not None:\n-                extension = self.validation_file.split(\".\")[-1]\n-                assert extension in [\"csv\", \"json\"], \"`validation_file` should be a csv or a json file.\"\n-        if self.val_max_target_length is None:\n-            self.val_max_target_length = self.max_target_length\n-\n-\n-# endregion\n-\n-# region Dataset name mappings\n-summarization_name_mapping = {\n-    \"amazon_reviews_multi\": (\"review_body\", \"review_title\"),\n-    \"big_patent\": (\"description\", \"abstract\"),\n-    \"cnn_dailymail\": (\"article\", \"highlights\"),\n-    \"orange_sum\": (\"text\", \"summary\"),\n-    \"pn_summary\": (\"article\", \"summary\"),\n-    \"psc\": (\"extract_text\", \"summary_text\"),\n-    \"samsum\": (\"dialogue\", \"summary\"),\n-    \"thaisum\": (\"body\", \"summary\"),\n-    \"xglue\": (\"news_body\", \"news_title\"),\n-    \"xsum\": (\"document\", \"summary\"),\n-    \"wiki_summary\": (\"article\", \"highlights\"),\n-    \"multi_news\": (\"document\", \"summary\"),\n-}\n-# endregion\n-\n-\n-def main():\n-    # region Argument parsing\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_summarization\", model_args, data_args, framework=\"tensorflow\")\n-    # endregion\n-\n-    # region Logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    logger.setLevel(logging.INFO)\n-    datasets.utils.logging.set_verbosity(logging.INFO)\n-    transformers.utils.logging.set_verbosity(logging.INFO)\n-\n-    # Log on each process the small summary:\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-    # endregion\n-\n-    # region T5 special-casing\n-    if data_args.source_prefix is None and model_args.model_name_or_path in [\n-        \"google-t5/t5-small\",\n-        \"google-t5/t5-base\",\n-        \"google-t5/t5-large\",\n-        \"google-t5/t5-3b\",\n-        \"google-t5/t5-11b\",\n-    ]:\n-        logger.warning(\n-            \"You're running a t5 model but didn't provide a source prefix, which is the expected, e.g. with \"\n-            \"`--source_prefix 'summarize: ' `\"\n-        )\n-    # endregion\n-\n-    # region Detecting last checkpoint\n-    last_checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        last_checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-    # endregion\n-\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # region Load datasets\n-    # Get the datasets: you can either provide your own CSV/JSON training and evaluation files (see below)\n-    # or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n-    # (the dataset will be downloaded automatically from the datasets Hub).\n-    #\n-    # For CSV/JSON files this script will use the first column for the full texts and the second column for the\n-    # summaries (unless you specify column names for this with the `text_column` and `summary_column` arguments).\n-    #\n-    # In distributed training, the load_dataset function guarantee that only one local process can concurrently\n-    # download the dataset.\n-    if data_args.dataset_name is not None:\n-        # Downloading and loading a dataset from the hub.\n-        raw_datasets = load_dataset(\n-            data_args.dataset_name,\n-            data_args.dataset_config_name,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-    else:\n-        data_files = {}\n-        if data_args.train_file is not None:\n-            data_files[\"train\"] = data_args.train_file\n-            extension = data_args.train_file.split(\".\")[-1]\n-        if data_args.validation_file is not None:\n-            data_files[\"validation\"] = data_args.validation_file\n-            extension = data_args.validation_file.split(\".\")[-1]\n-        if data_args.test_file is not None:\n-            data_files[\"test\"] = data_args.test_file\n-            extension = data_args.test_file.split(\".\")[-1]\n-        raw_datasets = load_dataset(\n-            extension,\n-            data_files=data_files,\n-            cache_dir=model_args.cache_dir,\n-            token=model_args.token,\n-        )\n-    # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-    # endregion\n-\n-    # region Load model config and tokenizer\n-    #\n-    # Distributed training:\n-    # The .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast_tokenizer,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-\n-    prefix = data_args.source_prefix if data_args.source_prefix is not None else \"\"\n-    # endregion\n-\n-    # region Dataset preprocessing\n-    # We need to tokenize inputs and targets.\n-    if training_args.do_train:\n-        column_names = raw_datasets[\"train\"].column_names\n-    elif training_args.do_eval:\n-        column_names = raw_datasets[\"validation\"].column_names\n-    else:\n-        logger.info(\"There is nothing to do. Please pass `do_train`, and/or `do_eval`.\")\n-        return\n-\n-    # Get the column names for input/target.\n-    dataset_columns = summarization_name_mapping.get(data_args.dataset_name, None)\n-    if data_args.text_column is None:\n-        text_column = dataset_columns[0] if dataset_columns is not None else column_names[0]\n-    else:\n-        text_column = data_args.text_column\n-        if text_column not in column_names:\n-            raise ValueError(\n-                f\"--text_column' value '{data_args.text_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-    if data_args.summary_column is None:\n-        summary_column = dataset_columns[1] if dataset_columns is not None else column_names[1]\n-    else:\n-        summary_column = data_args.summary_column\n-        if summary_column not in column_names:\n-            raise ValueError(\n-                f\"--summary_column' value '{data_args.summary_column}' needs to be one of: {', '.join(column_names)}\"\n-            )\n-\n-    # Temporarily set max_target_length for training.\n-    max_target_length = data_args.max_target_length\n-    padding = \"max_length\" if data_args.pad_to_max_length else False\n-\n-    def preprocess_function(examples):\n-        inputs = examples[text_column]\n-        targets = examples[summary_column]\n-        inputs = [prefix + inp for inp in inputs]\n-        model_inputs = tokenizer(inputs, max_length=data_args.max_source_length, padding=padding, truncation=True)\n-\n-        # Tokenize targets with the `text_target` keyword argument\n-        labels = tokenizer(text_target=targets, max_length=max_target_length, padding=padding, truncation=True)\n-\n-        # If we are padding here, replace all tokenizer.pad_token_id in the labels by -100 when we want to ignore\n-        # padding in the loss.\n-        if padding == \"max_length\" and data_args.ignore_pad_token_for_loss:\n-            labels[\"input_ids\"] = [\n-                [(l if l != tokenizer.pad_token_id else -100) for l in label] for label in labels[\"input_ids\"]\n-            ]\n-\n-        model_inputs[\"labels\"] = labels[\"input_ids\"]\n-        return model_inputs\n-\n-    if training_args.do_train:\n-        if \"train\" not in raw_datasets:\n-            raise ValueError(\"--do_train requires a train dataset\")\n-        train_dataset = raw_datasets[\"train\"]\n-        if data_args.max_train_samples is not None:\n-            max_train_samples = min(len(train_dataset), data_args.max_train_samples)\n-            train_dataset = train_dataset.select(range(max_train_samples))\n-        train_dataset = train_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on train dataset\",\n-        )\n-    else:\n-        train_dataset = None\n-\n-    if training_args.do_eval:\n-        max_target_length = data_args.val_max_target_length\n-        if \"validation\" not in raw_datasets:\n-            raise ValueError(\"--do_eval requires a validation dataset\")\n-        eval_dataset = raw_datasets[\"validation\"]\n-        if data_args.max_eval_samples is not None:\n-            max_eval_samples = min(len(eval_dataset), data_args.max_eval_samples)\n-            eval_dataset = eval_dataset.select(range(max_eval_samples))\n-        eval_dataset = eval_dataset.map(\n-            preprocess_function,\n-            batched=True,\n-            num_proc=data_args.preprocessing_num_workers,\n-            remove_columns=column_names,\n-            load_from_cache_file=not data_args.overwrite_cache,\n-            desc=\"Running tokenizer on validation dataset\",\n-        )\n-    else:\n-        eval_dataset = None\n-    # endregion\n-\n-    # region Text preprocessing\n-    def postprocess_text(preds, labels):\n-        preds = [pred.strip() for pred in preds]\n-        labels = [label.strip() for label in labels]\n-\n-        # rougeLSum expects newline after each sentence\n-        preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n-        labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n-\n-        return preds, labels\n-\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        # region Prepare model\n-        model = TFAutoModelForSeq2SeqLM.from_pretrained(\n-            model_args.model_name_or_path,\n-            config=config,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-\n-        # We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n-        # on a small vocab and want a smaller embedding size, remove this test.\n-        embeddings = model.get_input_embeddings()\n-\n-        # Matt: This is a temporary workaround as we transition our models to exclusively using Keras embeddings.\n-        #       As soon as the transition is complete, all embeddings should be keras.Embeddings layers, and\n-        #       the weights will always be in embeddings.embeddings.\n-        if hasattr(embeddings, \"embeddings\"):\n-            embedding_size = embeddings.embeddings.shape[0]\n-        else:\n-            embedding_size = embeddings.weight.shape[0]\n-        if len(tokenizer) > embedding_size:\n-            model.resize_token_embeddings(len(tokenizer))\n-        # endregion\n-\n-        # region Prepare TF Dataset objects\n-        if model.config.decoder_start_token_id is None:\n-            raise ValueError(\"Make sure that `config.decoder_start_token_id` is correctly defined\")\n-\n-        label_pad_token_id = -100 if data_args.ignore_pad_token_for_loss else tokenizer.pad_token_id\n-        data_collator = DataCollatorForSeq2Seq(\n-            tokenizer,\n-            model=model,\n-            label_pad_token_id=label_pad_token_id,\n-            pad_to_multiple_of=128,  # Reduce the number of unique shapes for XLA, especially for generation\n-            return_tensors=\"np\",\n-        )\n-\n-        dataset_options = tf.data.Options()\n-        dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-        total_train_batch_size = training_args.per_device_train_batch_size * num_replicas\n-        total_eval_batch_size = training_args.per_device_eval_batch_size * num_replicas\n-\n-        # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-        # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-        # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-        # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-        # using model.prepare_tf_dataset()\n-        # For more info see the docs:\n-        # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-        # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-\n-        tf_train_dataset = model.prepare_tf_dataset(\n-            train_dataset,\n-            collate_fn=data_collator,\n-            batch_size=total_train_batch_size,\n-            shuffle=True,\n-        ).with_options(dataset_options)\n-        tf_eval_dataset = model.prepare_tf_dataset(\n-            eval_dataset,\n-            collate_fn=data_collator,\n-            batch_size=total_eval_batch_size,\n-            shuffle=False,\n-        ).with_options(dataset_options)\n-        # endregion\n-\n-        # region Optimizer, loss and LR scheduling\n-        num_train_steps = int(len(tf_train_dataset) * training_args.num_train_epochs)\n-        if training_args.warmup_steps > 0:\n-            num_warmup_steps = training_args.warmup_steps\n-        elif training_args.warmup_ratio > 0:\n-            num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-        else:\n-            num_warmup_steps = 0\n-        if training_args.do_train:\n-            optimizer, lr_schedule = create_optimizer(\n-                init_lr=training_args.learning_rate,\n-                num_train_steps=num_train_steps,\n-                num_warmup_steps=num_warmup_steps,\n-                adam_beta1=training_args.adam_beta1,\n-                adam_beta2=training_args.adam_beta2,\n-                adam_epsilon=training_args.adam_epsilon,\n-                weight_decay_rate=training_args.weight_decay,\n-                adam_global_clipnorm=training_args.max_grad_norm,\n-            )\n-        else:\n-            optimizer = \"sgd\"  # Just write anything because we won't be using it\n-\n-        # endregion\n-\n-        # region Metric and KerasMetricCallback\n-        if training_args.do_eval:\n-            metric = evaluate.load(\"rouge\", cache_dir=model_args.cache_dir)\n-\n-            if data_args.val_max_target_length is None:\n-                data_args.val_max_target_length = data_args.max_target_length\n-\n-            gen_kwargs = {\n-                \"max_length\": data_args.val_max_target_length if data_args is not None else config.max_length,\n-                \"num_beams\": data_args.num_beams,\n-                \"no_repeat_ngram_size\": 0,  # Not supported under XLA right now, and some models set it by default\n-            }\n-\n-            def compute_metrics(preds):\n-                predictions, labels = preds\n-                if isinstance(predictions, tuple):\n-                    predictions = predictions[0]\n-                decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n-                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n-                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-                decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n-                metrics = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n-                # Only print the mid f-measures, but there are a lot of other statistics in there too!\n-                metrics = {key: round(val.mid.fmeasure * 100, 4) for key, val in metrics.items()}\n-                return metrics\n-\n-            # The KerasMetricCallback allows metrics that are too complex to write as standard Keras metrics\n-            # to be computed each epoch. Any Python code can be included in the metric_fn. This is especially\n-            # useful for metrics like BLEU and ROUGE that perform string comparisons on decoded model outputs.\n-            # For more information, see the docs at\n-            # https://huggingface.co/docs/transformers/main_classes/keras_callbacks#transformers.KerasMetricCallback\n-\n-            metric_callback = KerasMetricCallback(\n-                metric_fn=compute_metrics,\n-                eval_dataset=tf_eval_dataset,\n-                predict_with_generate=True,\n-                use_xla_generation=True,\n-                generate_kwargs=gen_kwargs,\n-            )\n-            callbacks = [metric_callback]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            if data_args.dataset_name is not None:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-{data_args.dataset_name}\"\n-            else:\n-                push_to_hub_model_id = f\"{model_name}-finetuned-summarization\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"summarization\"}\n-        if data_args.dataset_name is not None:\n-            model_card_kwargs[\"dataset_tags\"] = data_args.dataset_name\n-            if data_args.dataset_config_name is not None:\n-                model_card_kwargs[\"dataset_args\"] = data_args.dataset_config_name\n-                model_card_kwargs[\"dataset\"] = f\"{data_args.dataset_name} {data_args.dataset_config_name}\"\n-            else:\n-                model_card_kwargs[\"dataset\"] = data_args.dataset_name\n-\n-        if training_args.push_to_hub:\n-            # Because this training can be quite long, we save once per epoch.\n-            callbacks.append(\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            )\n-        # endregion\n-\n-        # region Training\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, jit_compile=training_args.xla)\n-        eval_metrics = None\n-        if training_args.do_train:\n-            logger.info(\"***** Running training *****\")\n-            logger.info(f\"  Num examples = {len(train_dataset)}\")\n-            logger.info(f\"  Num Epochs = {training_args.num_train_epochs}\")\n-            logger.info(f\"  Instantaneous batch size per device = {training_args.per_device_train_batch_size}\")\n-            logger.info(f\"  Total train batch size = {total_train_batch_size}\")\n-            logger.info(f\"  Total optimization steps = {num_train_steps}\")\n-\n-            if training_args.xla and not data_args.pad_to_max_length:\n-                logger.warning(\n-                    \"XLA training may be slow at first when --pad_to_max_length is not set \"\n-                    \"until all possible shapes have been compiled.\"\n-                )\n-            history = model.fit(tf_train_dataset, epochs=int(training_args.num_train_epochs), callbacks=callbacks)\n-            eval_metrics = {key: val[-1] for key, val in history.history.items()}\n-        # endregion\n-\n-        # region Validation\n-\n-        if training_args.do_eval and not training_args.do_train:\n-            # Do a standalone evaluation run\n-            logger.info(\"Evaluation...\")\n-\n-            # Compiling generation with XLA yields enormous speedups, see https://huggingface.co/blog/tf-xla-generate\n-            @tf.function(jit_compile=True)\n-            def generate(**kwargs):\n-                return model.generate(**kwargs)\n-\n-            for batch, labels in tf_eval_dataset:\n-                batch.update(gen_kwargs)\n-                generated_tokens = generate(**batch)\n-                if isinstance(generated_tokens, tuple):\n-                    generated_tokens = generated_tokens[0]\n-                decoded_preds = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n-                labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n-                decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n-                decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n-\n-                metric.add_batch(predictions=decoded_preds, references=decoded_labels)\n-\n-            eval_metrics = metric.compute(use_stemmer=True)\n-\n-            result = {key: round(val.mid.fmeasure * 100, 4) for key, val in eval_metrics.items()}\n-            logger.info(result)\n-        # endregion\n-\n-        if training_args.output_dir is not None and eval_metrics is not None:\n-            output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-            with open(output_eval_file, \"w\") as writer:\n-                writer.write(json.dumps(eval_metrics))\n-\n-        if training_args.output_dir is not None and not training_args.push_to_hub:\n-            # If we're not pushing to hub, at least save a local copy when we're done\n-            model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "03d0e32def07f46cf7b982a2b4b55f32c243026b",
            "filename": "examples/tensorflow/test_tensorflow_examples.py",
            "status": "removed",
            "additions": 0,
            "deletions": 336,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftest_tensorflow_examples.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftest_tensorflow_examples.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,336 +0,0 @@\n-# Copyright 2022 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import argparse\n-import json\n-import logging\n-import os\n-import sys\n-from unittest import skip\n-from unittest.mock import patch\n-\n-import tensorflow as tf\n-from packaging.version import parse\n-\n-\n-try:\n-    import tf_keras as keras\n-except (ModuleNotFoundError, ImportError):\n-    import keras\n-\n-    if parse(keras.__version__).major > 2:\n-        raise ValueError(\n-            \"Your currently installed version of Keras is Keras 3, but this is not yet supported in \"\n-            \"Transformers. Please install the backwards-compatible tf-keras package with \"\n-            \"`pip install tf-keras`.\"\n-        )\n-\n-from transformers.testing_utils import TestCasePlus, get_gpu_count, slow\n-\n-\n-SRC_DIRS = [\n-    os.path.join(os.path.dirname(__file__), dirname)\n-    for dirname in [\n-        \"text-generation\",\n-        \"text-classification\",\n-        \"token-classification\",\n-        \"language-modeling\",\n-        \"multiple-choice\",\n-        \"question-answering\",\n-        \"summarization\",\n-        \"translation\",\n-        \"image-classification\",\n-    ]\n-]\n-sys.path.extend(SRC_DIRS)\n-\n-\n-if SRC_DIRS is not None:\n-    import run_clm\n-    import run_image_classification\n-    import run_mlm\n-    import run_ner\n-    import run_qa as run_squad\n-    import run_summarization\n-    import run_swag\n-    import run_text_classification\n-    import run_translation\n-\n-\n-logging.basicConfig(level=logging.DEBUG)\n-\n-logger = logging.getLogger()\n-\n-\n-def get_setup_file():\n-    parser = argparse.ArgumentParser()\n-    parser.add_argument(\"-f\")\n-    args = parser.parse_args()\n-    return args.f\n-\n-\n-def get_results(output_dir):\n-    results = {}\n-    path = os.path.join(output_dir, \"all_results.json\")\n-    if os.path.exists(path):\n-        with open(path) as f:\n-            results = json.load(f)\n-    else:\n-        raise ValueError(f\"can't find {path}\")\n-    return results\n-\n-\n-def is_cuda_available():\n-    return bool(tf.config.list_physical_devices(\"GPU\"))\n-\n-\n-stream_handler = logging.StreamHandler(sys.stdout)\n-logger.addHandler(stream_handler)\n-\n-\n-class ExamplesTests(TestCasePlus):\n-    @skip(\"Skipping until shape inference for to_tf_dataset PR is merged.\")\n-    def test_run_text_classification(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_text_classification.py\n-            --model_name_or_path distilbert/distilbert-base-uncased\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --train_file ./tests/fixtures/tests_samples/MRPC/train.csv\n-            --validation_file ./tests/fixtures/tests_samples/MRPC/dev.csv\n-            --do_train\n-            --do_eval\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-            --learning_rate=1e-4\n-            --max_steps=10\n-            --warmup_steps=2\n-            --seed=42\n-            --max_seq_length=128\n-            \"\"\".split()\n-\n-        if is_cuda_available():\n-            testargs.append(\"--fp16\")\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_text_classification.main()\n-            # Reset the mixed precision policy so we don't break other tests\n-            keras.mixed_precision.set_global_policy(\"float32\")\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"eval_accuracy\"], 0.75)\n-\n-    def test_run_clm(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_clm.py\n-            --model_name_or_path distilbert/distilgpt2\n-            --train_file ./tests/fixtures/sample_text.txt\n-            --validation_file ./tests/fixtures/sample_text.txt\n-            --do_train\n-            --do_eval\n-            --block_size 128\n-            --per_device_train_batch_size 2\n-            --per_device_eval_batch_size 1\n-            --num_train_epochs 2\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            \"\"\".split()\n-\n-        if len(tf.config.list_physical_devices(\"GPU\")) > 1:\n-            # Skipping because there are not enough batches to train the model + would need a drop_last to work.\n-            return\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_clm.main()\n-            result = get_results(tmp_dir)\n-            self.assertLess(result[\"eval_perplexity\"], 100)\n-\n-    def test_run_mlm(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_mlm.py\n-            --model_name_or_path distilbert/distilroberta-base\n-            --train_file ./tests/fixtures/sample_text.txt\n-            --validation_file ./tests/fixtures/sample_text.txt\n-            --max_seq_length 64\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --do_train\n-            --do_eval\n-            --prediction_loss_only\n-            --num_train_epochs=1\n-            --learning_rate=1e-4\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_mlm.main()\n-            result = get_results(tmp_dir)\n-            self.assertLess(result[\"eval_perplexity\"], 42)\n-\n-    def test_run_ner(self):\n-        # with so little data distributed training needs more epochs to get the score on par with 0/1 gpu\n-        epochs = 7 if get_gpu_count() > 1 else 2\n-\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_ner.py\n-            --model_name_or_path google-bert/bert-base-uncased\n-            --train_file tests/fixtures/tests_samples/conll/sample.json\n-            --validation_file tests/fixtures/tests_samples/conll/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --do_train\n-            --do_eval\n-            --warmup_steps=2\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=2\n-            --num_train_epochs={epochs}\n-            --seed 7\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_ner.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"accuracy\"], 0.75)\n-\n-    def test_run_squad(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_qa.py\n-            --model_name_or_path google-bert/bert-base-uncased\n-            --version_2_with_negative\n-            --train_file tests/fixtures/tests_samples/SQUAD/sample.json\n-            --validation_file tests/fixtures/tests_samples/SQUAD/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --max_steps=10\n-            --warmup_steps=2\n-            --do_train\n-            --do_eval\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_squad.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"f1\"], 30)\n-            self.assertGreaterEqual(result[\"exact\"], 30)\n-\n-    def test_run_swag(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_swag.py\n-            --model_name_or_path google-bert/bert-base-uncased\n-            --train_file tests/fixtures/tests_samples/swag/sample.json\n-            --validation_file tests/fixtures/tests_samples/swag/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --max_steps=20\n-            --warmup_steps=2\n-            --do_train\n-            --do_eval\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_swag.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"val_accuracy\"], 0.8)\n-\n-    @slow\n-    def test_run_summarization(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_summarization.py\n-            --model_name_or_path google-t5/t5-small\n-            --train_file tests/fixtures/tests_samples/xsum/sample.json\n-            --validation_file tests/fixtures/tests_samples/xsum/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --max_steps=50\n-            --warmup_steps=8\n-            --do_train\n-            --do_eval\n-            --learning_rate=2e-4\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_summarization.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"rouge1\"], 10)\n-            self.assertGreaterEqual(result[\"rouge2\"], 2)\n-            self.assertGreaterEqual(result[\"rougeL\"], 7)\n-            self.assertGreaterEqual(result[\"rougeLsum\"], 7)\n-\n-    @slow\n-    def test_run_translation(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_translation.py\n-            --model_name_or_path Rocketknight1/student_marian_en_ro_6_1\n-            --source_lang en\n-            --target_lang ro\n-            --train_file tests/fixtures/tests_samples/wmt16/sample.json\n-            --validation_file tests/fixtures/tests_samples/wmt16/sample.json\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --warmup_steps=8\n-            --do_train\n-            --do_eval\n-            --learning_rate=3e-3\n-            --num_train_epochs 12\n-            --per_device_train_batch_size=2\n-            --per_device_eval_batch_size=1\n-            --source_lang en_XX\n-            --target_lang ro_RO\n-        \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_translation.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"bleu\"], 30)\n-\n-    def test_run_image_classification(self):\n-        tmp_dir = self.get_auto_remove_tmp_dir()\n-        testargs = f\"\"\"\n-            run_image_classification.py\n-            --dataset_name hf-internal-testing/cats_vs_dogs_sample\n-            --model_name_or_path microsoft/resnet-18\n-            --do_train\n-            --do_eval\n-            --learning_rate 1e-4\n-            --per_device_train_batch_size 2\n-            --per_device_eval_batch_size 1\n-            --output_dir {tmp_dir}\n-            --overwrite_output_dir\n-            --dataloader_num_workers 16\n-            --num_train_epochs 2\n-            --train_val_split 0.1\n-            --seed 42\n-            --ignore_mismatched_sizes True\n-            \"\"\".split()\n-\n-        with patch.object(sys, \"argv\", testargs):\n-            run_image_classification.main()\n-            result = get_results(tmp_dir)\n-            self.assertGreaterEqual(result[\"accuracy\"], 0.7)"
        },
        {
            "sha": "08d0324b51dd94729716aaa3c52b85ceb7259ec8",
            "filename": "examples/tensorflow/text-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 115,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,115 +0,0 @@\n-<!---\n-Copyright 2021 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\");\n-you may not use this file except in compliance with the License.\n-You may obtain a copy of the License at\n-\n-    http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software\n-distributed under the License is distributed on an \"AS IS\" BASIS,\n-WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-See the License for the specific language governing permissions and\n-limitations under the License.\n--->\n-\n-# Text classification examples\n-\n-This folder contains some scripts showing examples of *text classification* with the ðŸ¤— Transformers library.\n-For straightforward use-cases you may be able to use these scripts without modification, although we have also\n-included comments in the code to indicate areas that you may need to adapt to your own projects.\n-\n-## run_text_classification.py\n-\n-This script handles perhaps the single most common use-case for this entire library: Training an NLP classifier\n-on your own training data. This can be whatever you want - you could classify text as abusive/hateful or \n-allowable, or forum posts as spam or not-spam, or classify the genre of a headline as politics, sports or any \n-number of other categories. Any task that involves classifying natural language into two or more different categories \n-can work with this! You can even do regression, such as predicting the score on a 1-10 scale that a user gave,\n-given the text of their review.\n-\n-The preferred input format is either a CSV or newline-delimited JSON file that contains a `sentence1` and \n-`label` field. If your task involves comparing two texts (for example, if your classifier\n-is deciding whether two sentences are paraphrases of each other, or were written by the same author) then you should also include a `sentence2` field in each example. If you do not have a `sentence1` field then the script will assume the non-label fields are the input text, which\n-may not always be what you want, especially if you have more than two fields! \n-\n-Here is a snippet of a valid input JSON file, though note that your texts can be much longer than these, and are not constrained\n-(despite the field name) to being single grammatical sentences:\n-```json\n-{\"sentence1\": \"COVID-19 vaccine updates: How is the rollout proceeding?\", \"label\": \"news\"}\n-{\"sentence1\": \"Manchester United celebrates Europa League success\", \"label\": \"sports\"}\n-```\n-\n-### Usage notes\n-If your inputs are long (more than ~60-70 words), you may wish to increase the `--max_seq_length` argument\n-beyond the default value of 128. The maximum supported value for most models is 512 (about 200-300 words), \n-and some can handle even longer. This will come at a cost in runtime and memory use, however.\n-\n-We assume that your labels represent *categories*, even if they are integers, since text classification\n-is a much more common task than text regression. If your labels are floats, however, the script will assume\n-you want to do regression. This is something you can edit yourself if your use-case requires it!\n-\n-After training, the model will be saved to `--output_dir`. Once your model is trained, you can get predictions\n-by calling the script without a `--train_file` or `--validation_file`; simply pass it the output_dir containing\n-the trained model and a `--test_file` and it will write its predictions to a text file for you.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument.\n-\n-### Memory usage and data loading\n-\n-One thing to note is that all data is loaded into memory in this script. Most text classification datasets are small\n-enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\n-data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\n-required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and \n-README, but for more information you can see the 'Input Datasets' section of \n-[this document](https://www.tensorflow.org/guide/tpu).\n-\n-### Example command\n-```bash\n-python run_text_classification.py \\\n---model_name_or_path distilbert/distilbert-base-cased \\\n---train_file training_data.json \\\n---validation_file validation_data.json \\\n---output_dir output/ \\\n---test_file data_to_predict.json \\\n---do_train \\\n---do_eval \\\n---do_predict\n-```\n-\n-## run_glue.py\n-\n-This script handles training on the GLUE dataset for various text classification and regression tasks. The GLUE datasets will be loaded automatically, so you only need to specify the task you want (with the `--task_name` argument). You can also supply your own files for prediction with the `--predict_file` argument, for example if you want to train a model on GLUE for e.g. paraphrase detection and then predict whether your own data contains paraphrases or not. Please ensure the names of your input fields match the names of the features in the relevant GLUE dataset - you can see a list of the column names in the `task_to_keys` dict in the `run_glue.py` file.\n-\n-### Usage notes\n-\n-The `--do_train`, `--do_eval` and `--do_predict` arguments control whether training, evaluations or predictions are performed. After training, the model will be saved to `--output_dir`. Once your model is trained, you can call the script without the `--do_train` or `--do_eval` arguments to quickly get predictions from your saved model.\n-\n-### Multi-GPU and TPU usage\n-\n-By default, the script uses a `MirroredStrategy` and will use multiple GPUs effectively if they are available. TPUs\n-can also be used by passing the name of the TPU resource with the `--tpu` argument.\n-\n-### Memory usage and data loading\n-\n-One thing to note is that all data is loaded into memory in this script. Most text classification datasets are small\n-enough that this is not an issue, but if you have a very large dataset you will need to modify the script to handle\n-data streaming. This is particularly challenging for TPUs, given the stricter requirements and the sheer volume of data\n-required to keep them fed. A full explanation of all the possible pitfalls is a bit beyond this example script and \n-README, but for more information you can see the 'Input Datasets' section of \n-[this document](https://www.tensorflow.org/guide/tpu).\n-\n-### Example command\n-```bash\n-python run_glue.py \\\n---model_name_or_path distilbert/distilbert-base-cased \\\n---task_name mnli \\\n---do_train \\\n---do_eval \\\n---do_predict \\\n---predict_file data_to_predict.json\n-```"
        },
        {
            "sha": "494a82127ab06d3e2b49cd956117180fe1216a64",
            "filename": "examples/tensorflow/text-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,5 +0,0 @@\n-datasets >= 1.1.3\n-sentencepiece != 0.1.92\n-protobuf\n-tensorflow >= 2.3\n-evaluate >= 0.2.0\n\\ No newline at end of file"
        },
        {
            "sha": "74b263a7ff8af25fd246d09ce91f2998470662c9",
            "filename": "examples/tensorflow/text-classification/run_glue.py",
            "status": "removed",
            "additions": 0,
            "deletions": 599,
            "changes": 599,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b",
            "patch": "@@ -1,599 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Finetuning the library models for sequence classification on GLUE.\"\"\"\n-# You can also adapt this script on your own text classification task. Pointers for this are left as comments.\n-\n-import json\n-import logging\n-import os\n-import sys\n-from dataclasses import dataclass, field\n-from typing import Optional\n-\n-import evaluate\n-import numpy as np\n-import tensorflow as tf\n-from datasets import load_dataset\n-\n-import transformers\n-from transformers import (\n-    AutoConfig,\n-    AutoTokenizer,\n-    DataCollatorWithPadding,\n-    DefaultDataCollator,\n-    HfArgumentParser,\n-    PretrainedConfig,\n-    PushToHubCallback,\n-    TFAutoModelForSequenceClassification,\n-    TFTrainingArguments,\n-    create_optimizer,\n-    set_seed,\n-)\n-from transformers.trainer_utils import get_last_checkpoint, is_main_process\n-from transformers.utils import check_min_version, send_example_telemetry\n-\n-\n-# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n-check_min_version(\"4.57.0.dev0\")\n-\n-task_to_keys = {\n-    \"cola\": (\"sentence\", None),\n-    \"mnli\": (\"premise\", \"hypothesis\"),\n-    \"mrpc\": (\"sentence1\", \"sentence2\"),\n-    \"qnli\": (\"question\", \"sentence\"),\n-    \"qqp\": (\"question1\", \"question2\"),\n-    \"rte\": (\"sentence1\", \"sentence2\"),\n-    \"sst2\": (\"sentence\", None),\n-    \"stsb\": (\"sentence1\", \"sentence2\"),\n-    \"wnli\": (\"sentence1\", \"sentence2\"),\n-}\n-\n-logger = logging.getLogger(__name__)\n-\n-\n-# region Command-line arguments\n-@dataclass\n-class DataTrainingArguments:\n-    \"\"\"\n-    Arguments pertaining to what data we are going to input our model for training and eval.\n-\n-    Using `HfArgumentParser` we can turn this class\n-    into argparse arguments to be able to specify them on\n-    the command line.\n-    \"\"\"\n-\n-    task_name: str = field(\n-        metadata={\"help\": \"The name of the task to train on: \" + \", \".join(task_to_keys.keys())},\n-    )\n-    predict_file: str = field(\n-        metadata={\"help\": \"A file containing user-supplied examples to make predictions for\"},\n-        default=None,\n-    )\n-    max_seq_length: int = field(\n-        default=128,\n-        metadata={\n-            \"help\": (\n-                \"The maximum total input sequence length after tokenization. Sequences longer \"\n-                \"than this will be truncated, sequences shorter will be padded.\"\n-            )\n-        },\n-    )\n-    overwrite_cache: bool = field(\n-        default=False, metadata={\"help\": \"Overwrite the cached preprocessed datasets or not.\"}\n-    )\n-    pad_to_max_length: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to pad all samples to `max_seq_length`. \"\n-                \"If False, will pad the samples dynamically when batching to the maximum length in the batch.\"\n-            )\n-        },\n-    )\n-    max_train_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of training examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_eval_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of evaluation examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-    max_predict_samples: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"For debugging purposes or quicker training, truncate the number of prediction examples to this \"\n-                \"value if set.\"\n-            )\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        self.task_name = self.task_name.lower()\n-        if self.task_name not in task_to_keys:\n-            raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n-\n-\n-@dataclass\n-class ModelArguments:\n-    \"\"\"\n-    Arguments pertaining to which model/config/tokenizer we are going to fine-tune from.\n-    \"\"\"\n-\n-    model_name_or_path: str = field(\n-        metadata={\"help\": \"Path to pretrained model or model identifier from huggingface.co/models\"}\n-    )\n-    config_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained config name or path if not the same as model_name\"}\n-    )\n-    tokenizer_name: Optional[str] = field(\n-        default=None, metadata={\"help\": \"Pretrained tokenizer name or path if not the same as model_name\"}\n-    )\n-    cache_dir: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Where do you want to store the pretrained models downloaded from huggingface.co\"},\n-    )\n-    use_fast_tokenizer: bool = field(\n-        default=True,\n-        metadata={\"help\": \"Whether to use one of the fast tokenizer (backed by the tokenizers library) or not.\"},\n-    )\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"The specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    token: str = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"The token to use as HTTP bearer authorization for remote files. If not specified, will use the token \"\n-                \"generated when running `hf auth login` (stored in `~/.huggingface`).\"\n-            )\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether or not to allow for custom models defined on the Hub in their own modeling files. This option \"\n-                \"should only be set to `True` for repositories you trust and in which you have read the code, as it will \"\n-                \"execute code present on the Hub on your local machine.\"\n-            )\n-        },\n-    )\n-\n-\n-# endregion\n-\n-\n-def main():\n-    # region Argument parsing\n-    # See all possible arguments in src/transformers/training_args.py\n-    # or by passing the --help flag to this script.\n-    # We now keep distinct sets of args, for a cleaner separation of concerns.\n-\n-    parser = HfArgumentParser((ModelArguments, DataTrainingArguments, TFTrainingArguments))\n-    if len(sys.argv) == 2 and sys.argv[1].endswith(\".json\"):\n-        # If we pass only one argument to the script and it's the path to a json file,\n-        # let's parse it to get our arguments.\n-        model_args, data_args, training_args = parser.parse_json_file(json_file=os.path.abspath(sys.argv[1]))\n-    else:\n-        model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n-\n-    # Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n-    # information sent is the one passed as arguments along with your Python/PyTorch versions.\n-    send_example_telemetry(\"run_glue\", model_args, data_args, framework=\"tensorflow\")\n-\n-    if not (training_args.do_train or training_args.do_eval or training_args.do_predict):\n-        exit(\"Must specify at least one of --do_train, --do_eval or --do_predict!\")\n-    # endregion\n-\n-    # region Checkpoints\n-    checkpoint = None\n-    if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n-        checkpoint = get_last_checkpoint(training_args.output_dir)\n-        if checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n-            raise ValueError(\n-                f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n-                \"Use --overwrite_output_dir to overcome.\"\n-            )\n-        elif checkpoint is not None and training_args.resume_from_checkpoint is None:\n-            logger.info(\n-                f\"Checkpoint detected, resuming training at {checkpoint}. To avoid this behavior, change \"\n-                \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n-            )\n-    # endregion\n-\n-    # region Logging\n-    logging.basicConfig(\n-        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n-        datefmt=\"%m/%d/%Y %H:%M:%S\",\n-        handlers=[logging.StreamHandler(sys.stdout)],\n-    )\n-    logger.setLevel(logging.INFO if is_main_process(training_args.local_rank) else logging.WARN)\n-\n-    # Set the verbosity to info of the Transformers logger (on main process only):\n-    if is_main_process(training_args.local_rank):\n-        transformers.utils.logging.set_verbosity_info()\n-        transformers.utils.logging.enable_default_handler()\n-        transformers.utils.logging.enable_explicit_format()\n-    logger.info(f\"Training/evaluation parameters {training_args}\")\n-    # endregion\n-\n-    # region Dataset and labels\n-    # Set seed before initializing model.\n-    set_seed(training_args.seed)\n-\n-    # Downloading and loading a dataset from the hub. In distributed training, the load_dataset function guarantee\n-    # that only one local process can concurrently download the dataset.\n-    datasets = load_dataset(\n-        \"nyu-mll/glue\",\n-        data_args.task_name,\n-        cache_dir=model_args.cache_dir,\n-        token=model_args.token,\n-    )\n-    # See more about loading any type of standard or custom dataset at\n-    # https://huggingface.co/docs/datasets/loading_datasets.\n-\n-    is_regression = data_args.task_name == \"stsb\"\n-    if not is_regression:\n-        label_list = datasets[\"train\"].features[\"label\"].names\n-        num_labels = len(label_list)\n-    else:\n-        num_labels = 1\n-\n-    if data_args.predict_file is not None:\n-        logger.info(\"Preparing user-supplied file for predictions...\")\n-\n-        data_files = {\"data\": data_args.predict_file}\n-\n-        for key in data_files:\n-            logger.info(f\"Loading a local file for {key}: {data_files[key]}\")\n-\n-        if data_args.predict_file.endswith(\".csv\"):\n-            # Loading a dataset from local csv files\n-            user_dataset = load_dataset(\"csv\", data_files=data_files, cache_dir=model_args.cache_dir)\n-        else:\n-            # Loading a dataset from local json files\n-            user_dataset = load_dataset(\"json\", data_files=data_files, cache_dir=model_args.cache_dir)\n-        needed_keys = task_to_keys[data_args.task_name]\n-        for key in needed_keys:\n-            assert key in user_dataset[\"data\"].features, f\"Your supplied predict_file is missing the {key} key!\"\n-        datasets[\"user_data\"] = user_dataset[\"data\"]\n-    # endregion\n-\n-    # region Load model config and tokenizer\n-    #\n-    # In distributed training, the .from_pretrained methods guarantee that only one local process can concurrently\n-    # download model & vocab.\n-    config = AutoConfig.from_pretrained(\n-        model_args.config_name if model_args.config_name else model_args.model_name_or_path,\n-        num_labels=num_labels,\n-        finetuning_task=data_args.task_name,\n-        cache_dir=model_args.cache_dir,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    tokenizer = AutoTokenizer.from_pretrained(\n-        model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n-        cache_dir=model_args.cache_dir,\n-        use_fast=model_args.use_fast_tokenizer,\n-        revision=model_args.model_revision,\n-        token=model_args.token,\n-        trust_remote_code=model_args.trust_remote_code,\n-    )\n-    # endregion\n-\n-    # region Dataset preprocessing\n-    sentence1_key, sentence2_key = task_to_keys[data_args.task_name]\n-\n-    # Padding strategy\n-    if data_args.pad_to_max_length:\n-        padding = \"max_length\"\n-    else:\n-        # We will pad later, dynamically at batch creation, to the max sequence length in each batch\n-        padding = False\n-\n-    # Some models have set the order of the labels to use, so let's make sure we do use it.\n-    label_to_id = None\n-    if config.label2id != PretrainedConfig(num_labels=num_labels).label2id and not is_regression:\n-        # Some have all caps in their config, some don't.\n-        label_name_to_id = {k.lower(): v for k, v in config.label2id.items()}\n-        if sorted(label_name_to_id.keys()) == sorted(label_list):\n-            label_to_id = {i: int(label_name_to_id[label_list[i]]) for i in range(num_labels)}\n-        else:\n-            logger.warning(\n-                \"Your model seems to have been trained with labels, but they don't match the dataset: \"\n-                f\"model labels: {sorted(label_name_to_id.keys())}, dataset labels: {sorted(label_list)}.\"\n-                \"\\nIgnoring the model labels as a result.\",\n-            )\n-            label_to_id = {label: i for i, label in enumerate(label_list)}\n-    if label_to_id is not None:\n-        config.label2id = label_to_id\n-        config.id2label = {id: label for label, id in config.label2id.items()}\n-    elif data_args.task_name is not None and not is_regression:\n-        config.label2id = {l: i for i, l in enumerate(label_list)}\n-        config.id2label = {id: label for label, id in config.label2id.items()}\n-\n-    if data_args.max_seq_length > tokenizer.model_max_length:\n-        logger.warning(\n-            f\"The max_seq_length passed ({data_args.max_seq_length}) is larger than the maximum length for the \"\n-            f\"model ({tokenizer.model_max_length}). Using max_seq_length={tokenizer.model_max_length}.\"\n-        )\n-    max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n-\n-    def preprocess_function(examples):\n-        # Tokenize the texts\n-        args = (\n-            (examples[sentence1_key],) if sentence2_key is None else (examples[sentence1_key], examples[sentence2_key])\n-        )\n-        result = tokenizer(*args, padding=padding, max_length=max_seq_length, truncation=True)\n-\n-        return result\n-\n-    datasets = datasets.map(preprocess_function, batched=True, load_from_cache_file=not data_args.overwrite_cache)\n-\n-    if data_args.pad_to_max_length:\n-        data_collator = DefaultDataCollator(return_tensors=\"np\")\n-    else:\n-        data_collator = DataCollatorWithPadding(tokenizer, return_tensors=\"np\")\n-    # endregion\n-\n-    # region Metric function\n-    metric = evaluate.load(\"glue\", data_args.task_name, cache_dir=model_args.cache_dir)\n-\n-    def compute_metrics(preds, label_ids):\n-        preds = preds[\"logits\"]\n-        preds = np.squeeze(preds) if is_regression else np.argmax(preds, axis=1)\n-        result = metric.compute(predictions=preds, references=label_ids)\n-        if len(result) > 1:\n-            result[\"combined_score\"] = np.mean(list(result.values())).item()\n-        return result\n-\n-    # endregion\n-\n-    with training_args.strategy.scope():\n-        # region Load pretrained model\n-        if checkpoint is None:\n-            model_path = model_args.model_name_or_path\n-        else:\n-            model_path = checkpoint\n-        model = TFAutoModelForSequenceClassification.from_pretrained(\n-            model_path,\n-            config=config,\n-            cache_dir=model_args.cache_dir,\n-            revision=model_args.model_revision,\n-            token=model_args.token,\n-            trust_remote_code=model_args.trust_remote_code,\n-        )\n-        # endregion\n-\n-        # region Convert data to a tf.data.Dataset\n-        dataset_options = tf.data.Options()\n-        dataset_options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n-        num_replicas = training_args.strategy.num_replicas_in_sync\n-\n-        tf_data = {}\n-        max_samples = {\n-            \"train\": data_args.max_train_samples,\n-            \"validation\": data_args.max_eval_samples,\n-            \"validation_matched\": data_args.max_eval_samples,\n-            \"validation_mismatched\": data_args.max_eval_samples,\n-            \"test\": data_args.max_predict_samples,\n-            \"test_matched\": data_args.max_predict_samples,\n-            \"test_mismatched\": data_args.max_predict_samples,\n-            \"user_data\": None,\n-        }\n-        for key in datasets:\n-            if key == \"train\" or key.startswith(\"validation\"):\n-                assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n-            if key == \"train\":\n-                shuffle = True\n-                batch_size = training_args.per_device_train_batch_size * num_replicas\n-            else:\n-                shuffle = False\n-                batch_size = training_args.per_device_eval_batch_size * num_replicas\n-            samples_limit = max_samples[key]\n-            dataset = datasets[key]\n-            if samples_limit is not None:\n-                dataset = dataset.select(range(samples_limit))\n-\n-            # model.prepare_tf_dataset() wraps a Hugging Face dataset in a tf.data.Dataset which is ready to use in\n-            # training. This is the recommended way to use a Hugging Face dataset when training with Keras. You can also\n-            # use the lower-level dataset.to_tf_dataset() method, but you will have to specify things like column names\n-            # yourself if you use this method, whereas they are automatically inferred from the model input names when\n-            # using model.prepare_tf_dataset()\n-            # For more info see the docs:\n-            # https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset\n-            # https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset\n-            data = model.prepare_tf_dataset(\n-                dataset,\n-                shuffle=shuffle,\n-                batch_size=batch_size,\n-                collate_fn=data_collator,\n-                tokenizer=tokenizer,\n-            )\n-            data = data.with_options(dataset_options)\n-            tf_data[key] = data\n-        # endregion\n-\n-        # region Optimizer, loss and compilation\n-        if training_args.do_train:\n-            num_train_steps = len(tf_data[\"train\"]) * training_args.num_train_epochs\n-            if training_args.warmup_steps > 0:\n-                num_warmup_steps = training_args.warmup_steps\n-            elif training_args.warmup_ratio > 0:\n-                num_warmup_steps = int(num_train_steps * training_args.warmup_ratio)\n-            else:\n-                num_warmup_steps = 0\n-\n-            optimizer, schedule = create_optimizer(\n-                init_lr=training_args.learning_rate,\n-                num_train_steps=num_train_steps,\n-                num_warmup_steps=num_warmup_steps,\n-                adam_beta1=training_args.adam_beta1,\n-                adam_beta2=training_args.adam_beta2,\n-                adam_epsilon=training_args.adam_epsilon,\n-                weight_decay_rate=training_args.weight_decay,\n-                adam_global_clipnorm=training_args.max_grad_norm,\n-            )\n-        else:\n-            optimizer = \"sgd\"  # Just write anything because we won't be using it\n-        if is_regression:\n-            metrics = []\n-        else:\n-            metrics = [\"accuracy\"]\n-        # Transformers models compute the right loss for their task by default when labels are passed, and will\n-        # use this for training unless you specify your own loss function in compile().\n-        model.compile(optimizer=optimizer, metrics=metrics, jit_compile=training_args.xla)\n-        # endregion\n-\n-        # region Preparing push_to_hub and model card\n-        push_to_hub_model_id = training_args.push_to_hub_model_id\n-        model_name = model_args.model_name_or_path.split(\"/\")[-1]\n-        if not push_to_hub_model_id:\n-            push_to_hub_model_id = f\"{model_name}-finetuned-glue\"\n-\n-        model_card_kwargs = {\"finetuned_from\": model_args.model_name_or_path, \"tasks\": \"text-classification\"}\n-        model_card_kwargs[\"task_name\"] = data_args.task_name\n-\n-        if training_args.push_to_hub:\n-            callbacks = [\n-                PushToHubCallback(\n-                    output_dir=training_args.output_dir,\n-                    hub_model_id=push_to_hub_model_id,\n-                    hub_token=training_args.push_to_hub_token,\n-                    tokenizer=tokenizer,\n-                    **model_card_kwargs,\n-                )\n-            ]\n-        else:\n-            callbacks = []\n-        # endregion\n-\n-        # region Training and validation\n-        if training_args.do_train:\n-            if training_args.do_eval and data_args.task_name != \"mnli\":\n-                # Do both evaluation and training in the Keras fit loop, unless the task is MNLI\n-                # because MNLI has two validation sets\n-                validation_data = tf_data[\"validation\"]\n-            else:\n-                validation_data = None\n-            model.fit(\n-                tf_data[\"train\"],\n-                validation_data=validation_data,\n-                epochs=int(training_args.num_train_epochs),\n-                callbacks=callbacks,\n-            )\n-        # endregion\n-\n-        # region Evaluation\n-        if training_args.do_eval:\n-            # We normally do validation as part of the Keras fit loop, but we run it independently\n-            # if there was no fit() step (because we didn't train the model) or if the task is MNLI,\n-            # because MNLI has a separate validation-mismatched validation set\n-\n-            # In this example, we compute advanced metrics only at the end of training, and only compute\n-            # loss and accuracy on the validation set each epoch, but\n-            # if you'd like to compute metrics every epoch that are too complex to be written as\n-            # standard Keras metrics, you can use our KerasMetricCallback. See\n-            # https://huggingface.co/docs/transformers/main/en/main_classes/keras_callbacks\n-            logger.info(\"*** Evaluate ***\")\n-\n-            # Loop to handle MNLI double evaluation (matched, mis-matched)\n-            if data_args.task_name == \"mnli\":\n-                tasks = [\"mnli\", \"mnli-mm\"]\n-                tf_datasets = [tf_data[\"validation_matched\"], tf_data[\"validation_mismatched\"]]\n-                raw_datasets = [datasets[\"validation_matched\"], datasets[\"validation_mismatched\"]]\n-            else:\n-                tasks = [data_args.task_name]\n-                tf_datasets = [tf_data[\"validation\"]]\n-                raw_datasets = [datasets[\"validation\"]]\n-\n-            for raw_dataset, tf_dataset, task in zip(raw_datasets, tf_datasets, tasks):\n-                eval_predictions = model.predict(tf_dataset)\n-                eval_metrics = compute_metrics(eval_predictions, raw_dataset[\"label\"])\n-                print(f\"Evaluation metrics ({task}):\")\n-                print(eval_metrics)\n-                if training_args.output_dir is not None:\n-                    output_eval_file = os.path.join(training_args.output_dir, \"all_results.json\")\n-                    with open(output_eval_file, \"w\") as writer:\n-                        writer.write(json.dumps(eval_metrics))\n-\n-        # endregion\n-\n-        # region Prediction\n-        if training_args.do_predict or data_args.predict_file:\n-            logger.info(\"*** Predict ***\")\n-\n-            # Loop to handle MNLI double evaluation (matched, mis-matched)\n-            tasks = []\n-            tf_datasets = []\n-            raw_datasets = []\n-            if training_args.do_predict:\n-                if data_args.task_name == \"mnli\":\n-                    tasks.extend([\"mnli\", \"mnli-mm\"])\n-                    tf_datasets.extend([tf_data[\"test_matched\"], tf_data[\"test_mismatched\"]])\n-                    raw_datasets.extend([datasets[\"test_matched\"], datasets[\"test_mismatched\"]])\n-                else:\n-                    tasks.append(data_args.task_name)\n-                    tf_datasets.append(tf_data[\"test\"])\n-                    raw_datasets.append(datasets[\"test\"])\n-            if data_args.predict_file:\n-                tasks.append(\"user_data\")\n-                tf_datasets.append(tf_data[\"user_data\"])\n-                raw_datasets.append(datasets[\"user_data\"])\n-\n-            for raw_dataset, tf_dataset, task in zip(raw_datasets, tf_datasets, tasks):\n-                test_predictions = model.predict(tf_dataset)\n-                if \"label\" in raw_dataset:\n-                    test_metrics = compute_metrics(test_predictions, raw_dataset[\"label\"])\n-                    print(f\"Test metrics ({task}):\")\n-                    print(test_metrics)\n-\n-                if is_regression:\n-                    predictions_to_write = np.squeeze(test_predictions[\"logits\"])\n-                else:\n-                    predictions_to_write = np.argmax(test_predictions[\"logits\"], axis=1)\n-\n-                output_predict_file = os.path.join(training_args.output_dir, f\"predict_results_{task}.txt\")\n-                with open(output_predict_file, \"w\") as writer:\n-                    logger.info(f\"***** Writing prediction results for {task} *****\")\n-                    writer.write(\"index\\tprediction\\n\")\n-                    for index, item in enumerate(predictions_to_write):\n-                        if is_regression:\n-                            writer.write(f\"{index}\\t{item:3.3f}\\n\")\n-                        else:\n-                            item = model.config.id2label[item]\n-                            writer.write(f\"{index}\\t{item}\\n\")\n-        # endregion\n-\n-        if training_args.output_dir is not None and not training_args.push_to_hub:\n-            # If we're not pushing to hub, at least save a local copy when we're done\n-            model.save_pretrained(training_args.output_dir)\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "c87c9040080d4cc9b3ce855a8f3a0b862e7d0af7",
            "filename": "examples/tensorflow/text-classification/run_text_classification.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "6c8a15c00e813acc4fcba349c388e2c2d369d23d",
            "filename": "examples/tensorflow/token-classification/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftoken-classification%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "99aff2bb32b2bb92f7628eb9bab4c7535d4c7f92",
            "filename": "examples/tensorflow/token-classification/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftoken-classification%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "0bada558fb937c2bf21faafe2e5379e45e69eab0",
            "filename": "examples/tensorflow/token-classification/run_ner.py",
            "status": "removed",
            "additions": 0,
            "deletions": 635,
            "changes": 635,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftoken-classification%2Frun_ner.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "bbe6e27e9c78a41518e2c4704105001f20b9528e",
            "filename": "examples/tensorflow/translation/README.md",
            "status": "removed",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftranslation%2FREADME.md?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "99aff2bb32b2bb92f7628eb9bab4c7535d4c7f92",
            "filename": "examples/tensorflow/translation/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftranslation%2Frequirements.txt?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "ffce51b4cb4f6e9ec317ffcd8aac43feb7ff9346",
            "filename": "examples/tensorflow/translation/run_translation.py",
            "status": "removed",
            "additions": 0,
            "deletions": 715,
            "changes": 715,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2e496149300977299a16160c8b75f5c2aefb9b/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftranslation%2Frun_translation.py?ref=5e2e496149300977299a16160c8b75f5c2aefb9b"
        },
        {
            "sha": "56530dab8829a40ad13b55e81e40cf19da0bbbe0",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 0,
            "deletions": 38,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=fe1a9e0dba913062d6ab8a3ae9fd6095b9718b24"
        }
    ],
    "stats": {
        "total": 21962,
        "additions": 1,
        "deletions": 21961
    }
}