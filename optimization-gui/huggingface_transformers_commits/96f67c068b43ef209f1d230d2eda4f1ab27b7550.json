{
    "author": "cecheta",
    "message": "Feature: Add `MLFLOW_MAX_LOG_PARAMS` to `MLflowCallback` (#34279)",
    "sha": "96f67c068b43ef209f1d230d2eda4f1ab27b7550",
    "files": [
        {
            "sha": "a09116552c8e3410293a619dc1168fff829f6487",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/96f67c068b43ef209f1d230d2eda4f1ab27b7550/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/96f67c068b43ef209f1d230d2eda4f1ab27b7550/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=96f67c068b43ef209f1d230d2eda4f1ab27b7550",
            "patch": "@@ -1218,13 +1218,16 @@ def setup(self, args, state, model):\n             and other parameters are ignored.\n         - **MLFLOW_FLATTEN_PARAMS** (`str`, *optional*, defaults to `False`):\n             Whether to flatten the parameters dictionary before logging.\n+        - **MLFLOW_MAX_LOG_PARAMS** (`int`, *optional*):\n+            Set the maximum number of parameters to log in the run.\n         \"\"\"\n         self._log_artifacts = os.getenv(\"HF_MLFLOW_LOG_ARTIFACTS\", \"FALSE\").upper() in ENV_VARS_TRUE_VALUES\n         self._nested_run = os.getenv(\"MLFLOW_NESTED_RUN\", \"FALSE\").upper() in ENV_VARS_TRUE_VALUES\n         self._tracking_uri = os.getenv(\"MLFLOW_TRACKING_URI\", None)\n         self._experiment_name = os.getenv(\"MLFLOW_EXPERIMENT_NAME\", None)\n         self._flatten_params = os.getenv(\"MLFLOW_FLATTEN_PARAMS\", \"FALSE\").upper() in ENV_VARS_TRUE_VALUES\n         self._run_id = os.getenv(\"MLFLOW_RUN_ID\", None)\n+        self._max_log_params = os.getenv(\"MLFLOW_MAX_LOG_PARAMS\", None)\n \n         # \"synchronous\" flag is only available with mlflow version >= 2.8.0\n         # https://github.com/mlflow/mlflow/pull/9705\n@@ -1273,6 +1276,13 @@ def setup(self, args, state, model):\n                     del combined_dict[name]\n             # MLflow cannot log more than 100 values in one go, so we have to split it\n             combined_dict_items = list(combined_dict.items())\n+            if self._max_log_params and self._max_log_params.isdigit():\n+                max_log_params = int(self._max_log_params)\n+                if max_log_params < len(combined_dict_items):\n+                    logger.debug(\n+                        f\"Reducing the number of parameters to log from {len(combined_dict_items)} to {max_log_params}.\"\n+                    )\n+                    combined_dict_items = combined_dict_items[:max_log_params]\n             for i in range(0, len(combined_dict_items), self._MAX_PARAMS_TAGS_PER_BATCH):\n                 if self._async_log:\n                     self._ml_flow.log_params("
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}