{
    "author": "manueldeprada",
    "message": "Fix mamba regression (#39728)\n\n* fix mamba regression\n\n* fix compile test",
    "sha": "cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae",
    "files": [
        {
            "sha": "7630ebd6343ac968303fc0c31f2742bb352b4f8a",
            "filename": "src/transformers/models/falcon_mamba/configuration_falcon_mamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fconfiguration_falcon_mamba.py?ref=cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae",
            "patch": "@@ -141,7 +141,12 @@ def __init__(\n         self.layer_norm_epsilon = layer_norm_epsilon\n         self.conv_kernel = conv_kernel\n         self.expand = expand\n-        self.intermediate_size = int(expand * self.hidden_size)\n+        # This is needed since mamba overrides the intermediate_size attribute\n+        self.intermediate_size = (\n+            int(expand * self.hidden_size)\n+            if kwargs.get(\"intermediate_size\") is None\n+            else kwargs.get(\"intermediate_size\")\n+        )\n         self.bos_token_id = bos_token_id\n         self.eos_token_id = eos_token_id\n         self.pad_token_id = pad_token_id"
        },
        {
            "sha": "e28a84e2f0c28cf5361a2c858f83bba280c4852c",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae",
            "patch": "@@ -192,6 +192,12 @@ def __init__(\n             **kwargs,\n         )\n         self.mixer_rms_eps = mixer_rms_eps\n+        # This is needed since mamba overrides the intermediate_size attribute\n+        self.intermediate_size = (\n+            int(expand * self.hidden_size)\n+            if kwargs.get(\"intermediate_size\") is None\n+            else kwargs.get(\"intermediate_size\")\n+        )\n \n \n class FalconMambaCache(MambaCache):"
        },
        {
            "sha": "83de5dd6aebc48ef2fc881ce71b35dd60ae1e60a",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=cf97f6cfd1c9cff6f3f188e91b32dd3b2a1c5fae",
            "patch": "@@ -39,6 +39,10 @@\n \n logger = logging.get_logger(__name__)\n \n+if is_mambapy_available():\n+    from mambapy.pscan import pscan\n+else:\n+    pscan = None\n \n if is_mamba_ssm_available():\n     from mamba_ssm.ops.selective_scan_interface import mamba_inner_fn, selective_scan_fn\n@@ -330,10 +334,6 @@ def cuda_kernels_forward(\n \n     # fmt: off\n     def slow_forward(self, input_states, cache_params: Optional[MambaCache]=None, cache_position:Optional[torch.LongTensor]=None, attention_mask: Optional[torch.LongTensor] = None):\n-        if is_mambapy_available():\n-            from mambapy.pscan import pscan\n-        else:\n-            pscan = None\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated MLP's linear projection"
        }
    ],
    "stats": {
        "total": 21,
        "additions": 16,
        "deletions": 5
    }
}