{
    "author": "gante",
    "message": "VLM generate: tests can't generate image/video tokens (#33623)",
    "sha": "2fdb5e74cce41aef7d168df1dc2cc9fec348a127",
    "files": [
        {
            "sha": "08b40e71cf1f3c01496d6a98f0cd9987999ccdd8",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 8,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=2fdb5e74cce41aef7d168df1dc2cc9fec348a127",
            "patch": "@@ -132,7 +132,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n \n         return config, input_ids, attention_mask, inputs_dict\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {\n             \"bad_words_ids\": [[1, 0]],\n             \"repetition_penalty\": 1.2,\n@@ -146,6 +146,17 @@ def _get_logits_processor_kwargs(self, do_sample=False):\n                     \"temperature\": 0.7,\n                 }\n             )\n+        # TODO (joao, raushan): see this comment for a long-term fix\n+        # https://github.com/huggingface/transformers/pull/33593#issuecomment-2361824264)\n+        # This is a band-aid for VLM models, to ensure they don't generate image/video tokens which would cause them\n+        # to crash. On pretrained models this isn't a risk, as they are trained to not generate these tokens.\n+        if config is not None:\n+            image_token_index = config.image_token_index if hasattr(config, \"image_token_index\") else None\n+            video_token_index = config.video_token_index if hasattr(config, \"video_token_index\") else None\n+            if image_token_index is not None and image_token_index < config.get_text_config().vocab_size:\n+                logits_processor_kwargs[\"bad_words_ids\"].append([image_token_index])\n+            if video_token_index is not None and video_token_index < config.get_text_config().vocab_size:\n+                logits_processor_kwargs[\"bad_words_ids\"].append([video_token_index])\n \n         return logits_processor_kwargs\n \n@@ -211,7 +222,7 @@ def _greedy_generate(\n         return_dict_in_generate=False,\n         use_cache=True,\n     ):\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -246,7 +257,7 @@ def _sample_generate(\n         use_cache=True,\n     ):\n         torch.manual_seed(0)\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -281,7 +292,7 @@ def _beam_search_generate(\n         return_dict_in_generate=False,\n         use_cache=True,\n     ):\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -316,7 +327,7 @@ def _beam_sample_generate(\n         use_cache=True,\n     ):\n         torch.manual_seed(0)\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=True, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -350,7 +361,7 @@ def _group_beam_search_generate(\n         return_dict_in_generate=False,\n         use_cache=True,\n     ):\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -385,7 +396,7 @@ def _constrained_beam_search_generate(\n         return_dict_in_generate=False,\n         use_cache=True,\n     ):\n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -424,7 +435,7 @@ def _contrastive_generate(\n             \"top_k\": 5,\n         }\n \n-        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False)\n+        logits_processor_kwargs = self._get_logits_processor_kwargs(do_sample=False, config=model.config)\n         model_kwargs = {\"attention_mask\": attention_mask} if attention_mask is not None else {}\n         output_generate = model.generate(\n             input_ids,\n@@ -2052,6 +2063,7 @@ def test_generate_methods_with_num_logits_to_keep(self):\n             )\n             self.assertEqual(with_all_logits.tolist(), without_all_logits.tolist())\n \n+    @is_flaky()  # assisted generation tests are flaky (minor fp ops differences)\n     def test_assisted_decoding_with_num_logits_to_keep(self):\n         for model_class in self.all_generative_model_classes:\n             if \"num_logits_to_keep\" not in set(inspect.signature(model_class.forward).parameters.keys()):"
        },
        {
            "sha": "a385a18b91c5d5a2c885db7e9ade34a8bf0bce97",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=2fdb5e74cce41aef7d168df1dc2cc9fec348a127",
            "patch": "@@ -300,7 +300,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n         attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n         return config, input_ids, attention_mask, inputs_dict\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n@@ -1485,7 +1485,7 @@ def _sample_generate(\n \n         return output_generate\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n "
        },
        {
            "sha": "e8584e238d3cd9bccd015082c8512b05015df26a",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=2fdb5e74cce41aef7d168df1dc2cc9fec348a127",
            "patch": "@@ -303,7 +303,7 @@ def _get_input_ids_and_config(self, batch_size=2):\n         attention_mask = torch.ones((batch_size, sequence_length), dtype=torch.long)\n         return config, input_ids, attention_mask, inputs_dict\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n@@ -1469,7 +1469,7 @@ def _sample_generate(\n \n         return output_generate\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n "
        },
        {
            "sha": "bf0746a2927b0fb7d8cdebe2140795b8db91e9bf",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2fdb5e74cce41aef7d168df1dc2cc9fec348a127/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=2fdb5e74cce41aef7d168df1dc2cc9fec348a127",
            "patch": "@@ -411,9 +411,9 @@ def is_pipeline_test_to_skip(\n \n         return False\n \n-    def _get_logits_processor_kwargs(self, do_sample=False):\n+    def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         # Overwritten from `GenerationTesterMixin`, Whisper needs `\"temperature\": 0.0` to be able to do beam search\n-        logits_processor_kwargs = super()._get_logits_processor_kwargs(do_sample=do_sample)\n+        logits_processor_kwargs = super()._get_logits_processor_kwargs(do_sample=do_sample, config=config)\n         logits_processor_kwargs[\"temperature\"] = 0.0\n         return logits_processor_kwargs\n "
        }
    ],
    "stats": {
        "total": 40,
        "additions": 26,
        "deletions": 14
    }
}