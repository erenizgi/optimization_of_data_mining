{
    "author": "omahs",
    "message": "Fix typos (#37978)\n\nfix typos",
    "sha": "274e79b3262147e52968480bb0f03f4893559ab2",
    "files": [
        {
            "sha": "2afb922287583e14d36d6b12b0ebe20b1d208aa5",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -108,7 +108,7 @@ If in doubt about what args/kwargs a given model sends to the attention function\n ## Accessing current available implementations\n \n Most of the time, you will simply need to `register` a new function. If, however, you need to access an existing one,\n-and/or perform a few checks, the prefered way is to use the global `ALL_ATTENTION_FUNCTIONS`. It behaves the same way you\n+and/or perform a few checks, the preferred way is to use the global `ALL_ATTENTION_FUNCTIONS`. It behaves the same way you\n would expect from a usual Python dictionary:\n \n ```python"
        },
        {
            "sha": "25ee63997b5ebee5471041ae5e065fff9fc592f7",
            "filename": "docs/source/it/perf_train_cpu.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/docs%2Fsource%2Fit%2Fperf_train_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/docs%2Fsource%2Fit%2Fperf_train_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fperf_train_cpu.md?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -19,7 +19,7 @@ Questa guida si concentra su come addestrare in maniera efficiente grandi modell\n \n ## Mixed precision con IPEX\n \n-IPEX è ottimizzato per CPU con AVX-512 o superiore, e funziona per le CPU con solo AVX2. Pertanto, si prevede che le prestazioni saranno più vantaggiose per le le CPU Intel con AVX-512 o superiori, mentre le CPU con solo AVX2 (ad esempio, le CPU AMD o le CPU Intel più vecchie) potrebbero ottenere prestazioni migliori con IPEX, ma non sono garantite. IPEX offre ottimizzazioni delle prestazioni per l'addestramento della CPU sia con Float32 che con BFloat16. L'uso di BFloat16 è l'argomento principale delle seguenti sezioni.\n+IPEX è ottimizzato per CPU con AVX-512 o superiore, e funziona per le CPU con solo AVX2. Pertanto, si prevede che le prestazioni saranno più vantaggiose per le CPU Intel con AVX-512 o superiori, mentre le CPU con solo AVX2 (ad esempio, le CPU AMD o le CPU Intel più vecchie) potrebbero ottenere prestazioni migliori con IPEX, ma non sono garantite. IPEX offre ottimizzazioni delle prestazioni per l'addestramento della CPU sia con Float32 che con BFloat16. L'uso di BFloat16 è l'argomento principale delle seguenti sezioni.\n \n Il tipo di dati a bassa precisione BFloat16 è stato supportato in modo nativo su 3rd Generation Xeon® Scalable Processors (aka Cooper Lake) con AVX512 e sarà supportata dalla prossima generazione di Intel® Xeon® Scalable Processors con Intel® Advanced Matrix Extensions (Intel® AMX) instruction set con prestazioni ulteriormente migliorate. L'Auto Mixed Precision per il backende della CPU è stato abilitato da PyTorch-1.10. allo stesso tempo, il supporto di Auto Mixed Precision con BFloat16 per CPU e l'ottimizzazione degli operatori BFloat16 è stata abilitata in modo massiccio in Intel® Extension per PyTorch, and parzialmente aggiornato al branch master di PyTorch. Gli utenti possono ottenere prestazioni migliori ed users experience con IPEX Auto Mixed Precision..\n "
        },
        {
            "sha": "ea436706348bb660f73fc2935f82fc77cd879127",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -2277,7 +2277,7 @@ def generate(\n         generation_config, kwargs = self._prepare_generation_config(kwargs.pop(\"generation_config\", None), **kwargs)\n \n         input_ids, user_audio_codes, moshi_audio_codes, concat_unconditional_inputs = (\n-            self._check_and_maybe_initalize_inputs(\n+            self._check_and_maybe_initialize_inputs(\n                 input_ids=input_ids,\n                 user_input_values=user_input_values,\n                 user_audio_codes=user_audio_codes,\n@@ -2707,7 +2707,7 @@ def get_unconditional_inputs(self, num_samples=1):\n             attention_mask=attention_mask,\n         )\n \n-    def _check_and_maybe_initalize_inputs(\n+    def _check_and_maybe_initialize_inputs(\n         self,\n         input_ids=None,\n         user_input_values=None,"
        },
        {
            "sha": "e3b835492b970e434f60ecf8c0f81c831064bc02",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -593,8 +593,8 @@ def forward(\n                         context_input_ids,\n                         context_attention_mask,\n                         retrieved_doc_embeds,\n-                        retrived_doc_input_ids,\n-                        retrived_doc_attention_mask,\n+                        retrieved_doc_input_ids,\n+                        retrieved_doc_attention_mask,\n                         retrieved_doc_ids,\n                     ) = (\n                         retriever_outputs[\"context_input_ids\"],\n@@ -608,10 +608,10 @@ def forward(\n                     context_input_ids = context_input_ids.to(input_ids)\n                     context_attention_mask = context_attention_mask.to(input_ids)\n \n-                    retrived_doc_input_ids = retrived_doc_input_ids.to(input_ids)\n-                    retrived_doc_attention_mask = retrived_doc_attention_mask.to(input_ids)\n+                    retrieved_doc_input_ids = retrieved_doc_input_ids.to(input_ids)\n+                    retrieved_doc_attention_mask = retrieved_doc_attention_mask.to(input_ids)\n                     retrieved_doc_embeds = self.ctx_encoder(\n-                        retrived_doc_input_ids, attention_mask=retrived_doc_attention_mask, return_dict=True\n+                        retrieved_doc_input_ids, attention_mask=retrieved_doc_attention_mask, return_dict=True\n                     ).pooler_output\n                     retrieved_doc_embeds = retrieved_doc_embeds.view(\n                         -1, n_docs, question_encoder_last_hidden_state.shape[1]"
        },
        {
            "sha": "d610bc665799194ac8b4ad9960d2abfcfc7752dd",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -3391,7 +3391,7 @@ def generate(\n             `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:\n             - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n             - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n-              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              sequence_length)` and `waveform_lengths` which gives the length of each sample.\n         \"\"\"\n         batch_size = len(input_ids) if input_ids is not None else len(kwargs.get(\"inputs_embeds\"))\n \n@@ -3721,7 +3721,7 @@ def generate(\n             `Union[SeamlessM4TGenerationOutput, Tuple[Tensor]]`:\n             - If `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n             - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n-              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              sequence_length)` and `waveform_lengths` which gives the length of each sample.\n         \"\"\"\n         batch_size = len(input_features) if input_features is not None else len(kwargs.get(\"inputs_embeds\"))\n \n@@ -4132,7 +4132,7 @@ def generate(\n             `Union[SeamlessM4TGenerationOutput, Tuple[Tensor], ModelOutput]`:\n             - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4TGenerationOutput`].\n             - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of\n-              shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              shape `(batch_size, sequence_length)` and `waveform_lengths` which gives the length of each sample.\n             - If `generate_speech=False`, it will returns `ModelOutput`.\n         \"\"\"\n         if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\", None) is None:"
        },
        {
            "sha": "704b75450f7f34e7d1df6a1f9df6cfe922577b2d",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -3691,7 +3691,7 @@ def generate(\n             `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:\n             - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].\n             - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n-              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              sequence_length)` and `waveform_lengths` which gives the length of each sample.\n         \"\"\"\n         batch_size = len(input_ids) if input_ids is not None else len(kwargs.get(\"inputs_embeds\"))\n \n@@ -4062,7 +4062,7 @@ def generate(\n             `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor]]`:\n             - If `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].\n             - If not `return_intermediate_token_ids`, returns a tuple composed of waveforms of shape `(batch_size,\n-              sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              sequence_length)` and `waveform_lengths` which gives the length of each sample.\n         \"\"\"\n         batch_size = len(input_features) if input_features is not None else len(kwargs.get(\"inputs_embeds\"))\n \n@@ -4514,7 +4514,7 @@ def generate(\n             `Union[SeamlessM4Tv2GenerationOutput, Tuple[Tensor], ModelOutput]`:\n             - If `generate_speech` and `return_intermediate_token_ids`, returns [`SeamlessM4Tv2GenerationOutput`].\n             - If `generate_speech` and not `return_intermediate_token_ids`, returns a tuple composed of waveforms of\n-              shape `(batch_size, sequence_length)`and and `waveform_lengths` which gives the length of each sample.\n+              shape `(batch_size, sequence_length)` and `waveform_lengths` which gives the length of each sample.\n             - If `generate_speech=False`, it will returns `ModelOutput`.\n         \"\"\"\n         if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\", None) is None:"
        },
        {
            "sha": "48d1dbf20a3e2a9bdc77624e0bcebb734f50ac30",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -275,7 +275,7 @@ def test_model_parallelism(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"PaliGemmma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n+        reason=\"PaliGemma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n     )\n     def test_initialization(self):\n         pass"
        },
        {
            "sha": "e56dcc7d861119eb1c9ba17b391a5d51b46bfcb6",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -431,7 +431,7 @@ def test_model_rope_scaling(self):\n \n     def test_past_key_values_format(self):\n         \"\"\"\n-        Overwritting to pass the expected cache shapes (Deepseek-V3 uses MLA so the cache shapes are non-standard)\n+        Overwriting to pass the expected cache shapes (Deepseek-V3 uses MLA so the cache shapes are non-standard)\n         \"\"\"\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         batch_size, seq_length = inputs[\"input_ids\"].shape\n@@ -451,7 +451,7 @@ def test_past_key_values_format(self):\n     @slow\n     def test_eager_matches_sdpa_generate(self):\n         \"\"\"\n-        Overwritting the common test as the test is flaky on tiny models\n+        Overwriting the common test as the test is flaky on tiny models\n         \"\"\"\n         max_new_tokens = 30\n "
        },
        {
            "sha": "0f40f0daa86863504c5df8fb4d564e05de27c9f7",
            "filename": "tests/models/marian/test_tokenization_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_tokenization_marian.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -136,7 +136,7 @@ def test_tokenizer_integration(self):\n             decode_kwargs={\"use_source_tokenizer\": True},\n         )\n \n-    def test_tokenizer_integration_seperate_vocabs(self):\n+    def test_tokenizer_integration_separate_vocabs(self):\n         tokenizer = MarianTokenizer.from_pretrained(\"hf-internal-testing/test-marian-two-vocabs\")\n \n         source_text = \"Tämä on testi\""
        },
        {
            "sha": "d922775628a3e2f805ccbc412f298a3d3e0ca086",
            "filename": "tests/models/opt/test_modeling_flax_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -69,7 +69,7 @@ def __init__(\n         embed_dim=16,\n         word_embed_proj_dim=16,\n         initializer_range=0.02,\n-        attn_implemetation=\"eager\",\n+        attn_implementation=\"eager\",\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -92,7 +92,7 @@ def __init__(\n         self.word_embed_proj_dim = word_embed_proj_dim\n         self.initializer_range = initializer_range\n         self.is_encoder_decoder = False\n-        self.attn_implementation = attn_implemetation\n+        self.attn_implementation = attn_implementation\n \n     def prepare_config_and_inputs(self):\n         input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)"
        },
        {
            "sha": "4b9db1d75d1d6fa309c503395991470b89812dcd",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -297,7 +297,7 @@ def test_model_parallelism(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"PaliGemmma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n+        reason=\"PaliGemma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n     )\n     def test_initialization(self):\n         pass"
        },
        {
            "sha": "e13430f2b73c6b52f168e643cde538fc21b22329",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -294,7 +294,7 @@ def test_model_parallelism(self):\n         pass\n \n     @unittest.skip(\n-        reason=\"PaliGemmma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n+        reason=\"PaliGemma's SigLip encoder uses the same initialization scheme as the Flax original implementation\"\n     )\n     def test_initialization(self):\n         pass"
        },
        {
            "sha": "8a46871755123ad1cf8c9e360dd12f5ce9b9e51d",
            "filename": "utils/check_copies.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/274e79b3262147e52968480bb0f03f4893559ab2/utils%2Fcheck_copies.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/274e79b3262147e52968480bb0f03f4893559ab2/utils%2Fcheck_copies.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_copies.py?ref=274e79b3262147e52968480bb0f03f4893559ab2",
            "patch": "@@ -18,7 +18,7 @@\n - The list of models in the main README.md matches the ones in the localized READMEs,\n - Files that are registered as full copies of one another in the `FULL_COPIES` constant of this script.\n \n-This also checks the list of models in the README is complete (has all models) and add a line to complete if there is\n+This also checks the list of models in the README is complete (has all models) and adds a line to complete if there is\n a model missing.\n \n Use from the root of the repo with:\n@@ -420,7 +420,7 @@ def find_code_in_transformers(\n \n     # Detail: the `Copied from` statement is originally designed to work with the last part of `TRANSFORMERS_PATH`,\n     # (which is `transformers`). The same should be applied for `MODEL_TEST_PATH`. However, its last part is `models`\n-    # (to only check and search in it) which is a bit confusing. So we keep the copied statement staring with\n+    # (to only check and search in it) which is a bit confusing. So we keep the copied statement starting with\n     # `tests.models.` and change it to `tests` here.\n     if base_path == MODEL_TEST_PATH:\n         base_path = \"tests\""
        }
    ],
    "stats": {
        "total": 50,
        "additions": 25,
        "deletions": 25
    }
}