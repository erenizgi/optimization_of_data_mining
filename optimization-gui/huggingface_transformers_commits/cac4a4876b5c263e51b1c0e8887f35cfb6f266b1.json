{
    "author": "SunMarc",
    "message": "[Quantization] Switch to optimum-quanto  (#31732)\n\n* switch to optimum-quanto rebase squach\r\n\r\n* fix import check\r\n\r\n* again\r\n\r\n* test try-except\r\n\r\n* style",
    "sha": "cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
    "files": [
        {
            "sha": "0617ac8cdd779c21e79e1d91504b1d61f403afa5",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -56,7 +56,7 @@ RUN python3 -m pip install --no-cache-dir gguf\n RUN python3 -m pip install --no-cache-dir https://github.com/casper-hansen/AutoAWQ/releases/download/v0.2.3/autoawq-0.2.3+cu118-cp38-cp38-linux_x86_64.whl\n \n # Add quanto for quantization testing\n-RUN python3 -m pip install --no-cache-dir quanto\n+RUN python3 -m pip install --no-cache-dir optimum-quanto\n \n # Add eetq for quantization testing\n RUN python3 -m pip install git+https://github.com/NetEase-FuXi/EETQ.git"
        },
        {
            "sha": "223eda10a9671aa3d08b52629760bcef72998be7",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 13,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -9,14 +9,15 @@\n from packaging import version\n \n from .configuration_utils import PretrainedConfig\n-from .utils import is_hqq_available, is_quanto_available, is_torchdynamo_compiling, logging\n+from .utils import (\n+    is_hqq_available,\n+    is_optimum_quanto_available,\n+    is_quanto_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n \n \n-if is_quanto_available():\n-    quanto_version = version.parse(importlib.metadata.version(\"quanto\"))\n-    if quanto_version >= version.parse(\"0.2.0\"):\n-        from quanto import AffineQuantizer, MaxOptimizer, qint2, qint4\n-\n if is_hqq_available():\n     from hqq.core.quantize import Quantizer as HQQQuantizer\n \n@@ -754,12 +755,20 @@ class QuantoQuantizedCache(QuantizedCache):\n \n     def __init__(self, cache_config: CacheConfig) -> None:\n         super().__init__(cache_config)\n-        quanto_version = version.parse(importlib.metadata.version(\"quanto\"))\n-        if quanto_version < version.parse(\"0.2.0\"):\n-            raise ImportError(\n-                f\"You need quanto package version to be greater or equal than 0.2.0 to use `QuantoQuantizedCache`. Detected version {quanto_version}. \"\n-                f\"Please upgrade quanto with `pip install -U quanto`\"\n+\n+        if is_optimum_quanto_available():\n+            from optimum.quanto import MaxOptimizer, qint2, qint4\n+        elif is_quanto_available():\n+            logger.warning_once(\n+                \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instead `pip install optimum-quanto`\"\n             )\n+            quanto_version = version.parse(importlib.metadata.version(\"quanto\"))\n+            if quanto_version < version.parse(\"0.2.0\"):\n+                raise ImportError(\n+                    f\"You need quanto package version to be greater or equal than 0.2.0 to use `QuantoQuantizedCache`. Detected version {quanto_version}. \"\n+                    f\"Since quanto will be deprecated, please install optimum-quanto instead with `pip install -U optimum-quanto`\"\n+                )\n+            from quanto import MaxOptimizer, qint2, qint4\n \n         if self.nbits not in [2, 4]:\n             raise ValueError(f\"`nbits` for `quanto` backend has to be one of [`2`, `4`] but got {self.nbits}\")\n@@ -776,8 +785,22 @@ def __init__(self, cache_config: CacheConfig) -> None:\n         self.optimizer = MaxOptimizer()  # hardcode as it's the only one for per-channel quantization\n \n     def _quantize(self, tensor, axis):\n-        scale, zeropoint = self.optimizer(tensor, self.qtype.bits, axis, self.q_group_size)\n-        qtensor = AffineQuantizer.apply(tensor, self.qtype, axis, self.q_group_size, scale, zeropoint)\n+        # We have two different API since in optimum-quanto, we don't use AffineQuantizer anymore\n+        if is_optimum_quanto_available():\n+            from optimum.quanto import quantize_weight\n+\n+            scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n+            qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n+            return qtensor\n+        elif is_quanto_available():\n+            logger.warning_once(\n+                \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instead `pip install optimum-quanto`\"\n+            )\n+            from quanto import AffineQuantizer\n+\n+            scale, zeropoint = self.optimizer(tensor, self.qtype.bits, axis, self.q_group_size)\n+            qtensor = AffineQuantizer.apply(tensor, self.qtype, axis, self.q_group_size, scale, zeropoint)\n+\n         return qtensor\n \n     def _dequantize(self, qtensor):"
        },
        {
            "sha": "06b2654248fbcb06d0ea1b8d6f4534f85f9fe06d",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -42,6 +42,7 @@\n     ModelOutput,\n     is_accelerate_available,\n     is_hqq_available,\n+    is_optimum_quanto_available,\n     is_quanto_available,\n     is_torchdynamo_compiling,\n     logging,\n@@ -1674,10 +1675,10 @@ def _prepare_cache_for_generation(\n                 )\n                 cache_class = QUANT_BACKEND_CLASSES_MAPPING[cache_config.backend]\n \n-                if cache_config.backend == \"quanto\" and not is_quanto_available():\n+                if cache_config.backend == \"quanto\" and not (is_optimum_quanto_available() or is_quanto_available()):\n                     raise ImportError(\n-                        \"You need to install `quanto` in order to use KV cache quantization with quanto backend. \"\n-                        \"Please install it via  with `pip install quanto`\"\n+                        \"You need to install optimum-quanto in order to use KV cache quantization with optimum-quanto backend. \"\n+                        \"Please install it via  with `pip install optimum-quanto`\"\n                     )\n                 elif cache_config.backend == \"HQQ\" and not is_hqq_available():\n                     raise ImportError("
        },
        {
            "sha": "27b32de63bfe55c9348b65c0f829ea50b1fcd870",
            "filename": "src/transformers/integrations/quanto.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fintegrations%2Fquanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fquanto.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -12,12 +12,14 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ..utils import is_torch_available\n+from ..utils import is_optimum_quanto_available, is_quanto_available, is_torch_available, logging\n \n \n if is_torch_available():\n     import torch\n \n+logger = logging.get_logger(__name__)\n+\n \n def replace_with_quanto_layers(\n     model,\n@@ -45,7 +47,14 @@ def replace_with_quanto_layers(\n             should not be passed by the user.\n     \"\"\"\n     from accelerate import init_empty_weights\n-    from quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n+\n+    if is_optimum_quanto_available():\n+        from optimum.quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n+    elif is_quanto_available():\n+        logger.warning_once(\n+            \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instead `pip install optimum-quanto`\"\n+        )\n+        from quanto import QLayerNorm, QLinear, qfloat8, qint2, qint4, qint8\n \n     w_mapping = {\"float8\": qfloat8, \"int8\": qint8, \"int4\": qint4, \"int2\": qint2}\n     a_mapping = {None: None, \"float8\": qfloat8, \"int8\": qint8}"
        },
        {
            "sha": "0aacc18d2a1f40ce956a4e32c9fb5fcd8fbd6e52",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 30,
            "deletions": 10,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -23,7 +23,13 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_accelerate_available, is_quanto_available, is_torch_available, logging\n+from ..utils import (\n+    is_accelerate_available,\n+    is_optimum_quanto_available,\n+    is_quanto_available,\n+    is_torch_available,\n+    logging,\n+)\n from ..utils.quantization_config import QuantoConfig\n \n \n@@ -57,11 +63,13 @@ def post_init(self):\n             )\n \n     def validate_environment(self, *args, **kwargs):\n-        if not is_quanto_available():\n-            raise ImportError(\"Loading a quanto quantized model requires quanto library (`pip install quanto`)\")\n+        if not (is_optimum_quanto_available() or is_quanto_available()):\n+            raise ImportError(\n+                \"Loading an optimum-quanto quantized model requires optimum-quanto library (`pip install optimum-quanto`)\"\n+            )\n         if not is_accelerate_available():\n             raise ImportError(\n-                \"Loading a quanto quantized model requires accelerate library (`pip install accelerate`)\"\n+                \"Loading an optimum-quanto quantized model requires accelerate library (`pip install accelerate`)\"\n             )\n \n     def update_device_map(self, device_map):\n@@ -81,11 +89,17 @@ def update_torch_dtype(self, torch_dtype: \"torch.dtype\") -> \"torch.dtype\":\n         return torch_dtype\n \n     def update_missing_keys(self, model, missing_keys: List[str], prefix: str) -> List[str]:\n-        import quanto\n+        if is_optimum_quanto_available():\n+            from optimum.quanto import QModuleMixin\n+        elif is_quanto_available():\n+            logger.warning_once(\n+                \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instrad `pip install optimum-quanto`\"\n+            )\n+            from quanto import QModuleMixin\n \n         not_missing_keys = []\n         for name, module in model.named_modules():\n-            if isinstance(module, quanto.QModuleMixin):\n+            if isinstance(module, QModuleMixin):\n                 for missing in missing_keys:\n                     if (\n                         (name in missing or name in f\"{prefix}.{missing}\")\n@@ -106,7 +120,13 @@ def check_quantized_param(\n         \"\"\"\n         Check if a parameter needs to be quantized.\n         \"\"\"\n-        import quanto\n+        if is_optimum_quanto_available():\n+            from optimum.quanto import QModuleMixin\n+        elif is_quanto_available():\n+            logger.warning_once(\n+                \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instrad `pip install optimum-quanto`\"\n+            )\n+            from quanto import QModuleMixin\n \n         device_map = kwargs.get(\"device_map\", None)\n         param_device = kwargs.get(\"param_device\", None)\n@@ -119,7 +139,7 @@ def check_quantized_param(\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         # We only quantize the weights and the bias is not quantized.\n-        if isinstance(module, quanto.QModuleMixin) and \"weight\" in tensor_name:\n+        if isinstance(module, QModuleMixin) and \"weight\" in tensor_name:\n             # if the weights are quantized, don't need to recreate it again with `create_quantized_param`\n             return not module.frozen\n         else:\n@@ -162,7 +182,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             return target_dtype\n         else:\n             raise ValueError(\n-                \"You are using `device_map='auto'` on a quanto quantized model. To automatically compute\"\n+                \"You are using `device_map='auto'` on an optimum-quanto quantized model. To automatically compute\"\n                 \" the appropriate device map, you should upgrade your `accelerate` library,\"\n                 \"`pip install --upgrade accelerate` or install it from source.\"\n             )\n@@ -193,7 +213,7 @@ def _process_model_after_weight_loading(self, model):\n \n     @property\n     def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n-        return False\n+        return True\n \n     def is_serializable(self, safe_serialization=None):\n         return False"
        },
        {
            "sha": "8eda45bd40efb4da6f8972ba1e28fb39b9c1d665",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -94,6 +94,7 @@\n     is_nltk_available,\n     is_onnx_available,\n     is_optimum_available,\n+    is_optimum_quanto_available,\n     is_pandas_available,\n     is_peft_available,\n     is_phonemizer_available,\n@@ -102,7 +103,6 @@\n     is_pytesseract_available,\n     is_pytest_available,\n     is_pytorch_quantization_available,\n-    is_quanto_available,\n     is_rjieba_available,\n     is_sacremoses_available,\n     is_safetensors_available,\n@@ -1194,11 +1194,11 @@ def require_auto_awq(test_case):\n     return unittest.skipUnless(is_auto_awq_available(), \"test requires autoawq\")(test_case)\n \n \n-def require_quanto(test_case):\n+def require_optimum_quanto(test_case):\n     \"\"\"\n     Decorator for quanto dependency\n     \"\"\"\n-    return unittest.skipUnless(is_quanto_available(), \"test requires quanto\")(test_case)\n+    return unittest.skipUnless(is_optimum_quanto_available(), \"test requires optimum-quanto\")(test_case)\n \n \n def require_compressed_tensors(test_case):"
        },
        {
            "sha": "3b33127be4ba532761e762611e2ebf7dd1c10655",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -163,6 +163,7 @@\n     is_onnx_available,\n     is_openai_available,\n     is_optimum_available,\n+    is_optimum_quanto_available,\n     is_pandas_available,\n     is_peft_available,\n     is_phonemizer_available,"
        },
        {
            "sha": "519755489a3373f3301ade4b4b91838f157ce206",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -143,6 +143,12 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n # `importlib.metadata.version` doesn't work with `awq`\n _auto_awq_available = importlib.util.find_spec(\"awq\") is not None\n _quanto_available = _is_package_available(\"quanto\")\n+_is_optimum_quanto_available = False\n+try:\n+    importlib.metadata.version(\"optimum_quanto\")\n+    _is_optimum_quanto_available = True\n+except importlib.metadata.PackageNotFoundError:\n+    _is_optimum_quanto_available = False\n # For compressed_tensors, only check spec to allow compressed_tensors-nightly package\n _compressed_tensors_available = importlib.util.find_spec(\"compressed_tensors\") is not None\n _pandas_available = _is_package_available(\"pandas\")\n@@ -963,9 +969,17 @@ def is_auto_awq_available():\n \n \n def is_quanto_available():\n+    logger.warning_once(\n+        \"Importing from quanto will be deprecated in v4.47. Please install optimum-quanto instrad `pip install optimum-quanto`\"\n+    )\n     return _quanto_available\n \n \n+def is_optimum_quanto_available():\n+    # `importlib.metadata.version` doesn't work with `optimum.quanto`, need to put `optimum_quanto`\n+    return _is_optimum_quanto_available\n+\n+\n def is_compressed_tensors_available():\n     return _compressed_tensors_available\n "
        },
        {
            "sha": "b5feba6a3006b1c26d92bc4649f3576698c1d57d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -29,7 +29,7 @@\n     is_flaky,\n     require_accelerate,\n     require_auto_gptq,\n-    require_quanto,\n+    require_optimum_quanto,\n     require_torch,\n     require_torch_gpu,\n     require_torch_multi_accelerator,\n@@ -1941,7 +1941,7 @@ def test_generate_with_static_cache(self):\n             self.assertTrue(len(results.past_key_values.key_cache) == num_hidden_layers)\n             self.assertTrue(results.past_key_values.key_cache[0].shape == cache_shape)\n \n-    @require_quanto\n+    @require_optimum_quanto\n     @pytest.mark.generate\n     def test_generate_with_quant_cache(self):\n         for model_class in self.all_generative_model_classes:"
        },
        {
            "sha": "08cc48d0cccd36a1c876b06c45fc6a24c64a5c0d",
            "filename": "tests/quantization/quanto_integration/test_quanto.py",
            "status": "modified",
            "additions": 19,
            "deletions": 21,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cac4a4876b5c263e51b1c0e8887f35cfb6f266b1/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fquanto_integration%2Ftest_quanto.py?ref=cac4a4876b5c263e51b1c0e8887f35cfb6f266b1",
            "patch": "@@ -19,13 +19,13 @@\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n from transformers.testing_utils import (\n     require_accelerate,\n-    require_quanto,\n+    require_optimum_quanto,\n     require_read_token,\n     require_torch_gpu,\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_quanto_available, is_torch_available\n+from transformers.utils import is_accelerate_available, is_optimum_quanto_available, is_torch_available\n \n \n if is_torch_available():\n@@ -36,8 +36,8 @@\n if is_accelerate_available():\n     from accelerate import init_empty_weights\n \n-if is_quanto_available():\n-    from quanto import QLayerNorm, QLinear\n+if is_optimum_quanto_available():\n+    from optimum.quanto import QLayerNorm, QLinear\n \n     from transformers.integrations.quanto import replace_with_quanto_layers\n \n@@ -47,7 +47,7 @@ def test_attributes(self):\n         pass\n \n \n-@require_quanto\n+@require_optimum_quanto\n @require_accelerate\n class QuantoTestIntegration(unittest.TestCase):\n     model_id = \"facebook/opt-350m\"\n@@ -124,7 +124,7 @@ def test_conversion_with_modules_to_not_convert(self):\n \n @slow\n @require_torch_gpu\n-@require_quanto\n+@require_optimum_quanto\n @require_accelerate\n class QuantoQuantizationTest(unittest.TestCase):\n     \"\"\"\n@@ -187,7 +187,7 @@ def test_generate_quality_cuda(self):\n         self.check_inference_correctness(self.quantized_model, \"cuda\")\n \n     def test_quantized_model_layers(self):\n-        from quanto import QBitsTensor, QModuleMixin, QTensor\n+        from optimum.quanto import QBitsTensor, QModuleMixin, QTensor\n \n         \"\"\"\n         Suite of simple test to check if the layers are quantized and are working properly\n@@ -256,7 +256,7 @@ def check_same_model(self, model1, model2):\n             self.assertTrue(torch.equal(d0[k], d1[k].to(d0[k].device)))\n \n     def test_compare_with_quanto(self):\n-        from quanto import freeze, qint4, qint8, quantize\n+        from optimum.quanto import freeze, qint4, qint8, quantize\n \n         w_mapping = {\"int8\": qint8, \"int4\": qint4}\n         model = AutoModelForCausalLM.from_pretrained(\n@@ -272,7 +272,7 @@ def test_compare_with_quanto(self):\n \n     @unittest.skip\n     def test_load_from_quanto_saved(self):\n-        from quanto import freeze, qint4, qint8, quantize\n+        from optimum.quanto import freeze, qint4, qint8, quantize\n \n         from transformers import QuantoConfig\n \n@@ -356,21 +356,19 @@ def test_check_offload_quantized(self):\n         \"\"\"\n         We check that we have unquantized value in the cpu and in the disk\n         \"\"\"\n-        import quanto\n+        from optimum.quanto import QBitsTensor, QTensor\n \n         cpu_weights = self.quantized_model.transformer.h[22].self_attention.query_key_value._hf_hook.weights_map[\n             \"weight\"\n         ]\n         disk_weights = self.quantized_model.transformer.h[23].self_attention.query_key_value._hf_hook.weights_map[\n             \"weight\"\n         ]\n-        self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(cpu_weights, quanto.QTensor))\n-        self.assertTrue(isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, quanto.QTensor))\n+        self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(cpu_weights, QTensor))\n+        self.assertTrue(isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, QTensor))\n         if self.weights == \"int4\":\n-            self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(disk_weights, quanto.QBitsTensor))\n-            self.assertTrue(\n-                isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, quanto.QBitsTensor)\n-            )\n+            self.assertTrue(isinstance(cpu_weights, torch.Tensor) and not isinstance(disk_weights, QBitsTensor))\n+            self.assertTrue(isinstance(disk_weights, torch.Tensor) and not isinstance(disk_weights, QBitsTensor))\n \n \n @unittest.skip(reason=\"Skipping test class because serialization is not supported yet\")\n@@ -416,18 +414,18 @@ class QuantoQuantizationSerializationCudaTest(QuantoQuantizationTest):\n \n \n class QuantoQuantizationQBitsTensorTest(QuantoQuantizationTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is Nils, I am a student of the University\"\n+    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n     weights = \"int4\"\n \n \n class QuantoQuantizationQBitsTensorOffloadTest(QuantoQuantizationOffloadTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is Nils, I am a student of the University\"\n+    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n     weights = \"int4\"\n \n \n @unittest.skip(reason=\"Skipping test class because serialization is not supported yet\")\n class QuantoQuantizationQBitsTensorSerializationTest(QuantoQuantizationSerializationTest):\n-    EXPECTED_OUTPUTS = \"Hello my name is Nils, I am a student of the University\"\n+    EXPECTED_OUTPUTS = \"Hello my name is John, I am a professional photographer, I\"\n     weights = \"int4\"\n \n \n@@ -443,14 +441,14 @@ def test_quantize_activation(self):\n         self.assertIn(\"We don't support quantizing the activations with transformers library\", str(e.exception))\n \n \n-@require_quanto\n+@require_optimum_quanto\n @require_torch_gpu\n class QuantoKVCacheQuantizationTest(unittest.TestCase):\n     @slow\n     @require_read_token\n     def test_quantized_cache(self):\n         EXPECTED_TEXT_COMPLETION = [\n-            \"Simply put, the theory of relativity states that 1) the speed of light is the same for all observers, and 2) the laws of physics are the same for all observers.\\nThe first part of the theory of relativity\",\n+            \"Simply put, the theory of relativity states that 1) the speed of light is the same for all observers, and 2) the laws of physics are the same for all observers.\\nThe first part of the theory is the most\",\n             \"My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p\",\n         ]\n "
        }
    ],
    "stats": {
        "total": 176,
        "additions": 121,
        "deletions": 55
    }
}