{
    "author": "jerryzh168",
    "message": "Restructure torchao quantization examples (#37592)\n\n* Restructure torchao quantization examples\n\nSummary:\nMainly structured the examples by hardwares and then listed\nthe recommended quantization methods for each hardware H100 GPU, A100 GPU and CPU\n\nAlso added example for push_to_hub\n\nTest Plan:\nnot required\n\nReviewers:\n\nSubscribers:\n\nTasks:\n\nTags:\n\n* update\n\n* drop float8 cpu\n\n* address comments and simplify\n\n* small update\n\n* link update\n\n* minor update",
    "sha": "7eb1107cc29cef3028a03a5ad992621d94c061b9",
    "files": [
        {
            "sha": "62e3723403bde6335d3c9f42ca79c6c68a5821d6",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 65,
            "deletions": 83,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/7eb1107cc29cef3028a03a5ad992621d94c061b9/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7eb1107cc29cef3028a03a5ad992621d94c061b9/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=7eb1107cc29cef3028a03a5ad992621d94c061b9",
            "patch": "@@ -33,18 +33,19 @@ See the table below for additional torchao features.\n \n torchao supports the [quantization techniques](https://github.com/pytorch/ao/blob/main/torchao/quantization/README.md) below.\n \n-- A16W8 Int8 WeightOnly Quantization\n-- A16W4 WeightOnly Quantization\n-- A8W8 Int8 Dynamic Quantization\n+- A16W8 Float8 Dynamic Quantization\n - A16W8 Float8 WeightOnly Quantization\n+- A8W8 Int8 Dynamic Quantization\n+- A16W8 Int8 Weight Only Quantization\n+- A16W4 Int4 Weight Only Quantization\n - Autoquantization\n \n \n Check the table below to see if your hardware is compatible.\n \n | Component | Compatibility |\n |----------|----------------|\n-| CUDA Versions | âœ… cu118, cu124, cu126, cu128 |\n+| CUDA Versions | âœ… cu118, cu126, cu128 |\n | CPU | âœ… change `device_map=\"cpu\"` (see examples below) |\n \n \n@@ -56,14 +57,14 @@ Install torchao from PyPi or the PyTorch index with the following commands.\n \n ```bash\n # Updating ðŸ¤— Transformers to the latest version, as the example script below uses the new auto compilation\n-# Stable release from Pypi which will default to CUDA 12.4\n+# Stable release from Pypi which will default to CUDA 12.6\n pip install --upgrade torchao transformers\n ```\n </hfoption> \n <hfoption id=\"PyTorch Index\">\n Stable Release from the PyTorch index\n ```bash\n-pip install torchao --extra-index-url https://download.pytorch.org/whl/cu124 # options are cpu/cu118/cu124/cu126\n+pip install torchao --index-url https://download.pytorch.org/whl/cu126 # options are cpu/cu118/cu126/cu128\n ```\n </hfoption>\n </hfoptions>\n@@ -80,15 +81,19 @@ You can manually choose the quantization types and settings or automatically sel\n \n Create a [`TorchAoConfig`] and specify the quantization type and `group_size` of the weights to quantize (for int8 weight only and int4 weight only). Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method.\n \n-<hfoptions id=\"examples\">\n-<hfoption id=\"int8-weight-only cuda\">\n+We'll show examples for recommended quantization methods based on hardwares, e.g. A100 GPU, H100 GPU, CPU.\n \n+### H100 GPU\n+<hfoptions id=\"examples-H100-GPU\">\n+<hfoption id=\"float8-dynamic-and-weight-only\">\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8WeightOnlyConfig\n+from torchao.quantization import Float8DynamicActivationFloat8WeightConfig\n \n-quant_config = Int8WeightOnlyConfig(group_size=128)\n+quant_config = Float8DynamicActivationFloat8WeightConfig()\n+# or float8 weight only quantization\n+# quant_config = Float8WeightOnlyConfig()\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -109,41 +114,48 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n \n-<hfoption id=\"int8-weight-only cpu\">\n+</hfoption>\n+<hfoption id=\"int4-weight-only\">\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8WeightOnlyConfig\n+from torchao.quantization import GemliteUIntXWeightOnlyConfig\n \n-quant_config = Int8WeightOnlyConfig(group_size=128)\n+# We integrated with gemlite, which optimizes for batch size N on A100 and H100\n+quant_config = GemliteUIntXWeightOnlyConfig(group_size=128)\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n quantized_model = AutoModelForCausalLM.from_pretrained(\n     \"meta-llama/Llama-3.1-8B-Instruct\",\n     torch_dtype=\"auto\",\n-    device_map=\"cpu\",\n+    device_map=\"auto\",\n     quantization_config=quantization_config\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"int4-weight-only cuda\">\n+</hfoptions>\n \n+### A100 GPU\n+<hfoptions id=\"examples-A100-GPU\">\n+<hfoption id=\"int8-dynamic-and-weight-only\">\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.quantization import Int8WeightOnlyConfig\n \n-quant_config = Int4WeightOnlyConfig(group_size=128)\n+quant_config = Int8DynamicActivationInt8WeightConfig()\n+# or int8 weight only quantization\n+# quant_config = Int8WeightOnlyConfig()\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -164,45 +176,21 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n \n-<hfoption id=\"int4-weight-only cpu\">\n-\n-> [!TIP]\n-> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`.\n+<hfoption id=\"int4-weight-only\">\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n from torchao.quantization import Int4WeightOnlyConfig\n-from torchao.dtypes import Int4CPULayout\n \n-quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())\n-quantization_config = TorchAoConfig(quant_type=quant_config)\n+# For batch size N, we recommend gemlite, which may require autotuning\n+# default is 4 bit, 8 bit is also supported by passing `bit_width=8`\n+quant_config = GemliteUIntXWeightOnlyConfig(group_size=128)\n \n-# Load and quantize the model\n-quantized_model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n-    device_map=\"cpu\",\n-    quantization_config=quantization_config\n-)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n-input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\")\n-\n-# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n-output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n-print(tokenizer.decode(output[0], skip_special_tokens=True))\n-```\n-</hfoption>\n-<hfoption id=\"int8-dynamic-quantization cuda\">\n-\n-```py\n-import torch\n-from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n+# For batch size 1, we also have custom tinygemm kernel that's only optimized for this\n+# We can set `use_hqq` to `True` for better accuracy\n+# quant_config = Int4WeightOnlyConfig(group_size=128, use_hqq=True)\n \n-quant_config = Int8DynamicActivationInt8WeightConfig()\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -222,14 +210,18 @@ output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implemen\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"int8-dynamic-quantization cpu\">\n+</hfoptions>\n \n+### CPU\n+<hfoptions id=\"examples-CPU\">\n+<hfoption id=\"int8-dynamic-and-weight-only\">\n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Int8DynamicActivationInt8WeightConfig\n+from torchao.quantization import Int8WeightOnlyConfig\n \n quant_config = Int8DynamicActivationInt8WeightConfig()\n+# quant_config = Int8WeightOnlyConfig()\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -249,42 +241,18 @@ output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implemen\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-<hfoption id=\"float8-weight-only cuda\">\n+<hfoption id=\"int4-weight-only\">\n \n-```py\n-import torch\n-from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Float8WeightOnlyConfig\n-\n-quant_config = Float8WeightOnlyConfig()\n-quantization_config = TorchAoConfig(quant_type=quant_config)\n-\n-# Load and quantize the model\n-quantized_model = AutoModelForCausalLM.from_pretrained(\n-    \"meta-llama/Llama-3.1-8B-Instruct\",\n-    torch_dtype=\"auto\",\n-    device_map=\"auto\",\n-    quantization_config=quantization_config\n-)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n-input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n-\n-# auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n-output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n-print(tokenizer.decode(output[0], skip_special_tokens=True))\n-\n-```\n-</hfoption>\n-<hfoption id=\"float8-weight-only cpu\">\n+> [!TIP]\n+> Run the quantized model on a CPU by changing `device_map` to `\"cpu\"` and `layout` to `Int4CPULayout()`.\n \n ```py\n import torch\n from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n-from torchao.quantization import Float8WeightOnlyConfig\n+from torchao.quantization import Int4WeightOnlyConfig\n+from torchao.dtypes import Int4CPULayout\n \n-quant_config = Float8WeightOnlyConfig()\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -304,7 +272,6 @@ output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implemen\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```\n </hfoption>\n-\n </hfoptions>\n \n ### Autoquant\n@@ -313,6 +280,8 @@ If you want to automatically choose a quantization type for quantizable layers (\n \n The `autoquant` API automatically chooses a quantization type by micro-benchmarking on input type and shape and compiling a single linear layer.\n \n+Note: autoquant is for GPU only right now.\n+\n Create a [`TorchAoConfig`] and set to `\"autoquant\"`. Set the `cache_implementation` to `\"static\"` to automatically [torch.compile](https://pytorch.org/tutorials/intermediate/torch_compile_tutorial.html) the forward method. Finally, call `finalize_autoquant` on the quantized model to finalize the quantization and log the input shapes.\n \n \n@@ -346,11 +315,24 @@ torchao implements [torch.Tensor subclasses](https://pytorch.org/docs/stable/not\n \n To avoid arbitrary user code execution, torchao sets `weights_only=True` in [torch.load](https://pytorch.org/docs/stable/generated/torch.load.html) to ensure only tensors are loaded. Any known user functions can be whitelisted with [add_safe_globals](https://pytorch.org/docs/stable/notes/serialization.html#torch.serialization.add_safe_globals).\n \n+<hfoptions id=\"serialization-examples\">\n+<hfoption id=\"save-locally\">\n ```py\n # don't serialize model with Safetensors\n output_dir = \"llama3-8b-int4wo-128\"\n quantized_model.save_pretrained(\"llama3-8b-int4wo-128\", safe_serialization=False)\n ```\n+</hfoption>\n+<hfoption id=\"push-to-huggingface-hub\">\n+```py\n+# don't serialize model with Safetensors\n+USER_ID = \"your_huggingface_user_id\"\n+REPO_ID = \"llama3-8b-int4wo-128\"\n+quantized_model.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\", safe_serialization=False)\n+tokenizer.push_to_hub(f\"{USER_ID}/llama3-8b-int4wo-128\")\n+```\n+</hfoption>\n+\n \n ## Loading quantized models\n \n@@ -486,4 +468,4 @@ Refer to [Other Available Quantization Techniques](https://github.com/pytorch/ao\n \n ## Issues\n \n-If you encounter any issues with the Transformers integration, please open an issue on the [Transformers](https://github.com/huggingface/transformers/issues) repository. For issues directly related to torchao, please open an issue on the [torchao](https://github.com/pytorch/ao/issues) repository.\n\\ No newline at end of file\n+If you encounter any issues with the Transformers integration, please open an issue on the [Transformers](https://github.com/huggingface/transformers/issues) repository. For issues directly related to torchao, please open an issue on the [torchao](https://github.com/pytorch/ao/issues) repository."
        }
    ],
    "stats": {
        "total": 148,
        "additions": 65,
        "deletions": 83
    }
}