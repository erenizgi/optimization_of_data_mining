{
    "author": "VladOS95-cyber",
    "message": "Improve gguf tensor processing (#34515)\n\n* add tensor processing system to separate logic for models\r\n\r\n* format refactoring\r\n\r\n* small fix\r\n\r\n* make some methods private\r\n\r\n* move custom methods to processors\r\n\r\n* refactor tensor processing\r\n\r\n* format fix",
    "sha": "ae5cbf804bf8eeefa8f4a4359f0ea055b99369ad",
    "files": [
        {
            "sha": "cca6d548cdf3aca2931ce751fed3bbfe112bdb04",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 212,
            "deletions": 124,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae5cbf804bf8eeefa8f4a4359f0ea055b99369ad/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae5cbf804bf8eeefa8f4a4359f0ea055b99369ad/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=ae5cbf804bf8eeefa8f4a4359f0ea055b99369ad",
            "patch": "@@ -15,7 +15,7 @@\n # limitations under the License.\n \n import re\n-from typing import Dict, Optional\n+from typing import Dict, NamedTuple, Optional\n \n import numpy as np\n from tqdm import tqdm\n@@ -55,6 +55,200 @@\n GGUF_SUPPORTED_ARCHITECTURES = list(GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"].keys())\n \n \n+class GGUFTensor(NamedTuple):\n+    weights: np.ndarray\n+    name: str\n+    metadata: dict\n+\n+\n+class TensorProcessor:\n+    def __init__(self, config=None):\n+        self.config = config or {}\n+\n+    def process(self, weights, name, **kwargs):\n+        return GGUFTensor(weights, name, {})\n+\n+\n+class LlamaTensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        if \".attn_k.\" in name or \".attn_q.\" in name:\n+            num_heads = self.config.get(\"num_attention_heads\")\n+            num_kv_heads = self.config.get(\"num_key_value_heads\")\n+\n+            if None in (num_heads, num_kv_heads):\n+                return GGUFTensor(weights, name, {})\n+            if \".attn_q.\" in name:\n+                weights = self._reverse_permute_weights(weights, num_heads, num_heads)\n+            elif \".attn_k.\" in name:\n+                weights = self._reverse_permute_weights(weights, num_heads, num_kv_heads)\n+        return GGUFTensor(weights, name, {})\n+\n+    def _reverse_permute_weights(\n+        self, weights: np.ndarray, n_head: int, num_kv_heads: Optional[int] = None\n+    ) -> np.ndarray:\n+        # Original permutation implementation\n+        # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L1402-L1408\n+        if num_kv_heads is not None and n_head != num_kv_heads:\n+            n_head = num_kv_heads\n+\n+        dim = weights.shape[0] // n_head // 2\n+        w = weights.reshape(n_head, dim, 2, *weights.shape[1:])\n+        return w.swapaxes(2, 1).reshape(weights.shape)\n+\n+\n+class Qwen2MoeTensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        if \"_exp\" in name:\n+            tensor_key_mapping = kwargs.get(\"tensor_key_mapping\")\n+            parsed_parameters = kwargs.get(\"parsed_parameters\")\n+            if tensor_key_mapping:\n+                self._split_moe_expert_tensor(weights, parsed_parameters, name, tensor_key_mapping)\n+                return GGUFTensor(weights, None, {})\n+        if \"ffn_gate_inp_shexp\" in name:\n+            # for compatibility tensor shared_expert_gate must be (1, 2048) dim,\n+            # quantized one is (2048)\n+            weights = np.expand_dims(weights, axis=0)\n+        return GGUFTensor(weights, name, {})\n+\n+    def _split_moe_expert_tensor(\n+        self, weights: np.ndarray, parsed_parameters: Dict[str, Dict], name: str, tensor_key_mapping: dict\n+    ):\n+        # Original merge implementation\n+        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n+        exp_name = \"\"\n+        if \"ffn_gate_exps\" in name:\n+            exp_name = \"gate_proj\"\n+        elif \"ffn_down_exps\" in name:\n+            exp_name = \"down_proj\"\n+        elif \"ffn_up_exps\" in name:\n+            exp_name = \"up_proj\"\n+        else:\n+            raise ValueError(f\"Cannot map expert tensor {name} in Qwen2Moe architecture.\")\n+        for tensor_name in tensor_key_mapping:\n+            if tensor_name in name:\n+                name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n+        w_counter = self.config.get(\"num_experts\", 60)\n+        for i in range(0, w_counter):\n+            temp_name = name.replace(\".weight\", f\".{i}.{exp_name}.weight\")\n+            exp_weight = weights[i]\n+            parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))\n+\n+\n+class BloomTensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        if \"attn_qkv\" in name:\n+            num_heads = self.config[\"n_head\"]\n+            n_embed = self.config[\"hidden_size\"]\n+            if \"weight\" in name:\n+                weights = self._reverse_reshape_weights(weights, num_heads, n_embed)\n+            else:\n+                weights = self._reverse_reshape_bias(weights, num_heads, n_embed)\n+        return GGUFTensor(weights, name, {})\n+\n+    def _reverse_reshape_weights(self, weights: np.ndarray, n_head: int, n_embed: int):\n+        # Original reshape implementation\n+        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L972-L985\n+        q, k, v = np.array_split(weights, 3, axis=0)\n+\n+        q = q.reshape(n_head, n_embed // n_head, n_embed)\n+        k = k.reshape(n_head, n_embed // n_head, n_embed)\n+        v = v.reshape(n_head, n_embed // n_head, n_embed)\n+        qkv_weights = np.stack([q, k, v], axis=1)\n+\n+        return qkv_weights.reshape(n_head * 3 * (n_embed // n_head), n_embed)\n+\n+    def _reverse_reshape_bias(self, weights: np.ndarray, n_head: int, n_embed: int):\n+        # Original reshape implementation\n+        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L986-L998\n+        q_bias, k_bias, v_bias = np.array_split(weights, 3)\n+\n+        q_bias = q_bias.reshape(n_head, n_embed // n_head)\n+        k_bias = k_bias.reshape(n_head, n_embed // n_head)\n+        v_bias = v_bias.reshape(n_head, n_embed // n_head)\n+\n+        qkv_bias = np.stack([q_bias, k_bias, v_bias], axis=1).flatten()\n+        return qkv_bias\n+\n+\n+class T5TensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        bid = None\n+        for chunk in name.split(\".\"):\n+            if chunk.isdigit():\n+                bid = int(chunk)\n+                break\n+        return GGUFTensor(weights, name, {\"bid\": bid})\n+\n+\n+class GPT2TensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        # Original transpose implementation\n+        # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L2060-L2061\n+        if (\n+            \"attn_qkv.weight\" in name\n+            or \"ffn_down.weight\" in name\n+            or \"ffn_up.weight\" in name\n+            or \"attn_output.weight\" in name\n+        ):\n+            weights = weights.T\n+\n+        # Handle special case for output.weight\n+        if name == \"output.weight\":\n+            # output.weight has conflicts with attn_output.weight in name checking\n+            # Store the tensor directly and signal to skip further processing\n+            name = \"lm_head.weight\"\n+            parsed_parameters = kwargs.get(\"parsed_parameters\", {})\n+            parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n+            name = None  # Signal to skip further processing\n+        return GGUFTensor(weights, name, {})\n+\n+\n+class MambaTensorProcessor(TensorProcessor):\n+    def __init__(self, config=None):\n+        super().__init__(config=config)\n+\n+    def process(self, weights, name, **kwargs):\n+        if \"ssm_d\" in name and \"bias\" not in name and \"weight\" not in name:\n+            # ssm_d has conflicts with ssm_dt in name checking\n+            # we have to explicitly check that name is exactly ssm_d\n+            name = name.replace(\"ssm_d\", \"mixer.D\")\n+        if \"ssm_conv1d.weight\" in name:\n+            # for compatibility tensor ssm_conv1d must be (5120, 1, 4]) dim,\n+            # quantized one is (5120, 4)\n+            weights = np.expand_dims(weights, axis=1)\n+        if \"ssm_a\" in name:\n+            # Original exponential implementation\n+            # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L2975-L2977\n+            weights = np.log(-weights)\n+        return GGUFTensor(weights, name, {})\n+\n+\n+TENSOR_PROCESSORS = {\n+    \"llama\": LlamaTensorProcessor,\n+    \"qwen2moe\": Qwen2MoeTensorProcessor,\n+    \"bloom\": BloomTensorProcessor,\n+    \"t5\": T5TensorProcessor,\n+    \"t5encoder\": T5TensorProcessor,\n+    \"gpt2\": GPT2TensorProcessor,\n+    \"mamba\": MambaTensorProcessor,\n+}\n+\n+\n def read_field(reader, field):\n     value = reader.fields[field]\n     return [_gguf_parse_value(value.parts[_data_index], value.types) for _data_index in value.data]\n@@ -177,73 +371,28 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n \n     if return_tensors:\n         tensor_key_mapping = GGUF_TO_TRANSFORMERS_MAPPING[\"tensors\"][architecture + model_size]\n+        config = parsed_parameters.get(\"config\", {})\n+\n+        ProcessorClass = TENSOR_PROCESSORS.get(architecture, TensorProcessor)\n+        processor = ProcessorClass(config=config)\n \n         for tensor in tqdm(reader.tensors, desc=\"Converting and de-quantizing GGUF tensors...\"):\n             name = tensor.name\n-\n             weights = dequantize(tensor.data, tensor.tensor_type)\n \n-            if architecture == \"llama\" and (\".attn_k.\" in name or \".attn_q.\" in name):\n-                num_heads = parsed_parameters[\"config\"][\"num_attention_heads\"]\n-                num_kv_heads = parsed_parameters[\"config\"][\"num_key_value_heads\"]\n-                if \".attn_q.\" in name:\n-                    weights = reverse_permute_weights(weights, num_heads, num_heads)\n-                elif \".attn_k.\" in name:\n-                    weights = reverse_permute_weights(weights, num_heads, num_kv_heads)\n-\n-            if architecture == \"qwen2moe\":\n-                if \"_exp\" in name:\n-                    split_moe_expert_tensor(weights, parsed_parameters, name, tensor_key_mapping)\n-                    continue\n-                if \"ffn_gate_inp_shexp\" in name:\n-                    # for compatibility tensor shared_expert_gate must be (1, 2048) dim,\n-                    # quantized one is (2048)\n-                    weights = np.expand_dims(weights, axis=0)\n-\n-            if architecture == \"bloom\" and \"attn_qkv\" in name:\n-                num_heads = parsed_parameters[\"config\"][\"n_head\"]\n-                n_embed = parsed_parameters[\"config\"][\"hidden_size\"]\n-                if \"weight\" in name:\n-                    weights = reverse_reshape_weights(weights, num_heads, n_embed)\n-                else:\n-                    weights = reverse_reshape_bias(weights, num_heads, n_embed)\n-\n-            bid = None\n-            if architecture in (\"t5\", \"t5encoder\"):\n-                for chunk in name.split(\".\"):\n-                    if chunk.isdigit():\n-                        bid = int(chunk)\n-                        break\n-\n-            if architecture == \"gpt2\":\n-                if (\n-                    \"attn_qkv.weight\" in name\n-                    or \"ffn_down.weight\" in name\n-                    or \"ffn_up.weight\" in name\n-                    or \"attn_output.weight\" in name\n-                ):\n-                    # Original transpose implementation\n-                    # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L2060-L2061\n-                    weights = weights.T\n-                if name == \"output.weight\":\n-                    # output.weight has conflicts with attn_output.weight in name checking\n-                    # we have to explicitly check that name is exactly output.weight\n-                    name = \"lm_head.weight\"\n-                    parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))\n-                    continue\n-            if architecture == \"mamba\":\n-                if \"ssm_d\" in name and \"bias\" not in name and \"weight\" not in name:\n-                    # ssm_d has conflicts with ssm_dt in name checking\n-                    # we have to explicitly check that name is exactly ssm_d\n-                    name = name.replace(\"ssm_d\", \"mixer.D\")\n-                if \"ssm_conv1d.weight\" in name:\n-                    # for compatibility tensor ssm_conv1d must be (5120, 1, 4]) dim,\n-                    # quantized one is (5120, 4)\n-                    weights = np.expand_dims(weights, axis=1)\n-                if \"ssm_a\" in name:\n-                    # Original exponential implementation\n-                    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L2975-L2977\n-                    weights = np.log(-weights)\n+            result = processor.process(\n+                weights=weights,\n+                name=name,\n+                tensor_key_mapping=tensor_key_mapping,\n+                parsed_parameters=parsed_parameters,\n+            )\n+\n+            weights = result.weights\n+            name = result.name\n+            bid = result.metadata.get(\"bid\")\n+\n+            if name is None:\n+                continue\n \n             for tensor_name in tensor_key_mapping:\n                 if tensor_name.format(bid=bid) in name:\n@@ -256,64 +405,3 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n         logger.info(f\"Some keys of the GGUF file were not considered: {reader_keys}\")\n \n     return parsed_parameters\n-\n-\n-def reverse_permute_weights(weights: np.ndarray, n_head: int, num_kv_heads: Optional[int] = None) -> np.ndarray:\n-    # Original permutation implementation\n-    # https://github.com/ggerganov/llama.cpp/blob/a38b884c6c4b0c256583acfaaabdf556c62fabea/convert_hf_to_gguf.py#L1402-L1408\n-    if num_kv_heads is not None and n_head != num_kv_heads:\n-        n_head = num_kv_heads\n-\n-    dim = weights.shape[0] // n_head // 2\n-    w = weights.reshape(n_head, dim, 2, *weights.shape[1:])\n-    return w.swapaxes(2, 1).reshape(weights.shape)\n-\n-\n-def reverse_reshape_weights(weights: np.ndarray, n_head: int, n_embed: int):\n-    # Original reshape implementation\n-    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L972-L985\n-    q, k, v = np.array_split(weights, 3, axis=0)\n-\n-    q = q.reshape(n_head, n_embed // n_head, n_embed)\n-    k = k.reshape(n_head, n_embed // n_head, n_embed)\n-    v = v.reshape(n_head, n_embed // n_head, n_embed)\n-    qkv_weights = np.stack([q, k, v], axis=1)\n-\n-    return qkv_weights.reshape(n_head * 3 * (n_embed // n_head), n_embed)\n-\n-\n-def reverse_reshape_bias(weights: np.ndarray, n_head: int, n_embed: int):\n-    # Original reshape implementation\n-    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L986-L998\n-    q_bias, k_bias, v_bias = np.array_split(weights, 3)\n-\n-    q_bias = q_bias.reshape(n_head, n_embed // n_head)\n-    k_bias = k_bias.reshape(n_head, n_embed // n_head)\n-    v_bias = v_bias.reshape(n_head, n_embed // n_head)\n-\n-    qkv_bias = np.stack([q_bias, k_bias, v_bias], axis=1).flatten()\n-    return qkv_bias\n-\n-\n-def split_moe_expert_tensor(\n-    weights: np.ndarray, parsed_parameters: Dict[str, Dict], name: str, tensor_key_mapping: dict\n-):\n-    # Original merge implementation\n-    # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n-    exp_name = \"\"\n-    if \"ffn_gate_exps\" in name:\n-        exp_name = \"gate_proj\"\n-    elif \"ffn_down_exps\" in name:\n-        exp_name = \"down_proj\"\n-    elif \"ffn_up_exps\" in name:\n-        exp_name = \"up_proj\"\n-    else:\n-        raise ValueError(f\"Cannot map expert tensor {name} in Qwen2Moe architecture.\")\n-    for tensor_name in tensor_key_mapping:\n-        if tensor_name in name:\n-            name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n-    w_counter = parsed_parameters[\"config\"].get(\"num_experts\", 60)\n-    for i in range(0, w_counter):\n-        temp_name = name.replace(\".weight\", f\".{i}.{exp_name}.weight\")\n-        exp_weight = weights[i]\n-        parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))"
        }
    ],
    "stats": {
        "total": 336,
        "additions": 212,
        "deletions": 124
    }
}