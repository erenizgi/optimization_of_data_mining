{
    "author": "gathierry",
    "message": "support 3D attention mask in bert (#32105)\n\n* support 3D/4D attention mask in bert\r\n\r\n* test cases\r\n\r\n* update doc\r\n\r\n* fix doc",
    "sha": "342e800086821b375d8a779f9274059abdac5a8f",
    "files": [
        {
            "sha": "93d6d469b512267419fe43be53525cf52baab858",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/342e800086821b375d8a779f9274059abdac5a8f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342e800086821b375d8a779f9274059abdac5a8f/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=342e800086821b375d8a779f9274059abdac5a8f",
            "patch": "@@ -908,7 +908,7 @@ class BertForPreTrainingOutput(ModelOutput):\n             [`PreTrainedTokenizer.__call__`] for details.\n \n             [What are input IDs?](../glossary#input-ids)\n-        attention_mask (`torch.FloatTensor` of shape `({0})`, *optional*):\n+        attention_mask (`torch.FloatTensor` of shape `({0})`or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n             - 1 for tokens that are **not masked**,\n@@ -1023,7 +1023,7 @@ def forward(\n         encoder_hidden_states  (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n             Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\n             the model is configured as a decoder.\n-        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+        encoder_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)` or `(batch_size, sequence_length, target_length)`, *optional*):\n             Mask to avoid performing attention on the padding token indices of the encoder input. This mask is used in\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n \n@@ -1093,7 +1093,7 @@ def forward(\n         )\n \n         # Expand the attention mask\n-        if use_sdpa_attention_masks:\n+        if use_sdpa_attention_masks and attention_mask.dim() == 2:\n             # Expand the attention mask for SDPA.\n             # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n             if self.config.is_decoder:\n@@ -1120,7 +1120,7 @@ def forward(\n             if encoder_attention_mask is None:\n                 encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n \n-            if use_sdpa_attention_masks:\n+            if use_sdpa_attention_masks and encoder_attention_mask.dim() == 2:\n                 # Expand the attention mask for SDPA.\n                 # [bsz, seq_len] -> [bsz, 1, seq_len, seq_len]\n                 encoder_extended_attention_mask = _prepare_4d_attention_mask_for_sdpa("
        },
        {
            "sha": "5c87fbea8ee79536ee92352a4b4c28e0886c5313",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/342e800086821b375d8a779f9274059abdac5a8f/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/342e800086821b375d8a779f9274059abdac5a8f/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=342e800086821b375d8a779f9274059abdac5a8f",
            "patch": "@@ -498,6 +498,14 @@ def test_model_various_embeddings(self):\n             config_and_inputs[0].position_embedding_type = type\n             self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_model_3d_mask_shapes(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        # manipulate input_mask\n+        config_and_inputs = list(config_and_inputs)\n+        batch_size, seq_length = config_and_inputs[3].shape\n+        config_and_inputs[3] = random_attention_mask([batch_size, seq_length, seq_length])\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n     def test_model_as_decoder(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n@@ -535,6 +543,36 @@ def test_model_as_decoder_with_default_input_mask(self):\n             encoder_attention_mask,\n         )\n \n+    def test_model_as_decoder_with_3d_input_mask(self):\n+        (\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+        ) = self.model_tester.prepare_config_and_inputs_for_decoder()\n+\n+        batch_size, seq_length = input_mask.shape\n+        input_mask = random_attention_mask([batch_size, seq_length, seq_length])\n+        batch_size, seq_length = encoder_attention_mask.shape\n+        encoder_attention_mask = random_attention_mask([batch_size, seq_length, seq_length])\n+\n+        self.model_tester.create_and_check_model_as_decoder(\n+            config,\n+            input_ids,\n+            token_type_ids,\n+            input_mask,\n+            sequence_labels,\n+            token_labels,\n+            choice_labels,\n+            encoder_hidden_states,\n+            encoder_attention_mask,\n+        )\n+\n     def test_for_causal_lm(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_for_causal_lm(*config_and_inputs)"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 42,
        "deletions": 4
    }
}