{
    "author": "MekkCyber",
    "message": "Fix Failing GPTQ tests (#36666)\n\nfix tests",
    "sha": "0013ba61e5e90c30835aaa946ce9e6c3e00a9070",
    "files": [
        {
            "sha": "7d8d3501ce0e209a97ba3beb352f79fb56ebc88e",
            "filename": "tests/quantization/gptq/test_gptq.py",
            "status": "modified",
            "additions": 16,
            "deletions": 5,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/0013ba61e5e90c30835aaa946ce9e6c3e00a9070/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0013ba61e5e90c30835aaa946ce9e6c3e00a9070/tests%2Fquantization%2Fgptq%2Ftest_gptq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fgptq%2Ftest_gptq.py?ref=0013ba61e5e90c30835aaa946ce9e6c3e00a9070",
            "patch": "@@ -94,6 +94,7 @@ class GPTQTest(unittest.TestCase):\n     EXPECTED_OUTPUTS.add(\"Hello my name is Aiden, I am a student at the University\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is Nate and I am a member of the N\")\n     EXPECTED_OUTPUTS.add(\"Hello my name is Nellie and I am a student at the\")\n+    EXPECTED_OUTPUTS.add(\"Hello my name is Nate and I am a new member of the\")\n \n     # this seems a little small considering that we are doing 4bit quant but we have a small model and ww don't quantize the embeddings\n     EXPECTED_RELATIVE_DIFFERENCE = 1.664253062\n@@ -260,7 +261,9 @@ def test_serialization(self):\n                 if self.device_map == \"cpu\":\n                     quant_type = \"ipex\" if is_ipex_available() else \"torch\"\n                 else:\n-                    quant_type = \"exllama\"\n+                    # We expecte tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n+                    # TODO: Remove this once GPTQModel exllama kernels supports packing\n+                    quant_type = \"tritonv2\"\n                 quantized_model_from_saved = AutoModelForCausalLM.from_pretrained(\n                     tmpdirname, device_map=self.device_map\n                 )\n@@ -424,10 +427,18 @@ def setUpClass(cls):\n         cls.tokenizer = AutoTokenizer.from_pretrained(cls.model_name, use_fast=True)\n \n     def test_quantized_layers_type(self):\n-        self.assertEqual(\n-            self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n-            \"exllama\" if is_gptqmodel_available() else \"exllamav2\",\n-        )\n+        if is_auto_gptq_available() and not is_gptqmodel_available():\n+            self.assertEqual(\n+                self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n+                \"exllamav2\",\n+            )\n+        else:\n+            # We expecte tritonv2 to be used here, because exllama backend doesn't support packing https://github.com/ModelCloud/GPTQModel/issues/1354\n+            # TODO: Remove this once GPTQModel exllama kernels supports packing\n+            self.assertEqual(\n+                self.quantized_model.model.layers[0].self_attn.k_proj.QUANT_TYPE,\n+                \"tritonv2\",\n+            )\n \n     def check_inference_correctness(self, model):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 21,
        "additions": 16,
        "deletions": 5
    }
}