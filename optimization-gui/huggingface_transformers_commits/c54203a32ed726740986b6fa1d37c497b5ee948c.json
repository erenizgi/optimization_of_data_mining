{
    "author": "LysandreJik",
    "message": "gpt_oss last chat template changes (#39925)\n\nLast chat template changes",
    "sha": "c54203a32ed726740986b6fa1d37c497b5ee948c",
    "files": [
        {
            "sha": "37c054dc620db352d989bb03189aa994342ae3bb",
            "filename": "src/transformers/models/gpt_oss/convert_gpt_oss_weights_to_hf.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c54203a32ed726740986b6fa1d37c497b5ee948c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c54203a32ed726740986b6fa1d37c497b5ee948c/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconvert_gpt_oss_weights_to_hf.py?ref=c54203a32ed726740986b6fa1d37c497b5ee948c",
            "patch": "@@ -561,7 +561,7 @@ def write_tokenizer(tokenizer_path: str, save_dir: str, instruct: bool = False):\n                 {%- if not loop.last %}\n                     {{- \",\\n\" }}\n                 {%- else %}\n-                    {{- \"\\n\" }}\n+                    {{- \",\\n\" }}\n                 {%- endif -%}\n             {%- endfor %}\n             {{- \"}) => any;\\n\\n\" }}\n@@ -696,6 +696,15 @@ def write_tokenizer(tokenizer_path: str, save_dir: str, instruct: bool = False):\n             {%- endif %}\n         {%- endif %}\n         {%- if \"tool_calls\" in message %}\n+            {#- We need very careful handling here - we want to drop the tool call analysis message if the model #}\n+            {#- has output a later <|final|> message, but otherwise we want to retain it. This is the only case #}\n+            {#- when we render CoT/analysis messages in inference. #}\n+            {%- set future_final_message = namespace(found=false) %}\n+            {%- for future_message in loop_messages[loop.index:] %}\n+                {%- if future_message.role == 'assistant' and \"tool_calls\" not in future_message %}\n+                    {%- set future_final_message.found = true %}\n+                {%- endif %}\n+            {%- endfor %}\n             {#- We assume max 1 tool call per message, and so we infer the tool call name #}\n             {#- in \"tool\" messages from the most recent assistant tool call name #}\n             {%- set tool_call = message.tool_calls[0] %}\n@@ -704,9 +713,9 @@ def write_tokenizer(tokenizer_path: str, save_dir: str, instruct: bool = False):\n             {%- endif %}\n             {%- if message.content and message.thinking %}\n                 {{- raise_exception(\"Cannot pass both content and thinking in an assistant message with tool calls! Put the analysis message in one or the other, but not both.\") }}\n-            {%- elif message.content %}\n+            {%- elif message.content and not future_final_message.found %}\n                 {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.content + \"<|end|>\" }}\n-            {%- elif message.thinking %}\n+            {%- elif message.thinking and not future_final_message.found %}\n                 {{- \"<|start|>assistant<|channel|>analysis<|message|>\" + message.thinking + \"<|end|>\" }}\n             {%- endif %}\n             {{- \"<|start|>assistant to=\" }}"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 12,
        "deletions": 3
    }
}