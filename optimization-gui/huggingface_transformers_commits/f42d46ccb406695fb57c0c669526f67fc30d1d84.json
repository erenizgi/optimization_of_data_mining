{
    "author": "qubvel",
    "message": "Add common test for `torch.export` and fix some vision models (#35124)\n\n* Add is_torch_greater_or_equal test decorator\r\n\r\n* Add common test for torch.export\r\n\r\n* Fix bit\r\n\r\n* Fix focalnet\r\n\r\n* Fix imagegpt\r\n\r\n* Fix seggpt\r\n\r\n* Fix swin2sr\r\n\r\n* Enable torch.export test for vision models\r\n\r\n* Enable test for video models\r\n\r\n* Remove json\r\n\r\n* Enable for hiera\r\n\r\n* Enable for ijepa\r\n\r\n* Fix detr\r\n\r\n* Fic conditional_detr\r\n\r\n* Fix maskformer\r\n\r\n* Enable test maskformer\r\n\r\n* Fix test for deformable detr\r\n\r\n* Fix custom kernels for export in rt-detr and deformable-detr\r\n\r\n* Enable test for all DPT\r\n\r\n* Remove custom test for deformable detr\r\n\r\n* Simplify test to use only kwargs for export\r\n\r\n* Add comment\r\n\r\n* Move compile_compatible_method_lru_cache to utils\r\n\r\n* Fix beit export\r\n\r\n* Fix deformable detr\r\n\r\n* Fix copies data2vec<->beit\r\n\r\n* Fix typos, update test to work with dict\r\n\r\n* Add seed to the test\r\n\r\n* Enable test for vit_mae\r\n\r\n* Fix beit tests\r\n\r\n* [run-slow] beit, bit, conditional_detr, data2vec, deformable_detr, detr, focalnet, imagegpt, maskformer, rt_detr, seggpt, swin2sr\r\n\r\n* Add vitpose test\r\n\r\n* Add textnet test\r\n\r\n* Add dinov2 with registers\r\n\r\n* Update tests/test_modeling_common.py\r\n\r\n* Switch to torch.testing.assert_close\r\n\r\n* Fix masformer\r\n\r\n* Remove save-load from test\r\n\r\n* Add dab_detr\r\n\r\n* Add depth_pro\r\n\r\n* Fix and test RT-DETRv2\r\n\r\n* Fix dab_detr",
    "sha": "f42d46ccb406695fb57c0c669526f67fc30d1d84",
    "files": [
        {
            "sha": "40b01d34a8f5b245db0cc6b71f9a614d5c84b4fc",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 27,
            "deletions": 29,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -34,7 +34,7 @@\n     SemanticSegmenterOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -297,10 +297,9 @@ def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> N\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-        if window_size:\n+        self.has_relative_position_bias = bool(window_size)\n+        if self.has_relative_position_bias:\n             self.relative_position_bias = BeitRelativePositionBias(config, window_size=window_size)\n-        else:\n-            self.relative_position_bias = None\n \n     def transpose_for_scores(self, x):\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n@@ -312,7 +311,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"BeitRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -328,7 +327,7 @@ def forward(\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n \n         # Add relative position bias if present.\n-        if self.relative_position_bias is not None:\n+        if self.has_relative_position_bias:\n             height, width = resolution\n             window_size = (height // self.config.patch_size, width // self.config.patch_size)\n             attention_scores = attention_scores + self.relative_position_bias(\n@@ -367,7 +366,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"BeitRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -393,7 +392,7 @@ def forward(\n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n         attn_bias = None\n-        if self.relative_position_bias is not None:\n+        if self.has_relative_position_bias:\n             height, width = resolution\n             window_size = (height // self.config.patch_size, width // self.config.patch_size)\n             attn_bias = self.relative_position_bias(\n@@ -477,7 +476,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"BeitRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -546,7 +545,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"BeitRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -595,8 +594,7 @@ def __init__(self, config: BeitConfig, window_size: tuple) -> None:\n         )  # 2*Wh-1 * 2*Ww-1, nH\n         # cls to token & token 2 cls & cls to cls\n \n-        self.relative_position_indices = {}\n-\n+    @compile_compatible_method_lru_cache(maxsize=10)\n     def generate_relative_position_index(self, window_size: Tuple[int, int]) -> torch.Tensor:\n         \"\"\"\n         This method creates the relative position index, modified to support arbitrary window sizes,\n@@ -648,11 +646,9 @@ def forward(self, window_size, interpolate_pos_encoding: bool = False, dim_size=\n             [new_sub_table, old_relative_position_bias_table[old_num_relative_distance - 3 :]]\n         )\n \n-        key = window_size\n-        if key not in self.relative_position_indices.keys():\n-            self.relative_position_indices[key] = self.generate_relative_position_index(window_size)\n+        relative_position_index = self.generate_relative_position_index(window_size)\n+        relative_position_bias = new_relative_position_bias_table[relative_position_index.view(-1)]\n \n-        relative_position_bias = new_relative_position_bias_table[self.relative_position_indices[key].view(-1)]\n         # patch_size*num_patches_height, patch_size*num_patches_width, num_attention_heads\n         relative_position_bias = relative_position_bias.view(\n             window_size[0] * window_size[1] + 1, window_size[0] * window_size[1] + 1, -1\n@@ -675,10 +671,9 @@ class BeitEncoder(nn.Module):\n     def __init__(self, config: BeitConfig, window_size: Optional[tuple] = None) -> None:\n         super().__init__()\n         self.config = config\n-        if config.use_shared_relative_position_bias:\n+        self.has_relative_position_bias = config.use_shared_relative_position_bias\n+        if self.has_relative_position_bias:\n             self.relative_position_bias = BeitRelativePositionBias(config, window_size=window_size)\n-        else:\n-            self.relative_position_bias = None\n \n         # stochastic depth decay rule\n         dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n@@ -701,7 +696,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n+        resolution: Optional[Tuple[int, int]] = None,\n         return_dict: bool = True,\n     ) -> Union[tuple, BaseModelOutput]:\n         all_hidden_states = () if output_hidden_states else None\n@@ -711,6 +706,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n+            if self.has_relative_position_bias:\n+                height, width = resolution\n+                window_size = (height // self.config.patch_size, width // self.config.patch_size)\n+                relative_position_bias = self.relative_position_bias(\n+                    window_size, interpolate_pos_encoding=interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n+                )\n+            else:\n+                relative_position_bias = None\n+\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             if self.gradient_checkpointing and self.training:\n@@ -719,17 +723,11 @@ def forward(\n                     hidden_states,\n                     layer_head_mask,\n                     output_attentions,\n+                    relative_position_bias,\n+                    interpolate_pos_encoding,\n+                    resolution,\n                 )\n             else:\n-                height, width = resolution\n-                window_size = (height // self.config.patch_size, width // self.config.patch_size)\n-                relative_position_bias = (\n-                    self.relative_position_bias(\n-                        window_size, interpolate_pos_encoding=interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n-                    )\n-                    if self.relative_position_bias is not None\n-                    else None\n-                )\n                 layer_outputs = layer_module(\n                     hidden_states,\n                     layer_head_mask,"
        },
        {
            "sha": "d71e3bf94635215b93811afe9eca2fa21c2d5584",
            "filename": "src/transformers/models/bit/modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fmodeling_bit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -192,7 +192,7 @@ def compute_padding(x, kernel_size, stride, dilation):\n \n         self.compute_padding = compute_padding\n \n-    def __call__(self, input):\n+    def forward(self, input):\n         # Get width and height\n         input_height, input_width = input.size()[-2:]\n "
        },
        {
            "sha": "5a839f9513a870c77db54afdc2edb79923eface5",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -1735,7 +1735,11 @@ def forward(\n         # class logits + predicted bounding boxes\n         logits = self.class_labels_classifier(sequence_output)\n \n-        reference = outputs.reference_points if return_dict else outputs[-1]\n+        # Index [-2] is valid only if `output_attentions` and `output_hidden_states`\n+        # are not specified, otherwise it will be another index which is hard to determine.\n+        # Leave it as is, because it's not a common case to use\n+        # return_dict=False + output_attentions=True / output_hidden_states=True\n+        reference = outputs.reference_points if return_dict else outputs[-2]\n         reference_before_sigmoid = inverse_sigmoid(reference).transpose(0, 1)\n \n         hs = sequence_output\n@@ -2105,7 +2109,7 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)\n \n         if mask is not None:\n-            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n+            weights = weights.masked_fill(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights"
        },
        {
            "sha": "3e3294db07a1fccaf4ef8144207636b2dbb114be",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -1537,7 +1537,7 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)\n \n         if mask is not None:\n-            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n+            weights = weights.masked_fill(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights"
        },
        {
            "sha": "8e4d9c0bb26e1d8914094a5563d87b9d4d17fcb0",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 26,
            "deletions": 28,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -32,7 +32,7 @@\n     SemanticSegmenterOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n+from ...pytorch_utils import compile_compatible_method_lru_cache, find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n     add_start_docstrings,\n@@ -298,10 +298,9 @@ def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] =\n \n         self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n \n-        if window_size:\n+        self.has_relative_position_bias = bool(window_size)\n+        if self.has_relative_position_bias:\n             self.relative_position_bias = Data2VecVisionRelativePositionBias(config, window_size=window_size)\n-        else:\n-            self.relative_position_bias = None\n \n     def transpose_for_scores(self, x):\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n@@ -313,7 +312,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"Data2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -329,7 +328,7 @@ def forward(\n         attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n \n         # Add relative position bias if present.\n-        if self.relative_position_bias is not None:\n+        if self.has_relative_position_bias:\n             height, width = resolution\n             window_size = (height // self.config.patch_size, width // self.config.patch_size)\n             attention_scores = attention_scores + self.relative_position_bias(\n@@ -369,7 +368,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"Data2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -395,7 +394,7 @@ def forward(\n         query_layer = self.transpose_for_scores(mixed_query_layer)\n \n         attn_bias = None\n-        if self.relative_position_bias is not None:\n+        if self.has_relative_position_bias:\n             height, width = resolution\n             window_size = (height // self.config.patch_size, width // self.config.patch_size)\n             attn_bias = self.relative_position_bias(\n@@ -557,7 +556,7 @@ def forward(\n         hidden_states: torch.Tensor,\n         head_mask: Optional[torch.Tensor] = None,\n         output_attentions: bool = False,\n-        relative_position_bias: Optional[\"Data2VecVisionRelativePositionBias\"] = None,\n+        relative_position_bias: Optional[torch.Tensor] = None,\n         interpolate_pos_encoding: bool = False,\n         resolution: Optional[Tuple[int]] = None,\n     ) -> Union[Tuple[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]]:\n@@ -607,8 +606,7 @@ def __init__(self, config: Data2VecVisionConfig, window_size: tuple) -> None:\n         )  # 2*Wh-1 * 2*Ww-1, nH\n         # cls to token & token 2 cls & cls to cls\n \n-        self.relative_position_indices = {}\n-\n+    @compile_compatible_method_lru_cache(maxsize=10)\n     def generate_relative_position_index(self, window_size: Tuple[int, int]) -> torch.Tensor:\n         \"\"\"\n         This method creates the relative position index, modified to support arbitrary window sizes,\n@@ -660,11 +658,9 @@ def forward(self, window_size, interpolate_pos_encoding: bool = False, dim_size=\n             [new_sub_table, old_relative_position_bias_table[old_num_relative_distance - 3 :]]\n         )\n \n-        key = window_size\n-        if key not in self.relative_position_indices.keys():\n-            self.relative_position_indices[key] = self.generate_relative_position_index(window_size)\n+        relative_position_index = self.generate_relative_position_index(window_size)\n+        relative_position_bias = new_relative_position_bias_table[relative_position_index.view(-1)]\n \n-        relative_position_bias = new_relative_position_bias_table[self.relative_position_indices[key].view(-1)]\n         # patch_size*num_patches_height, patch_size*num_patches_width, num_attention_heads\n         relative_position_bias = relative_position_bias.view(\n             window_size[0] * window_size[1] + 1, window_size[0] * window_size[1] + 1, -1\n@@ -688,10 +684,9 @@ class Data2VecVisionEncoder(nn.Module):\n     def __init__(self, config: Data2VecVisionConfig, window_size: Optional[tuple] = None) -> None:\n         super().__init__()\n         self.config = config\n-        if config.use_shared_relative_position_bias:\n+        self.has_relative_position_bias = config.use_shared_relative_position_bias\n+        if self.has_relative_position_bias:\n             self.relative_position_bias = Data2VecVisionRelativePositionBias(config, window_size=window_size)\n-        else:\n-            self.relative_position_bias = None\n \n         # stochastic depth decay rule\n         dpr = [x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)]\n@@ -714,7 +709,7 @@ def forward(\n         output_attentions: bool = False,\n         output_hidden_states: bool = False,\n         interpolate_pos_encoding: bool = False,\n-        resolution: Optional[Tuple[int]] = None,\n+        resolution: Optional[Tuple[int, int]] = None,\n         return_dict: bool = True,\n     ) -> Union[tuple, BaseModelOutput]:\n         all_hidden_states = () if output_hidden_states else None\n@@ -724,6 +719,15 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n+            if self.has_relative_position_bias:\n+                height, width = resolution\n+                window_size = (height // self.config.patch_size, width // self.config.patch_size)\n+                relative_position_bias = self.relative_position_bias(\n+                    window_size, interpolate_pos_encoding=interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n+                )\n+            else:\n+                relative_position_bias = None\n+\n             layer_head_mask = head_mask[i] if head_mask is not None else None\n \n             if self.gradient_checkpointing and self.training:\n@@ -732,17 +736,11 @@ def forward(\n                     hidden_states,\n                     layer_head_mask,\n                     output_attentions,\n+                    relative_position_bias,\n+                    interpolate_pos_encoding,\n+                    resolution,\n                 )\n             else:\n-                height, width = resolution\n-                window_size = (height // self.config.patch_size, width // self.config.patch_size)\n-                relative_position_bias = (\n-                    self.relative_position_bias(\n-                        window_size, interpolate_pos_encoding=interpolate_pos_encoding, dim_size=hidden_states.shape[1]\n-                    )\n-                    if self.relative_position_bias is not None\n-                    else None\n-                )\n                 layer_outputs = layer_module(\n                     hidden_states,\n                     layer_head_mask,"
        },
        {
            "sha": "39d189418fea52e09b3159638d1b21407872b15f",
            "filename": "src/transformers/models/deformable_detr/modeling_deformable_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fmodeling_deformable_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -40,6 +40,7 @@\n     is_ninja_available,\n     is_timm_available,\n     is_torch_cuda_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n     requires_backends,\n@@ -705,7 +706,7 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n+        if self.disable_custom_kernels or MultiScaleDeformableAttention is None or is_torchdynamo_compiling():\n             # PyTorch implementation\n             output = multi_scale_deformable_attention(\n                 value, spatial_shapes_list, sampling_locations, attention_weights\n@@ -1606,7 +1607,7 @@ def gen_encoder_output_proposals(self, enc_output, padding_mask, spatial_shapes)\n         Args:\n             enc_output (Tensor[batch_size, sequence_length, hidden_size]): Output of the encoder.\n             padding_mask (Tensor[batch_size, sequence_length]): Padding mask for `enc_output`.\n-            spatial_shapes (Tensor[num_feature_levels, 2]): Spatial shapes of the feature maps.\n+            spatial_shapes (List[Tuple[int, int]]): Spatial shapes of the feature maps.\n \n         Returns:\n             `tuple(torch.FloatTensor)`: A tuple of feature map and bbox prediction.\n@@ -1786,7 +1787,7 @@ def forward(\n         enc_outputs_coord_logits = None\n         if self.config.two_stage:\n             object_query_embedding, output_proposals = self.gen_encoder_output_proposals(\n-                encoder_outputs[0], ~mask_flatten, spatial_shapes\n+                encoder_outputs[0], ~mask_flatten, spatial_shapes_list\n             )\n \n             # hack implementation for two-stage Deformable DETR"
        },
        {
            "sha": "0b006c44ad98a16ba0dab27a63d39fa3f975aa97",
            "filename": "src/transformers/models/detr/modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fmodeling_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -1801,7 +1801,7 @@ def forward(self, q, k, mask: Optional[Tensor] = None):\n         weights = torch.einsum(\"bqnc,bnchw->bqnhw\", queries_per_head * self.normalize_fact, keys_per_head)\n \n         if mask is not None:\n-            weights.masked_fill_(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n+            weights = weights.masked_fill(mask.unsqueeze(1).unsqueeze(1), torch.finfo(weights.dtype).min)\n         weights = nn.functional.softmax(weights.flatten(2), dim=-1).view(weights.size())\n         weights = self.dropout(weights)\n         return weights"
        },
        {
            "sha": "687654a22da3a9642832184ef99153ef560d6cde",
            "filename": "src/transformers/models/focalnet/modeling_focalnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fmodeling_focalnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -358,23 +358,23 @@ def forward(self, hidden_state):\n \n         # pre linear projection\n         x = self.projection_in(hidden_state).permute(0, 3, 1, 2).contiguous()\n-        q, ctx, self.gates = torch.split(x, (num_channels, num_channels, self.focal_level + 1), 1)\n+        q, ctx, gates = torch.split(x, (num_channels, num_channels, self.focal_level + 1), 1)\n \n         # context aggreation\n         ctx_all = 0\n         for level in range(self.focal_level):\n             ctx = self.focal_layers[level](ctx)\n-            ctx_all = ctx_all + ctx * self.gates[:, level : level + 1]\n+            ctx_all = ctx_all + ctx * gates[:, level : level + 1]\n         ctx_global = self.activation(ctx.mean(2, keepdim=True).mean(3, keepdim=True))\n-        ctx_all = ctx_all + ctx_global * self.gates[:, self.focal_level :]\n+        ctx_all = ctx_all + ctx_global * gates[:, self.focal_level :]\n \n         # normalize context\n         if self.normalize_modulator:\n             ctx_all = ctx_all / (self.focal_level + 1)\n \n         # focal modulation\n-        self.modulator = self.projection_context(ctx_all)\n-        x_out = q * self.modulator\n+        modulator = self.projection_context(ctx_all)\n+        x_out = q * modulator\n         x_out = x_out.permute(0, 2, 3, 1).contiguous()\n         if self.use_post_layernorm_in_modulation:\n             x_out = self.layernorm(x_out)"
        },
        {
            "sha": "dab0b0188a3eaa9f02e645e33fc1d110aa0ab1aa",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -164,13 +164,11 @@ def __init__(self, hidden_size: Tuple[int], eps: float = 1e-5):\n         self.eps = eps\n         self.weight = nn.Parameter(torch.Tensor(hidden_size))\n \n-    def forward(self, tensor: torch.Tensor) -> tuple:\n+    def forward(self, tensor: torch.Tensor) -> torch.Tensor:\n         # input is not mean centered\n-        return (\n-            tensor\n-            / torch.sqrt(torch.mean(torch.square(tensor), axis=-1, keepdim=True) + self.eps)\n-            * self.weight.data[..., :]\n-        )\n+        tensor = tensor / torch.sqrt(torch.mean(torch.square(tensor), axis=-1, keepdim=True) + self.eps)\n+        tensor = tensor * self.weight\n+        return tensor\n \n \n class ImageGPTAttention(nn.Module):"
        },
        {
            "sha": "b29672d7de36fbfa29be6e32e8783a8a5eacd5f9",
            "filename": "src/transformers/models/maskformer/modeling_maskformer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -1424,7 +1424,11 @@ def forward(\n         # repeat the queries \"q c -> b q c\"\n         batch_size = image_features.shape[0]\n         queries_embeddings = self.queries_embedder.weight.unsqueeze(0).repeat(batch_size, 1, 1)\n-        inputs_embeds = torch.zeros_like(queries_embeddings, requires_grad=True)\n+        inputs_embeds = torch.zeros_like(queries_embeddings, requires_grad=self.training)\n+\n+        # torch.export.export does no support requires_grad\n+        if self.training:\n+            inputs_embeds.requires_grad_(True)\n \n         batch_size, num_channels, height, width = image_features.shape\n         # rearrange both image_features and object_queries \"b c h w -> b (h w) c\""
        },
        {
            "sha": "3ff82503494592315640fcb4a81588283bf911a8",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 24,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -18,7 +18,7 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from functools import lru_cache, partial, wraps\n+from functools import partial\n from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n@@ -32,12 +32,14 @@\n from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_ninja_available,\n     is_torch_cuda_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -870,7 +872,7 @@ def forward(\n         else:\n             raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n \n-        if self.disable_custom_kernels or MultiScaleDeformableAttention is None:\n+        if self.disable_custom_kernels or MultiScaleDeformableAttention is None or is_torchdynamo_compiling():\n             # PyTorch implementation\n             output = multi_scale_deformable_attention(\n                 value, spatial_shapes_list, sampling_locations, attention_weights\n@@ -1590,27 +1592,6 @@ def forward(\n         )\n \n \n-def compile_compatible_lru_cache(*lru_args, **lru_kwargs):\n-    def decorator(func):\n-        @wraps(func)\n-        def wrapper(self, *args, **kwargs):\n-            if not torch.compiler.is_compiling():\n-                # Cache the function only if the model is not being compiled\n-                # check if the function is already cached, otherwise create it\n-                if not hasattr(self, f\"_cached_{func.__name__}\"):\n-                    self.__setattr__(\n-                        f\"_cached_{func.__name__}\", lru_cache(*lru_args, **lru_kwargs)(func.__get__(self))\n-                    )\n-                return self.__getattribute__(f\"_cached_{func.__name__}\")(*args, **kwargs)\n-            else:\n-                # Otherwise, just call the original function\n-                return func(self, *args, **kwargs)\n-\n-        return wrapper\n-\n-    return decorator\n-\n-\n # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n class RTDetrMLPPredictionHead(nn.Module):\n     \"\"\"\n@@ -1728,7 +1709,7 @@ def unfreeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(True)\n \n-    @compile_compatible_lru_cache(maxsize=32)\n+    @compile_compatible_method_lru_cache(maxsize=32)\n     def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dtype=torch.float32):\n         if spatial_shapes is None:\n             spatial_shapes = ["
        },
        {
            "sha": "0f9fddea5c29e9d123a4435cb9eb3950e5cac017",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 27,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -22,7 +22,7 @@\n import os\n import warnings\n from dataclasses import dataclass\n-from functools import lru_cache, partial, wraps\n+from functools import partial\n from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n@@ -34,12 +34,14 @@\n from ...image_transforms import center_to_corners_format, corners_to_center_format\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n+from ...pytorch_utils import compile_compatible_method_lru_cache\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_ninja_available,\n     is_torch_cuda_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -97,7 +99,7 @@ def multi_scale_deformable_attention_v2(\n     value_list = (\n         value.permute(0, 2, 3, 1)\n         .flatten(0, 1)\n-        .split([height.item() * width.item() for height, width in value_spatial_shapes], dim=-1)\n+        .split([height * width for height, width in value_spatial_shapes], dim=-1)\n     )\n     # sampling_offsets [8, 480, 8, 12, 2]\n     if method == \"default\":\n@@ -226,17 +228,17 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n-        **kwargs,\n     ):\n         # Process inputs up to sampling locations calculation using parent class logic\n         if position_embeddings is not None:\n             hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+        if not is_torchdynamo_compiling() and (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n             )\n@@ -272,7 +274,7 @@ def forward(\n \n         # V2-specific attention implementation choice\n         output = multi_scale_deformable_attention_v2(\n-            value, spatial_shapes, sampling_locations, attention_weights, self.n_points_list, self.method\n+            value, spatial_shapes_list, sampling_locations, attention_weights, self.n_points_list, self.method\n         )\n \n         output = self.output_proj(output)\n@@ -1329,27 +1331,6 @@ def get_contrastive_denoising_training_group(\n \"\"\"\n \n \n-def compile_compatible_lru_cache(*lru_args, **lru_kwargs):\n-    def decorator(func):\n-        @wraps(func)\n-        def wrapper(self, *args, **kwargs):\n-            if not torch.compiler.is_compiling():\n-                # Cache the function only if the model is not being compiled\n-                # check if the function is already cached, otherwise create it\n-                if not hasattr(self, f\"_cached_{func.__name__}\"):\n-                    self.__setattr__(\n-                        f\"_cached_{func.__name__}\", lru_cache(*lru_args, **lru_kwargs)(func.__get__(self))\n-                    )\n-                return self.__getattribute__(f\"_cached_{func.__name__}\")(*args, **kwargs)\n-            else:\n-                # Otherwise, just call the original function\n-                return func(self, *args, **kwargs)\n-\n-        return wrapper\n-\n-    return decorator\n-\n-\n def _get_clones(partial_module, N):\n     return nn.ModuleList([partial_module() for i in range(N)])\n \n@@ -1669,7 +1650,7 @@ def unfreeze_backbone(self):\n         for param in self.backbone.parameters():\n             param.requires_grad_(True)\n \n-    @compile_compatible_lru_cache(maxsize=32)\n+    @compile_compatible_method_lru_cache(maxsize=32)\n     def generate_anchors(self, spatial_shapes=None, grid_size=0.05, device=\"cpu\", dtype=torch.float32):\n         if spatial_shapes is None:\n             spatial_shapes = ["
        },
        {
            "sha": "c6a192bd3e2b8135e72e62fe29ab584fc1589962",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -20,7 +20,7 @@\n from torch import Tensor, nn\n \n from ...configuration_utils import PretrainedConfig\n-from ...utils import logging\n+from ...utils import is_torchdynamo_compiling, logging\n from ...utils.backbone_utils import (\n     verify_backbone_config_arguments,\n )\n@@ -404,7 +404,7 @@ def multi_scale_deformable_attention_v2(\n     value_list = (\n         value.permute(0, 2, 3, 1)\n         .flatten(0, 1)\n-        .split([height.item() * width.item() for height, width in value_spatial_shapes], dim=-1)\n+        .split([height * width for height, width in value_spatial_shapes], dim=-1)\n     )\n     # sampling_offsets [8, 480, 8, 12, 2]\n     if method == \"default\":\n@@ -497,17 +497,17 @@ def forward(\n         position_embeddings: Optional[torch.Tensor] = None,\n         reference_points=None,\n         spatial_shapes=None,\n+        spatial_shapes_list=None,\n         level_start_index=None,\n         output_attentions: bool = False,\n-        **kwargs,\n     ):\n         # Process inputs up to sampling locations calculation using parent class logic\n         if position_embeddings is not None:\n             hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape\n-        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+        if not is_torchdynamo_compiling() and (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n             raise ValueError(\n                 \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n             )\n@@ -543,7 +543,7 @@ def forward(\n \n         # V2-specific attention implementation choice\n         output = multi_scale_deformable_attention_v2(\n-            value, spatial_shapes, sampling_locations, attention_weights, self.n_points_list, self.method\n+            value, spatial_shapes_list, sampling_locations, attention_weights, self.n_points_list, self.method\n         )\n \n         output = self.output_proj(output)"
        },
        {
            "sha": "d1c3d2eb6dcb98dbcd94d1def7efaf9a18022f91",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -817,8 +817,11 @@ def forward(\n         # and reconstructed together (In-Context Painting).\n         if bool_masked_pos is None:\n             num_patches = self.embeddings.patch_embeddings.num_patches\n-            bool_masked_pos = torch.zeros(num_patches, dtype=torch.bool).to(pixel_values.device)\n-            bool_masked_pos[num_patches // 2 :] = 1\n+            bool_masked_pos_zeros = torch.zeros(num_patches // 2, dtype=torch.bool, device=pixel_values.device)\n+            bool_masked_pos_ones = torch.ones(\n+                num_patches - num_patches // 2, dtype=torch.bool, device=pixel_values.device\n+            )\n+            bool_masked_pos = torch.cat([bool_masked_pos_zeros, bool_masked_pos_ones])\n             bool_masked_pos = bool_masked_pos.unsqueeze(0)\n \n         embedding_output = self.embeddings(\n@@ -975,8 +978,11 @@ def forward(\n \n         if bool_masked_pos is None:\n             num_patches = self.model.embeddings.patch_embeddings.num_patches\n-            bool_masked_pos = torch.zeros(num_patches, dtype=torch.bool).to(pixel_values.device)\n-            bool_masked_pos[num_patches // 2 :] = 1\n+            bool_masked_pos_zeros = torch.zeros(num_patches // 2, dtype=torch.bool, device=pixel_values.device)\n+            bool_masked_pos_ones = torch.ones(\n+                num_patches - num_patches // 2, dtype=torch.bool, device=pixel_values.device\n+            )\n+            bool_masked_pos = torch.cat([bool_masked_pos_zeros, bool_masked_pos_ones])\n             bool_masked_pos = bool_masked_pos.unsqueeze(0)\n \n         outputs = self.model("
        },
        {
            "sha": "784367a01478281692716d09988b206e672b79ab",
            "filename": "src/transformers/models/swin2sr/modeling_swin2sr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fmodeling_swin2sr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -813,10 +813,11 @@ def __init__(self, config):\n         self.config = config\n \n         if config.num_channels == 3 and config.num_channels_out == 3:\n-            rgb_mean = (0.4488, 0.4371, 0.4040)\n-            self.mean = torch.Tensor(rgb_mean).view(1, 3, 1, 1)\n+            mean = torch.tensor([0.4488, 0.4371, 0.4040]).view(1, 3, 1, 1)\n         else:\n-            self.mean = torch.zeros(1, 1, 1, 1)\n+            mean = torch.zeros(1, 1, 1, 1)\n+        self.register_buffer(\"mean\", mean, persistent=False)\n+\n         self.img_range = config.img_range\n \n         self.first_convolution = nn.Conv2d(config.num_channels, config.embed_dim, 3, 1, 1)\n@@ -851,8 +852,8 @@ def pad_and_normalize(self, pixel_values):\n         pixel_values = nn.functional.pad(pixel_values, (0, modulo_pad_width, 0, modulo_pad_height), \"reflect\")\n \n         # 2. normalize\n-        self.mean = self.mean.type_as(pixel_values)\n-        pixel_values = (pixel_values - self.mean) * self.img_range\n+        mean = self.mean.type_as(pixel_values)\n+        pixel_values = (pixel_values - mean) * self.img_range\n \n         return pixel_values\n "
        },
        {
            "sha": "c36adffd97224a098c947ae17805e29dfbc8f2fa",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 1,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -14,14 +14,15 @@\n from __future__ import annotations\n \n import inspect\n+from functools import lru_cache, wraps\n from typing import Callable, List, Optional, Set, Tuple, Union\n \n import torch\n from packaging import version\n from safetensors.torch import storage_ptr, storage_size\n from torch import nn\n \n-from .utils import is_torch_greater_or_equal, is_torch_xla_available, logging\n+from .utils import is_torch_greater_or_equal, is_torch_xla_available, is_torchdynamo_compiling, logging\n \n \n ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n@@ -364,3 +365,29 @@ def translate_to_torch_parallel_style(style: str):\n         return RowwiseParallel(input_layouts=Replicate())\n     else:\n         raise ValueError(f\"Unsupported parallel style value: {style}\")\n+\n+\n+def compile_compatible_method_lru_cache(*lru_args, **lru_kwargs):\n+    \"\"\"\n+    LRU cache decorator from standard functools library, but with a workaround to disable\n+    caching when torchdynamo is compiling. Expected to work with class methods.\n+    \"\"\"\n+\n+    def decorator(func):\n+        @wraps(func)\n+        def wrapper(self, *args, **kwargs):\n+            if not is_torchdynamo_compiling():\n+                # Cache the function only if the model is not being compiled\n+                # check if the function is already cached, otherwise create it\n+                if not hasattr(self, f\"_cached_{func.__name__}\"):\n+                    self.__setattr__(\n+                        f\"_cached_{func.__name__}\", lru_cache(*lru_args, **lru_kwargs)(func.__get__(self))\n+                    )\n+                return self.__getattribute__(f\"_cached_{func.__name__}\")(*args, **kwargs)\n+            else:\n+                # Otherwise, just call the original function\n+                return func(self, *args, **kwargs)\n+\n+        return wrapper\n+\n+    return decorator"
        },
        {
            "sha": "bf7c5e8ea3404e02772aa11a0bd67a08c8d6a9fa",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -135,6 +135,7 @@\n     is_torch_bf16_gpu_available,\n     is_torch_deterministic,\n     is_torch_fp16_available_on_device,\n+    is_torch_greater_or_equal,\n     is_torch_neuroncore_available,\n     is_torch_npu_available,\n     is_torch_sdpa_available,\n@@ -556,6 +557,21 @@ def require_torch(test_case):\n     return unittest.skipUnless(is_torch_available(), \"test requires PyTorch\")(test_case)\n \n \n+def require_torch_greater_or_equal(version: str):\n+    \"\"\"\n+    Decorator marking a test that requires PyTorch version >= `version`.\n+\n+    These tests are skipped when PyTorch version is less than `version`.\n+    \"\"\"\n+\n+    def decorator(test_case):\n+        return unittest.skipUnless(is_torch_greater_or_equal(version), f\"test requires PyTorch version >= {version}\")(\n+            test_case\n+        )\n+\n+    return decorator\n+\n+\n def require_flash_attn(test_case):\n     \"\"\"\n     Decorator marking a test that requires Flash Attention."
        },
        {
            "sha": "89245c7009bf6d77b72e0a9d1bcef712881ddb39",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -271,6 +271,7 @@ class BeitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = BeitModelTester(self)\n@@ -292,6 +293,10 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n+    @unittest.skip(reason=\"BEiT can't compile dynamic\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n     def test_model_get_set_embeddings(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -764,13 +769,6 @@ def test_inference_interpolate_pos_encoding(self):\n         inputs = processor(images=image, return_tensors=\"pt\", size={\"height\": 480, \"width\": 480})\n         pixel_values = inputs.pixel_values.to(torch_device)\n \n-        # with interpolate_pos_encoding being False an exception should be raised with higher resolution\n-        # images than what the model supports.\n-        self.assertFalse(processor.do_center_crop)\n-        with torch.no_grad():\n-            with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n-                model(pixel_values, interpolate_pos_encoding=False)\n-\n         # with interpolate_pos_encoding being True the model should process the higher resolution image\n         # successfully and produce the expected output.\n         with torch.no_grad():"
        },
        {
            "sha": "31effc266d99455057c18a9322102eb6b405440f",
            "filename": "tests/models/bit/test_modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -170,6 +170,7 @@ class BitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = BitModelTester(self)"
        },
        {
            "sha": "80360e8177e7b02fa182933c77c83ac23109a8d2",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -194,6 +194,7 @@ class ConditionalDetrModelTest(ModelTesterMixin, GenerationTesterMixin, Pipeline\n     test_head_masking = False\n     test_missing_keys = False\n     zero_init_hidden_state = True\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "694d3931a9ae359a726b47b9f16f06f8466b8fa5",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -180,6 +180,7 @@ class ConvNextModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ConvNextModelTester(self)"
        },
        {
            "sha": "4a163ddcd7c1d182940f17efd3f651d38b6084fb",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -188,6 +188,7 @@ class ConvNextV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ConvNextV2ModelTester(self)"
        },
        {
            "sha": "a63789d8871bf748ee459dd0b243bece5e81646e",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -159,6 +159,7 @@ class CvtModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = CvtModelTester(self)"
        },
        {
            "sha": "35d43123bfd828e15c23d94cd5a35621aa994cc6",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -197,6 +197,7 @@ class DabDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     test_head_masking = False\n     test_missing_keys = False\n     zero_init_hidden_state = True\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "b9404e08a9df9245f5e3066536c397a8310d5358",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -200,6 +200,7 @@ class DeformableDetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n     test_pruning = False\n     test_head_masking = False\n     test_missing_keys = False\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "bf58e1cd3228ea60518812fe6ab034184224af7c",
            "filename": "tests/models/deit/test_modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -222,6 +222,7 @@ class DeiTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DeiTModelTester(self)"
        },
        {
            "sha": "95026c1054e9954c332522be50119dbd658eee23",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -146,6 +146,7 @@ class DepthAnythingModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Tes\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DepthAnythingModelTester(self)"
        },
        {
            "sha": "89db2a57ccda6feddeb6efa4b17d6d58c63486ab",
            "filename": "tests/models/depth_pro/test_modeling_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_pro%2Ftest_modeling_depth_pro.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -212,6 +212,7 @@ class DepthProModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DepthProModelTester(self)"
        },
        {
            "sha": "bfeded558b6dc36ad219bac0f4891fef844e952f",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -194,6 +194,7 @@ class DetrModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     test_head_masking = False\n     test_missing_keys = False\n     zero_init_hidden_state = True\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "7c4982705161c5ed0d9c91e9b1f332dc83b7dec4",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -216,6 +216,7 @@ class DinatModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DinatModelTester(self)"
        },
        {
            "sha": "5cbcbe77d907d973d4b10ed1beb92c60b0771146",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -212,6 +212,8 @@ class Dinov2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     attention_mask and seq_length.\n     \"\"\"\n \n+    test_torch_exportable = True\n+\n     all_model_classes = (\n         (\n             Dinov2Model,"
        },
        {
            "sha": "a276eedd3a164958284a5eb28339f47ac060e182",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -237,6 +237,7 @@ class Dinov2WithRegistersModelTest(ModelTesterMixin, PipelineTesterMixin, unitte\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = Dinov2WithRegistersModelTester(self)"
        },
        {
            "sha": "44fb2afb3bfdf0e7fd5119a0d0144a93e52a3251",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -172,6 +172,7 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DPTModelTester(self)"
        },
        {
            "sha": "62240d24bced5f9ad035e2fbd07e931da10680fa",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -140,6 +140,7 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DPTModelTester(self)"
        },
        {
            "sha": "568b05e2d4a57e9cf1b40da62d8b84f12b5211b3",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -186,6 +186,7 @@ class DPTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = DPTModelTester(self)"
        },
        {
            "sha": "68bc175ad47cbff36e8b1109d3f7857168762fec",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -139,6 +139,7 @@ class EfficientNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = EfficientNetModelTester(self)"
        },
        {
            "sha": "2960234c3bd78ece6a22989bbbfc181e24e8addb",
            "filename": "tests/models/focalnet/test_modeling_focalnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -247,6 +247,7 @@ class FocalNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = FocalNetModelTester(self)"
        },
        {
            "sha": "c4dd380a0dd767e52f5a0be6739fe37d3c1ae8bc",
            "filename": "tests/models/glpn/test_modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -152,6 +152,7 @@ class GLPNModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_head_masking = False\n     test_pruning = False\n     test_resize_embeddings = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = GLPNModelTester(self)"
        },
        {
            "sha": "bbd35f29efba3ac6111af2c43c2d2e140fc8214c",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -250,6 +250,7 @@ class HieraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = HieraModelTester(self)"
        },
        {
            "sha": "4088f355a42650c270f49db4b86e95406eef1d69",
            "filename": "tests/models/ijepa/test_modeling_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -207,6 +207,7 @@ class IJepaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = IJepaModelTester(self)"
        },
        {
            "sha": "b21054525c51697cabffb0ecc8c1367e5d042443",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -237,6 +237,7 @@ class ImageGPTModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterM\n         else {}\n     )\n     test_missing_keys = False\n+    test_torch_exportable = True\n \n     # as ImageGPTForImageClassification isn't included in any auto mapping, we add labels here\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "b1fecd925308959f12a18264e8d9e6713914e678",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -205,6 +205,7 @@ class Mask2FormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_pruning = False\n     test_head_masking = False\n     test_missing_keys = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = Mask2FormerModelTester(self)"
        },
        {
            "sha": "284cdb5f4935858f6e6ea8195f6f073b87cfbce8",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -209,6 +209,7 @@ class MaskFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     test_head_masking = False\n     test_missing_keys = False\n     zero_init_hidden_state = True\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MaskFormerModelTester(self)"
        },
        {
            "sha": "502660b191efae562b3db0ba60d559668228bcdf",
            "filename": "tests/models/maskformer/test_modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -181,6 +181,7 @@ class MaskFormerSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MaskFormerSwinModelTester(self)"
        },
        {
            "sha": "9488700193807ef1f48074e1f3aae20bda18b7c4",
            "filename": "tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -154,6 +154,7 @@ class MobileNetV1ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MobileNetV1ModelTester(self)"
        },
        {
            "sha": "b5a12edd7b8fec1f55ef7c813e6a66aecefaae1e",
            "filename": "tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -205,6 +205,7 @@ class MobileNetV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MobileNetV2ModelTester(self)"
        },
        {
            "sha": "b67abce9341c62435c76bb67b6acc3903a32a086",
            "filename": "tests/models/mobilevit/test_modeling_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -198,6 +198,7 @@ class MobileViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MobileViTModelTester(self)"
        },
        {
            "sha": "70fc70124f6b0fced95e74ed0bad58bc7dce3b68",
            "filename": "tests/models/mobilevitv2/test_modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -200,6 +200,7 @@ class MobileViTV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = MobileViTV2ModelTester(self)"
        },
        {
            "sha": "6f4ead91a2e7f591acc32c37c9760f7811b84d08",
            "filename": "tests/models/poolformer/test_modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -132,6 +132,7 @@ class PoolFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     test_resize_embeddings = False\n     test_torchscript = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = PoolFormerModelTester(self)"
        },
        {
            "sha": "df2b3bcc85aa85058669edb19d8179dfc7f7da82",
            "filename": "tests/models/pvt/test_modeling_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -166,6 +166,7 @@ class PvtModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_torchscript = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = PvtModelTester(self)"
        },
        {
            "sha": "d850da24f3ca85679e001d47b94165256c677324",
            "filename": "tests/models/pvt_v2/test_modeling_pvt_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -202,6 +202,7 @@ class PvtV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_torchscript = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = PvtV2ModelTester(self)"
        },
        {
            "sha": "62faeb58b29403bb638a38160d352ee7d6018b55",
            "filename": "tests/models/regnet/test_modeling_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -133,6 +133,7 @@ class RegNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = RegNetModelTester(self)"
        },
        {
            "sha": "d8790130dff847e7321d33a5350259af3b0789b6",
            "filename": "tests/models/resnet/test_modeling_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -178,6 +178,7 @@ class ResNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ResNetModelTester(self)"
        },
        {
            "sha": "ab465065f168e222fb53b0339a084f25e8186370",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -261,6 +261,7 @@ class RTDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_head_masking = False\n     test_missing_keys = False\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "d5388cf41a99053e8813046358c84ad4cd0b3bb9",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -259,6 +259,7 @@ class RTDetrV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_pruning = False\n     test_head_masking = False\n     test_missing_keys = False\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "30ea5c63f4fd445c0909ae6df4e25cf5fe731e97",
            "filename": "tests/models/segformer/test_modeling_segformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -180,6 +180,7 @@ class SegformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     test_head_masking = False\n     test_pruning = False\n     test_resize_embeddings = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = SegformerModelTester(self)"
        },
        {
            "sha": "3a31557971666891d0dc444d1af2a24944bab95b",
            "filename": "tests/models/seggpt/test_modeling_seggpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -172,6 +172,8 @@ class SegGptModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torchscript = False\n+    test_torch_exportable = True\n+\n     pipeline_model_mapping = (\n         {\"feature-extraction\": SegGptModel, \"mask-generation\": SegGptModel} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "0d6f471e96c5f29a244e068915cecff17a870490",
            "filename": "tests/models/swiftformer/test_modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -147,6 +147,7 @@ class SwiftFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_resize_embeddings = False\n     test_head_masking = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = SwiftFormerModelTester(self)"
        },
        {
            "sha": "cf33a3f3df9f4d5caeea62023ed5ac0465054992",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -240,6 +240,7 @@ class SwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = SwinModelTester(self)"
        },
        {
            "sha": "3752f6ef30047bc792124a43845f4cf3ecaeff4b",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -172,6 +172,7 @@ class Swin2SRModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torchscript = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = Swin2SRModelTester(self)"
        },
        {
            "sha": "a4e93eeb3dfe1990f271d6ad39875fc9be0d898e",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -226,6 +226,7 @@ class Swinv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = Swinv2ModelTester(self)"
        },
        {
            "sha": "cbed595f66ffe807166a5b59396e7f72898cedda",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -209,6 +209,7 @@ class TableTransformerModelTest(ModelTesterMixin, GenerationTesterMixin, Pipelin\n     test_head_masking = False\n     test_missing_keys = False\n     zero_init_hidden_state = True\n+    test_torch_exportable = True\n \n     # special case for head models\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "5d560f919b12ca4626dfba7bb571b9bf5bd1cf37",
            "filename": "tests/models/textnet/test_modeling_textnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -217,6 +217,7 @@ class TextNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n     has_attentions = False\n \n     def setUp(self):"
        },
        {
            "sha": "2cbe3a6dcecd54642e19c351a6035864e0971133",
            "filename": "tests/models/timesformer/test_modeling_timesformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -167,6 +167,7 @@ class TimesformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = TimesformerModelTester(self)"
        },
        {
            "sha": "1b337460f8d5183f5fd7236f0e68b2e6a3e5c49c",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -157,6 +157,7 @@ class UperNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_head_masking = False\n     test_torchscript = False\n     has_attentions = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = UperNetModelTester(self)"
        },
        {
            "sha": "f2171f37ad71ff39c21c48a39d2120fc47bd964d",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -186,6 +186,7 @@ class VideoMAEModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VideoMAEModelTester(self)"
        },
        {
            "sha": "929d7fec959a7babacccacfdf6fe5fef14c61586",
            "filename": "tests/models/vit/test_modeling_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -207,6 +207,7 @@ class ViTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ViTModelTester(self)"
        },
        {
            "sha": "1b836900142802f4b5976c1fd7d8f4027ab34b14",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -174,6 +174,7 @@ class ViTMAEModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ViTMAEModelTester(self)"
        },
        {
            "sha": "8c94a137719d9bc7ca91947f98f85a34fd7973a4",
            "filename": "tests/models/vit_msn/test_modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -162,6 +162,7 @@ class ViTMSNModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ViTMSNModelTester(self)"
        },
        {
            "sha": "2c46b60f7e73fa1e206a2ab160cf29cd9d6f584e",
            "filename": "tests/models/vitdet/test_modeling_vitdet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitdet%2Ftest_modeling_vitdet.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -169,6 +169,7 @@ class VitDetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VitDetModelTester(self)"
        },
        {
            "sha": "035e1a65b8a433f5323158381c4f3dcb89cc310f",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -143,6 +143,7 @@ class VitMatteModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VitMatteModelTester(self)"
        },
        {
            "sha": "a47e9ca737ec47cd730b12cc736ae01ffe1a16d9",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -154,6 +154,7 @@ class VitPoseModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VitPoseModelTester(self)"
        },
        {
            "sha": "91c3d37abbea78c0606311d3d8f9cd2c72962d15",
            "filename": "tests/models/vitpose_backbone/test_modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose_backbone%2Ftest_modeling_vitpose_backbone.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -18,7 +18,7 @@\n import unittest\n \n from transformers import VitPoseBackboneConfig\n-from transformers.testing_utils import require_torch\n+from transformers.testing_utils import require_torch, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_backbone_common import BackboneTesterMixin\n@@ -27,6 +27,8 @@\n \n \n if is_torch_available():\n+    import torch\n+\n     from transformers import VitPoseBackbone\n \n \n@@ -129,6 +131,7 @@ class VitPoseBackboneModelTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VitPoseBackboneModelTester(self)\n@@ -187,6 +190,17 @@ def test_forward_signature(self):\n             expected_arg_names = [\"pixel_values\"]\n             self.assertListEqual(arg_names[:1], expected_arg_names)\n \n+    def test_torch_export(self):\n+        # Dense architecture\n+        super().test_torch_export()\n+\n+        # MOE architecture\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.num_experts = 2\n+        config.part_features = config.hidden_size // config.num_experts\n+        inputs_dict[\"dataset_index\"] = torch.tensor([0] * self.model_tester.batch_size, device=torch_device)\n+        super().test_torch_export(config=config, inputs_dict=inputs_dict)\n+\n \n @require_torch\n class VitPoseBackboneTest(unittest.TestCase, BackboneTesterMixin):"
        },
        {
            "sha": "364998eb65bb24df278cd624d765ba3aa72688e9",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -175,6 +175,7 @@ class VivitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_torchscript = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = VivitModelTester(self)"
        },
        {
            "sha": "bfc428961c7d2bbc1e00b5bd017584c1c82af097",
            "filename": "tests/models/yolos/test_modeling_yolos.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -178,6 +178,7 @@ class YolosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     test_resize_embeddings = False\n     test_head_masking = False\n     test_torchscript = False\n+    test_torch_exportable = True\n \n     # special case for head model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "782b99af436437005eabae47c4b9c09771ed49b2",
            "filename": "tests/models/zoedepth/test_modeling_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -147,6 +147,7 @@ class ZoeDepthModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     test_pruning = False\n     test_resize_embeddings = False\n     test_head_masking = False\n+    test_torch_exportable = True\n \n     def setUp(self):\n         self.model_tester = ZoeDepthModelTester(self)"
        },
        {
            "sha": "0b437d9356ff79844bdd1bbdcb479fba55e95752",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f42d46ccb406695fb57c0c669526f67fc30d1d84/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f42d46ccb406695fb57c0c669526f67fc30d1d84",
            "patch": "@@ -86,6 +86,7 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n+    require_torch_greater_or_equal,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n@@ -221,6 +222,7 @@ class ModelTesterMixin:\n     test_mismatched_shapes = True\n     test_missing_keys = True\n     test_model_parallel = False\n+    test_torch_exportable = False\n     # Used in `check_training_gradient_checkpointing` to NOT check all params having gradient (e.g. for some MOE models)\n     test_all_params_have_gradient = True\n     is_encoder_decoder = False\n@@ -4865,6 +4867,72 @@ def test_forward_with_logits_to_keep(self):\n             # Assert the last tokens are actually the same (except for the natural fluctuation due to order of FP ops)\n             torch.testing.assert_close(all_logits[:, -1:, :], last_token_logits, rtol=1e-5, atol=1e-5)\n \n+    @slow\n+    @require_torch_greater_or_equal(\"2.5\")\n+    def test_torch_export(self, config=None, inputs_dict=None, tolerance=1e-4):\n+        \"\"\"\n+        Test if model can be exported with torch.export.export()\n+\n+        Args:\n+            config (PretrainedConfig):\n+                Config to use for the model, if None, use default config from model_tester\n+            inputs_dict (dict):\n+                Inputs to use for the model, if None, use default inputs from model_tester\n+            tolerance (float):\n+                `atol` for torch.allclose(), defined in signature for test overriding\n+        \"\"\"\n+        if not self.test_torch_exportable:\n+            self.skipTest(reason=\"test_torch_exportable=False for this model.\")\n+\n+        def recursively_check(eager_outputs, exported_outputs):\n+            is_tested = False\n+            if isinstance(eager_outputs, torch.Tensor):\n+                torch.testing.assert_close(eager_outputs, exported_outputs, atol=tolerance, rtol=tolerance)\n+                return True\n+            elif isinstance(eager_outputs, (tuple, list)):\n+                for eager_output, exported_output in zip(eager_outputs, exported_outputs):\n+                    is_tested = is_tested or recursively_check(eager_output, exported_output)\n+                return is_tested\n+            elif isinstance(eager_outputs, dict):\n+                for key in eager_outputs:\n+                    is_tested = is_tested or recursively_check(eager_outputs[key], exported_outputs[key])\n+                return is_tested\n+            return is_tested\n+\n+        default_config, default_inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config = config or default_config\n+        inputs_dict = inputs_dict or default_inputs_dict\n+\n+        for model_class in self.all_model_classes:\n+            if model_class.__name__.endswith(\"ForPreTraining\"):\n+                continue\n+\n+            with self.subTest(model_class.__name__):\n+                model = model_class(config).eval().to(torch_device)\n+\n+                # Export model\n+                exported_model = torch.export.export(\n+                    model,\n+                    args=(),\n+                    kwargs=inputs_dict,\n+                    strict=True,\n+                )\n+\n+                # Run exported model and eager model\n+                with torch.no_grad():\n+                    # set seed in case anything is not deterministic in model (e.g. vit_mae noise)\n+                    torch.manual_seed(1234)\n+                    eager_outputs = model(**inputs_dict)\n+                    torch.manual_seed(1234)\n+                    exported_outputs = exported_model.module().forward(**inputs_dict)\n+\n+                # Check if outputs are close:\n+                # is_tested is a boolean flag idicating if we comapre any outputs,\n+                # e.g. there might be a situation when outputs are empty list, then is_tested will be False.\n+                # In case of outputs are different the error will be rasied in `recursively_check` function.\n+                is_tested = recursively_check(eager_outputs, exported_outputs)\n+                self.assertTrue(is_tested, msg=f\"No outputs were compared for {model_class.__name__}\")\n+\n     @require_torch_gpu\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:"
        }
    ],
    "stats": {
        "total": 456,
        "additions": 305,
        "deletions": 151
    }
}