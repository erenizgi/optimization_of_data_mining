{
    "author": "yonigozlan",
    "message": "Fix post processing methods in  keypoints matching models (#42018)\n\nFix out of bound issue and incorrect post processing function in fast processors",
    "sha": "decde58eda17aec894f17b15e7a5cdf4bf82d46a",
    "files": [
        {
            "sha": "c799ae828995e819c47e303a4393ba23ee6314db",
            "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -1,8 +1,71 @@\n+from typing import Union\n+\n+import torch\n+\n+from ...utils import TensorType\n from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n+from .modeling_efficientloftr import KeypointMatchingOutput\n \n \n class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n-    pass\n+    def post_process_keypoint_matching(\n+        self,\n+        outputs: \"KeypointMatchingOutput\",\n+        target_sizes: Union[TensorType, list[tuple]],\n+        threshold: float = 0.0,\n+    ) -> list[dict[str, torch.Tensor]]:\n+        \"\"\"\n+        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+        Args:\n+            outputs ([`KeypointMatchingOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+                target size `(height, width)` of each image in the batch. This must be the original image size (before\n+                any processing).\n+            threshold (`float`, *optional*, defaults to 0.0):\n+                Threshold to filter out the matches with low scores.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            of the pair, the matching scores and the matching indices.\n+        \"\"\"\n+        if outputs.matches.shape[0] != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+        if not all(len(target_size) == 2 for target_size in target_sizes):\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        if isinstance(target_sizes, list):\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+        else:\n+            if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_pair_sizes = target_sizes\n+\n+        keypoints = outputs.keypoints.clone()\n+        keypoints = keypoints * image_pair_sizes.flip(-1).reshape(-1, 2, 1, 2)\n+        keypoints = keypoints.to(torch.int32)\n+\n+        results = []\n+        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n+            # Filter out matches with low scores\n+            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n+\n+            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n+            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n+            matching_scores = scores[0][valid_matches[0]]\n+\n+            results.append(\n+                {\n+                    \"keypoints0\": matched_keypoints0,\n+                    \"keypoints1\": matched_keypoints1,\n+                    \"matching_scores\": matching_scores,\n+                }\n+            )\n+\n+        return results\n \n \n __all__ = [\"EfficientLoFTRImageProcessorFast\"]"
        },
        {
            "sha": "0dbd68bb78c8a57802da6cc31ede26b9859a872d",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -390,8 +390,8 @@ def post_process_keypoint_matching(\n             matches0 = matches[mask0]\n             scores0 = scores[mask0]\n \n-            # Filter out matches with low scores\n-            valid_matches = torch.logical_and(scores0 > threshold, matches0 > -1)\n+            # Filter out matches with low scores, invalid matches, and out-of-bounds indices\n+            valid_matches = (scores0 > threshold) & (matches0 > -1) & (matches0 < keypoints1.shape[0])\n \n             matched_keypoints0 = keypoints0[valid_matches]\n             matched_keypoints1 = keypoints1[matches0[valid_matches]]"
        },
        {
            "sha": "a38cc3288271055a812dcaf7bb3a58a56d7f7f64",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 12,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -188,23 +188,23 @@ def post_process_keypoint_matching(\n         Args:\n             outputs ([`KeypointMatchingOutput`]):\n                 Raw outputs of the model.\n-            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n-                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+            target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the\n                 target size `(height, width)` of each image in the batch. This must be the original image size (before\n                 any processing).\n             threshold (`float`, *optional*, defaults to 0.0):\n                 Threshold to filter out the matches with low scores.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            `list[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n             of the pair, the matching scores and the matching indices.\n         \"\"\"\n-        if outputs.matches.shape[0] != len(target_sizes):\n+        if outputs.mask.shape[0] != len(target_sizes):\n             raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n         if not all(len(target_size) == 2 for target_size in target_sizes):\n             raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n \n         if isinstance(target_sizes, list):\n-            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n         else:\n             if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n                 raise ValueError(\n@@ -217,13 +217,22 @@ def post_process_keypoint_matching(\n         keypoints = keypoints.to(torch.int32)\n \n         results = []\n-        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n-            # Filter out matches with low scores\n-            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n-\n-            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n-            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n-            matching_scores = scores[0][valid_matches[0]]\n+        for mask_pair, keypoints_pair, matches, scores in zip(\n+            outputs.mask, keypoints, outputs.matches[:, 0], outputs.matching_scores[:, 0]\n+        ):\n+            mask0 = mask_pair[0] > 0\n+            mask1 = mask_pair[1] > 0\n+            keypoints0 = keypoints_pair[0][mask0]\n+            keypoints1 = keypoints_pair[1][mask1]\n+            matches0 = matches[mask0]\n+            scores0 = scores[mask0]\n+\n+            # Filter out matches with low scores, invalid matches, and out-of-bounds indices\n+            valid_matches = (scores0 > threshold) & (matches0 > -1) & (matches0 < keypoints1.shape[0])\n+\n+            matched_keypoints0 = keypoints0[valid_matches]\n+            matched_keypoints1 = keypoints1[matches0[valid_matches]]\n+            matching_scores = scores0[valid_matches]\n \n             results.append(\n                 {"
        },
        {
            "sha": "67f073b97c39048653c0f8175379e3444ef8c5cc",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -394,8 +394,8 @@ def post_process_keypoint_matching(\n             matches0 = matches[mask0]\n             scores0 = scores[mask0]\n \n-            # Filter out matches with low scores\n-            valid_matches = torch.logical_and(scores0 > threshold, matches0 > -1)\n+            # Filter out matches with low scores, invalid matches, and out-of-bounds indices\n+            valid_matches = (scores0 > threshold) & (matches0 > -1) & (matches0 < keypoints1.shape[0])\n \n             matched_keypoints0 = keypoints0[valid_matches]\n             matched_keypoints1 = keypoints1[matches0[valid_matches]]"
        },
        {
            "sha": "606667d515cb4838690583e34076ee7f5304193a",
            "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 12,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -177,23 +177,23 @@ def post_process_keypoint_matching(\n         Args:\n             outputs ([`KeypointMatchingOutput`]):\n                 Raw outputs of the model.\n-            target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n-                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the\n+            target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n+                Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the\n                 target size `(height, width)` of each image in the batch. This must be the original image size (before\n                 any processing).\n             threshold (`float`, *optional*, defaults to 0.0):\n                 Threshold to filter out the matches with low scores.\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n+            `list[Dict]`: A list of dictionaries, each dictionary containing the keypoints in the first and second image\n             of the pair, the matching scores and the matching indices.\n         \"\"\"\n-        if outputs.matches.shape[0] != len(target_sizes):\n+        if outputs.mask.shape[0] != len(target_sizes):\n             raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n         if not all(len(target_size) == 2 for target_size in target_sizes):\n             raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n \n         if isinstance(target_sizes, list):\n-            image_pair_sizes = torch.tensor(target_sizes, device=outputs.matches.device)\n+            image_pair_sizes = torch.tensor(target_sizes, device=outputs.mask.device)\n         else:\n             if target_sizes.shape[1] != 2 or target_sizes.shape[2] != 2:\n                 raise ValueError(\n@@ -206,13 +206,22 @@ def post_process_keypoint_matching(\n         keypoints = keypoints.to(torch.int32)\n \n         results = []\n-        for keypoints_pair, matches, scores in zip(keypoints, outputs.matches, outputs.matching_scores):\n-            # Filter out matches with low scores\n-            valid_matches = torch.logical_and(scores > threshold, matches > -1)\n-\n-            matched_keypoints0 = keypoints_pair[0][valid_matches[0]]\n-            matched_keypoints1 = keypoints_pair[1][valid_matches[1]]\n-            matching_scores = scores[0][valid_matches[0]]\n+        for mask_pair, keypoints_pair, matches, scores in zip(\n+            outputs.mask, keypoints, outputs.matches[:, 0], outputs.matching_scores[:, 0]\n+        ):\n+            mask0 = mask_pair[0] > 0\n+            mask1 = mask_pair[1] > 0\n+            keypoints0 = keypoints_pair[0][mask0]\n+            keypoints1 = keypoints_pair[1][mask1]\n+            matches0 = matches[mask0]\n+            scores0 = scores[mask0]\n+\n+            # Filter out matches with low scores, invalid matches, and out-of-bounds indices\n+            valid_matches = (scores0 > threshold) & (matches0 > -1) & (matches0 < keypoints1.shape[0])\n+\n+            matched_keypoints0 = keypoints0[valid_matches]\n+            matched_keypoints1 = keypoints1[matches0[valid_matches]]\n+            matching_scores = scores0[valid_matches]\n \n             results.append(\n                 {"
        },
        {
            "sha": "93cc2e5f12ba5d75800997c53385c9d4e06a5cc4",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "modified",
            "additions": 57,
            "deletions": 0,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/decde58eda17aec894f17b15e7a5cdf4bf82d46a/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/decde58eda17aec894f17b15e7a5cdf4bf82d46a/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=decde58eda17aec894f17b15e7a5cdf4bf82d46a",
            "patch": "@@ -412,6 +412,63 @@ def check_post_processed_output(post_processed_output, image_pair_size):\n \n             check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n \n+    @require_torch\n+    def test_post_processing_keypoint_matching_with_padded_match_indices(self):\n+        \"\"\"\n+        Test that post_process_keypoint_matching correctly handles matches pointing to padded keypoints.\n+        This tests the edge case where a match index points beyond the actual number of real keypoints,\n+        which would cause an out-of-bounds error without proper filtering.\n+        \"\"\"\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+\n+            # Create a specific scenario with intentional padding issues\n+            batch_size = 1\n+            max_number_keypoints = 50\n+\n+            # Image 0 has 10 real keypoints, image 1 has only 5 real keypoints\n+            num_keypoints0 = 10\n+            num_keypoints1 = 5\n+\n+            mask = torch.zeros((batch_size, 2, max_number_keypoints), dtype=torch.int)\n+            keypoints = torch.zeros((batch_size, 2, max_number_keypoints, 2))\n+            matches = torch.full((batch_size, 2, max_number_keypoints), -1, dtype=torch.int)\n+            scores = torch.zeros((batch_size, 2, max_number_keypoints))\n+\n+            # Set up real keypoints\n+            mask[0, 0, :num_keypoints0] = 1\n+            mask[0, 1, :num_keypoints1] = 1\n+            keypoints[0, 0, :num_keypoints0] = torch.rand((num_keypoints0, 2))\n+            keypoints[0, 1, :num_keypoints1] = torch.rand((num_keypoints1, 2))\n+\n+            # Create a match that points to a padded keypoint in image 1\n+            # This would cause IndexError before the fix\n+            matches[0, 0, 0] = 8  # Points to index 8, but image 1 only has 5 real keypoints (indices 0-4)\n+            scores[0, 0, 0] = 0.9  # High confidence score\n+\n+            # Create a valid match for comparison\n+            matches[0, 0, 1] = 2  # Points to index 2, which is valid\n+            scores[0, 0, 1] = 0.8\n+\n+            outputs = KeypointMatchingOutput(mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores)\n+\n+            image_sizes = [((480, 640), (480, 640))]\n+\n+            # This should not raise an IndexError and should filter out the invalid match\n+            post_processed = image_processor.post_process_keypoint_matching(outputs, image_sizes)\n+\n+            # Check that we got results\n+            self.assertEqual(len(post_processed), 1)\n+            result = post_processed[0]\n+\n+            # Should only have 1 valid match (index 1), the out-of-bounds match (index 0) should be filtered out\n+            self.assertEqual(result[\"keypoints0\"].shape[0], 1)\n+            self.assertEqual(result[\"keypoints1\"].shape[0], 1)\n+            self.assertEqual(result[\"matching_scores\"].shape[0], 1)\n+\n+            # Verify the match score corresponds to the valid match\n+            self.assertAlmostEqual(result[\"matching_scores\"][0].item(), 0.8, places=5)\n+\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\""
        }
    ],
    "stats": {
        "total": 196,
        "additions": 167,
        "deletions": 29
    }
}