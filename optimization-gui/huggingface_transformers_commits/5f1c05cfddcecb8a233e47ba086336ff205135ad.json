{
    "author": "Cyrilvallez",
    "message": "[loading][TP] Fix device placement at loading-time, and simplify sharding primitives (#43003)\n\n* why was it so complicated\n\n* device\n\n* simplify\n\n* fix\n\n* imports\n\n* simplify quite a bit\n\n* fix",
    "sha": "5f1c05cfddcecb8a233e47ba086336ff205135ad",
    "files": [
        {
            "sha": "3864ebe94c189aa3a15f245e1113aab377545c9a",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=5f1c05cfddcecb8a233e47ba086336ff205135ad",
            "patch": "@@ -712,13 +712,13 @@ def _job():\n \n \n def spawn_tp_materialize(\n-    thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, sharding_method, tensor_idx, dtype=None\n+    thread_pool: ThreadPoolExecutor | None, tensor: torch.Tensor, sharding_method, tensor_idx, device=None, dtype=None\n ) -> Future | Callable:\n     \"\"\"Materialize and shard a tensor (according to the TP-plan) from file asynchronously if `thread_pool` is provided, or\n     return a Callable that will load the tensor synchronously when called.\"\"\"\n \n     def _job():\n-        return sharding_method.shard_tensor(tensor, param_casting_dtype=dtype, tensor_idx=tensor_idx)[0]\n+        return sharding_method.shard_tensor(tensor, tensor_idx=tensor_idx, device=device, dtype=dtype)\n \n     if thread_pool is not None:\n         return thread_pool.submit(_job)\n@@ -796,20 +796,17 @@ def set_param_for_module(\n     if ref is None:\n         unexpected_keys.add(target_name)\n     else:\n-        use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n         if not isinstance(param_value, torch.nn.Parameter):\n             if distributed_operation is not None:\n-                param_value = DTensor.from_local(\n-                    param_value,\n-                    distributed_operation.device_mesh,\n-                    getattr(distributed_operation, \"shard\", Replicate()),\n-                    run_check=False,\n-                    shape=ref.size(),\n-                    stride=ref.stride(),\n-                )\n-                if not use_dtensor:\n-                    # we convert to local\n-                    param_value = param_value.to_local()\n+                if getattr(distributed_operation, \"use_dtensor\", False):\n+                    param_value = DTensor.from_local(\n+                        param_value,\n+                        distributed_operation.device_mesh,\n+                        getattr(distributed_operation, \"shard\", Replicate()),\n+                        run_check=False,\n+                        shape=ref.size(),\n+                        stride=ref.stride(),\n+                    )\n             if param_name not in module_obj._buffers:\n                 param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n \n@@ -1079,6 +1076,7 @@ def convert_and_load_state_dict_in_model(\n                         tensor,\n                         mapping.distributed_operation,\n                         shard_index,\n+                        device_map[\"\"],\n                         _dtype,\n                     )\n "
        },
        {
            "sha": "101e6209e568e3c962f226cf3e9390d5a06f6a8f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 112,
            "deletions": 211,
            "changes": 323,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=5f1c05cfddcecb8a233e47ba086336ff205135ad",
            "patch": "@@ -19,6 +19,9 @@\n import re\n from functools import partial, reduce\n \n+from ..distributed import DistributedConfig\n+from ..utils import is_torch_greater_or_equal, logging\n+from ..utils.generic import GeneralInterface\n from ..utils.import_utils import is_torch_available\n \n \n@@ -27,21 +30,16 @@\n     import torch.distributed as dist\n     from torch import nn\n \n-from ..distributed import DistributedConfig\n-from ..utils import is_torch_greater_or_equal, logging\n-from ..utils.generic import GeneralInterface\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-if is_torch_available():\n     # Cache this result has it's a C FFI call which can be pretty time-consuming\n     _torch_distributed_available = torch.distributed.is_available()\n \n     if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n         from torch.distributed.tensor import DTensor, Placement, Replicate, Shard\n \n \n+logger = logging.get_logger(__name__)\n+\n+\n def initialize_tensor_parallelism(\n     tp_plan: str | dict[str, str] | None, tp_size: int | None = None, device_mesh=None, device_map=None\n ):\n@@ -470,7 +468,12 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n     @staticmethod\n     def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh): ...\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def shard_tensor(\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        raise NotImplementedError\n+\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         raise NotImplementedError\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n@@ -519,19 +522,10 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         return outputs\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        shard = [Replicate()]\n-        parameter = param[...].to(param_casting_dtype)\n-        self.shard = shard\n-        return parameter, shard\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        self.shard = [Replicate()]\n+        return param[...].to(device=device, dtype=dtype)\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         distribute_module(\n@@ -562,29 +556,20 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         return outputs\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        mesh = device_mesh or self.device_mesh\n-        parameter = param[...].to(param_casting_dtype)\n-        if mesh is not None:\n-            parameter = parameter / mesh.size()\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        parameter = param[...].to(device=device, dtype=dtype)\n+        if self.device_mesh is not None:\n+            parameter = parameter / self.device_mesh.size()\n         self.shard = None\n-        return parameter, None\n+        return parameter\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        param = param[...].to(param_casting_dtype)\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n-            param = param.contiguous()\n-        param = param / device_mesh.size()  # TODO should be optionable\n+            parameter = parameter.contiguous()\n         # TODO: assumes parent module will allreduce the output afterwards (e.g rowlinear bias is IsolatedParallel and parent module is GatherParallel)\n-        return param\n+        return parameter\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         distribute_module(\n@@ -623,31 +608,15 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         return outputs.to_local() if use_local_output and isinstance(outputs, DTensor) else outputs\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        parameter = param[...].to(param_casting_dtype)\n-        shard = [Replicate()]\n-        self.shard = shard\n-        return parameter, shard\n-\n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        parameter, shard = self.shard_tensor(\n-            param,\n-            param_type=param_type,\n-            param_casting_dtype=param_casting_dtype,\n-            to_contiguous=to_contiguous,\n-            rank=rank,\n-            device_mesh=device_mesh,\n-        )\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        self.shard = [Replicate()]\n+        return param[...].to(device=device, dtype=dtype)\n+\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, shard, run_check=False)\n+            parameter = DTensor.from_local(parameter, self.device_mesh, self.shard, run_check=False)\n         return parameter\n \n \n@@ -685,38 +654,34 @@ def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_\n         return input_tensor\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        device_mesh = self.device_mesh\n-        empty_param = self.empty_param\n-        rank = self.rank\n-        if param_type == \"bias\":\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1, tensor_idx)\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        # If only 1 dim, shard this one (usually it's a `bias`)\n+        dim = param.dim() if isinstance(param, torch.Tensor) else len(param.get_shape())\n+        if dim == 1:\n+            parameter = get_tensor_shard(param, self.empty_param, self.device_mesh, self.rank, -1, tensor_idx)\n             shard = [Shard(-1)]\n         else:\n             shard = [Shard(-2)]\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -2, tensor_idx)\n-        parameter = parameter.to(param_casting_dtype)\n+            parameter = get_tensor_shard(param, self.empty_param, self.device_mesh, self.rank, -2, tensor_idx)\n         self.shard = shard\n-        return parameter, shard\n+        return parameter.to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n         # weight would become Shard(1)\n-        parameter, shard = self.shard_tensor(param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(\n-                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+                parameter,\n+                self.device_mesh,\n+                self.shard,\n+                run_check=False,\n+                shape=self.empty_param.size(),\n+                stride=self.empty_param.stride(),\n             )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n@@ -731,30 +696,20 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n \n class PackedColwiseParallel(ColwiseParallel):\n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        device_mesh = device_mesh or self.device_mesh\n-        empty_param = self.empty_param\n-        rank = rank if rank is not None else self.rank\n-        return get_packed_weights(param, empty_param, device_mesh, rank, -2).to(param_casting_dtype), [Shard(-2)]\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        parameter = get_packed_weights(param, self.empty_param, self.device_mesh, self.rank, -2)\n+        return parameter.to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n         # weight would become Shard(1)\n-        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -2)\n-        parameter = parameter.to(param_casting_dtype)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-2)], run_check=False)\n+            parameter = DTensor.from_local(parameter, self.device_mesh, [Shard(-2)], run_check=False)\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n@@ -810,45 +765,36 @@ def __init__(\n         self.use_dtensor = use_dtensor\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        device_mesh = device_mesh or self.device_mesh\n-        empty_param = self.empty_param\n-        rank = rank if rank is not None else self.rank\n-        if param_type == \"bias\":\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        # If only 1 dim, it should not be sharded (usually it's a `bias`)\n+        dim = param.dim() if isinstance(param, torch.Tensor) else len(param.get_shape())\n+        if dim == 1:\n             shard = [Replicate()]\n             parameter = param[...]\n         else:\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1, tensor_idx=tensor_idx)\n+            parameter = get_tensor_shard(\n+                param, self.empty_param, self.device_mesh, self.rank, -1, tensor_idx=tensor_idx\n+            )\n             shard = [Shard(-1)]\n-        parameter = parameter.to(param_casting_dtype)\n         self.shard = shard\n-        return parameter, shard\n+        return parameter.to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # Rowwise shard weight to Shard(1), bias to Replicate(), weight be Shard(1)\n         # means Rowwise as nn.Linear is input * weight^T + bias, where\n         # weight would become Shard(0)\n-        if param_type != \"bias\":\n-            parameter = get_tensor_shard(param, empty_param, device_mesh, rank, -1)\n-            shard = [Shard(-1)]\n-        else:\n-            shard = [Replicate()]\n-            parameter = param[:]\n-\n-        parameter = parameter.to(param_casting_dtype)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n             parameter = DTensor.from_local(\n-                parameter, device_mesh, shard, run_check=False, shape=empty_param.size(), stride=empty_param.stride()\n+                parameter,\n+                self.device_mesh,\n+                self.shard,\n+                run_check=False,\n+                shape=self.empty_param.size(),\n+                stride=self.empty_param.stride(),\n             )\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n@@ -904,30 +850,20 @@ def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n \n class PackedRowwiseParallel(RowwiseParallel):\n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        device_mesh = device_mesh or self.device_mesh\n-        empty_param = self.empty_param\n-        rank = rank if rank is not None else self.rank\n-        return get_packed_weights(param, empty_param, device_mesh, rank, -1), [Shard(-1)]\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        parameter = get_packed_weights(param, self.empty_param, self.device_mesh, self.rank, -1)\n+        return parameter.to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n         # weight would become Shard(1)\n-        parameter = get_packed_weights(param, empty_param, device_mesh, rank, -1)\n-        parameter = parameter.to(param_casting_dtype)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, [Shard(-1)], run_check=False)\n+            parameter = DTensor.from_local(parameter, self.device_mesh, [Shard(-1)], run_check=False)\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n@@ -1015,18 +951,13 @@ def __init__(self, sequence_dim: int = 1, use_local_output: bool = False, use_dt\n \n     def shard_tensor(\n         self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n+        param: torch.Tensor,\n         tensor_idx=None,\n-    ):\n-        parameter = param[...].to(param_casting_dtype)\n-        shard = [Replicate()]\n-        self.shard = shard\n-        return parameter, shard\n+        device=None,\n+        dtype=None,\n+    ) -> torch.Tensor:\n+        self.shard = [Replicate()]\n+        return param[...].to(device=device, dtype=dtype)\n \n     @staticmethod\n     def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n@@ -1044,16 +975,15 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         )  # maybe we have to replicate ? because next layer is not sharded\n         return outputs.to_local()  # if use_local_output else outputs\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # colwise shard weight/bias to Shard(0), weight be Shard(-2) (0 if you have 1 dim only)\n         # means Colwise as Linear is input * weight^T + bias, where\n         # weight would become Shard(1)\n-        parameter = param[...]\n-        parameter = parameter.to(param_casting_dtype)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n             parameter = parameter.contiguous()\n         if self.use_dtensor:\n-            parameter = DTensor.from_local(parameter, device_mesh, [Replicate()], run_check=False)\n+            parameter = DTensor.from_local(parameter, self.device_mesh, [Replicate()], run_check=False)\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n@@ -1067,41 +997,23 @@ def __init__(self, **kwargs):\n         self.use_dtensor = False\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        empty_param = self.empty_param\n-        ep_rank = self.rank\n-        device_mesh = self.device_mesh\n-\n-        global_num_experts = empty_param.shape[0]\n-        if global_num_experts % device_mesh.size() != 0:\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n+        global_num_experts = self.empty_param.shape[0]\n+        if global_num_experts % self.device_mesh.size() != 0:\n             raise ValueError(\n-                f\"Global number of experts must be divisible by number of devices: {global_num_experts} % {device_mesh.size()} != 0\"\n+                f\"Global number of experts must be divisible by number of devices: {global_num_experts} % {self.device_mesh.size()} != 0\"\n             )\n-        local_num_experts = global_num_experts // device_mesh.size()\n-        parameter = param[ep_rank * local_num_experts : (ep_rank + 1) * local_num_experts].to(param_casting_dtype)\n+        local_num_experts = global_num_experts // self.device_mesh.size()\n+        parameter = param[self.rank * local_num_experts : (self.rank + 1) * local_num_experts]\n         self.shard = None\n-        return parameter, None\n+        return parameter.to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n-        ep_rank = rank\n-        global_num_experts = empty_param.shape[0]\n-        if global_num_experts % device_mesh.size() != 0:\n-            raise ValueError(\n-                f\"Global number of experts must be divisible by number of devices: {global_num_experts} % {device_mesh.size()} != 0\"\n-            )\n-        local_num_experts = global_num_experts // device_mesh.size()\n-        param = param[ep_rank * local_num_experts : (ep_rank + 1) * local_num_experts].to(param_casting_dtype)\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n-            param = param.contiguous()\n-        return param\n+            parameter = parameter.contiguous()\n+        return parameter\n \n \n class RouterParallel(TensorParallelLayer):\n@@ -1177,25 +1089,17 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n         return router_logits, router_scores, router_indices\n \n     def shard_tensor(\n-        self,\n-        param,\n-        param_type=None,\n-        param_casting_dtype=None,\n-        to_contiguous=None,\n-        rank=None,\n-        device_mesh=None,\n-        tensor_idx=None,\n-    ):\n-        parameter = param[...].to(param_casting_dtype)\n+        self, param: torch.Tensor, tensor_idx: int | None = None, device=None, dtype=None\n+    ) -> torch.Tensor:\n         self.shard = None\n-        return parameter, None\n+        return param[...].to(device=device, dtype=dtype)\n \n-    def partition_tensor(self, param, empty_param, param_type, param_casting_dtype, to_contiguous, rank, device_mesh):\n+    def partition_tensor(self, param: torch.Tensor, dtype, to_contiguous: bool):\n         # TODO: i'd like for this to be the default\n-        param = param[...].to(param_casting_dtype)\n+        parameter = self.shard_tensor(param, dtype=dtype)\n         if to_contiguous:\n-            param = param.contiguous()\n-        return param\n+            parameter = parameter.contiguous()\n+        return parameter\n \n     def prepare_module_tp(self, module: nn.Module, device_mesh) -> nn.Module:\n         # TODO: need an abstract Parallel class that is different from TensorParallelLayer\n@@ -1331,13 +1235,10 @@ def shard_and_distribute_module(\n \n     if current_shard_plan is not None:\n         try:\n-            tp_layer = ALL_PARALLEL_STYLES[current_shard_plan]\n-            tp_layer.empty_param = empty_param\n-            tp_layer.device_mesh = device_mesh\n-            tp_layer.rank = rank\n-            param = tp_layer.partition_tensor(\n-                param, empty_param, param_type, param_casting_dtype, is_contiguous, rank, device_mesh\n+            tp_layer = ALL_PARALLEL_STYLES[current_shard_plan](\n+                empty_param=empty_param, device_mesh=device_mesh, rank=rank\n             )\n+            param = tp_layer.partition_tensor(param, param_casting_dtype, is_contiguous)\n         except NotImplementedError as e:\n             print(\n                 f\"Trying to prepare {parameter_name}, but it's not supported. Corresponding module: {module_to_tp} Fix it's TP plan, current layer: {tp_layer} : {e}\""
        },
        {
            "sha": "0b58e96a39e9a38869a94d7d2c11e85c899c742c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 10,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f1c05cfddcecb8a233e47ba086336ff205135ad/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5f1c05cfddcecb8a233e47ba086336ff205135ad",
            "patch": "@@ -4541,13 +4541,7 @@ def get_total_byte_count(\n \n     total_byte_count = defaultdict(lambda: 0)\n     tied_param_names = model.all_tied_weights_keys.keys()\n-\n-    tp_plan = getattr(model, \"_tp_plan\", []) or []\n-    tp_plan_regex = (\n-        re.compile(\"|\".join([re.escape(plan) for plan in tp_plan]))\n-        if _torch_distributed_available and torch.distributed.is_initialized()\n-        else None\n-    )\n+    tp_plan = model._tp_plan\n \n     for param_name, device in accelerator_device_map.items():\n         # Skip if the parameter has already been accounted for (tied weights)\n@@ -4563,9 +4557,9 @@ def get_total_byte_count(\n \n         param_byte_count = param.numel() * dtype_size\n \n-        if tp_plan_regex is not None:\n-            generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n-            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n+        if len(tp_plan) > 0:\n+            is_part_of_plan = _get_parameter_tp_plan(param_name, tp_plan, is_weight=True) is not None\n+            param_byte_count //= torch.distributed.get_world_size() if is_part_of_plan else 1\n \n         total_byte_count[device] += param_byte_count\n     return total_byte_count"
        }
    ],
    "stats": {
        "total": 363,
        "additions": 128,
        "deletions": 235
    }
}