{
    "author": "qubvel",
    "message": "[vision] Improve keypoint-matching models docs (#40497)\n\nfix options and add inference_mode",
    "sha": "476cd7bab17d83224c19cd17e70d0e9209fe4c1c",
    "files": [
        {
            "sha": "83d5dd000adac47ab9a14ecd6889feb934c1b428",
            "filename": "docs/source/en/model_doc/efficientloftr.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fefficientloftr.md?ref=476cd7bab17d83224c19cd17e70d0e9209fe4c1c",
            "patch": "@@ -45,7 +45,7 @@ results = keypoint_matcher([url_0, url_1], threshold=0.9)\n print(results[0])\n # {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n ```\n-<hfoption id=\"AutoModel\">\n+</hfoption>\n <hfoption id=\"AutoModel\">\n \n ```py\n@@ -65,7 +65,7 @@ processor = AutoImageProcessor.from_pretrained(\"zju-community/efficientloftr\")\n model = AutoModelForKeypointMatching.from_pretrained(\"zju-community/efficientloftr\")\n \n inputs = processor(images, return_tensors=\"pt\")\n-with torch.no_grad():\n+with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Post-process to get keypoints and matches\n@@ -92,7 +92,8 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # EfficientLoFTR requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n-    outputs = model(**inputs)\n+    with torch.inference_mode():\n+        outputs = model(**inputs)\n     \n     # Extract matching information\n     keypoints = outputs.keypoints        # Keypoints in both images"
        },
        {
            "sha": "13ac58a1b842cff3cf00e644149c328398d358d8",
            "filename": "docs/source/en/model_doc/lightglue.md",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flightglue.md?ref=476cd7bab17d83224c19cd17e70d0e9209fe4c1c",
            "patch": "@@ -47,6 +47,8 @@ results = keypoint_matcher([url_0, url_1], threshold=0.9)\n print(results[0])\n # {'keypoint_image_0': {'x': ..., 'y': ...}, 'keypoint_image_1': {'x': ..., 'y': ...}, 'score': ...}\n ```\n+\n+</hfoption>\n <hfoption id=\"AutoModel\">\n \n ```py\n@@ -66,7 +68,7 @@ processor = AutoImageProcessor.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n model = AutoModel.from_pretrained(\"ETH-CVG/lightglue_superpoint\")\n \n inputs = processor(images, return_tensors=\"pt\")\n-with torch.no_grad():\n+with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Post-process to get keypoints and matches\n@@ -93,7 +95,8 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # LightGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n-    outputs = model(**inputs)\n+    with torch.inference_mode():\n+        outputs = model(**inputs)\n     \n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image"
        },
        {
            "sha": "3e42b002ec6a558aacd15f1e5ba20642ad917018",
            "filename": "docs/source/en/model_doc/superglue.md",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/476cd7bab17d83224c19cd17e70d0e9209fe4c1c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperglue.md?ref=476cd7bab17d83224c19cd17e70d0e9209fe4c1c",
            "patch": "@@ -68,7 +68,7 @@ processor = AutoImageProcessor.from_pretrained(\"magic-leap-community/superglue_o\n model = AutoModel.from_pretrained(\"magic-leap-community/superglue_outdoor\")\n \n inputs = processor(images, return_tensors=\"pt\")\n-with torch.no_grad():\n+with torch.inference_mode():\n     outputs = model(**inputs)\n \n # Post-process to get keypoints and matches\n@@ -95,7 +95,8 @@ processed_outputs = processor.post_process_keypoint_matching(outputs, image_size\n     # SuperGlue requires pairs of images\n     images = [image1, image2]\n     inputs = processor(images, return_tensors=\"pt\")\n-    outputs = model(**inputs)\n+    with torch.inference_mode():\n+        outputs = model(**inputs)\n     \n     # Extract matching information\n     keypoints0 = outputs.keypoints0  # Keypoints in first image"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 12,
        "deletions": 7
    }
}