{
    "author": "pbelevich",
    "message": "Llama4: remove redundant transpose of router_logits (#37468)\n\n* Llama4: remove redundant transpose of router_logits\n\n* Fix formatting",
    "sha": "ecaeee66bc663a66c8b5de0a050477a4c3263c1f",
    "files": [
        {
            "sha": "021bae7f62c62e669cc54b8dc5f144e9a583f41f",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ecaeee66bc663a66c8b5de0a050477a4c3263c1f/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ecaeee66bc663a66c8b5de0a050477a4c3263c1f/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=ecaeee66bc663a66c8b5de0a050477a4c3263c1f",
            "patch": "@@ -159,14 +159,12 @@ def __init__(self, config):\n     def forward(self, hidden_states):\n         batch, seq_len, hidden_dim = hidden_states.shape\n         hidden_states = hidden_states.view(-1, self.hidden_dim)\n-        router_logits = self.router(hidden_states).transpose(0, 1)\n+        router_logits = self.router(hidden_states)\n         tokens_per_expert = batch * seq_len\n \n-        router_top_value, router_indices = torch.topk(router_logits.transpose(0, 1), self.top_k, dim=1)\n+        router_top_value, router_indices = torch.topk(router_logits, self.top_k, dim=1)\n         router_scores = (\n-            torch.full_like(router_logits.transpose(0, 1), float(\"-inf\"))\n-            .scatter_(1, router_indices, router_top_value)\n-            .transpose(0, 1)\n+            torch.full_like(router_logits, float(\"-inf\")).scatter_(1, router_indices, router_top_value).transpose(0, 1)\n         )\n         # We do this to make sure we have -inf for non topK tokens before going through the !\n         # Here we are just creating a tensor to index each and every single one of the hidden states. Let s maybe register a buffer for this!"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 3,
        "deletions": 5
    }
}