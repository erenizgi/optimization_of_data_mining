{
    "author": "jiqing-feng",
    "message": "fix gpt-oss out shape (#40535)\n\n* fix out shape\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* reset gpt-oss modeling\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix copies\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "1067577ad204e649514ff3a5d3af0f7d52a63f14",
    "files": [
        {
            "sha": "7cf4d42ea58fac967d3f1f8ec2dfab2109c294de",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1067577ad204e649514ff3a5d3af0f7d52a63f14/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1067577ad204e649514ff3a5d3af0f7d52a63f14/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=1067577ad204e649514ff3a5d3af0f7d52a63f14",
            "patch": "@@ -116,7 +116,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 glu = gate * torch.sigmoid(gate * self.alpha)\n                 gated_output = (up + 1) * glu\n                 out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n-                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n+                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n                 next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n             next_states = next_states.view(batch_size, -1, self.hidden_size)\n         else:"
        },
        {
            "sha": "9203860cc5e04e5469212ab9b8f5993be48f5b95",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1067577ad204e649514ff3a5d3af0f7d52a63f14/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1067577ad204e649514ff3a5d3af0f7d52a63f14/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=1067577ad204e649514ff3a5d3af0f7d52a63f14",
            "patch": "@@ -115,7 +115,7 @@ def forward(self, hidden_states: torch.Tensor, router_indices=None, routing_weig\n                 glu = gate * torch.sigmoid(gate * self.alpha)\n                 gated_output = (up + 1) * glu\n                 out = gated_output @ self.down_proj[expert_idx] + self.down_proj_bias[expert_idx]\n-                weighted_output = out[0] * routing_weights[token_idx, expert_idx, None]\n+                weighted_output = out * routing_weights[token_idx, expert_idx, None]\n                 next_states.index_add_(0, token_idx, weighted_output.to(hidden_states.dtype))\n             next_states = next_states.view(batch_size, -1, self.hidden_size)\n         else:"
        },
        {
            "sha": "35e8f707c4b8c019a491998d2d9bde4262048592",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1067577ad204e649514ff3a5d3af0f7d52a63f14/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1067577ad204e649514ff3a5d3af0f7d52a63f14/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=1067577ad204e649514ff3a5d3af0f7d52a63f14",
            "patch": "@@ -128,9 +128,6 @@ def test_flex_attention_with_grads(self):\n     def test_generate_compile_model_forward_fullgraph(self):\n         return super().test_generate_compile_model_forward_fullgraph()\n \n-    def test_batching_equivalence(self, **kwargs):\n-        return super().test_batching_equivalence(atol=5e-4, rtol=1e-3)\n-\n \n RESULTS_PATH = Path(__file__).parent.parent.parent / \"fixtures/gpt_oss/integration_tests.json\"\n "
        }
    ],
    "stats": {
        "total": 7,
        "additions": 2,
        "deletions": 5
    }
}