{
    "author": "kimishpatel",
    "message": "Allow customization of sdpa in executorch.py (#38827)\n\nEarlier PR put executorch specific sdpa and mask function in the export function. This prevent any customization that can be done to sdpa, prior to export. By moving this to __init__, we still keep the original behavior but allow users like optimum-executorch to override sdpa by setting model.config._attn_implementation.",
    "sha": "37367c7d9fd23401c26e79a2b26253ab2d1b7236",
    "files": [
        {
            "sha": "0df283c83b71d1ad55cc3ea4080044c4c15b1fc0",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/37367c7d9fd23401c26e79a2b26253ab2d1b7236/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/37367c7d9fd23401c26e79a2b26253ab2d1b7236/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=37367c7d9fd23401c26e79a2b26253ab2d1b7236",
            "patch": "@@ -65,6 +65,10 @@ def __init__(\n                 \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n             )\n             self.model = TorchExportableModuleWithStaticCache(model)\n+        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n+        ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n+        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n+        self.model.model.config._attn_implementation = \"sdpa_without_vmap\"\n \n     def forward(\n         self,\n@@ -103,10 +107,6 @@ def export(\n             strict(`Optional[bool]`):\n                 Flag to instruct `torch.export` to use `torchdynamo`.\n         \"\"\"\n-        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n-        ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n-        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n-        self.model.model.config._attn_implementation = \"sdpa_without_vmap\"\n \n         example_input_ids = input_ids if input_ids is not None else torch.tensor([[1]], dtype=torch.long)\n         example_cache_position = cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long)"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}