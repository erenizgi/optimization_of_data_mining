{
    "author": "Rocketknight1",
    "message": "No more dtype_byte_size() (#37144)\n\n* No more dtype_byte_size()\n\n* Remove function once again\n\n* Fix rebase cruft\n\n* Trigger tests",
    "sha": "cbfa14823b4ef762ebf138822cacc00c733be845",
    "files": [
        {
            "sha": "c775ee85bbefb6a70481863bb8c546ef3efa43d3",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 19,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -17,7 +17,6 @@\n import gc\n import json\n import os\n-import re\n import warnings\n from functools import partial\n from pickle import UnpicklingError\n@@ -83,23 +82,6 @@ def quick_gelu(x):\n }\n \n \n-def dtype_byte_size(dtype):\n-    \"\"\"\n-    Returns the size (in bytes) occupied by one parameter of type `dtype`. Example:\n-    ```py\n-    >>> dtype_byte_size(np.float32)\n-    4\n-    ```\n-    \"\"\"\n-    if dtype is bool:\n-        return 1 / 8\n-    bit_search = re.search(r\"[^\\d](\\d+)$\", dtype.name)\n-    if bit_search is None:\n-        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n-    bit_size = int(bit_search.groups()[0])\n-    return bit_size // 8\n-\n-\n def flax_shard_checkpoint(params, max_shard_size=\"10GB\"):\n     \"\"\"\n     Splits a model state dictionary in sub-checkpoints so that the final size of each sub-checkpoint does not exceed a\n@@ -131,7 +113,7 @@ def flax_shard_checkpoint(params, max_shard_size=\"10GB\"):\n     # flatten the weights to chunk\n     weights = flatten_dict(params, sep=\"/\")\n     for item in weights:\n-        weight_size = weights[item].size * dtype_byte_size(weights[item].dtype)\n+        weight_size = weights[item].size * weights[item].dtype.itemsize\n \n         # If this weight is going to tip up over the maximal size, we split.\n         if current_block_size + weight_size > max_shard_size:"
        },
        {
            "sha": "a09bc430a44b4924b28953f9081b501c78afb190",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -617,26 +617,6 @@ def input_processing(func, config, **kwargs):\n     return output\n \n \n-def dtype_byte_size(dtype):\n-    \"\"\"\n-    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n-\n-    Example:\n-\n-    ```py\n-    >>> dtype_byte_size(tf.float32)\n-    4\n-    ```\n-    \"\"\"\n-    if dtype == tf.bool:\n-        return 1 / 8\n-    bit_search = re.search(r\"[^\\d](\\d+)$\", dtype.name)\n-    if bit_search is None:\n-        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n-    bit_size = int(bit_search.groups()[0])\n-    return bit_size // 8\n-\n-\n def strip_model_name_and_prefix(name, _prefix=None):\n     if _prefix is not None and name.startswith(_prefix):\n         name = name[len(_prefix) :]\n@@ -678,7 +658,7 @@ def tf_shard_checkpoint(weights, max_shard_size=\"10GB\", weights_name: str = TF2_\n     total_size = 0\n \n     for item in weights:\n-        weight_size = item.numpy().size * dtype_byte_size(item.dtype)\n+        weight_size = item.numpy().size * item.dtype.size\n \n         # If this weight is going to tip up over the maximal size, we split.\n         if current_block_size + weight_size > max_shard_size:"
        },
        {
            "sha": "331248fbf997396c8e96db91db8e4953e4670f7b",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -385,26 +385,6 @@ def get_state_dict_dtype(state_dict):\n         return next(state_dict.values()).dtype\n \n \n-def dtype_byte_size(dtype):\n-    \"\"\"\n-    Returns the size (in bytes) occupied by one parameter of type `dtype`.\n-\n-    Example:\n-\n-    ```py\n-    >>> dtype_byte_size(torch.float32)\n-    4\n-    ```\n-    \"\"\"\n-    if dtype == torch.bool:\n-        return 1 / 8\n-    bit_search = re.search(r\"[^\\d](\\d+)_?\", str(dtype))\n-    if bit_search is None:\n-        raise ValueError(f\"`dtype` is not a valid dtype: {dtype}.\")\n-    bit_size = int(bit_search.groups()[0])\n-    return bit_size // 8\n-\n-\n def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     \"\"\"\n     This is the same as\n@@ -5820,7 +5800,7 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n     for param_name, device in accelerator_device_map.items():\n         param = model.get_parameter_or_buffer(param_name)\n         # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n-        param_byte_count = math.prod(param.shape) * dtype_byte_size(param.dtype)\n+        param_byte_count = math.prod(param.shape) * param.element_size()\n \n         if tp_plan_regex is not None:\n             generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)"
        },
        {
            "sha": "a84138a62461cf1a54224cadab215b843cd0f392",
            "filename": "src/transformers/models/nllb_moe/convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -19,7 +19,6 @@\n from torch import nn\n \n from transformers import NllbMoeConfig, NllbMoeModel\n-from transformers.modeling_utils import dtype_byte_size\n from transformers.utils import WEIGHTS_INDEX_NAME, WEIGHTS_NAME\n \n \n@@ -86,8 +85,8 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, num_experts, dtype, weig\n             )\n             torch.save(expert_state, save_path)\n             sharded_state_dicts.append(expert_state.keys())\n-            total_size += sum([value.numel() for key, value in expert_state.items()]) * dtype_byte_size(\n-                expert_state[list(expert_state)[0]].dtype\n+            total_size += sum([value.numel() for key, value in expert_state.items()]) * (\n+                expert_state[list(expert_state)[0]].element_size()\n             )\n \n     # Add the last block"
        },
        {
            "sha": "6f422439fc7e5c0cd09b97affd99e74ba4c741c6",
            "filename": "src/transformers/models/switch_transformers/convert_big_switch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -8,7 +8,6 @@\n from flax.traverse_util import flatten_dict, unflatten_dict\n from tensorflow.io import gfile\n \n-from transformers.modeling_utils import dtype_byte_size\n from transformers.models.switch_transformers.convert_switch_transformers_original_flax_checkpoint_to_pytorch import (\n     rename_keys,\n )\n@@ -94,7 +93,7 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n         # open tensorstore file\n         raw_weights = ts.open(unflatten_dict(all_layers[key])).result().read().result()\n         raw_weights = torch.tensor(raw_weights)\n-        weight_size = raw_weights.numel() * dtype_byte_size(raw_weights.dtype)\n+        weight_size = raw_weights.numel() * raw_weights.element_size()\n \n         # use the renaming pattern from the small conversion scripts\n         key, raw_weights = rename_base_flax_keys(tuple(key.split(\"/\")), raw_weights)"
        },
        {
            "sha": "4ec2c4974403e6bfcd34cc819695f658e89a3967",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbfa14823b4ef762ebf138822cacc00c733be845/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbfa14823b4ef762ebf138822cacc00c733be845/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=cbfa14823b4ef762ebf138822cacc00c733be845",
            "patch": "@@ -116,7 +116,6 @@\n     from transformers.modeling_utils import (\n         _find_disjoint,\n         _find_identical,\n-        dtype_byte_size,\n     )\n     from transformers.pytorch_utils import isin_mps_friendly\n \n@@ -704,31 +703,6 @@ def test_model_from_config_attn_implementation(self):\n             model = AutoModelForCausalLM.from_config(config=config, attn_implementation=requested_attn_implementation)\n             self.assertEqual(model.config._attn_implementation, requested_attn_implementation)\n \n-    def test_torch_dtype_byte_sizes(self):\n-        torch_dtypes_and_bytes = [\n-            (torch.double, 8),\n-            (torch.float64, 8),\n-            (torch.float, 4),\n-            (torch.float32, 4),\n-            (torch.half, 2),\n-            (torch.float16, 2),\n-            (torch.bfloat16, 2),\n-            (torch.long, 8),\n-            (torch.int64, 8),\n-            (torch.int, 4),\n-            (torch.int32, 4),\n-            (torch.short, 2),\n-            (torch.int16, 2),\n-            (torch.uint8, 1),\n-            (torch.int8, 1),\n-            (torch.float8_e4m3fn, 1),\n-            (torch.float8_e5m2, 1),\n-            (torch.bool, 0.125),\n-        ]\n-\n-        for torch_dtype, bytes_per_element in torch_dtypes_and_bytes:\n-            self.assertEqual(dtype_byte_size(torch_dtype), bytes_per_element)\n-\n     def test_no_super_init_config_and_model(self):\n         config = NoSuperInitConfig(attribute=32)\n         model = NoSuperInitModel(config)"
        }
    ],
    "stats": {
        "total": 98,
        "additions": 6,
        "deletions": 92
    }
}