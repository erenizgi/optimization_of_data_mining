{
    "author": "ducviet00",
    "message": "Fix contrastive search to correctly handle input with padding (#33507)\n\n* fix: handle padding in contrastive search for decoder-only models\r\n\r\n* fix: handle padding in contrastive search for encoder-decoder models\r\n\r\n* tests: move padding contrastive test to test_util, add t5 test\r\n\r\n* fix: handle if model_kwargs[\"decoder_attention_mask\"] is None\r\n\r\n* refactor: improve padding input contrastive search generation tests\r\n\r\n* chore: _ranking_fast to use LongTensor for cosine_matrix_mask",
    "sha": "dc8b6eaeeeb59dd3089b478cc09b577f2c62a297",
    "files": [
        {
            "sha": "2fe92d3e3ed64bb6c6841faec892a281ec039b82",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc8b6eaeeeb59dd3089b478cc09b577f2c62a297/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc8b6eaeeeb59dd3089b478cc09b577f2c62a297/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=dc8b6eaeeeb59dd3089b478cc09b577f2c62a297",
            "patch": "@@ -2604,6 +2604,15 @@ def _contrastive_search(\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n+        # Create cosine_matrix_mask based on the attention_mask\n+        cosine_matrix_mask = torch.ones_like(input_ids, dtype=torch.long)\n+        if self.config.is_encoder_decoder:\n+            if \"decoder_attention_mask\" in model_kwargs and model_kwargs[\"decoder_attention_mask\"] is not None:\n+                cosine_matrix_mask = model_kwargs[\"decoder_attention_mask\"]\n+        else:\n+            cosine_matrix_mask = model_kwargs[\"attention_mask\"]\n+        cosine_matrix_mask = cosine_matrix_mask.repeat_interleave(top_k, dim=0)\n+\n         this_peer_finished = False\n \n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n@@ -2771,7 +2780,12 @@ def _contrastive_search(\n             # compute the degeneration penalty and re-rank the candidates based on the degeneration penalty and the\n             # model confidence. Keeping `selected_idx` on CPU enables multi-device contrastive search and doesn't\n             # introduce (noticeable) slowdowns on single-device runs.\n-            selected_idx = _ranking_fast(context_hidden, next_hidden, top_k_probs, penalty_alpha, top_k)\n+            selected_idx = _ranking_fast(\n+                context_hidden, next_hidden, top_k_probs, cosine_matrix_mask, penalty_alpha, top_k\n+            )\n+            cosine_matrix_mask = torch.cat(\n+                [cosine_matrix_mask, cosine_matrix_mask.new_ones((cosine_matrix_mask.shape[0], 1))], dim=-1\n+            )\n             selected_idx = selected_idx.to(\"cpu\")\n \n             # This will be used instead of the previous inneficient torch.stack(torch.split())\n@@ -4283,6 +4297,7 @@ def _ranking_fast(\n     context_hidden: torch.FloatTensor,\n     next_hidden: torch.FloatTensor,\n     next_top_k_probs: torch.FloatTensor,\n+    cosine_matrix_mask: torch.LongTensor,\n     alpha: float,\n     beam_width: int,\n ) -> torch.FloatTensor:\n@@ -4294,6 +4309,13 @@ def _ranking_fast(\n     norm_context_hidden = context_hidden / context_hidden.norm(dim=2, keepdim=True)\n     norm_next_hidden = next_hidden / next_hidden.norm(dim=2, keepdim=True)\n     cosine_matrix = torch.matmul(norm_context_hidden, norm_next_hidden.transpose(1, 2)).squeeze(-1)  # [B*K, S]\n+\n+    # Penalize cosine_matrix based on the cosine_matrix_mask (ignore padding positions)\n+    # Using a large negative value for masked positions\n+    cosine_matrix_mask = cosine_matrix_mask.to(dtype=cosine_matrix.dtype)\n+    cosine_matrix_mask = (1 - cosine_matrix_mask) * torch.finfo(cosine_matrix.dtype).min\n+    cosine_matrix = cosine_matrix + cosine_matrix_mask\n+\n     degeneration_penalty, _ = torch.max(cosine_matrix, dim=-1)  # [B*K]\n     next_top_k_probs = next_top_k_probs.view(-1)  # [B*K]\n     contrastive_score = (1.0 - alpha) * next_top_k_probs - alpha * degeneration_penalty"
        },
        {
            "sha": "2f8e60c79151e962bc8e162be7770de13c308af1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 135,
            "deletions": 0,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc8b6eaeeeb59dd3089b478cc09b577f2c62a297/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc8b6eaeeeb59dd3089b478cc09b577f2c62a297/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=dc8b6eaeeeb59dd3089b478cc09b577f2c62a297",
            "patch": "@@ -44,6 +44,7 @@\n \n if is_torch_available():\n     import torch\n+    import torch.nn.functional as F\n \n     from transformers import (\n         AutoModelForCausalLM,\n@@ -59,6 +60,7 @@\n         GPT2Tokenizer,\n         ImageGPTForCausalImageModeling,\n         SpeechEncoderDecoderModel,\n+        T5ForConditionalGeneration,\n     )\n     from transformers.cache_utils import DynamicCache, EncoderDecoderCache, QuantoQuantizedCache, StaticCache\n     from transformers.generation import (\n@@ -3644,6 +3646,139 @@ def test_init_static_cache_multi_gpu(self):\n         value_cache_1 = results.past_key_values.value_cache[1]\n         self.assertTrue(key_cache_1.device == value_cache_1.device == torch.device(1))\n \n+    @slow\n+    def test_padding_input_contrastive_search_gpt2(self):\n+        # Load the pre-trained GPT-2 model and tokenizer\n+        model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n+        model.to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", clean_up_tokenization_spaces=True)\n+\n+        # Set the tokenizer to left-pad the sequences\n+        tokenizer.padding_side = \"left\"\n+\n+        # Define the PAD token as the EOS token\n+        tokenizer.pad_token = tokenizer.eos_token\n+        model.generation_config.pad_token_id = model.generation_config.eos_token_id\n+\n+        # Define the input prompt\n+        prompt_text = \"The whispered legends of the haunted mansion spoke\"\n+\n+        # Tokenize the input prompt\n+        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\", padding=True)\n+        input_ids = encoded_prompt.input_ids.to(torch_device)\n+        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n+\n+        # Define the contrastive search params\n+        penalty_alpha = 0.6\n+        top_k = 4\n+\n+        # Define the padding length to add to the input IDs and attention mask\n+        padding_length = 10\n+\n+        # Generate text without padding\n+        outputs = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            do_sample=False,\n+            penalty_alpha=penalty_alpha,\n+            top_k=top_k,\n+            max_new_tokens=64,\n+        )\n+        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n+\n+        # Pad the input IDs and attention mask on the left\n+        padded_input_ids = F.pad(\n+            input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n+        )\n+        padded_attention_mask = F.pad(attention_mask, (padding_length, 0), \"constant\", value=0)\n+\n+        # Generate text with padded inputs\n+        outputs_with_padding = model.generate(\n+            input_ids=padded_input_ids,\n+            attention_mask=padded_attention_mask,\n+            do_sample=False,\n+            penalty_alpha=penalty_alpha,\n+            top_k=top_k,\n+            max_new_tokens=64,\n+        )\n+        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n+\n+        # Assert that the generated texts are identical for padded and non-padded inputs\n+        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n+        self.assertEqual(\n+            generated_text_with_padding,\n+            'The whispered legends of the haunted mansion spoke of the \"souls of the dead\" who were \"falling '\n+            'out of the sky\" and \"falling into the sea.\"\\n\\nThe ghostly apparitions were said to have been '\n+            'created by the spirits of the dead, who were \"falling out of the sky\" and \"falling into the sea',\n+        )\n+\n+    @slow\n+    def test_padding_input_contrastive_search_t5(self):\n+        # Load the pre-trained T5 model and tokenizer\n+        model = T5ForConditionalGeneration.from_pretrained(\"google-t5/t5-small\")\n+        model.to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\", clean_up_tokenization_spaces=True)\n+\n+        # Define the input prompt\n+        prompt_text = \"translate English to German: I need to finish this task before the end of the day.\"\n+\n+        # Tokenize the input prompt\n+        encoded_prompt = tokenizer(prompt_text, return_tensors=\"pt\")\n+        input_ids = encoded_prompt.input_ids.to(torch_device)\n+        attention_mask = encoded_prompt.attention_mask.to(torch_device)\n+\n+        # Define the decoder prompt\n+        decoder_prompt_text = \"Ich muss diese Aufgabe\"\n+        encoded_decoder_prompt = tokenizer(decoder_prompt_text, add_special_tokens=False, return_tensors=\"pt\")\n+        decoder_input_ids = encoded_decoder_prompt.input_ids.to(torch_device)\n+        decoder_attention_mask = encoded_decoder_prompt.attention_mask.to(torch_device)\n+\n+        # Define the contrastive search params\n+        penalty_alpha = 0.6\n+        top_k = 4\n+\n+        # Generate text without padding\n+        outputs = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=decoder_input_ids,\n+            decoder_attention_mask=decoder_attention_mask,\n+            do_sample=False,\n+            penalty_alpha=penalty_alpha,\n+            top_k=top_k,\n+            max_new_tokens=64,\n+        )\n+        generated_text_no_padding = tokenizer.decode(outputs[0], skip_special_tokens=True)\n+\n+        # Define the padding length to add to the input IDs and attention mask\n+        padding_length = 10\n+\n+        # Pad the decoder input IDs and attention mask on the left\n+        padded_decoder_input_ids = F.pad(\n+            decoder_input_ids, (padding_length, 0), \"constant\", value=model.generation_config.pad_token_id\n+        )\n+        padded_decoder_attention_mask = F.pad(decoder_attention_mask, (padding_length, 0), \"constant\", value=0)\n+        # Since the decoder_start_token_id is the same as the pad_token_id,\n+        # the last padded token represents the decoder start token.\n+        # Set the attention mask for the decoder_start_token_id to True (1).\n+        padded_decoder_attention_mask[:, padding_length - 1] = 1\n+        # Generate text with padded inputs\n+        outputs_with_padding = model.generate(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            decoder_input_ids=padded_decoder_input_ids,\n+            decoder_attention_mask=padded_decoder_attention_mask,\n+            do_sample=False,\n+            penalty_alpha=penalty_alpha,\n+            top_k=top_k,\n+            max_new_tokens=64,\n+        )\n+        generated_text_with_padding = tokenizer.decode(outputs_with_padding[0], skip_special_tokens=True)\n+\n+        # Assert that the generated texts are identical for padded and non-padded inputs\n+        self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n+        self.assertEqual(generated_text_no_padding, \"Ich muss diese Aufgabe vor Ende des Tages beenden.\")\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 159,
        "additions": 158,
        "deletions": 1
    }
}