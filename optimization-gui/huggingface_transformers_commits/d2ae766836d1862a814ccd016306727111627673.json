{
    "author": "guangy10",
    "message": "Export SmolvLM (#39614)\n\nExport SmolVLM for ExecuTorch",
    "sha": "d2ae766836d1862a814ccd016306727111627673",
    "files": [
        {
            "sha": "a56dceaa24f45db316fce6cb6234f5fcfe8ad642",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 184,
            "deletions": 6,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=d2ae766836d1862a814ccd016306727111627673",
            "patch": "@@ -27,6 +27,167 @@\n from ..pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n \n \n+# Add this to src/transformers/integrations/executorch.py\n+\n+\n+class TorchExportableModuleForVLM:\n+    \"\"\"\n+    A wrapper class for exporting Vision-Language Models (VLMs) like SmolVLM2 for ExecuTorch.\n+\n+    This class handles the export of three main components:\n+        1. Vision encoder (processes images to visual features)\n+        2. Connector/projector (maps visual features to text embedding space)\n+        3. Text decoder (generates text from combined visual and text tokens)\n+    \"\"\"\n+\n+    def __init__(self, model, max_batch_size: int = 1, max_cache_len: int = 1024):\n+        \"\"\"\n+        Initialize the exportable VLM module.\n+\n+        Args:\n+            model: The VLM (e.g. SmolVLM) model instance\n+            max_batch_size: Maximum batch size. Always 1 for ExecuTorch\n+            max_cache_len: Maximum cache length for text generation\n+        \"\"\"\n+        self.model = model\n+        self.max_batch_size = max_batch_size\n+        self.max_cache_len = max_cache_len\n+        self.config = model.config\n+\n+        # Extract individual components\n+        self.vision_encoder = model.model.vision_model\n+        self.connector = model.model.connector\n+        self.text_decoder = model.model.text_model\n+\n+        # Store exported programs\n+        self.exported_vision_encoder = None\n+        self.exported_connector = None\n+        self.exported_text_decoder = None\n+\n+    def export_vision_encoder(self):\n+        \"\"\"Export the vision encoder component.\"\"\"\n+        self.vision_encoder.eval()\n+\n+        # Create example input\n+        pixel_values = torch.randn(1, 3, 384, 384, dtype=torch.float32)\n+\n+        # Define dynamic shapes\n+        dynamic_shapes = {\n+            \"pixel_values\": {\n+                2: torch.export.Dim.AUTO,\n+                3: torch.export.Dim.AUTO,\n+            }\n+        }\n+\n+        self.exported_vision_encoder = torch.export.export(\n+            self.vision_encoder,\n+            args=(pixel_values,),\n+            dynamic_shapes=dynamic_shapes,\n+            strict=False,\n+        )\n+\n+        return self.exported_vision_encoder\n+\n+    def export_connector(self):\n+        \"\"\"Export the connector component.\"\"\"\n+        self.connector.eval()\n+\n+        # Vision encoder output shape: [batch_size, num_patches, vision_hidden_size]\n+        vision_hidden_size = self.config.vision_config.hidden_size\n+        image_size = self.config.vision_config.image_size\n+        patch_size = self.config.vision_config.patch_size\n+        patches_per_dim = image_size // patch_size\n+        num_patches = patches_per_dim * patches_per_dim\n+        image_hidden_states = torch.randn(1, num_patches, vision_hidden_size, dtype=torch.float32)\n+\n+        # Define dynamic shapes - static batch_size=1, dynamic num_patches\n+        dynamic_shapes = {\"image_hidden_states\": {1: torch.export.Dim.AUTO}}\n+\n+        # Export the connector using torch.export\n+        self.exported_connector = torch.export.export(\n+            self.connector,\n+            args=(image_hidden_states,),\n+            dynamic_shapes=dynamic_shapes,\n+            strict=False,\n+        )\n+\n+        return self.exported_connector\n+\n+    def export_text_decoder(self):\n+        \"\"\"Export the text decoder component.\"\"\"\n+\n+        # Create text decoder exportable wrapper\n+        self.exportable_text_decoder = TorchExportableModuleForDecoderOnlyLM(\n+            model=self.text_decoder,\n+            max_batch_size=self.max_batch_size,\n+            max_cache_len=self.max_cache_len,\n+        )\n+\n+        # Use the existing text decoder exportable wrapper\n+        seq_length = 3\n+        input_ids = torch.zeros((1, seq_length), dtype=torch.long)\n+        cache_position = torch.arange(seq_length, dtype=torch.long)\n+        max_seq_length = min(self.max_cache_len, self.config.text_config.max_position_embeddings)\n+        seq_len_dim = torch.export.Dim(\"seq_length_dim\", max=max_seq_length - 1)\n+\n+        dynamic_shapes = {\n+            \"input_ids\": {1: seq_len_dim},\n+            \"cache_position\": {0: seq_len_dim},\n+        }\n+\n+        self.exported_text_decoder = self.exportable_text_decoder.export(\n+            input_ids=input_ids,\n+            cache_position=cache_position,\n+            dynamic_shapes=dynamic_shapes,\n+            strict=False,\n+        )\n+\n+        return self.exported_text_decoder\n+\n+    def export(self, **kwargs):\n+        \"\"\"Export all components of the VLM model.\"\"\"\n+        self.export_vision_encoder(**kwargs)\n+        self.export_connector(**kwargs)\n+        self.export_text_decoder(**kwargs)\n+        return {\n+            \"vision_encoder\": self.exported_vision_encoder,\n+            \"connector\": self.exported_connector,\n+            \"text_decoder\": self.exported_text_decoder,\n+        }\n+\n+    def forward(self, pixel_values, input_ids, cache_position):\n+        \"\"\"\n+        Simplified forward pass for inference with guaranteed non-null input_ids and cache_position.\n+\n+        Args:\n+            pixel_values: Input images [1, channels, height, width] (optional)\n+            input_ids: Text token IDs [1, seq_len] (required - won't be None)\n+            cache_position: Cache positions [seq_len] (required - won't be None)\n+\n+        Returns:\n+            Output with logits for text generation\n+        \"\"\"\n+        pass\n+\n+    def generate(\n+        self, pixel_values=None, input_ids=None, max_new_tokens=50, do_sample=False, temperature=1.0, **kwargs\n+    ):\n+        \"\"\"\n+        Simplified generate method with guaranteed non-null input_ids.\n+\n+        Args:\n+            pixel_values: Input images [1, channels, height, width] (optional)\n+            input_ids: Initial text tokens [1, seq_len] (required - won't be None)\n+            max_new_tokens: Maximum number of tokens to generate\n+            do_sample: Whether to use sampling or greedy decoding\n+            temperature: Temperature for sampling\n+\n+        Returns:\n+            Generated sequences\n+        \"\"\"\n+        pass\n+\n+\n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     \"\"\"\n     A recipe module designed to make a `PreTrainedModel` exportable with `torch.export`,\n@@ -64,7 +225,7 @@ def __init__(\n             logging.info(\n                 \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n             )\n-            self.model = TorchExportableModuleWithStaticCache(model)\n+            self.model = TorchExportableModuleWithStaticCache(model, max_batch_size, max_cache_len)\n         # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n         ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n         ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n@@ -254,7 +415,12 @@ class TorchExportableModuleWithStaticCache(torch.nn.Module):\n         in a way that ensures the model can be further lowered and run efficiently in `ExecuTorch`.\n     \"\"\"\n \n-    def __init__(self, model: PreTrainedModel):\n+    def __init__(\n+        self,\n+        model: PreTrainedModel,\n+        max_batch_size: int = 1,\n+        max_cache_len: int = 4096,\n+    ):\n         \"\"\"\n         Initializes the wrapper module with the pretrained model.\n \n@@ -270,9 +436,16 @@ def __init__(self, model: PreTrainedModel):\n \n         # Sanity checks\n         if model.generation_config is None:\n-            raise AssertionError(\n-                \"The model must have a generation config to be exported with static caching. \"\n-                \"Please set `generation_config`.\"\n+            # Use default generation config if not specified\n+            model.generation_config = GenerationConfig(\n+                use_cache=model.config.use_cache,\n+                cache_implementation=\"static\",\n+                max_length=max_cache_len,\n+                cache_config={\n+                    \"batch_size\": max_batch_size,\n+                    \"max_cache_len\": max_cache_len,\n+                    \"device\": \"cpu\",\n+                },\n             )\n \n         if not model.generation_config.use_cache:\n@@ -332,7 +505,12 @@ def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n             past_key_values=past_key_values,\n             use_cache=True,\n         )\n-        return outs.logits\n+        if hasattr(outs, \"logits\"):\n+            # Returned outputs is `CausalLMOutputWithPast`\n+            return outs.logits\n+        else:\n+            # Returned the `last_hidden_state` from `BaseModelOutputWithPast`\n+            return outs.last_hidden_state\n \n     @staticmethod\n     def generate("
        },
        {
            "sha": "d25cf5e2f2a1654bbab23880cd2e621cc521e303",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=d2ae766836d1862a814ccd016306727111627673",
            "patch": "@@ -147,8 +147,11 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n             nb_patches_h = p_attn_mask[:, 0].sum()\n             nb_patches_w = p_attn_mask[0].sum()\n \n-            fractional_coords_h = torch.arange(0, 1 - 1e-6, 1 / nb_patches_h)\n-            fractional_coords_w = torch.arange(0, 1 - 1e-6, 1 / nb_patches_w)\n+            h_indices = torch.arange(nb_patches_h, device=pixel_values.device, dtype=pixel_values.dtype)\n+            w_indices = torch.arange(nb_patches_w, device=pixel_values.device, dtype=pixel_values.dtype)\n+\n+            fractional_coords_h = h_indices / nb_patches_h * (1 - 1e-6)\n+            fractional_coords_w = w_indices / nb_patches_w * (1 - 1e-6)\n \n             bucket_coords_h = torch.bucketize(fractional_coords_h, boundaries, right=True)\n             bucket_coords_w = torch.bucketize(fractional_coords_w, boundaries, right=True)"
        },
        {
            "sha": "c2d41aac02d76477d0347b147e119562e1fb0cc7",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=d2ae766836d1862a814ccd016306727111627673",
            "patch": "@@ -147,8 +147,11 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n             nb_patches_h = p_attn_mask[:, 0].sum()\n             nb_patches_w = p_attn_mask[0].sum()\n \n-            fractional_coords_h = torch.arange(0, 1 - 1e-6, 1 / nb_patches_h)\n-            fractional_coords_w = torch.arange(0, 1 - 1e-6, 1 / nb_patches_w)\n+            h_indices = torch.arange(nb_patches_h, device=pixel_values.device, dtype=pixel_values.dtype)\n+            w_indices = torch.arange(nb_patches_w, device=pixel_values.device, dtype=pixel_values.dtype)\n+\n+            fractional_coords_h = h_indices / nb_patches_h * (1 - 1e-6)\n+            fractional_coords_w = w_indices / nb_patches_w * (1 - 1e-6)\n \n             bucket_coords_h = torch.bucketize(fractional_coords_h, boundaries, right=True)\n             bucket_coords_w = torch.bucketize(fractional_coords_w, boundaries, right=True)\n@@ -558,10 +561,10 @@ def forward(\n         # The call to `_upad_input` in `_flash_attention_forward` is expensive\n         # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n         # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if not torch.any(~patch_attention_mask):\n-            patch_attention_mask = None\n-        elif not self._use_flash_attention_2:\n+        if not self._use_flash_attention_2:\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n+        elif not torch.any(~patch_attention_mask):\n+            patch_attention_mask = None\n \n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,"
        },
        {
            "sha": "745206868581a6f69c8fbbe8662cf76b514349a2",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2ae766836d1862a814ccd016306727111627673/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=d2ae766836d1862a814ccd016306727111627673",
            "patch": "@@ -142,8 +142,11 @@ def forward(self, pixel_values: torch.FloatTensor, patch_attention_mask: torch.B\n             nb_patches_h = p_attn_mask[:, 0].sum()\n             nb_patches_w = p_attn_mask[0].sum()\n \n-            fractional_coords_h = torch.arange(0, 1 - 1e-6, 1 / nb_patches_h)\n-            fractional_coords_w = torch.arange(0, 1 - 1e-6, 1 / nb_patches_w)\n+            h_indices = torch.arange(nb_patches_h, device=pixel_values.device, dtype=pixel_values.dtype)\n+            w_indices = torch.arange(nb_patches_w, device=pixel_values.device, dtype=pixel_values.dtype)\n+\n+            fractional_coords_h = h_indices / nb_patches_h * (1 - 1e-6)\n+            fractional_coords_w = w_indices / nb_patches_w * (1 - 1e-6)\n \n             bucket_coords_h = torch.bucketize(fractional_coords_h, boundaries, right=True)\n             bucket_coords_w = torch.bucketize(fractional_coords_w, boundaries, right=True)\n@@ -445,10 +448,10 @@ def forward(\n         # The call to `_upad_input` in `_flash_attention_forward` is expensive\n         # So when the `patch_attention_mask` is full of 1s (i.e. attending to the whole sequence),\n         # avoiding passing the attention_mask, which is equivalent to attending to the full sequence\n-        if not torch.any(~patch_attention_mask):\n-            patch_attention_mask = None\n-        elif not self._use_flash_attention_2:\n+        if not self._use_flash_attention_2:\n             patch_attention_mask = _prepare_4d_attention_mask(patch_attention_mask, hidden_states.dtype)\n+        elif not torch.any(~patch_attention_mask):\n+            patch_attention_mask = None\n \n         encoder_outputs = self.encoder(\n             inputs_embeds=hidden_states,"
        },
        {
            "sha": "d1140b6ec114189f7a731b619b29da011ebb68ae",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 77,
            "deletions": 0,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/d2ae766836d1862a814ccd016306727111627673/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d2ae766836d1862a814ccd016306727111627673/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=d2ae766836d1862a814ccd016306727111627673",
            "patch": "@@ -595,3 +595,80 @@ def test_integration_test_video(self):\n \n         expected_generated_text = 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video depicts a large language model architecture, specifically a language model with a \"quick brown\" feature'  # fmt: skip\n         self.assertEqual(generated_texts[0], expected_generated_text)\n+\n+    @slow\n+    def test_export_smolvlm_vision_encoder(self):\n+        from transformers import AutoConfig\n+        from transformers.integrations.executorch import TorchExportableModuleForVLM\n+\n+        model_id = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n+\n+        # NOTE: The attention_mask is prepared internally in the vision encoder, depending on whether flash attention is used or not\n+        # For ExecuTorch, flash attention is not supported, so the way of exporting vison encoder should be compatible with text-decoder\n+        config = AutoConfig.from_pretrained(model_id)\n+        config.text_config._flash_attn_2_enabled = False\n+\n+        # Load model and extract vision encoder\n+        model = SmolVLMForConditionalGeneration.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.float32,\n+            config=config,\n+        )\n+\n+        exportable_module = TorchExportableModuleForVLM(model)\n+        exported_program = exportable_module.export_vision_encoder()\n+        self.assertIsInstance(exported_program, torch.export.ExportedProgram)\n+\n+    @slow\n+    def test_export_smolvlm_connector(self):\n+        from transformers import AutoConfig\n+        from transformers.integrations.executorch import TorchExportableModuleForVLM\n+\n+        model_id = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n+\n+        # NOTE: The attention_mask is prepared internally in the vision encoder, depending on whether flash attention is used or not\n+        # For ExecuTorch, flash attention is not supported, so the way of exporting vison encoder should be compatible with text-decoder\n+        config = AutoConfig.from_pretrained(model_id)\n+        config.text_config._flash_attn_2_enabled = False\n+\n+        # Load the model and extract the connector (multi-modal projector)\n+        model = SmolVLMForConditionalGeneration.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.float32,\n+            config=config,\n+        )\n+\n+        connector = model.model.connector\n+        connector.eval()\n+\n+        exportable_module = TorchExportableModuleForVLM(model)\n+        exported_program = exportable_module.export_connector()\n+        self.assertIsInstance(exported_program, torch.export.ExportedProgram)\n+\n+    @slow\n+    def test_export_smolvlm_text_decoder(self):\n+        from transformers import AutoConfig\n+        from transformers.integrations.executorch import TorchExportableModuleForVLM\n+\n+        model_id = \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\"\n+\n+        # NOTE: The attention_mask is prepared internally in the vision encoder, depending on whether flash attention is used or not\n+        # For ExecuTorch, flash attention is not supported, so the way of exporting vison encoder should be compatible with text-decoder\n+        config = AutoConfig.from_pretrained(model_id)\n+        config.text_config._flash_attn_2_enabled = False\n+        config.text_config.use_cache = True\n+        config.text_config.attn_implementation = \"sdpa\"\n+\n+        # Load the model and extract the text decoder\n+        model = SmolVLMForConditionalGeneration.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.float32,\n+            config=config,\n+        )\n+\n+        text_decoder = model.model.text_model\n+        text_decoder.eval()\n+\n+        exportable_module = TorchExportableModuleForVLM(model)\n+        exported_program = exportable_module.export_text_decoder()\n+        self.assertIsInstance(exported_program, torch.export.ExportedProgram)"
        }
    ],
    "stats": {
        "total": 300,
        "additions": 282,
        "deletions": 18
    }
}