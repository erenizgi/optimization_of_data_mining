{
    "author": "ritsumei-aoi",
    "message": "Remove Japanese sequence_classification doc and update references (#38246)",
    "sha": "5c13cc0f942510761158518b0564dbd0f2d50409",
    "files": [
        {
            "sha": "a8a01dbd9cd4e4de1a7e868934c1cb5c4ff8d9ab",
            "filename": "docs/source/ja/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2F_toctree.yml?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -29,8 +29,6 @@\n - sections:\n   - isExpanded: false\n     sections:\n-    - local: tasks/sequence_classification\n-      title: ãƒ†ã‚­ã‚¹ãƒˆã®åˆ†é¡\n     - local: tasks/token_classification\n       title: ãƒˆãƒ¼ã‚¯ãƒ³ã®åˆ†é¡\n     - local: tasks/question_answering"
        },
        {
            "sha": "a77e17a64008dd39b8e0bac0dffcb01f1a7e32df",
            "filename": "docs/source/ja/model_doc/albert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Falbert.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -47,7 +47,7 @@ ALBERTãƒ¢ãƒ‡ãƒ«ã¯ã€ã€Œ[ALBERT: A Lite BERT for Self-supervised Learning of Lan\n \n ## å‚è€ƒè³‡æ–™\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒ«ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/masked_language_modeling)"
        },
        {
            "sha": "5c25d6a0c7ac1727df2723d6b6ef359bcbc72ecd",
            "filename": "docs/source/ja/model_doc/bart.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbart.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -129,7 +129,7 @@ BART ã‚’å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n - [ç¿»è¨³ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/translation)\n \n ä»¥ä¸‹ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)\n - [æŠ½å‡ºã•ã‚ŒãŸãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ](https://huggingface.co/models?search=distilbart) ã¯ã€ã“ã® [è«–æ–‡](https://arxiv.org/abs/2010.13002) ã§èª¬æ˜ã•ã‚Œã¦ã„ã¾ã™ã€‚"
        },
        {
            "sha": "6e6947bd04114d3f427c7a6673e5b1fa7a916800",
            "filename": "docs/source/ja/model_doc/bert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbert.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -76,7 +76,7 @@ BERT ã‚’å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£\n - [`BertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ã€‚\n - [`TFBertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ã€‚\n - [`FlaxBertForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb)ã€‚\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n \n <PipelineTag pipeline=\"token-classification\"/>\n "
        },
        {
            "sha": "960d19146c228160e1df77b8c442b515390d96fc",
            "filename": "docs/source/ja/model_doc/big_bird.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbig_bird.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -58,7 +58,7 @@ BigBird ã¯ã€è³ªå•å¿œç­”ã‚„è¦ç´„ãªã©ã®ã•ã¾ã–ã¾ãª NLP ã‚¿ã‚¹ã‚¯ã®ãƒ‘\n \n ## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ãƒªã‚½ãƒ¼ã‚¹\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)"
        },
        {
            "sha": "5314aed1bcd6a99a13d9a6a9a1600aa835ab611f",
            "filename": "docs/source/ja/model_doc/bigbird_pegasus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbigbird_pegasus.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -58,7 +58,7 @@ BigBird ã¯ã€è³ªå•å¿œç­”ã‚„è¦ç´„ãªã©ã®ã•ã¾ã–ã¾ãª NLP ã‚¿ã‚¹ã‚¯ã®ãƒ‘\n \n ## ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ãƒªã‚½ãƒ¼ã‚¹\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)\n - [ç¿»è¨³ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/translation)"
        },
        {
            "sha": "f7cb66ab953288b2b840b1986d57a4ad84919bc1",
            "filename": "docs/source/ja/model_doc/bloom.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbloom.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -39,7 +39,7 @@ BLOOM ã‚’ä½¿ã„å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥ãƒ‹\n \n ä»¥ä¸‹ã‚‚å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n "
        },
        {
            "sha": "4f59700954316a6fa2dde3b3334edd657a209ba2",
            "filename": "docs/source/ja/model_doc/camembert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcamembert.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -46,7 +46,7 @@ Bi-direction Encoders for Transformers (BERT) ã®ãƒ•ãƒ©ãƒ³ã‚¹èªç‰ˆã§ã‚ã‚‹ Cam\n \n ## Resources\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)"
        },
        {
            "sha": "b45f1e4f7e2041ef8244717eaed0ffead12d85c6",
            "filename": "docs/source/ja/model_doc/canine.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fcanine.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fcanine.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fcanine.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -98,7 +98,7 @@ CANINE ã¯ç”Ÿã®æ–‡å­—ã§å‹•ä½œã™ã‚‹ãŸã‚ã€**ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ãªã—**ã§\n \n ## Resources\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å¤šè‚¢é¸æŠã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/multiple_choice)"
        },
        {
            "sha": "7d790e4069dab8f972e29a116b0e0b746615f4a7",
            "filename": "docs/source/ja/model_doc/convbert.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fconvbert.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -53,7 +53,7 @@ ConvBERT ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒ’ãƒ³ãƒˆã¯ BERT ã®ãƒ’ãƒ³ãƒˆã¨ä¼¼ã¦ã„ã¾ã™\n \n ## Resources\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [ãƒã‚¹ã‚¯ã•ã‚ŒãŸè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/masked_lang_modeling)"
        },
        {
            "sha": "e20e49d91868c16976c335b1b1efe89ab2e73e6d",
            "filename": "docs/source/ja/model_doc/ctrl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fctrl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fctrl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fctrl.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -61,7 +61,7 @@ CTRL ãƒ¢ãƒ‡ãƒ«ã¯ã€Nitish Shirish Keskar*ã€Bryan McCann*ã€Lav R. Varshneyã€C\n \n ## Resources\n \n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)\n \n ## CTRLConfig"
        },
        {
            "sha": "c16d913881a22af2ea27892e6f7cfaa45b2e10fd",
            "filename": "docs/source/ja/model_doc/data2vec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdata2vec.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -58,7 +58,7 @@ Data2Vec ã®ä½¿ç”¨ã‚’é–‹å§‹ã™ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³\n - ã‚«ã‚¹ã‚¿ãƒ  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ [`TFData2VecVisionForImageClassification`] ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€[ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/sayakpaul/TF-2.0-Hacks/blob/master/data2vec_vision_image_classification.ipynb) ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚ ï¼‰ã€‚\n \n **Data2VecText ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ ãƒªã‚½ãƒ¼ã‚¹**\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [å› æœè¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/language_modeling)"
        },
        {
            "sha": "bdbaf3c21bb163929f67131beee43134ed56d70c",
            "filename": "docs/source/ja/model_doc/deberta-v2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta-v2.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -61,7 +61,7 @@ v2 ã®æ–°æ©Ÿèƒ½:\n [kamalkraj](https://huggingface.co/kamalkraj) ã«ã‚ˆã‚‹æŠ•ç¨¿ã€‚å…ƒã®ã‚³ãƒ¼ãƒ‰ã¯ [ã“ã¡ã‚‰](https://github.com/microsoft/DeBERTa) ã«ã‚ã‚Šã¾ã™ã€‚\n \n ## Resources\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n - [ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/token_classification)\n - [è³ªå•å›ç­”ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/question_answering)\n - [ãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚° ã‚¿ã‚¹ã‚¯ ã‚¬ã‚¤ãƒ‰](../tasks/masked_language_modeling)"
        },
        {
            "sha": "ef55a6458120dacc57ee3b7f2225369d86a11512",
            "filename": "docs/source/ja/model_doc/deberta.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeberta.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -52,7 +52,7 @@ DeBERTa ã‚’ä½¿ã„å§‹ã‚ã‚‹ã®ã«å½¹ç«‹ã¤å…¬å¼ Hugging Face ãŠã‚ˆã³ã‚³ãƒŸãƒ¥\n - DeBERTa ã«ã‚ˆã‚‹ [æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒãƒ£ãƒ¼ã‚¸ã•ã‚ŒãŸé¡§å®¢ã‚µãƒ¼ãƒ“ã‚¹](https://huggingface.co/blog/supercharge-customer-service-with-machine-learning) ã«é–¢ã™ã‚‹ãƒ–ãƒ­ã‚°æŠ•ç¨¿ã€‚\n - [`DebertaForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ã€‚\n - [`TFDebertaForSequenceClassification`] ã¯ã€ã“ã® [ã‚µãƒ³ãƒ—ãƒ« ã‚¹ã‚¯ãƒªãƒ—ãƒˆ](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification) ãŠã‚ˆã³ [ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ã€‚\n-- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰](../tasks/sequence_classification)\n+- [ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../../en/tasks/sequence_classification)\n \n <PipelineTag pipeline=\"token-classification\" />\n "
        },
        {
            "sha": "ba2e39282b00f1d32a09c5b768141787ab1faa60",
            "filename": "docs/source/ja/tasks/sequence_classification.md",
            "status": "removed",
            "additions": 0,
            "deletions": 604,
            "changes": 604,
            "blob_url": "https://github.com/huggingface/transformers/blob/71009e4b6849069ecabc80f04fd485b26e111145/docs%2Fsource%2Fja%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/71009e4b6849069ecabc80f04fd485b26e111145/docs%2Fsource%2Fja%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fsequence_classification.md?ref=71009e4b6849069ecabc80f04fd485b26e111145",
            "patch": "@@ -1,604 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Sequence classification\n-\n-[[open-in-colab]]\n-\n-<Youtube id=\"dKE8SIt9C-w\"/>\n-\n-ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã§ã¯ã€ç”»åƒã®å€‹ã€…ã®ãƒ”ã‚¯ã‚»ãƒ«ã«ãƒ©ãƒ™ãƒ«ã¾ãŸã¯ã‚¯ãƒ©ã‚¹ã‚’å‰²ã‚Šå½“ã¦ã¾ã™ã€‚ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã„ãã¤ã‹ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ãŒã€ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®å ´åˆã€åŒã˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ä¸€æ„ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹é–“ã®åŒºåˆ¥ã¯è¡Œã‚ã‚Œã¾ã›ã‚“ã€‚ä¸¡æ–¹ã®ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«åŒã˜ãƒ©ãƒ™ãƒ«ãŒä»˜ã‘ã‚‰ã‚Œã¾ã™ (ãŸã¨ãˆã°ã€ã€Œcar-1ã€ã¨ã€Œcar-2ã€ã®ä»£ã‚ã‚Šã«ã€Œcarã€)ã€‚ã‚»ãƒãƒ³ãƒ†ã‚£ãƒƒã‚¯ ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã®ä¸€èˆ¬çš„ãªç¾å®Ÿä¸–ç•Œã®ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ã€æ­©è¡Œè€…ã‚„é‡è¦ãªäº¤é€šæƒ…å ±ã‚’è­˜åˆ¥ã™ã‚‹ãŸã‚ã®è‡ªå‹•é‹è»¢è»Šã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã€åŒ»ç™‚ç”»åƒå†…ã®ç´°èƒã¨ç•°å¸¸ã®è­˜åˆ¥ã€è¡›æ˜Ÿç”»åƒã‹ã‚‰ã®ç’°å¢ƒå¤‰åŒ–ã®ç›£è¦–ãªã©ãŒå«ã¾ã‚Œã¾ã™ã€‚\n-\n-ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n-\n-1. [SceneParse150](https://huggingface.co/datasets/scene_parse_150) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-<Tip>\n-\n-ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/text-classification) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-</Tip>\n-\n-å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-```bash\n-pip install -q datasets transformers evaluate\n-```\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚\n-\n-```py\n->>> from huggingface_hub import notebook_login\n-\n->>> notebook_login()\n-```\n-\n-## Load SceneParse150 dataset\n-\n-\n-ã¾ãšã€SceneParse150 ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’ ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n-\n-```py\n->>> from datasets import load_dataset\n-\n->>> ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n-```\n-\n-[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚\n-\n-```py\n->>> ds = ds.train_test_split(test_size=0.2)\n->>> train_ds = ds[\"train\"]\n->>> test_ds = ds[\"test\"]\n-```\n-\n-æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n-\n-```py\n->>> train_ds[0]\n-{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=512x683 at 0x7F9B0C201F90>,\n- 'annotation': <PIL.PngImagePlugin.PngImageFile image mode=L size=512x683 at 0x7F9B0C201DD0>,\n- 'scene_category': 368}\n-```\n-\n-- `image`: ã‚·ãƒ¼ãƒ³ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚\n-- `annotation`: ã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã® PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã€‚ãƒ¢ãƒ‡ãƒ«ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã§ã‚‚ã‚ã‚Šã¾ã™ã€‚\n-- `scene_category`: ã€Œã‚­ãƒƒãƒãƒ³ã€ã‚„ã€Œã‚ªãƒ•ã‚£ã‚¹ã€ãªã©ã®ç”»åƒã‚·ãƒ¼ãƒ³ã‚’èª¬æ˜ã™ã‚‹ã‚«ãƒ†ã‚´ãƒª IDã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€ã€Œimageã€ã¨ã€Œannotationã€ã®ã¿ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚ã©ã¡ã‚‰ã‚‚ PIL ã‚¤ãƒ¡ãƒ¼ã‚¸ã§ã™ã€‚\n-\n-ã¾ãŸã€ãƒ©ãƒ™ãƒ« ID ã‚’ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹ã«ãƒãƒƒãƒ—ã™ã‚‹è¾æ›¸ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ã“ã‚Œã¯ã€å¾Œã§ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã™ã‚‹ã¨ãã«å½¹ç«‹ã¡ã¾ã™ã€‚ãƒãƒ–ã‹ã‚‰ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã€`id2label` ãŠã‚ˆã³ `label2id` ãƒ‡ã‚£ã‚¯ã‚·ãƒ§ãƒŠãƒªã‚’ä½œæˆã—ã¾ã™ã€‚\n-\n-```py\n->>> import json\n->>> from pathlib import Path\n->>> from huggingface_hub import hf_hub_download\n-\n->>> repo_id = \"huggingface/label-files\"\n->>> filename = \"ade20k-id2label.json\"\n->>> id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n->>> id2label = {int(k): v for k, v in id2label.items()}\n->>> label2id = {v: k for k, v in id2label.items()}\n->>> num_labels = len(id2label)\n-```\n-\n-## Preprocess\n-\n-æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€SegFormer ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã—ã¾ã™ã€‚ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚ˆã†ãªä¸€éƒ¨ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ã€ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰ ã‚¯ãƒ©ã‚¹ã¨ã—ã¦ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ãŸã ã—ã€å®Ÿéš›ã«ã¯èƒŒæ™¯ã‚¯ãƒ©ã‚¹ã¯ 150 å€‹ã®ã‚¯ãƒ©ã‚¹ã«å«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€`do_reduce_labels=True`ã‚’è¨­å®šã—ã¦ã™ã¹ã¦ã®ãƒ©ãƒ™ãƒ«ã‹ã‚‰ 1 ã¤ã‚’å¼•ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã‚¼ãƒ­ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã¯ `255` ã«ç½®ãæ›ãˆã‚‰ã‚Œã‚‹ãŸã‚ã€SegFormer ã®æå¤±é–¢æ•°ã«ã‚ˆã£ã¦ç„¡è¦–ã•ã‚Œã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> checkpoint = \"nvidia/mit-b0\"\n->>> image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)\n-```\n-\n-<frameworkcontent>\n-<pt>\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[torchvision](https://pytorch.org) ã® [`ColorJitter`](https://pytorch.org/vision/stable/generated/torchvision.transforms.ColorJitter.html) é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ /vision/stable/index.html) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ç”»åƒãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from torchvision.transforms import ColorJitter\n-\n->>> jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)\n-```\n-\n-æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã‚’æº–å‚™ã™ã‚‹ãŸã‚ã® 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®é–¢æ•°ã¯ã€ç”»åƒã‚’`pixel_values`ã«å¤‰æ›ã—ã€æ³¨é‡ˆã‚’`labels`ã«å¤‰æ›ã—ã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ã‚»ãƒƒãƒˆã®å ´åˆã€ç”»åƒã‚’ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã«æä¾›ã™ã‚‹å‰ã«`jitter`ãŒé©ç”¨ã•ã‚Œã¾ã™ã€‚ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã®å ´åˆã€ãƒ†ã‚¹ãƒˆä¸­ã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µãŒé©ç”¨ã•ã‚Œãªã„ãŸã‚ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã¯`images`ã‚’åˆ‡ã‚Šå–ã£ã¦æ­£è¦åŒ–ã—ã€`labels` ã®ã¿ã‚’åˆ‡ã‚Šå–ã‚Šã¾ã™ã€‚\n-\n-```py\n->>> def train_transforms(example_batch):\n-...     images = [jitter(x) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-\n-\n->>> def val_transforms(example_batch):\n-...     images = [x for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-```\n-\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«`jitter`ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚\n-\n-```py\n->>> train_ds.set_transform(train_transforms)\n->>> test_ds.set_transform(val_transforms)\n-```\n-\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚\n-ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒ\n-å¥½ããªå›³æ›¸é¤¨ã€‚\n-2 ã¤ã®åˆ¥ã€…ã®å¤‰æ›é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-- ç”»åƒæ‹¡å¼µã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿å¤‰æ›\n-- ğŸ¤— Transformers ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ£ãƒãƒ«å„ªå…ˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€ç”»åƒã‚’è»¢ç½®ã™ã‚‹ã ã‘ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å¤‰æ›\n-\n-```py\n->>> import tensorflow as tf\n-\n-\n->>> def aug_transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.image.random_brightness(image, 0.25)\n-...     image = tf.image.random_contrast(image, 0.5, 2.0)\n-...     image = tf.image.random_saturation(image, 0.75, 1.25)\n-...     image = tf.image.random_hue(image, 0.1)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-\n-\n->>> def transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-```\n-\n-æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã®ãƒãƒƒãƒã‚’æº–å‚™ã™ã‚‹ 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ãŒé©ç”¨ã•ã‚Œã¾ã™\n-ç”»åƒå¤‰æ›ã‚’è¡Œã„ã€ä»¥å‰ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ `image_processor` ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ `pixel_values` ã«å¤‰æ›ã—ã€\n-`labels`ã¸ã®æ³¨é‡ˆã€‚ `ImageProcessor` ã¯ã€ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ã¨æ­£è¦åŒ–ã‚‚å‡¦ç†ã—ã¾ã™ã€‚\n-\n-```py\n->>> def train_transforms(example_batch):\n-...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-\n-\n->>> def val_transforms(example_batch):\n-...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-```\n-\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚\n-\n-```py\n->>> train_ds.set_transform(train_transforms)\n->>> test_ds.set_transform(val_transforms)\n-```\n-</tf>\n-</frameworkcontent>\n-\n-## Evaluate\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[Mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (ğŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co) ã‚’å‚ç…§ã—ã¦ãã ã•ã„) /docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚\n-\n-```py\n->>> import evaluate\n-\n->>> metric = evaluate.load(\"mean_iou\")\n-```\n-\n-æ¬¡ã«ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ [`~evaluate.EvaluationModule.compute`] ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚äºˆæ¸¬ã‚’æ¬¡ã®ã‚ˆã†ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™\n-æœ€åˆã«ãƒ­ã‚¸ãƒƒãƒˆã‚’ä½œæˆã—ã€æ¬¡ã« [`~evaluate.EvaluationModule.compute`] ã‚’å‘¼ã³å‡ºã™å‰ã«ãƒ©ãƒ™ãƒ«ã®ã‚µã‚¤ã‚ºã«ä¸€è‡´ã™ã‚‹ã‚ˆã†ã«å†å½¢æˆã—ã¾ã™ã€‚\n-\n-<frameworkcontent>\n-<pt>\n-\n-```py\n->>> import numpy as np\n->>> import torch\n->>> from torch import nn\n-\n->>> def compute_metrics(eval_pred):\n-...     with torch.no_grad():\n-...         logits, labels = eval_pred\n-...         logits_tensor = torch.from_numpy(logits)\n-...         logits_tensor = nn.functional.interpolate(\n-...             logits_tensor,\n-...             size=labels.shape[-2:],\n-...             mode=\"bilinear\",\n-...             align_corners=False,\n-...         ).argmax(dim=1)\n-\n-...         pred_labels = logits_tensor.detach().cpu().numpy()\n-...         metrics = metric.compute(\n-...             predictions=pred_labels,\n-...             references=labels,\n-...             num_labels=num_labels,\n-...             ignore_index=255,\n-...             reduce_labels=False,\n-...         )\n-...         for key, value in metrics.items():\n-...             if type(value) is np.ndarray:\n-...                 metrics[key] = value.tolist()\n-...         return metrics\n-```\n-\n-</pt>\n-</frameworkcontent>\n-\n-\n-<frameworkcontent>\n-<tf>\n-\n-```py\n->>> def compute_metrics(eval_pred):\n-...     logits, labels = eval_pred\n-...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n-...     logits_resized = tf.image.resize(\n-...         logits,\n-...         size=tf.shape(labels)[1:],\n-...         method=\"bilinear\",\n-...     )\n-\n-...     pred_labels = tf.argmax(logits_resized, axis=-1)\n-...     metrics = metric.compute(\n-...         predictions=pred_labels,\n-...         references=labels,\n-...         num_labels=num_labels,\n-...         ignore_index=-1,\n-...         reduce_labels=image_processor.do_reduce_labels,\n-...     )\n-\n-...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n-...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n-\n-...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n-...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n-...     return {\"val_\" + k: v for k, v in metrics.items()}\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n-ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚\n-\n-## Train\n-<frameworkcontent>\n-<pt>\n-<Tip>\n-\n-[`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#finetune-with-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-\n-</Tip>\n-\n-ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForSemanticSegmentation`] ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ©ãƒ™ãƒ« ID ã¨ãƒ©ãƒ™ãƒ« ã‚¯ãƒ©ã‚¹é–“ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n-\n->>> model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)\n-```\n-\n-ã“ã®æ™‚ç‚¹ã§æ®‹ã£ã¦ã„ã‚‹æ‰‹é †ã¯æ¬¡ã® 3 ã¤ã ã‘ã§ã™ã€‚\n-\n-1. [`TrainingArguments`] ã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚ `image` åˆ—ãŒå‰Šé™¤ã•ã‚Œã‚‹ãŸã‚ã€æœªä½¿ç”¨ã®åˆ—ã‚’å‰Šé™¤ã—ãªã„ã“ã¨ãŒé‡è¦ã§ã™ã€‚ `image` åˆ—ãŒãªã„ã¨ã€`pixel_values` ã‚’ä½œæˆã§ãã¾ã›ã‚“ã€‚ã“ã®å‹•ä½œã‚’é˜²ãã«ã¯ã€`remove_unused_columns=False`ã‚’è¨­å®šã—ã¦ãã ã•ã„ã€‚ä»–ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ `output_dir` ã ã‘ã§ã™ã€‚ `push_to_hub=True`ã‚’è¨­å®šã—ã¦ã€ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ (ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã«ã¯ã€Hugging Face ã«ã‚µã‚¤ãƒ³ã‚¤ãƒ³ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™)ã€‚å„ã‚¨ãƒãƒƒã‚¯ã®çµ‚äº†æ™‚ã«ã€[`Trainer`] ã¯ IoU ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’è©•ä¾¡ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã—ã¾ã™ã€‚\n-2. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¼•æ•°ã‚’ã€ãƒ¢ãƒ‡ãƒ«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã€ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã€ãŠã‚ˆã³ `compute_metrics` é–¢æ•°ã¨ã¨ã‚‚ã« [`Trainer`] ã«æ¸¡ã—ã¾ã™ã€‚\n-3. [`~Trainer.train`] ã‚’å‘¼ã³å‡ºã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> training_args = TrainingArguments(\n-...     output_dir=\"segformer-b0-scene-parse-150\",\n-...     learning_rate=6e-5,\n-...     num_train_epochs=50,\n-...     per_device_train_batch_size=2,\n-...     per_device_eval_batch_size=2,\n-...     save_total_limit=3,\n-...     eval_strategy=\"steps\",\n-...     save_strategy=\"steps\",\n-...     save_steps=20,\n-...     eval_steps=20,\n-...     logging_steps=1,\n-...     eval_accumulation_steps=5,\n-...     remove_unused_columns=False,\n-...     push_to_hub=True,\n-... )\n-\n->>> trainer = Trainer(\n-...     model=model,\n-...     args=training_args,\n-...     train_dataset=train_ds,\n-...     eval_dataset=test_ds,\n-...     compute_metrics=compute_metrics,\n-... )\n-\n->>> trainer.train()\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã—ãŸã‚‰ã€ [`~transformers.Trainer.push_to_hub`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«å…±æœ‰ã—ã€èª°ã‚‚ãŒãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ã—ã¾ã™ã€‚\n-\n-```py\n->>> trainer.push_to_hub()\n-```\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã¾ãš [åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./training#train-a-tensorflow-model-with-keras) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚\n-1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n-2. äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚\n-3. ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` ã«å¤‰æ›ã—ã¾ã™ã€‚\n-4. ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n-5. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™\n-6. `fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n-\n-ã¾ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 2\n->>> num_epochs = 50\n->>> num_train_steps = len(train_ds) * num_epochs\n->>> learning_rate = 6e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-æ¬¡ã«ã€ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã¨ã‚‚ã« [`TFAutoModelForSemanticSegmentation`] ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n-ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-[`~datasets.Dataset.to_tf_dataset`] ã¨ [`DefaultDataCollatâ€‹â€‹or`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-\n->>> tf_train_dataset = train_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_eval_dataset = test_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€[Keras callbacks](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-`compute_metrics` é–¢æ•°ã‚’ [`KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-ãã—ã¦ [`PushToHubCallback`] ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(\n-...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n-... )\n-\n->>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", image_processor=image_processor)\n-\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚`fit()`ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€\n-ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯:\n-\n-```py\n->>> model.fit(\n-...     tf_train_dataset,\n-...     validation_data=tf_eval_dataset,\n-...     callbacks=callbacks,\n-...     epochs=num_epochs,\n-... )\n-```\n-\n-ãŠã‚ã§ã¨ã†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ğŸ¤— Hub ã§å…±æœ‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã§æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n-\n-</tf>\n-</frameworkcontent>\n-\n-\n-## Inference\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n-\n-æ¨è«–ã®ãŸã‚ã«ç”»åƒã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n-\n-```py\n->>> image = ds[0][\"image\"]\n->>> image\n-```\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-image.png\" alt=\"Image of bedroom\"/>\n-</div>\n-\n-<frameworkcontent>\n-<pt>\n-\n-æ¨è«–ç”¨ã«å¾®èª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã™æœ€ã‚‚ç°¡å˜ãªæ–¹æ³•ã¯ã€ãã‚Œã‚’ [`pipeline`] ã§ä½¿ç”¨ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ç”¨ã® `pipeline` ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã«ç”»åƒã‚’æ¸¡ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import pipeline\n-\n->>> segmenter = pipeline(\"image-segmentation\", model=\"my_awesome_seg_model\")\n->>> segmenter(image)\n-[{'score': None,\n-  'label': 'wall',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062690>},\n- {'score': None,\n-  'label': 'sky',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A50>},\n- {'score': None,\n-  'label': 'floor',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062B50>},\n- {'score': None,\n-  'label': 'ceiling',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062A10>},\n- {'score': None,\n-  'label': 'bed ',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E90>},\n- {'score': None,\n-  'label': 'windowpane',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062390>},\n- {'score': None,\n-  'label': 'cabinet',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062550>},\n- {'score': None,\n-  'label': 'chair',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062D90>},\n- {'score': None,\n-  'label': 'armchair',\n-  'mask': <PIL.Image.Image image mode=L size=640x427 at 0x7FD5B2062E10>}]\n-```\n-\n-å¿…è¦ã«å¿œã˜ã¦ã€`pipeline` ã®çµæœã‚’æ‰‹å‹•ã§è¤‡è£½ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã§ç”»åƒã‚’å‡¦ç†ã—ã€`pixel_values`ã‚’ GPU ã«é…ç½®ã—ã¾ã™ã€‚\n-\n-```py\n->>> device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # use GPU if available, otherwise use a CPU\n->>> encoding = image_processor(image, return_tensors=\"pt\")\n->>> pixel_values = encoding.pixel_values.to(device)\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ã€Œlogitsã€ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> outputs = model(pixel_values=pixel_values)\n->>> logits = outputs.logits.cpu()\n-```\n-\n-æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> upsampled_logits = nn.functional.interpolate(\n-...     logits,\n-...     size=image.size[::-1],\n-...     mode=\"bilinear\",\n-...     align_corners=False,\n-... )\n-\n->>> pred_seg = upsampled_logits.argmax(dim=1)[0]\n-```\n-\n-</pt>\n-</frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-\n-ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’å‰å‡¦ç†ã—ã€å…¥åŠ›ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n->>> logits = model(**inputs).logits\n-```\n-\n-æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã€ã‚¯ãƒ©ã‚¹æ¬¡å…ƒã« argmax ã‚’é©ç”¨ã—ã¾ã™ã€‚\n-\n-```py\n->>> logits = tf.transpose(logits, [0, 2, 3, 1])\n-\n->>> upsampled_logits = tf.image.resize(\n-...     logits,\n-...     # We reverse the shape of `image` because `image.size` returns width and height.\n-...     image.size[::-1],\n-... )\n-\n->>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n-çµæœã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚«ãƒ©ãƒ¼ ãƒ‘ãƒ¬ãƒƒãƒˆ](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ã‚’ã€ãã‚Œãã‚Œã‚’ãƒãƒƒãƒ—ã™ã‚‹ `ade_palette()` ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚’ RGB å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚æ¬¡ã«ã€ç”»åƒã¨äºˆæ¸¬ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™ã€‚\n-\n-```py\n->>> import matplotlib.pyplot as plt\n->>> import numpy as np\n-\n->>> color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n->>> palette = np.array(ade_palette())\n->>> for label, color in enumerate(palette):\n-...     color_seg[pred_seg == label, :] = color\n->>> color_seg = color_seg[..., ::-1]  # convert to BGR\n-\n->>> img = np.array(image) * 0.5 + color_seg * 0.5  # plot the image with the segmentation map\n->>> img = img.astype(np.uint8)\n-\n->>> plt.figure(figsize=(15, 10))\n->>> plt.imshow(img)\n->>> plt.show()\n-```\n-\n-<div class=\"flex justify-center\">\n-    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/semantic-seg-preds.png\" alt=\"Image of bedroom overlaid with segmentation map\"/>\n-</div>"
        },
        {
            "sha": "bdfb3ec0ac8ba034c64c1e28e96d6758dd1d1956",
            "filename": "docs/source/ja/tasks_explained.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Ftasks_explained.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c13cc0f942510761158518b0564dbd0f2d50409/docs%2Fsource%2Fja%2Ftasks_explained.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks_explained.md?ref=5c13cc0f942510761158518b0564dbd0f2d50409",
            "patch": "@@ -221,7 +221,7 @@ Transformerã¯æœ€åˆã«æ©Ÿæ¢°ç¿»è¨³ã®ãŸã‚ã«è¨­è¨ˆã•ã‚Œã€ãã‚Œä»¥é™ã€ã»\n \n äº‹å‰è¨“ç·´æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã«ä½¿ç”¨ã™ã‚‹ã«ã¯ã€ãƒ™ãƒ¼ã‚¹ã®BERTãƒ¢ãƒ‡ãƒ«ã®ä¸Šã«ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’è¿½åŠ ã—ã¾ã™ã€‚ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¯æœ€çµ‚çš„ãªéš ã‚ŒãŸçŠ¶æ…‹ã‚’å—ã‘å…¥ã‚Œã€ãã‚Œã‚‰ã‚’ãƒ­ã‚¸ãƒƒãƒˆã«å¤‰æ›ã™ã‚‹ãŸã‚ã®ç·šå½¢å±¤ã§ã™ã€‚ã‚¯ãƒ­ã‚¹ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼æå¤±ã¯ã€ãƒ­ã‚¸ãƒƒãƒˆã¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé–“ã§æœ€ã‚‚å¯èƒ½æ€§ã®é«˜ã„ãƒ©ãƒ™ãƒ«ã‚’è¦‹ã¤ã‘ã‚‹ãŸã‚ã«è¨ˆç®—ã•ã‚Œã¾ã™ã€‚\n \n-ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚’è©¦ã—ã¦ã¿ã‚‹æº–å‚™ã¯ã§ãã¾ã—ãŸã‹ï¼ŸDistilBERTã‚’å¾®èª¿æ•´ã—ã€æ¨è«–ã«ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ãŸã‚ã«ã€å®Œå…¨ãª[ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¬ã‚¤ãƒ‰](tasks/sequence_classification)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ï¼\n+ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚’è©¦ã—ã¦ã¿ã‚‹æº–å‚™ã¯ã§ãã¾ã—ãŸã‹ï¼ŸDistilBERTã‚’å¾®èª¿æ•´ã—ã€æ¨è«–ã«ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’å­¦ã¶ãŸã‚ã«ã€å®Œå…¨ãª[ãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¬ã‚¤ãƒ‰(è‹±èªç‰ˆ)](../en/tasks/sequence_classification)ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ã¿ã¦ãã ã•ã„ï¼\n \n ### Token classification\n "
        }
    ],
    "stats": {
        "total": 634,
        "additions": 14,
        "deletions": 620
    }
}