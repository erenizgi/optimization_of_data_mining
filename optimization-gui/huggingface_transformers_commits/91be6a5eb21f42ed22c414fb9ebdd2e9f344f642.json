{
    "author": "Cyrilvallez",
    "message": "Modular: support for importing functions from any file (#35692)\n\n* fix function imports\r\n\r\n* improve comment\r\n\r\n* Update modeling_switch_function.py\r\n\r\n* make checks more robust\r\n\r\n* improvement\r\n\r\n* rename\r\n\r\n* final test update",
    "sha": "91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
    "files": [
        {
            "sha": "acf140f025d93422bef5e717c2d8c6565641ce2c",
            "filename": "examples/modular-transformers/modeling_add_function.py",
            "status": "added",
            "additions": 66,
            "deletions": 0,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_add_function.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -0,0 +1,66 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_add_function.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_add_function.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Note that zamba does not have the `apply_rotary_pos_emb` function!\n+from typing import Optional, Tuple\n+\n+import torch\n+from torch import nn\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+class TestAttention(nn.Module):\n+    \"\"\"\n+    Multi-headed attention from 'Attention Is All You Need' paper. Modified to use sliding window attention: Longformer\n+    and \"Generating Long Sequences with Sparse Transformers\".\n+\n+    Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:\n+    The input dimension here is attention_hidden_size = 2 * hidden_size, and head_dim = attention_hidden_size // num_heads.\n+    The extra factor of 2 comes from the input being the concatenation of original_hidden_states with the output of the previous (mamba) layer\n+    (see fig. 2 in https://arxiv.org/pdf/2405.16712).\n+    Additionally, replaced\n+    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim) with\n+    attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim/2)\n+    \"\"\"\n+\n+    def __init__(self):\n+        pass\n+\n+    def forward(self) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "0c61848924a4ccf198998766839fefb257abda48",
            "filename": "examples/modular-transformers/modeling_dummy.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_dummy.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_dummy.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -45,13 +45,8 @@ def extra_repr(self):\n \n \n class DummyRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: DummyConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: DummyConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -63,7 +58,7 @@ def __init__(\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -75,13 +70,14 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n \n         if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n             self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n             self.max_seq_len_cached = self.original_max_seq_len\n \n@@ -356,6 +352,7 @@ class DummyPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "45b10a5b206ad3a4819a4506e43f0a4d85241403",
            "filename": "examples/modular-transformers/modeling_multimodal1.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_multimodal1.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -45,13 +45,8 @@ def extra_repr(self):\n \n \n class Multimodal1TextRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Multimodal1TextConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: Multimodal1TextConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -63,7 +58,7 @@ def __init__(\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -75,13 +70,14 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n \n         if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n             self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n             self.max_seq_len_cached = self.original_max_seq_len\n \n@@ -356,6 +352,7 @@ class Multimodal1TextPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "ae71d724c25ac8912496a390847f25bf44acc338",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -61,13 +61,8 @@ def forward(self, x):\n \n \n class MyNewModel2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: MyNewModel2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: MyNewModel2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -79,7 +74,7 @@ def __init__(\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -91,13 +86,14 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n \n         if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n             self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n             self.max_seq_len_cached = self.original_max_seq_len\n \n@@ -356,6 +352,7 @@ class MyNewModel2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "f07ac7f3348bd43601e74b33cacd1a011ae586d5",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -107,7 +107,6 @@ class NewTaskModelPreTrainedModel(PreTrainedModel):\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True\n-    _supports_cache_class = True\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n "
        },
        {
            "sha": "e44c4bde1987aaa97e12e6340ff3aa8e79db5914",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 7,
            "deletions": 10,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -45,13 +45,8 @@ def extra_repr(self):\n \n \n class SuperRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: SuperConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: SuperConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -63,7 +58,7 @@ def __init__(\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -75,13 +70,14 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n \n         if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            # This .to() is needed if the model has been moved to a device after being initialized (because\n+            # the buffer is automatically moved, but not the original copy)\n+            self.original_inv_freq = self.original_inv_freq.to(device)\n             self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n             self.max_seq_len_cached = self.original_max_seq_len\n \n@@ -356,6 +352,7 @@ class SuperPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_flex_attn = True\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "3b89284537ae54b28bed679b54616862fe6c3330",
            "filename": "examples/modular-transformers/modeling_switch_function.py",
            "status": "added",
            "additions": 170,
            "deletions": 0,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodeling_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_switch_function.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -0,0 +1,170 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from examples/modular-transformers/modular_switch_function.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_switch_function.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# Note that llama and cohere have different definitions for rotate_half\n+from typing import Callable, Optional, Tuple\n+\n+import torch\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from .configuration_switch_function import SwitchFunctionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+def rotate_half(x):\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n+    x1 = x[..., ::2]\n+    x2 = x[..., 1::2]\n+    rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n+    return rot_x\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class SwitchFunctionAttention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: SwitchFunctionConfig, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights"
        },
        {
            "sha": "6a2426a67236bf4c4bb8db553382ace5f5df9407",
            "filename": "examples/modular-transformers/modular_add_function.py",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodular_add_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodular_add_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_add_function.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -0,0 +1,15 @@\n+# Note that zamba does not have the `apply_rotary_pos_emb` function!\n+from transformers.models.llama.modeling_llama import apply_rotary_pos_emb\n+from transformers.models.zamba.modeling_zamba import ZambaAttention\n+\n+\n+# When following ZambaAttention dependencies, the function `apply_rotary_pos_emb` is not present\n+# by default as it is absent from the class definition (and the file altogether).\n+# Note that this syntax should be able to add both `apply_rotary_pos_emb` as imported directly, but\n+# `rotate_half` as well as a dependency from the imported function!!\n+class TestAttention(ZambaAttention):\n+    def __init__(self):\n+        pass\n+\n+    def forward(self):\n+        _ = apply_rotary_pos_emb(1, 1, 1, 1)"
        },
        {
            "sha": "3c0c716a43979b3a18d7d48ec0b09a960e40ca0a",
            "filename": "examples/modular-transformers/modular_switch_function.py",
            "status": "added",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodular_switch_function.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/examples%2Fmodular-transformers%2Fmodular_switch_function.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodular_switch_function.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -0,0 +1,10 @@\n+# Note that llama and cohere have different definitions for rotate_half\n+from transformers.models.cohere.modeling_cohere import rotate_half  # noqa\n+from transformers.models.llama.modeling_llama import LlamaAttention\n+\n+\n+# When following LlamaAttention dependencies, we will grab the function `rotate_half` defined\n+# in `modeling_llama.py`. But here we imported it explicitly from Cohere, so it should use Cohere's\n+# definition instead\n+class SwitchFunctionAttention(LlamaAttention):\n+    pass"
        },
        {
            "sha": "8126d130ae728ed77f6c5133cc626ebea055def1",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 16,
            "deletions": 2,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/91be6a5eb21f42ed22c414fb9ebdd2e9f344f642/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=91be6a5eb21f42ed22c414fb9ebdd2e9f344f642",
            "patch": "@@ -776,7 +776,7 @@ def compute_relative_order(self, missing_dependencies: set[str]) -> dict[str, in\n                 else:\n                     merged_dependencies.append(class_dep)\n             # Sort both list according to the order in their respective file\n-            original_dependencies = sorted(original_dependencies, key=lambda x: self.start_lines[x])\n+            original_dependencies = sorted(original_dependencies, key=lambda x: self.start_lines.get(x, 1e10))\n             merged_dependencies = sorted(merged_dependencies, key=lambda x: self.modular_file_start_lines[x])\n \n             # Add all original node first, then merged ones\n@@ -801,7 +801,7 @@ def compute_relative_order(self, missing_dependencies: set[str]) -> dict[str, in\n             else:\n                 original_dependencies.append(dep)\n         # Sort both list according to the order in their respective file\n-        original_dependencies = sorted(original_dependencies, key=lambda x: self.start_lines[x])\n+        original_dependencies = sorted(original_dependencies, key=lambda x: self.start_lines.get(x, 1e10))\n         merged_dependencies = sorted(merged_dependencies, key=lambda x: self.modular_file_start_lines[x])\n \n         # Add all original node first, then merged ones\n@@ -1321,6 +1321,20 @@ def merge_model_specific_imports(self, visited_modules):\n                             self.added_objects_file_mapping[dep] = file\n                             self.functions[dep] = visited_module.global_nodes[dep]\n \n+                # Add/overwrite the imported functions to other visited modules as well, in case it is absent/different\n+                # in he modeling source file of the inherited class. See `examples/modular-tranformers/modular_switch_function.py`\n+                # and `examples/modular-tranformers/modular_add_function.py` for examples\n+                recursive_dependencies = visited_module.object_recursive_dependency_mapping.get(object_name, set())\n+                node_recursive_dependencies_mapping = {\n+                    dep: visited_module.global_nodes[dep] for dep in recursive_dependencies\n+                }\n+                for filename, module_mapper in self.visited_modules.items():\n+                    if filename != file:\n+                        module_mapper.global_nodes[object_name] = visited_module.functions[object_name]\n+                        if len(recursive_dependencies) > 0:\n+                            module_mapper.object_recursive_dependency_mapping[object_name] = recursive_dependencies\n+                            module_mapper.global_nodes.update(node_recursive_dependencies_mapping)\n+\n             # Add assignments and their dependencies\n             elif object_name in visited_module.assignments and object_name not in self.assignments:\n                 self.assignments[object_name] = visited_module.assignments[object_name]"
        }
    ],
    "stats": {
        "total": 348,
        "additions": 305,
        "deletions": 43
    }
}