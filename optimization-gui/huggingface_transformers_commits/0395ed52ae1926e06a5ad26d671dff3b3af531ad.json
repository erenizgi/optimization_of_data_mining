{
    "author": "ArthurZucker",
    "message": "[`CB`] Refactors the way we access paged (#41370)\n\n* up\n\n* refactor the way we handle paged attention\n\n* affect serve as well\n\n* update\n\n* fix\n\n* cup",
    "sha": "0395ed52ae1926e06a5ad26d671dff3b3af531ad",
    "files": [
        {
            "sha": "39fad6cb7a4ea122772c8a4f45f604f7dd87fbbf",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -184,9 +184,7 @@ def batch_generate(\n     parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n     parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n \n-    parser.add_argument(\n-        \"--attn\", type=str, default=\"paged_attention|kernels-community/flash-attn\", help=\"Attention implementation\"\n-    )\n+    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n     parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n     parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")"
        },
        {
            "sha": "8048042eb4858952c4531431842b90fe63ba8aa6",
            "filename": "examples/pytorch/continuous_batching_simple.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/examples%2Fpytorch%2Fcontinuous_batching_simple.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching_simple.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -31,9 +31,7 @@\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\"--num-blocks\", \"-n\", type=int, default=None)\n     parser.add_argument(\"--max-batch-tokens\", \"-b\", type=int, default=None)\n-    parser.add_argument(\n-        \"--attn\", type=str, default=\"paged_attention|kernels-community/flash-attn\", help=\"Attention implementation\"\n-    )\n+    parser.add_argument(\"--attn\", type=str, default=\"kernels-community/flash-attn\", help=\"Attention implementation\")\n     parser.add_argument(\"--samples\", type=int, default=500)\n     args = parser.parse_args()\n "
        },
        {
            "sha": "d46b8b55694a29a485fd9146e8327f8ca752e236",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -489,19 +489,6 @@ def __init__(self, args: ServeArguments):\n         # Store and process input arguments\n         self.args = args\n         self.use_continuous_batching = self.args.continuous_batching\n-        if self.use_continuous_batching:\n-            default_attn_impl = ContinuousBatchingManager.default_attention_implementation()\n-            # checking if attn_implementation is supported by continuous batching\n-            if self.args.attn_implementation is None:\n-                self.args.attn_implementation = default_attn_impl  # default to sdpa_paged\n-                logger.info(f\"No attn_implementation passed, defaulting to {default_attn_impl}\")\n-            supported_attn_impl = ContinuousBatchingManager.supported_attention_implementations()\n-            if self.args.attn_implementation not in supported_attn_impl:\n-                raise ValueError(\n-                    f\"Continuous batching only supports {supported_attn_impl} as attn_implementation, got \"\n-                    f\"{self.args.attn_implementation}\"\n-                    f\"Try setting `--attn_implementation={default_attn_impl}`\"\n-                )\n         self.enable_cors = self.args.enable_cors\n \n         if self.args.default_seed is not None:"
        },
        {
            "sha": "d8cafd6a0c3e3a92439c0a62249b9151ff70ee50",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -172,7 +172,7 @@ def __init__(\n         # Infer number of blocks and max batch tokens\n         page_size = self.head_dim * self.num_key_value_heads\n \n-        if getattr(config, \"attn_implementation\", None) == \"paged_attention\":\n+        if \"flash\" in self.config._attn_implementation:\n             num_attention_masks = 0\n         else:\n             # TODO: when we generalize to allow for block-attn, we can use `num_attention_masks=sum(set(group_types))`"
        },
        {
            "sha": "e9adc98fc6aff9bfc7535070b926b7dd526aa7e3",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n+from ...integrations.hub_kernels import load_and_register_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n@@ -241,7 +242,10 @@ def setup_static_tensors(self, num_groups: int) -> None:\n         self.reset_static_tensors(full_reset=True)\n \n     def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n+        return self.config._attn_implementation in [\n+            \"paged|eager\",\n+            \"paged|sdpa\",\n+        ]  # we set `is_causal` to True in paged call\n \n     @traced\n     @torch.no_grad()\n@@ -604,6 +608,10 @@ def __init__(\n             streaming: Whether to stream tokens as they are generated\n         \"\"\"\n         self.model = model.eval()\n+        if \"paged|\" not in model.config._attn_implementation:\n+            attn_implementation = \"paged|\" + self.model.config._attn_implementation\n+            load_and_register_kernel(attn_implementation)\n+            model.set_attn_implementation(attn_implementation)\n         generation_config = model.generation_config if generation_config is None else generation_config\n         self.generation_config = generation_config\n         self.input_queue = queue.Queue(maxsize=max_queue_size)\n@@ -758,14 +766,6 @@ def request_id_iter(self, request_id):\n             if self.batch_processor is not None:\n                 request_cancelled = self.batch_processor.scheduler.request_is_cancelled(request_id)\n \n-    @staticmethod\n-    def supported_attention_implementations() -> set[str]:\n-        return {\"eager_paged\", \"sdpa_paged\", \"flash_attention_2\"}\n-\n-    @staticmethod\n-    def default_attention_implementation() -> str:\n-        return \"sdpa_paged\"\n-\n     @traced\n     def warmup(self, batch_processor):\n         stream = torch.cuda.Stream(device=self.model.device)"
        },
        {
            "sha": "ca50984bde15ce7ac57cfbb7b0dca282ac371039",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -2509,7 +2509,7 @@ def _check_and_adjust_attn_implementation(\n         # If FA not installed, do not fail but use kernels instead\n         if (\n             attn_implementation is not None\n-            and attn_implementation.startswith(\"flash_attention\")\n+            and \"flash\" in attn_implementation\n             and self._supports_flash_attn\n             and not (is_flash_attn_2_available() or is_flash_attn_3_available())\n             and is_kernels_available()\n@@ -2617,8 +2617,6 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n             else attn_implementation.get(\"\", self.config._attn_implementation)\n         )\n \n-        # At this point, the model was already instantiated, so instead of crashing on bad value, let's simply\n-        # warn the user that the requested value is not working\n         if requested_implementation != self.config._attn_implementation:\n             # In this case, raise\n             if not self._can_set_attn_implementation():\n@@ -5834,10 +5832,10 @@ class AttentionInterface(GeneralInterface):\n         \"flash_attention_3\": flash_attention_forward,\n         \"flash_attention_2\": flash_attention_forward,\n         \"flex_attention\": flex_attention_forward,\n-        \"paged_attention\": paged_attention_forward,\n         \"sdpa\": sdpa_attention_forward,\n-        \"sdpa_paged\": sdpa_attention_paged_forward,\n-        \"eager_paged\": eager_paged_attention_forward,\n+        \"paged|flash_attention_2\": paged_attention_forward,\n+        \"paged|sdpa\": sdpa_attention_paged_forward,\n+        \"paged|eager\": eager_paged_attention_forward,\n     }\n \n "
        },
        {
            "sha": "b5476aa5f39871a4f2598a46854e84519070e8c9",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -328,6 +328,15 @@ def test_continuous_batching_parity_gpt_oss_flash(self) -> None:\n             \"openai/gpt-oss-20b\", \"paged_attention|kernels-community/flash-attn\", expected_outputs\n         )\n \n+    def test_attn_implementation(self) -> None:\n+        model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n+        manager = model.init_continuous_batching()\n+        assert \"paged|sdpa\" == manager.model.config._attn_implementation\n+\n+        model = AutoModelForCausalLM.from_pretrained(\"gpt2\", _attn_implementation=\"eager\")\n+        manager = model.init_continuous_batching()\n+        assert \"paged|eager\" == manager.model.config._attn_implementation\n+\n \n # FIXME: the gemma test seem broken, there is a message about cuda graphs and the sdpa and flash expecteations are\n # inverted on CUDA. On AMD they do fine."
        },
        {
            "sha": "0cb13eb1dc2343d00702e048cb8f94509c5c170a",
            "filename": "tests/generation/test_paged_attention.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0395ed52ae1926e06a5ad26d671dff3b3af531ad/tests%2Fgeneration%2Ftest_paged_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0395ed52ae1926e06a5ad26d671dff3b3af531ad/tests%2Fgeneration%2Ftest_paged_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_paged_attention.py?ref=0395ed52ae1926e06a5ad26d671dff3b3af531ad",
            "patch": "@@ -44,10 +44,10 @@ def setUpClass(cls):\n \n     @parameterized.expand(\n         [\n-            (\"eager_paged\", 64, 128, 64),\n-            (\"sdpa_paged\", 32, 256, 128),\n-            (\"paged_attention\", 16, 512, 256),\n-            (\"flex_paged\", 64, 128, 64),\n+            (\"paged|eager\", 64, 128, 64),\n+            (\"paged|sdpa\", 32, 256, 128),\n+            (\"paged|flash_attention_2\", 16, 512, 256),\n+            (\"paged|flex_attention\", 64, 128, 64),\n         ]\n     )\n     def test_generate_batch_consistency(self, attn_impl, num_blocks, block_size, max_batch_tokens):\n@@ -89,10 +89,10 @@ def test_generate_batch_consistency(self, attn_impl, num_blocks, block_size, max\n \n     @parameterized.expand(\n         [\n-            (\"eager_paged\", 64, 128, 64),\n-            (\"sdpa_paged\", 32, 256, 128),\n-            (\"paged_attention\", 16, 512, 256),\n-            (\"flex_paged\", 64, 128, 64),\n+            (\"paged|eager\", 64, 128, 64),\n+            (\"paged|sdpa\", 32, 256, 128),\n+            (\"paged|flash_attention_2\", 16, 512, 256),\n+            (\"paged|flex_attention\", 64, 128, 64),\n         ]\n     )\n     def test_generate_batch_with_sampling(self, attn_impl, num_blocks, block_size, max_batch_tokens):"
        }
    ],
    "stats": {
        "total": 76,
        "additions": 33,
        "deletions": 43
    }
}