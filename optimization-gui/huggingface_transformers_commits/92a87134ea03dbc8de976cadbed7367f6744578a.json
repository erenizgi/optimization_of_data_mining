{
    "author": "ydshieh",
    "message": "update `ColQwen2ModelIntegrationTest` (#38583)\n\n* update\n\n* update\n\n* update\n\n* update\n\n* 4 bit\n\n* 8 bit\n\n* final\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "92a87134ea03dbc8de976cadbed7367f6744578a",
    "files": [
        {
            "sha": "3ed42afd99a6438b3acc7d583e84883f9d5851da",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 9,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/92a87134ea03dbc8de976cadbed7367f6744578a/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/92a87134ea03dbc8de976cadbed7367f6744578a/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=92a87134ea03dbc8de976cadbed7367f6744578a",
            "patch": "@@ -26,7 +26,15 @@\n from transformers.models.colqwen2.configuration_colqwen2 import ColQwen2Config\n from transformers.models.colqwen2.modeling_colqwen2 import ColQwen2ForRetrieval, ColQwen2ForRetrievalOutput\n from transformers.models.colqwen2.processing_colqwen2 import ColQwen2Processor\n-from transformers.testing_utils import cleanup, require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    require_bitsandbytes,\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n \n \n if is_torch_available():\n@@ -283,6 +291,7 @@ def setUp(self):\n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n+    @require_bitsandbytes\n     @slow\n     def test_model_integration_test(self):\n         \"\"\"\n@@ -291,7 +300,7 @@ def test_model_integration_test(self):\n         model = ColQwen2ForRetrieval.from_pretrained(\n             self.model_name,\n             torch_dtype=torch.bfloat16,\n-            device_map=torch_device,\n+            load_in_8bit=True,\n         ).eval()\n \n         # Load the test dataset\n@@ -319,13 +328,20 @@ def test_model_integration_test(self):\n         self.assertTrue((scores.argmax(axis=1) == torch.arange(len(ds), device=scores.device)).all())\n \n         # Further validation: fine-grained check, with a hardcoded score from the original Hf implementation.\n-        expected_scores = torch.tensor(\n-            [\n-                [16.2500, 7.8750, 14.6875],\n-                [9.5000, 17.1250, 10.5000],\n-                [14.9375, 10.9375, 20.0000],\n-            ],\n-            dtype=scores.dtype,\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    [15.5000, 8.1250, 14.9375],\n+                    [9.0625, 17.1250, 10.6875],\n+                    [15.9375, 12.1875, 20.2500],\n+                ],\n+                (\"cuda\", 8): [\n+                    [15.1250, 8.6875, 15.0625],\n+                    [9.2500, 17.2500, 10.3750],\n+                    [15.9375, 12.3750, 20.2500],\n+                ],\n+            }\n         )\n+        expected_scores = torch.tensor(expectations.get_expectation(), dtype=scores.dtype)\n \n         assert torch.allclose(scores, expected_scores, atol=1e-3), f\"Expected scores {expected_scores}, got {scores}\""
        }
    ],
    "stats": {
        "total": 34,
        "additions": 25,
        "deletions": 9
    }
}