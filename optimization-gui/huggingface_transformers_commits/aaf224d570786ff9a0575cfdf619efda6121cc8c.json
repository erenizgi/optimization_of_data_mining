{
    "author": "zucchini-nlp",
    "message": "[video processor] fix tests (#38104)\n\n* fix tests\n\n* delete\n\n* fix one more test\n\n* fix qwen + some tests are failing irrespective of `VideoProcessor`\n\n* delete file",
    "sha": "aaf224d570786ff9a0575cfdf619efda6121cc8c",
    "files": [
        {
            "sha": "e7d08239fe9f39627f64579dc8e202ce39164517",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -46,12 +46,15 @@\n else:\n     VIDEO_PROCESSOR_MAPPING_NAMES = OrderedDict(\n         [\n+            (\"instructblip\", \"InstructBlipVideoVideoProcessor\"),\n             (\"instructblipvideo\", \"InstructBlipVideoVideoProcessor\"),\n             (\"internvl\", \"InternVLVideoProcessor\"),\n             (\"llava_next_video\", \"LlavaNextVideoVideoProcessor\"),\n             (\"llava_onevision\", \"LlavaOnevisionVideoProcessor\"),\n-            (\"qwen2_5_vl\", \"Qwen2_5_VLVideoProcessor\"),\n+            (\"qwen2_5_omni\", \"Qwen2VLVideoProcessor\"),\n+            (\"qwen2_5_vl\", \"Qwen2VLVideoProcessor\"),\n             (\"qwen2_vl\", \"Qwen2VLVideoProcessor\"),\n+            (\"smolvlm\", \"SmolVLMVideoProcessor\"),\n             (\"video_llava\", \"VideoLlavaVideoProcessor\"),\n         ]\n     )"
        },
        {
            "sha": "d83af3a9f6563d94f11cab79b8a4d3b6cfe5fcec",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -156,29 +156,28 @@ def __call__(\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n             - **pixel_values_videos** -- Pixel values to be fed to a model. Returned when `videos` is not `None`.\n         \"\"\"\n-        data = {}\n-        if images is not None:\n-            encoded_images = self.image_processor(images=images, return_tensors=return_tensors)\n-            data.update(encoded_images)\n-\n-        if videos is not None:\n-            encoded_videos = self.video_processor(videos=videos, return_tensors=return_tensors)\n-            data.update(encoded_videos)\n \n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):\n             raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n-        if encoded_images is not None:\n+        data = {}\n+        if images is not None:\n+            encoded_images = self.image_processor(images=images, return_tensors=return_tensors)\n+            data.update(encoded_images)\n+\n             height, width = get_image_size(to_numpy_array(encoded_images.get(\"pixel_values_images\")[0]))\n             num_image_tokens = (height // self.patch_size) * (width // self.patch_size)\n             num_image_tokens += self.num_additional_image_tokens\n             if self.vision_feature_select_strategy == \"default\":\n                 num_image_tokens -= 1\n             text = [sample.replace(self.image_token, self.image_token * num_image_tokens) for sample in text]\n \n-        if encoded_videos is not None:\n+        if videos is not None:\n+            encoded_videos = self.video_processor(videos=videos, return_tensors=return_tensors)\n+            data.update(encoded_videos)\n+\n             one_video = encoded_videos.get(\"pixel_values_videos\")[0]\n             if isinstance(encoded_videos.get(\"pixel_values_videos\")[0], (list, tuple)):\n                 one_video = np.array(one_video)"
        },
        {
            "sha": "3f52d8291d7fdfdbe3e52a33207a2947c0ad756e",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -415,7 +415,7 @@ def test_small_model_integration_test(self):\n             \"llava-hf/LLaVA-NeXT-Video-7B-hf\", load_in_4bit=True, cache_dir=\"./\"\n         )\n \n-        inputs = self.processor(self.prompt_video, videos=self.video, return_tensors=\"pt\")\n+        inputs = self.processor(text=self.prompt_video, videos=self.video, return_tensors=\"pt\")\n         # verify single forward pass\n         inputs = inputs.to(torch_device)\n         with torch.no_grad():\n@@ -438,7 +438,7 @@ def test_small_model_integration_test_batch(self):\n         )\n \n         inputs = self.processor(\n-            [self.prompt_video, self.prompt_video],\n+            text=[self.prompt_video, self.prompt_video],\n             videos=[self.video, self.video],\n             return_tensors=\"pt\",\n             padding=True,\n@@ -465,7 +465,7 @@ def test_small_model_integration_test_batch_different_vision_types(self):\n         )\n \n         inputs = self.processor(\n-            [self.prompt_image, self.prompt_video],\n+            text=[self.prompt_image, self.prompt_video],\n             images=self.image,\n             videos=self.video,\n             return_tensors=\"pt\",\n@@ -491,7 +491,7 @@ def test_small_model_integration_test_batch_matches_single(self):\n         )\n \n         inputs_batched = self.processor(\n-            [self.prompt_video, self.prompt_image],\n+            text=[self.prompt_video, self.prompt_image],\n             images=[self.image],\n             videos=[self.video],\n             return_tensors=\"pt\","
        },
        {
            "sha": "747993cbb684fd64cd7e4216fa0984a4036cc0d4",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 13,
            "deletions": 3,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -648,7 +648,12 @@ def test_small_model_integration_test_multiturn(self):\n             self.messages[0],\n             {\n                 \"role\": \"assistant\",\n-                \"content\": \"The sound is glass shattering, and the dog appears to be a Labrador Retriever.\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"The sound is glass shattering, and the dog appears to be a Labrador Retriever.\",\n+                    }\n+                ],\n             },\n             {\n                 \"role\": \"user\",\n@@ -687,7 +692,12 @@ def test_small_model_integration_test_w_audio(self):\n         messages = [\n             {\n                 \"role\": \"system\",\n-                \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"text\",\n+                        \"text\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\",\n+                    }\n+                ],\n             },\n             {\n                 \"role\": \"user\",\n@@ -697,7 +707,7 @@ def test_small_model_integration_test_w_audio(self):\n         audio, _ = librosa.load(BytesIO(urlopen(audio_url).read()), sr=self.processor.feature_extractor.sampling_rate)\n \n         text = self.processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        inputs = self.processor(text=[text], audio=[audio], return_tensors=\"pt\", padding=True).to(torch_device)\n+        inputs = self.processor(text=text, audio=[audio], return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, thinker_temperature=0, thinker_do_sample=False)\n "
        },
        {
            "sha": "3e81044e6b69f245cc6b1172e245dfe6d776e0b7",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -466,7 +466,7 @@ def test_small_model_integration_test(self):\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n         video_file = np.load(video_file)\n-        inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device)\n+        inputs = self.processor(text=prompt, videos=video_file, return_tensors=\"pt\").to(torch_device)\n \n         EXPECTED_INPUT_IDS = torch.tensor([1,  3148, 1001, 29901, 29871, 13, 11008, 338, 445, 4863, 2090, 1460, 29973, 319, 1799, 9047, 13566, 29901], device=torch_device)  # fmt: skip\n         non_video_inputs = inputs[\"input_ids\"][inputs[\"input_ids\"] != 32001]\n@@ -496,9 +496,9 @@ def test_small_model_integration_test_mixed_inputs(self):\n         url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n \n-        inputs = self.processor(prompts, images=[image], videos=[video_file], padding=True, return_tensors=\"pt\").to(\n-            torch_device\n-        )\n+        inputs = self.processor(\n+            text=prompts, images=[image], videos=[video_file], padding=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n \n         EXPECTED_DECODED_TEXT = [\n@@ -522,7 +522,7 @@ def test_small_model_integration_test_llama(self):\n             repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo.npy\", repo_type=\"dataset\"\n         )\n         video_file = np.load(video_file)\n-        inputs = self.processor(prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n+        inputs = self.processor(text=prompt, videos=video_file, return_tensors=\"pt\").to(torch_device, torch.float16)\n \n         output = model.generate(**inputs, max_new_tokens=900, do_sample=False)\n         EXPECTED_DECODED_TEXT = \"USER: \\nDescribe the video in details. ASSISTANT: The video features a young child sitting on a bed, holding a book and reading it. \" \\\n@@ -554,7 +554,7 @@ def test_small_model_integration_test_llama_batched(self):\n             hf_hub_download(repo_id=\"raushan-testing-hf/videos-test\", filename=\"video_demo_2.npy\", repo_type=\"dataset\")\n         )\n \n-        inputs = processor(prompts, videos=[video_1, video_2], return_tensors=\"pt\", padding=True).to(torch_device)\n+        inputs = processor(text=prompts, videos=[video_1, video_2], return_tensors=\"pt\", padding=True).to(torch_device)\n \n         output = model.generate(**inputs, max_new_tokens=20)\n "
        },
        {
            "sha": "441838ffcab467b16df355d135a84014cfa81f23",
            "filename": "tests/utils/test_video_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Futils%2Ftest_video_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aaf224d570786ff9a0575cfdf619efda6121cc8c/tests%2Futils%2Ftest_video_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_video_utils.py?ref=aaf224d570786ff9a0575cfdf619efda6121cc8c",
            "patch": "@@ -71,17 +71,17 @@ def test_make_batched_videos_pil(self):\n \n         # Test a list of videos is converted to a list of 1 video\n         video = get_random_video(16, 32)\n-        video = [PIL.Image.fromarray(frame) for frame in video]\n-        videos_list = make_batched_videos(video)\n+        pil_video = [PIL.Image.fromarray(frame) for frame in video]\n+        videos_list = make_batched_videos(pil_video)\n         self.assertIsInstance(videos_list, list)\n         self.assertIsInstance(videos_list[0], np.ndarray)\n         self.assertEqual(videos_list[0].shape, (8, 16, 32, 3))\n         self.assertTrue(np.array_equal(videos_list[0], video))\n \n         # Test a nested list of videos is not modified\n         video = get_random_video(16, 32)\n-        video = [PIL.Image.fromarray(frame) for frame in video]\n-        videos = [video, video]\n+        pil_video = [PIL.Image.fromarray(frame) for frame in video]\n+        videos = [pil_video, pil_video]\n         videos_list = make_batched_videos(videos)\n         self.assertIsInstance(videos_list, list)\n         self.assertIsInstance(videos_list[0], np.ndarray)"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 40,
        "deletions": 28
    }
}