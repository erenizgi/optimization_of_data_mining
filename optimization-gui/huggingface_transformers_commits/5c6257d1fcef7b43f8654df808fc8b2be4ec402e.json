{
    "author": "benniekiss",
    "message": "[whisper] Clarify error message when setting max_new_tokens (#33324)\n\n* clarify error message when setting max_new_tokens\r\n\r\n* sync error message in test_generate_with_prompt_ids_max_length\r\n\r\n* there is no self",
    "sha": "5c6257d1fcef7b43f8654df808fc8b2be4ec402e",
    "files": [
        {
            "sha": "91812155c5719b7ef8bbbc2deb8b29546f52b136",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c6257d1fcef7b43f8654df808fc8b2be4ec402e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c6257d1fcef7b43f8654df808fc8b2be4ec402e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=5c6257d1fcef7b43f8654df808fc8b2be4ec402e",
            "patch": "@@ -1698,8 +1698,8 @@ def _set_max_new_tokens_and_length(self, config, decoder_input_ids, generation_c\n         max_new_tokens = generation_config.max_new_tokens if generation_config.max_new_tokens is not None else 0\n         if max_new_tokens + decoder_input_ids.shape[-1] > self.config.max_target_positions:\n             raise ValueError(\n-                f\"The length of `decoder_input_ids` equal `prompt_ids` plus special start tokens is {decoder_input_ids.shape[-1]}, and the `max_new_tokens` \"\n-                f\"is {max_new_tokens}. Thus, the combined length of \"\n+                f\"The length of `decoder_input_ids`, including special start tokens, prompt tokens, and previous tokens, is {decoder_input_ids.shape[-1]}, \"\n+                f\" and `max_new_tokens` is {max_new_tokens}. Thus, the combined length of \"\n                 f\"`decoder_input_ids` and `max_new_tokens` is: {max_new_tokens + decoder_input_ids.shape[-1]}. This exceeds the \"\n                 f\"`max_target_positions` of the Whisper model: {self.config.max_target_positions}. \"\n                 \"You should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, \""
        },
        {
            "sha": "09be23a0d381f9f4a20887c9035e9a307da1446a",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c6257d1fcef7b43f8654df808fc8b2be4ec402e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c6257d1fcef7b43f8654df808fc8b2be4ec402e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=5c6257d1fcef7b43f8654df808fc8b2be4ec402e",
            "patch": "@@ -1349,8 +1349,8 @@ def test_generate_with_prompt_ids_max_length(self):\n \n         with self.assertRaisesRegex(\n             ValueError,\n-            f\"The length of `decoder_input_ids` equal `prompt_ids` plus special start tokens is {decoder_input_ids.shape[-1]}, and the `max_new_tokens` \"\n-            f\"is {max_new_tokens}. Thus, the combined length of \"\n+            f\"The length of `decoder_input_ids`, including special start tokens, prompt tokens, and previous tokens, is {decoder_input_ids.shape[-1]}, \"\n+            f\" and `max_new_tokens` is {max_new_tokens}. Thus, the combined length of \"\n             f\"`decoder_input_ids` and `max_new_tokens` is: {max_new_tokens + decoder_input_ids.shape[-1]}. This exceeds the \"\n             f\"`max_target_positions` of the Whisper model: {config.max_target_positions}. \"\n             \"You should either reduce the length of your prompt, or reduce the value of `max_new_tokens`, \""
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}