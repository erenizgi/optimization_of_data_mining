{
    "author": "HollowMan6",
    "message": "Fix `max_length_q` and `max_length_k` types to `flash_attn_varlen_func` (#37206)\n\nAlso add notes asking users to set `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1`\nor call `torch._dynamo.config.capture_scalar_outputs = True`, as currently\nthis will cause a graph break.\n\nSigned-off-by: Hollow Man <hollowman@opensuse.org>",
    "sha": "2ef59646b8466a6e47cbf42754637a9f4a82484f",
    "files": [
        {
            "sha": "46a48fb11485b3a200934f32034b9db2c6dda841",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/2ef59646b8466a6e47cbf42754637a9f4a82484f/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2ef59646b8466a6e47cbf42754637a9f4a82484f/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=2ef59646b8466a6e47cbf42754637a9f4a82484f",
            "patch": "@@ -208,6 +208,8 @@ def _get_unpad_data(attention_mask: torch.Tensor) -> tuple[torch.Tensor, torch.T\n     \"\"\"\n     seqlens_in_batch = attention_mask.sum(dim=-1, dtype=torch.int32)\n     indices = torch.nonzero(attention_mask.flatten(), as_tuple=False).flatten()\n+    # NOTE: Similar to the `.item()` in prepare_fa2_from_position_ids, with torch compile,\n+    # this might cause a graph break\n     max_seqlen_in_batch = seqlens_in_batch.max().item()\n     cu_seqlens = F.pad(torch.cumsum(seqlens_in_batch, dim=0, dtype=torch.int32), (1, 0))\n     return (\n@@ -342,7 +344,13 @@ def _prepare_flash_attention_from_position_ids(query, key, value, position_ids):\n         )\n     )\n \n-    max_length = position_ids.max() + 1\n+    # NOTE: With torch compile, this will cause a graph break if you don't set\n+    # `TORCHDYNAMO_CAPTURE_SCALAR_OUTPUTS=1` in the environment or call\n+    # `torch._dynamo.config.capture_scalar_outputs = True` before doing the forward pass.\n+    # This is a limitation of flash attention API, as the function `flash_attn_varlen_func`\n+    # requires `max_length_q`, `max_length_k` to be passed as `int` and not `torch.Tensor`.\n+    # https://github.com/Dao-AILab/flash-attention/blob/2dd8078adc1d9b74e315ee99718c0dea0de8eeb6/flash_attn/flash_attn_interface.py#L1423-L1424\n+    max_length = position_ids.max().item() + 1\n \n     return (query, key, value, indices_q, (cu_seq_lens, cu_seq_lens), (max_length, max_length))\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 9,
        "deletions": 1
    }
}