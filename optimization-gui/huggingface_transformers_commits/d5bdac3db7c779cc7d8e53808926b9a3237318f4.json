{
    "author": "sizhky",
    "message": "Improve Error Messaging for Flash Attention 2 on CPU (#33655)\n\nUpdate flash-attn error message on CPU\r\n\r\nRebased to latest branch",
    "sha": "d5bdac3db7c779cc7d8e53808926b9a3237318f4",
    "files": [
        {
            "sha": "3e3d789087d23153a777ec0f57a2bfd8deba6821",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5bdac3db7c779cc7d8e53808926b9a3237318f4/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5bdac3db7c779cc7d8e53808926b9a3237318f4/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d5bdac3db7c779cc7d8e53808926b9a3237318f4",
            "patch": "@@ -1698,6 +1698,10 @@ def _check_and_enable_flash_attn_2(\n                     raise ImportError(\n                         f\"{preface} you need flash_attn package version to be greater or equal than 2.1.0. Detected version {flash_attention_version}. {install_message}\"\n                     )\n+                elif not torch.cuda.is_available():\n+                    raise ValueError(\n+                        f\"{preface} Flash Attention 2 is not available on CPU. Please make sure torch can access a CUDA device.\"\n+                    )\n                 else:\n                     raise ImportError(f\"{preface} Flash Attention 2 is not available. {install_message}\")\n             elif torch.version.hip:"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 4,
        "deletions": 0
    }
}