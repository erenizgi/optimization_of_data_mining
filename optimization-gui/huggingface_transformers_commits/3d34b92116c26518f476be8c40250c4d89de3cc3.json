{
    "author": "ydshieh",
    "message": "Switch to use A10 progressively (#38936)\n\n* try\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "3d34b92116c26518f476be8c40250c4d89de3cc3",
    "files": [
        {
            "sha": "c9096bec3c6661078e4dda0140e584029e1018a3",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=3d34b92116c26518f476be8c40250c4d89de3cc3",
            "patch": "@@ -12,8 +12,8 @@ on:\n       slice_id:\n         required: true\n         type: number\n-      runner:\n-        required: true\n+      runner_map:\n+        required: false\n         type: string\n       docker:\n         required: true\n@@ -45,7 +45,7 @@ jobs:\n       matrix:\n         folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}\n     runs-on:\n-      group: '${{ inputs.machine_type }}'\n+      group: ${{ fromJson(inputs.runner_map)[matrix.folders][inputs.machine_type] }}\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/"
        },
        {
            "sha": "c90181ec6f1b6df86a90f9215ff13e64e7a2f781",
            "filename": ".github/workflows/model_jobs_amd.yml",
            "status": "removed",
            "additions": 0,
            "deletions": 128,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8059e1f8f9ad245d71fbe2d18723d735ffccfec/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8059e1f8f9ad245d71fbe2d18723d735ffccfec/.github%2Fworkflows%2Fmodel_jobs_amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs_amd.yml?ref=b8059e1f8f9ad245d71fbe2d18723d735ffccfec",
            "patch": "@@ -1,128 +0,0 @@\n-name: model jobs\n-\n-on:\n-  workflow_call:\n-    inputs:\n-      folder_slices:\n-        required: true\n-        type: string\n-      machine_type:\n-        required: true\n-        type: string\n-      slice_id:\n-        required: true\n-        type: number\n-      runner:\n-        required: true\n-        type: string\n-      docker:\n-        required: true\n-        type: string\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  RUN_SLOW: yes\n-  # For gated repositories, we still need to agree to share information on the Hub repo. page in order to get access.\n-  # This token is created under the bot `hf-transformers-bot`.\n-  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n-  TF_FORCE_GPU_ALLOW_GROWTH: true\n-  CUDA_VISIBLE_DEVICES: 0,1\n-\n-jobs:\n-  run_models_gpu:\n-    name: \" \"\n-    strategy:\n-      max-parallel: 1  # For now, not to parallelize. Can change later if it works well.\n-      fail-fast: false\n-      matrix:\n-        folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}\n-    runs-on: ['${{ inputs.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: ${{ inputs.docker }}\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Echo input and matrix info\n-        shell: bash\n-        run: |\n-          echo \"${{ inputs.folder_slices }}\"\n-          echo \"${{ matrix.folders }}\"\n-          echo \"${{ toJson(fromJson(inputs.folder_slices)[inputs.slice_id]) }}\"\n-\n-      - name: Echo folder ${{ matrix.folders }}\n-        shell: bash\n-        # For folders like `models/bert`, set an env. var. (`matrix_folders`) to `models_bert`, which will be used to\n-        # set the artifact folder names (because the character `/` is not allowed).\n-        run: |\n-          echo \"${{ matrix.folders }}\"\n-          matrix_folders=${{ matrix.folders }}\n-          matrix_folders=${matrix_folders/'models/'/'models_'}\n-          echo \"$matrix_folders\"\n-          echo \"matrix_folders=$matrix_folders\" >> $GITHUB_ENV\n-\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: Update / Install some packages (for Past CI)\n-        if: ${{ contains(inputs.docker, '-past-') }}\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pip install -U datasets\n-\n-      - name: Update / Install some packages (for Past CI)\n-        if: ${{ contains(inputs.docker, '-past-') && contains(inputs.docker, '-pytorch-') }}\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pip install --no-cache-dir git+https://github.com/huggingface/accelerate@main#egg=accelerate\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: python3 -m pytest -rsfE -v --make-reports=${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}  -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n-\n-      - name: Run test\n-        shell: bash\n-        run: |\n-          mkdir -p /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n-\n-      - name: \"Test suite reports artifacts: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "88eee91a3bc2d85d4c193d5635e94546c29d6067",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=3d34b92116c26518f476be8c40250c4d89de3cc3",
            "patch": "@@ -22,7 +22,7 @@ on:\n         default: \"\"\n \n \n-# Used for `push` to easily modiffy the target workflow runs to compare against\n+# Used for `push` to easily modify the target workflow runs to compare against\n env:\n     prev_workflow_run_id: \"\"\n     other_workflow_run_id: \"\"\n@@ -51,7 +51,6 @@ jobs:\n     with:\n       job: run_models_gpu\n       slack_report_channel: \"#transformers-ci-daily-models\"\n-      runner: daily-ci\n       docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n@@ -63,7 +62,6 @@ jobs:\n     with:\n       job: run_pipelines_torch_gpu\n       slack_report_channel: \"#transformers-ci-daily-pipeline-torch\"\n-      runner: daily-ci\n       docker: huggingface/transformers-pytorch-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n@@ -75,7 +73,6 @@ jobs:\n     with:\n       job: run_examples_gpu\n       slack_report_channel: \"#transformers-ci-daily-examples\"\n-      runner: daily-ci\n       docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n@@ -87,7 +84,6 @@ jobs:\n     with:\n       job: run_trainer_and_fsdp_gpu\n       slack_report_channel: \"#transformers-ci-daily-training\"\n-      runner: daily-ci\n       docker: huggingface/transformers-all-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n@@ -99,7 +95,6 @@ jobs:\n     with:\n       job: run_torch_cuda_extensions_gpu\n       slack_report_channel: \"#transformers-ci-daily-training\"\n-      runner: daily-ci\n       docker: huggingface/transformers-pytorch-deepspeed-latest-gpu\n       ci_event: Daily CI\n       working-directory-prefix: /workspace\n@@ -112,7 +107,6 @@ jobs:\n     with:\n       job: run_quantization_torch_gpu\n       slack_report_channel: \"#transformers-ci-daily-quantization\"\n-      runner: daily-ci\n       docker: huggingface/transformers-quantization-latest-gpu\n       ci_event: Daily CI\n       report_repo_id: hf-internal-testing/transformers_daily_ci"
        },
        {
            "sha": "2ddf1071710a57436c45af5488f71cbf5e5557a6",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d34b92116c26518f476be8c40250c4d89de3cc3/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=3d34b92116c26518f476be8c40250c4d89de3cc3",
            "patch": "@@ -15,9 +15,6 @@ on:\n       slack_report_channel:\n         required: true\n         type: string\n-      runner:\n-        required: true\n-        type: string\n       docker:\n         required: true\n         type: string\n@@ -62,6 +59,7 @@ jobs:\n     outputs:\n       folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}\n       slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}\n+      runner_map: ${{ steps.set-matrix.outputs.runner_map }}\n       quantization_matrix: ${{ steps.set-matrix-quantization.outputs.quantization_matrix }}\n     steps:\n       - name: Update clone\n@@ -88,6 +86,7 @@ jobs:\n           if [ \"${{ inputs.job }}\" = \"run_models_gpu\" ]; then\n             echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n             echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n+            echo \"runner_map=$(python3 ../utils/get_runner_map.py)\" >> $GITHUB_OUTPUT\n           elif [ \"${{ inputs.job }}\" = \"run_trainer_and_fsdp_gpu\" ]; then\n             echo \"folder_slices=[['trainer'], ['fsdp']]\" >> $GITHUB_OUTPUT\n             echo \"slice_ids=[0, 1]\" >> $GITHUB_OUTPUT\n@@ -111,14 +110,14 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n+        machine_type: [single-gpu, multi-gpu]\n         slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n     uses: ./.github/workflows/model_jobs.yml\n     with:\n       folder_slices: ${{ needs.setup.outputs.folder_slices }}\n       machine_type: ${{ matrix.machine_type }}\n       slice_id: ${{ matrix.slice_id }}\n-      runner: ${{ inputs.runner }}\n+      runner_map: ${{ needs.setup.outputs.runner_map }}\n       docker: ${{ inputs.docker }}\n     secrets: inherit\n \n@@ -136,7 +135,6 @@ jobs:\n       folder_slices: ${{ needs.setup.outputs.folder_slices }}\n       machine_type: ${{ matrix.machine_type }}\n       slice_id: ${{ matrix.slice_id }}\n-      runner: ${{ inputs.runner }}\n       docker: ${{ inputs.docker }}\n       report_name_prefix: run_trainer_and_fsdp_gpu\n     secrets: inherit"
        },
        {
            "sha": "7b36651165bc40b0bc96a7126068f0766d8fc421",
            "filename": "utils/get_runner_map.py",
            "status": "added",
            "additions": 65,
            "deletions": 0,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/3d34b92116c26518f476be8c40250c4d89de3cc3/utils%2Fget_runner_map.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3d34b92116c26518f476be8c40250c4d89de3cc3/utils%2Fget_runner_map.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fget_runner_map.py?ref=3d34b92116c26518f476be8c40250c4d89de3cc3",
            "patch": "@@ -0,0 +1,65 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"\n+This script is used to get a map containing the information of runners to use in GitHub Actions workflow files.\n+This is meant to be a temporary file that helps us to switch progressively from T4 to A10 runners.\n+\n+The data is stored in a Hub repository [hf-internal-testing/transformers_daily_ci](https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/blob/main/runner_map.json).\n+Currently, in that file, we specify the models for which we want to run the tests with T4 runners to avoid many test failures showing on the CI reports.\n+We will work on the tests toward to use A10 for all CI jobs.\n+\"\"\"\n+\n+import os\n+\n+import requests\n+\n+\n+if __name__ == \"__main__\":\n+    # T4\n+    t4_runners = {\n+        \"single-gpu\": \"aws-g4dn-4xlarge-cache\",\n+        \"multi-gpu\": \"aws-g4dn-12xlarge-cache\",\n+    }\n+\n+    # A10\n+    a10_runners = {\n+        \"single-gpu\": \"aws-g5-4xlarge-cache\",\n+        \"multi-gpu\": \"aws-g5-12xlarge-cache\",\n+    }\n+\n+    tests = os.getcwd()\n+    model_tests = os.listdir(os.path.join(tests, \"models\"))\n+    d1 = sorted(filter(os.path.isdir, os.listdir(tests)))\n+    d2 = sorted(filter(os.path.isdir, [f\"models/{x}\" for x in model_tests]))\n+    d1.remove(\"models\")\n+    d = d2 + d1\n+\n+    response = requests.get(\n+        \"https://huggingface.co/datasets/hf-internal-testing/transformers_daily_ci/resolve/main/runner_map.json\"\n+    )\n+    # The models that we want to run with T4 runners\n+    jobs_using_t4 = response.json()\n+\n+    runner_map = {}\n+    for key in d:\n+        modified_key = key\n+        if modified_key.startswith(\"models/\"):\n+            modified_key = key[len(\"models/\") :]\n+        if modified_key in jobs_using_t4:\n+            runner_map[key] = t4_runners\n+        else:\n+            runner_map[key] = a10_runners\n+\n+    print(runner_map)"
        }
    ],
    "stats": {
        "total": 217,
        "additions": 73,
        "deletions": 144
    }
}