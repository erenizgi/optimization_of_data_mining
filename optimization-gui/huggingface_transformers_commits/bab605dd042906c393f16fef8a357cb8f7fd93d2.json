{
    "author": "gante",
    "message": "[Cache] rename dtype attribute ðŸš¨ ðŸš¨  (#37044)\n\n* yoink\n\n* same pattern in all cache",
    "sha": "bab605dd042906c393f16fef8a357cb8f7fd93d2",
    "files": [
        {
            "sha": "6fa96e5c8e43eb5dae09464f49b59314f1bcd572",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/bab605dd042906c393f16fef8a357cb8f7fd93d2/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bab605dd042906c393f16fef8a357cb8f7fd93d2/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=bab605dd042906c393f16fef8a357cb8f7fd93d2",
            "patch": "@@ -1199,7 +1199,7 @@ def __init__(\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n         )\n \n-        self.dtype = dtype\n+        self._dtype = dtype\n         self.num_key_value_heads = (\n             config.num_attention_heads\n             if getattr(config, \"num_key_value_heads\", None) is None\n@@ -1216,8 +1216,8 @@ def __init__(\n                 layer_device = layer_device_map[idx]\n             else:\n                 layer_device = device\n-            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n-            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n             # preventing compiled graph breaks when updating the cache.\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n@@ -1680,7 +1680,7 @@ def __init__(\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n         )\n \n-        self.dtype = dtype\n+        self._dtype = dtype\n         self.num_key_value_heads = (\n             config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n         )\n@@ -1707,8 +1707,8 @@ def __init__(\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n             cache_shape = global_cache_shape if not self.is_sliding[i] else sliding_cache_shape\n-            new_layer_key_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n-            new_layer_value_cache = torch.zeros(cache_shape, dtype=self.dtype, device=layer_device)\n+            new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n+            new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n             torch._dynamo.mark_static_address(new_layer_value_cache)\n             self.key_cache.append(new_layer_key_cache)\n@@ -1853,8 +1853,8 @@ def __init__(\n         dtype: torch.dtype = torch.float16,\n         device: Union[torch.device, str, None] = None,\n     ):\n-        self.dtype = dtype\n         self.max_batch_size = max_batch_size\n+        self._dtype = dtype\n         self.intermediate_size = config.intermediate_size\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n@@ -1868,14 +1868,14 @@ def __init__(\n                 self.intermediate_size,\n                 self.conv_kernel_size,\n                 device=device,\n-                dtype=dtype,\n+                dtype=self._dtype,\n             )\n             ssm_state: torch.Tensor = torch.zeros(\n                 self.max_batch_size,\n                 self.intermediate_size,\n                 self.ssm_state_size,\n                 device=device,\n-                dtype=dtype,\n+                dtype=self._dtype,\n             )\n \n             torch._dynamo.mark_static_address(conv_state)\n@@ -1972,7 +1972,7 @@ def __init__(\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n         self.device = torch.device(device) if layer_device_map is None else torch.device(layer_device_map[0])\n         self.offload_device = torch.device(offload_device)\n-        self.dtype = dtype if dtype is not None else torch.float32\n+        self._dtype = dtype if dtype is not None else torch.float32\n \n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n@@ -2144,8 +2144,8 @@ def _create_key_value_cache_tensors(\n \n         is_cpu_device = device == torch.device(\"cpu\")\n \n-        key_cache = torch.zeros(shape, dtype=self.dtype, device=device, pin_memory=is_cpu_device)\n-        value_cache = torch.zeros(shape, dtype=self.dtype, device=device, pin_memory=is_cpu_device)\n+        key_cache = torch.zeros(shape, dtype=self._dtype, device=device, pin_memory=is_cpu_device)\n+        value_cache = torch.zeros(shape, dtype=self._dtype, device=device, pin_memory=is_cpu_device)\n \n         # Note: `mark_static_address` is used to tag the cache as a fixed data pointer,\n         # preventing compiled graph breaks when updating the cache."
        }
    ],
    "stats": {
        "total": 24,
        "additions": 12,
        "deletions": 12
    }
}