{
    "author": "faaany",
    "message": "[docs] fix bugs in the bitsandbytes documentation (#35868)\n\n* fix doc\n\n* update model",
    "sha": "7399f8021edcdc074792986539c965562d434809",
    "files": [
        {
            "sha": "368a649bae3b1a4b2459f82635b00393b7051722",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7399f8021edcdc074792986539c965562d434809/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7399f8021edcdc074792986539c965562d434809/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=7399f8021edcdc074792986539c965562d434809",
            "patch": "@@ -208,7 +208,8 @@ from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n model_id = \"bigscience/bloom-1b7\"\n \n quantization_config = BitsAndBytesConfig(\n-    llm_int8_threshold=10,\n+    llm_int8_threshold=10.0,\n+    llm_int8_enable_fp32_cpu_offload=True\n )\n \n model_8bit = AutoModelForCausalLM.from_pretrained(\n@@ -285,7 +286,7 @@ For inference, the `bnb_4bit_quant_type` does not have a huge impact on performa\n \n ### Nested quantization\n \n-Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter. For example, with nested quantization, you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b) model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps.\n+Nested quantization is a technique that can save additional memory at no additional performance cost. This feature performs a second quantization of the already quantized weights to save an additional 0.4 bits/parameter. For example, with nested quantization, you can finetune a [Llama-13b](https://huggingface.co/meta-llama/Llama-2-13b-chat-hf) model on a 16GB NVIDIA T4 GPU with a sequence length of 1024, a batch size of 1, and enabling gradient accumulation with 4 steps.\n \n ```py\n from transformers import BitsAndBytesConfig\n@@ -295,7 +296,7 @@ double_quant_config = BitsAndBytesConfig(\n     bnb_4bit_use_double_quant=True,\n )\n \n-model_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b\", torch_dtype=\"auto\", quantization_config=double_quant_config)\n+model_double_quant = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", torch_dtype=\"auto\", quantization_config=double_quant_config)\n ```\n \n ## Dequantizing `bitsandbytes` models\n@@ -307,7 +308,7 @@ from transformers import AutoModelForCausalLM, BitsAndBytesConfig, AutoTokenizer\n \n model_id = \"facebook/opt-125m\"\n \n-model = AutoModelForCausalLM.from_pretrained(model_id, BitsAndBytesConfig(load_in_4bit=True))\n+model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=BitsAndBytesConfig(load_in_4bit=True))\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model.dequantize()"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 5,
        "deletions": 4
    }
}