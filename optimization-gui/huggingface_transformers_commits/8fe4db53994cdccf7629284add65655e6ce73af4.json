{
    "author": "MekkCyber",
    "message": "[kernels] rm mra kernels (#41507)\n\n* fix modeling\n\n* remove kernel\n\n* fix style",
    "sha": "8fe4db53994cdccf7629284add65655e6ce73af4",
    "files": [
        {
            "sha": "87ed89052873813153786bd416a981d3e5279af9",
            "filename": "src/transformers/kernels/mra/cuda_kernel.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 383,
            "changes": 383,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.cu?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1,383 +0,0 @@\n-#include \"cuda_kernel.h\"\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.x;\n-\n-  long thread_idx = threadIdx.x;\n-  long num_thread = blockDim.x;\n-\n-  extern __shared__ float buffer[];\n-  int *max_buffer = (int*)buffer;\n-\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_buffer[idx] = -1e8;\n-    }\n-  }\n-  __syncthreads();\n-\n-  int *indices_pt = &indices[batch_idx * num_block];\n-  float *index_vals_pt = &index_vals[batch_idx * num_block * 32];\n-\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    atomicMax(&max_buffer[A_block_idx * 32 + idx / num_block], (int)(index_vals_pt[idx] * 1000));\n-  }\n-  __syncthreads();\n-  \n-  float *max_vals_pt = &max_vals[batch_idx * A_num_block * 32];\n-  for (int i = 0; i < A_num_block * 32; i = i + num_thread) {\n-    int idx = i + thread_idx;\n-    if (idx < A_num_block * 32) {\n-      max_vals_pt[idx] = (float)max_buffer[idx] / 1000.;\n-    }\n-  }\n-  \n-  float *max_vals_scatter_pt = &max_vals_scatter[batch_idx * num_block * 32];\n-  for (int idx_start = 0; idx_start < 32 * num_block; idx_start = idx_start + num_thread) {\n-    int idx = idx_start + thread_idx;\n-    int A_block_idx = indices_pt[idx % num_block] / B_num_block;\n-    max_vals_scatter_pt[idx] = (float)max_buffer[A_block_idx * 32 + idx / num_block] / 1000.;\n-  }\n-\n-}\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[4096];\n-  float *A_buffer = &buffer[threadIdx.y * 1024]; // [2, 8, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 1024 + 512]; // [2, 8, 32]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * dim * 32];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * dim * 32];\n-\n-  int reg_1_idx = thread_idx / 8;    // [0000000011111111222222223333333344444444555555556666666677777777]\n-  int reg_2_idx = thread_idx % 8;    // [0123456701234567012345670123456701234567012345670123456701234567]\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0};\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    A_buffer[i * 64 + thread_idx] = dense_A_pt[i * 64 + thread_idx];\n-    B_buffer[i * 64 + thread_idx] = dense_B_pt[i * 64 + thread_idx];\n-  }\n-\n-  __syncthreads();\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    reg_1[i] = A_buffer[reg_1_idx * 4 + i];\n-    reg_2[i] = B_buffer[reg_2_idx * 4 + i];\n-  }\n-\n-  for (int dim_stride = 1; dim_stride < (dim / 8); dim_stride++) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      A_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_A_pt[dim_stride * 256 + i * 64 + thread_idx];\n-      B_buffer[(dim_stride % 2) * 256 + i * 64 + thread_idx] = dense_B_pt[dim_stride * 256 + i * 64 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[((dim_stride - 1) % 2) * 256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = A_buffer[(dim_stride % 2) * 256 + reg_1_idx * 4 + i];\n-      reg_2[i] = B_buffer[(dim_stride % 2) * 256 + reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-  }\n-\n-  #pragma unroll\n-  for (int mini_dim_idx = 1; mini_dim_idx < 8; mini_dim_idx++) {\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[(mini_dim_idx % 2) * 4 + i] = A_buffer[256 + mini_dim_idx * 32 + reg_1_idx * 4 + i];\n-      reg_2[(mini_dim_idx % 2) * 4 + i] = B_buffer[256 + mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-      }\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *C_buffer = &buffer[threadIdx.y * 1024]; // [32, 32]\n-\n-  #pragma unroll\n-  for (int i = 0; i < 4; i++) {\n-    #pragma unroll\n-    for (int j = 0; j < 4; j++) {\n-      C_buffer[(reg_2_idx * 4 + j) * 32 + reg_1_idx * 4 + i] = reg_array[i * 4 + j];\n-    }\n-  }\n-  __syncthreads();\n-\n-  float *sparse_C_pt = &sparse_C[batch_idx__block_idx * 1024];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 16; i++) {\n-    sparse_C_pt[i * 64 + thread_idx] = C_buffer[i * 64 + thread_idx];\n-  }\n-\n-}\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  __shared__ float buffer[6144];\n-  float *A_buffer = &buffer[threadIdx.y * 3072]; // [32, 32]\n-  float *B_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [32, 64]\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    A_buffer[i * 128 + thread_idx] = sparse_A_pt[i * 128 + thread_idx];\n-  }\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_B_pt = &dense_B[(batch_idx * B_num_block + AB_block_idx % B_num_block) * 32 * dim];\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32 * dim];\n-\n-  // [0000000011111111222222223333333344444444555555556666666677777777]\n-  // [0123456701234567012345670123456701234567012345670123456701234567]\n-  int reg_1_idx = thread_idx / 8;\n-  int reg_2_idx = thread_idx % 8;\n-\n-  float reg_1[8];\n-  float reg_2[8];\n-\n-  float reg_array[16];\n-\n-  for (int dim_stride = 0; dim_stride < dim; dim_stride = dim_stride + 64) {\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      B_buffer[i * 128 + thread_idx] = dense_B_pt[dim_stride * 32 + i * 128 + thread_idx];\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      reg_array[i] = 0;\n-    }\n-\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      reg_1[i] = B_buffer[(reg_1_idx * 4 + i) * 32];\n-      reg_2[i] = A_buffer[reg_2_idx * 4 + i];\n-    }\n-\n-    #pragma unroll\n-    for (int mini_dim_idx = 1; mini_dim_idx < 32; mini_dim_idx++) {\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        reg_1[(mini_dim_idx % 2) * 4 + i] = B_buffer[(reg_1_idx * 4 + i) * 32 + mini_dim_idx];\n-        reg_2[(mini_dim_idx % 2) * 4 + i] = A_buffer[mini_dim_idx * 32 + reg_2_idx * 4 + i];\n-      }\n-      #pragma unroll\n-      for (int i = 0; i < 4; i++) {\n-        #pragma unroll\n-        for (int j = 0; j < 4; j++) {\n-          reg_array[i * 4 + j] += reg_1[((mini_dim_idx - 1) % 2) * 4 + i] * reg_2[((mini_dim_idx - 1) % 2) * 4 + j];\n-        }\n-      }\n-    }\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        reg_array[i * 4 + j] += reg_1[4 + i] * reg_2[4 + j];\n-      }\n-    }\n-\n-    __syncthreads();\n-\n-    float *C_buffer = &buffer[threadIdx.y * 3072 + 1024]; // [64, 32]\n-\n-    #pragma unroll\n-    for (int i = 0; i < 4; i++) {\n-      #pragma unroll\n-      for (int j = 0; j < 4; j++) {\n-        C_buffer[(reg_1_idx * 4 + i) * 32 + reg_2_idx * 4 + j] = reg_array[i * 4 + j];\n-      }\n-    }\n-    __syncthreads();\n-\n-    #pragma unroll\n-    for (int i = 0; i < 16; i++) {\n-      atomicAdd(&dense_C_pt[dim_stride * 32 + i * 128 + thread_idx], C_buffer[i * 128 + thread_idx]);\n-    }\n-    __syncthreads();\n-\n-  }\n-\n-}\n-\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *sparse_A_pt = &sparse_A[batch_idx__block_idx * 1024];\n-\n-  float reg_array[16];\n-  float value = 0;\n-\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    reg_array[i] = sparse_A_pt[i * 32 + thread_idx];\n-  }\n-  #pragma unroll\n-  for (int stride = 8; stride < 32; stride = stride + 8) {\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      reg_array[(stride + i) % 16] = sparse_A_pt[(stride + i) * 32 + thread_idx];\n-    }\n-    #pragma unroll\n-    for (int i = 0; i < 8; i++) {\n-      value = value + reg_array[(stride - 8 + i) % 16];\n-    }\n-  }\n-  #pragma unroll\n-  for (int i = 0; i < 8; i++) {\n-    value = value + reg_array[8 + i];\n-  }\n-\n-  float *dense_C_pt = &dense_C[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-\n-  atomicAdd(&dense_C_pt[thread_idx], value);\n-\n-}\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-) {\n-\n-  long batch_idx = blockIdx.y;\n-  long block_idx = blockIdx.x * blockDim.y + threadIdx.y;\n-\n-  long thread_idx = threadIdx.x;\n-\n-  long batch_idx__block_idx = batch_idx * num_block + block_idx;\n-\n-  long AB_block_idx = indices[batch_idx__block_idx];\n-  float *dense_A_pt = &dense_A[(batch_idx * A_num_block + AB_block_idx / B_num_block) * 32];\n-  float *sparse_C_pt = &sparse_C[(batch_idx * num_block + block_idx) * 1024];\n-\n-  float value = dense_A_pt[thread_idx];\n-\n-  #pragma unroll\n-  for (int i = 0; i < 32; i++) {\n-    sparse_C_pt[i * 32 + thread_idx] = value;\n-  }\n-\n-}"
        },
        {
            "sha": "a95b46f7d159b11851143710034cf80c20aa6bf8",
            "filename": "src/transformers/kernels/mra/cuda_kernel.h",
            "status": "removed",
            "additions": 0,
            "deletions": 59,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_kernel.h?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1,59 +0,0 @@\n-\n-#define WARP_SIZE 32\n-#define FULL_MASK 0xffffffff\n-#define OPTIMAL_THREADS 256\n-\n-__global__ void index_max_cuda_kernel(\n-  float *index_vals,       // [batch_size, 32, num_block]\n-  int   *indices,        // [batch_size, num_block]\n-  float *max_vals,        // [batch_size, A_num_block * 32]\n-  float *max_vals_scatter,   // [batch_size, 32, num_block]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void mm_to_sparse_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, dim, 32]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void sparse_dense_mm_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_B,   // [batch_size, B_num_block, dim, 32]\n-  float *dense_C,   // [batch_size, A_num_block, dim, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long dim,\n-  long num_block\n-);\n-\n-__global__ void reduce_sum_cuda_kernel(\n-  float *sparse_A,  // [batch_size, num_block, 32, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *dense_C,   // [batch_size, A_num_block, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);\n-\n-__global__ void scatter_cuda_kernel(\n-  float *dense_A,   // [batch_size, A_num_block, 32]\n-  int   *indices,   // [batch_size, num_block]\n-  float *sparse_C,  // [batch_size, num_block, 32, 32]\n-  long batch_size,\n-  long A_num_block,\n-  long B_num_block,\n-  long num_block\n-);"
        },
        {
            "sha": "ba2a0cacfe614e75e06d2dde80dc77a6e8a4ec1a",
            "filename": "src/transformers/kernels/mra/cuda_launch.cu",
            "status": "removed",
            "additions": 0,
            "deletions": 154,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.cu",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.cu",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.cu?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1,154 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include \"cuda_kernel.h\"\n-#include <vector>\n-\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-//////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,  // [batch_size, 32, num_block]\n-  at::Tensor indices,     // [batch_size, num_block],\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = indices.size(0);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor max_vals = at::zeros({batch_size, A_num_block * 32}, index_vals.options());\n-  at::Tensor max_vals_scatter = at::zeros({batch_size, 32, num_block}, index_vals.options());\n-\n-  dim3 threads(256);\n-  dim3 blocks(batch_size);\n-  int shared_mem = A_num_block * 32 * sizeof(float);\n-\n-  index_max_cuda_kernel<<<blocks, threads, shared_mem>>>(\n-    index_vals.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    max_vals.data_ptr<float>(),\n-    max_vals_scatter.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return {max_vals, max_vals_scatter};\n-}\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,  // [batch_size, A_num_block, dim, 32]\n-  at::Tensor dense_B,  // [batch_size, B_num_block, dim, 32]\n-  at::Tensor indices   // [batch_size, num_block]\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_A.size(2);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(64, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  mm_to_sparse_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    dense_B.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  at::Tensor dense_B,   // [batch_size, B_num_block, dim, 32]\n-  int A_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-  int B_num_block = dense_B.size(1);\n-  int dim = dense_B.size(2);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, dim, 32}, dense_B.options());\n-\n-  dim3 threads(128, 2);\n-  dim3 blocks(num_block / 2, batch_size);\n-\n-  sparse_dense_mm_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_B.data_ptr<float>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    dim,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,  // [batch_size, num_block, 32, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  int batch_size = sparse_A.size(0);\n-  int num_block = sparse_A.size(1);\n-\n-  at::Tensor dense_C = at::zeros({batch_size, A_num_block, 32}, sparse_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  reduce_sum_cuda_kernel<<<blocks, threads>>>(\n-    sparse_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    dense_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return dense_C;\n-}\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,   // [batch_size, A_num_block, 32]\n-  at::Tensor indices,   // [batch_size, num_block]\n-  int B_num_block\n-) {\n-  int batch_size = dense_A.size(0);\n-  int A_num_block = dense_A.size(1);\n-  int num_block = indices.size(1);\n-\n-  at::Tensor sparse_C = at::zeros({batch_size, num_block, 32, 32}, dense_A.options());\n-\n-  dim3 threads(32, 4);\n-  dim3 blocks(num_block / 4, batch_size);\n-\n-  scatter_cuda_kernel<<<blocks, threads>>>(\n-    dense_A.data_ptr<float>(),\n-    indices.data_ptr<int>(),\n-    sparse_C.data_ptr<float>(),\n-    batch_size,\n-    A_num_block,\n-    B_num_block,\n-    num_block\n-  );\n-\n-  return sparse_C;\n-}"
        },
        {
            "sha": "0200140ee337b8c5d9583767bbad1e842e9d4677",
            "filename": "src/transformers/kernels/mra/cuda_launch.h",
            "status": "removed",
            "additions": 0,
            "deletions": 39,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.h",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.h",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fmra%2Fcuda_launch.h?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1,39 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include <vector>\n-\n-#define min(a, b) ((a)<(b)?(a):(b))\n-#define max(a, b) ((a)>(b)?(a):(b))\n-\n-std::vector<at::Tensor> index_max_kernel(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor mm_to_sparse_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-);\n-\n-at::Tensor sparse_dense_mm_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-);\n-\n-at::Tensor reduce_sum_kernel(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-);\n-\n-at::Tensor scatter_kernel(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-);"
        },
        {
            "sha": "60c9262b779270a6e95ae54f53a67daa6d740a9e",
            "filename": "src/transformers/kernels/mra/torch_extension.cpp",
            "status": "removed",
            "additions": 0,
            "deletions": 78,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Ftorch_extension.cpp",
            "raw_url": "https://github.com/huggingface/transformers/raw/c620c38bb04c80bb12350bec739c298918b5675d/src%2Ftransformers%2Fkernels%2Fmra%2Ftorch_extension.cpp",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkernels%2Fmra%2Ftorch_extension.cpp?ref=c620c38bb04c80bb12350bec739c298918b5675d",
            "patch": "@@ -1,78 +0,0 @@\n-#include <torch/extension.h>\n-#include <ATen/ATen.h>\n-#include \"cuda_launch.h\"\n-#include <vector>\n-\n-std::vector<at::Tensor> index_max(\n-  at::Tensor index_vals,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return index_max_kernel(\n-    index_vals,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor mm_to_sparse(\n-  at::Tensor dense_A,\n-  at::Tensor dense_B,\n-  at::Tensor indices\n-) {\n-  return mm_to_sparse_kernel(\n-    dense_A,\n-    dense_B,\n-    indices\n-  );\n-}\n-\n-at::Tensor sparse_dense_mm(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  at::Tensor dense_B,\n-  int A_num_block\n-) {\n-  return sparse_dense_mm_kernel(\n-    sparse_A,\n-    indices,\n-    dense_B,\n-    A_num_block\n-  );\n-}\n-\n-at::Tensor reduce_sum(\n-  at::Tensor sparse_A,\n-  at::Tensor indices,\n-  int A_num_block,\n-  int B_num_block\n-) {\n-  return reduce_sum_kernel(\n-    sparse_A,\n-    indices,\n-    A_num_block,\n-    B_num_block\n-  );\n-}\n-\n-at::Tensor scatter(\n-  at::Tensor dense_A,\n-  at::Tensor indices,\n-  int B_num_block\n-) {\n-  return scatter_kernel(\n-    dense_A,\n-    indices,\n-    B_num_block\n-  );\n-}\n-\n-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n-  m.def(\"index_max\", &index_max, \"index_max (CUDA)\");\n-  m.def(\"mm_to_sparse\", &mm_to_sparse, \"mm_to_sparse (CUDA)\");\n-  m.def(\"sparse_dense_mm\", &sparse_dense_mm, \"sparse_dense_mm (CUDA)\");\n-  m.def(\"reduce_sum\", &reduce_sum, \"reduce_sum (CUDA)\");\n-  m.def(\"scatter\", &scatter, \"scatter (CUDA)\");\n-}"
        },
        {
            "sha": "478d6678185173753bb7e6b043a1afad6fd5e988",
            "filename": "src/transformers/models/mra/modeling_mra.py",
            "status": "modified",
            "additions": 12,
            "deletions": 10,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8fe4db53994cdccf7629284add65655e6ce73af4/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8fe4db53994cdccf7629284add65655e6ce73af4/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fmodeling_mra.py?ref=8fe4db53994cdccf7629284add65655e6ce73af4",
            "patch": "@@ -15,13 +15,11 @@\n \"\"\"PyTorch MRA model.\"\"\"\n \n import math\n-from pathlib import Path\n from typing import Optional, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n-from torch.utils.cpp_extension import load\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -35,7 +33,14 @@\n )\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import apply_chunking_to_forward\n-from ...utils import auto_docstring, is_cuda_platform, is_ninja_available, is_torch_cuda_available, logging\n+from ...utils import (\n+    auto_docstring,\n+    is_cuda_platform,\n+    is_kernels_available,\n+    is_ninja_available,\n+    is_torch_cuda_available,\n+    logging,\n+)\n from .configuration_mra import MraConfig\n \n \n@@ -46,14 +51,11 @@\n \n def load_cuda_kernels():\n     global mra_cuda_kernel\n-    src_folder = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"mra\"\n-\n-    def append_root(files):\n-        return [src_folder / file for file in files]\n-\n-    src_files = append_root([\"cuda_kernel.cu\", \"cuda_launch.cu\", \"torch_extension.cpp\"])\n+    if not is_kernels_available():\n+        raise ImportError(\"kernels is not installed, please install it with `pip install kernels`\")\n+    from kernels import get_kernel\n \n-    mra_cuda_kernel = load(\"cuda_kernel\", src_files, verbose=True)\n+    mra_cuda_kernel = get_kernel(\"kernels-community/mra\")\n \n \n def sparse_max(sparse_qk_prod, indices, query_num_block, key_num_block):"
        }
    ],
    "stats": {
        "total": 735,
        "additions": 12,
        "deletions": 723
    }
}