{
    "author": "zheliuyu",
    "message": "Add local kernel loading support to KernelConfig(). (#42800)\n\n* add add_to_mapping_local for KernelConfig\n\n* refactor kernel_mapping format\n\n* lint code\n\n* specify the kernel path\n\n* fix `abs/path`\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* rename check_kernel_from_local to use_local_kernel\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "24275124c69477762e1dd6af2bb05d9513098a7e",
    "files": [
        {
            "sha": "ce7208bfc69d673987b57497e65d6ae8a8d312da",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/24275124c69477762e1dd6af2bb05d9513098a7e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24275124c69477762e1dd6af2bb05d9513098a7e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=24275124c69477762e1dd6af2bb05d9513098a7e",
            "patch": "@@ -3533,7 +3533,9 @@ def set_use_kernels(self, use_kernels, kernel_config):\n \n                 # This is a context manager to override the default kernel mapping\n                 # We are calling kernelize inside this context manager using the use_kernels setter\n-                with use_kernel_mapping(kernel_config.kernel_mapping):\n+                # Param inherit_mapping should be False to avoid still loading kernel from remote\n+                inherit_mapping = not kernel_config.use_local_kernel\n+                with use_kernel_mapping(kernel_config.kernel_mapping, inherit_mapping=inherit_mapping):\n                     self.use_kernels = True\n             # We use the default kernel mapping in .integrations.hub_kernels\n             else:"
        },
        {
            "sha": "6ebb39c9f64de2a90bce47364cd3786d46c948b9",
            "filename": "src/transformers/utils/kernel_config.py",
            "status": "modified",
            "additions": 71,
            "deletions": 18,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/24275124c69477762e1dd6af2bb05d9513098a7e/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/24275124c69477762e1dd6af2bb05d9513098a7e/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fkernel_config.py?ref=24275124c69477762e1dd6af2bb05d9513098a7e",
            "patch": "@@ -71,14 +71,36 @@ def add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping):\n     }\n \n \n+def add_to_mapping_local(layer_name, device, repo_name, mode, compatible_mapping):\n+    from pathlib import Path\n+\n+    from kernels import LocalLayerRepository\n+\n+    if device not in [\"cuda\", \"rocm\", \"xpu\", \"npu\"]:\n+        raise ValueError(f\"Only cuda, rocm, xpu and npu devices supported, got: {device}\")\n+    repo_layer_name = repo_name.split(\":\")[1]\n+    repo_path = repo_name.split(\":\")[0]\n+    repo_package_name = repo_path.split(\"/\")[-1]\n+    compatible_mapping[layer_name] = {\n+        device: {\n+            mode: LocalLayerRepository(\n+                repo_path=Path(repo_path),\n+                package_name=repo_package_name,\n+                layer_name=repo_layer_name,\n+            )\n+        }\n+    }\n+\n+\n class KernelConfig(PushToHubMixin):\n     \"\"\"\n     Kernel configuration class. This class is used to configure the kernel mapping for a model.\n     \"\"\"\n \n-    def __init__(self, kernel_mapping={}):\n+    def __init__(self, kernel_mapping={}, use_local_kernel=False):\n         self.kernel_mapping = kernel_mapping\n         self.registered_layer_names = {}\n+        self.use_local_kernel = use_local_kernel\n \n     def update_kernel(self, repo_id, registered_name, layer_name, device, mode, revision=None):\n         from kernels import LayerRepository\n@@ -105,6 +127,7 @@ def sanitize_kernel_mapping(self, model):\n         2. Each kernel value is either a string of the form 'org/repo:layer_name' or a dict mapping device types (\"cuda\", \"rocm\", \"xpu\", \"npu\") to such strings.\n         3. Each device key in a dict is one of \"cuda\", \"rocm\", \"xpu\", or \"npu\".\n         4. Each repo_name is a valid repository and layer name in the format 'org/repo:layer_name' (i.e., a string containing both a slash and a colon).\n+        5. If a local path is detected, it should be in the format '/abs/path:layer_name'. The absolute path must include the `package_name`, like \"/home/user/layer_norm\".\n \n         Args:\n             model: The model instance whose modules are checked for registered kernel_layer_name attributes.\n@@ -114,14 +137,13 @@ def sanitize_kernel_mapping(self, model):\n                         or if a repo_name is not a valid 'org/repo:layer_name' string.\n         \"\"\"\n         MAPPING_FORMAT = \"\"\"\n+        For single device form remote\n         {\n             \"RMSNorm\":\n                 \"kernels-community/layer_norm:LlamaRMSNorm\",\n             ...\n         },\n-\n-        or\n-\n+        For multiple devices form remote\n         {\n             \"RMSNorm\": {\n                 \"cuda\":\n@@ -132,6 +154,23 @@ def sanitize_kernel_mapping(self, model):\n             },\n             ...\n         }\n+        For single device form local\n+        {\n+            \"RMSNorm\":\n+                \"/abs/path:LlamaRMSNorm\",\n+            ...\n+        },\n+        For multiple devices form local\n+        {\n+            \"RMSNorm\": {\n+                \"cuda\":\n+                    \"/abs/path:LlamaRMSNorm\",\n+                \"rocm\":\n+                    \"/abs/path:LlamaRMSNorm\",\n+                ...\n+            },\n+            ...\n+        }\n         \"\"\"\n         self.store_registered_layer_names(model)\n         # Validate that the kernel mapping is a dict\n@@ -149,7 +188,7 @@ def sanitize_kernel_mapping(self, model):\n             if isinstance(kernel, str):\n                 if \"/\" not in kernel or \":\" not in kernel:\n                     raise ValueError(\n-                        f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name'), got: {kernel}\"\n+                        f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name' or '/abs/path:layer_name'), got: {kernel}\"\n                     )\n \n             elif isinstance(kernel, dict):\n@@ -159,9 +198,8 @@ def sanitize_kernel_mapping(self, model):\n \n                     if not isinstance(repo_name, str) or \"/\" not in repo_name or \":\" not in repo_name:\n                         raise ValueError(\n-                            f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name'), got: {repo_name}\"\n+                            f\"Kernel mapping for '{layer_name}' must be a valid repo name with a layer name (e.g., 'org/repo:layer_name' or '/abs/path:layer_name'), got: {repo_name}\"\n                         )\n-\n             else:\n                 raise ValueError(f\"Kernel mapping must follow the format: {MAPPING_FORMAT}, got: {kernel}\")\n \n@@ -174,18 +212,13 @@ def create_compatible_mapping(self, model, compile=False):\n                 ...\n             },\n \n-            or\n+            or for local path:\n \n             {\n-                \"RMSNorm\": {\n-                    \"cuda\":\n-                        \"kernels-community/layer_norm:LlamaRMSNorm\",\n-                    \"rocm\":\n-                        \"kernels-community/layer_norm:LlamaRMSNorm\",\n-                    ...\n-                },\n+                \"RMSNorm\":\n+                    \"/home/user/liger_kernels:LigerRMSNorm\",\n                 ...\n-            }\n+            },\n \n         into a nested mapping:\n \n@@ -200,6 +233,20 @@ def create_compatible_mapping(self, model, compile=False):\n                 }\n             }\n \n+            or for local path:\n+\n+            {\n+                \"RMSNorm\": {\n+                    \"cuda\": {\n+                        Mode.INFERENCE: LocalLayerRepository(\n+                            repo_path=Path(\"/home/user/liger_kernels\"),\n+                            package_name=\"liger_kernels\",\n+                            layer_name=\"LigerRMSNorm\",\n+                        )\n+                    }\n+                }\n+            }\n+\n         that's compatible with the kernels library.\n \n         The device is inferred from the model's parameters if not provided.\n@@ -217,11 +264,17 @@ def create_compatible_mapping(self, model, compile=False):\n \n             if isinstance(kernel, str):\n                 repo_name = kernel\n-                add_to_mapping(layer_name, current_device, repo_name, mode, compatible_mapping)\n+                if not self.use_local_kernel:\n+                    add_to_mapping(layer_name, current_device, repo_name, mode, compatible_mapping)\n+                else:\n+                    add_to_mapping_local(layer_name, current_device, repo_name, mode, compatible_mapping)\n             elif isinstance(kernel, dict):\n                 for device, repo_name in kernel.items():\n                     if device != current_device:\n                         continue\n-                    add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping)\n+                    if not self.use_local_kernel:\n+                        add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping)\n+                    else:\n+                        add_to_mapping_local(layer_name, device, repo_name, mode, compatible_mapping)\n \n         self.kernel_mapping = compatible_mapping"
        }
    ],
    "stats": {
        "total": 93,
        "additions": 74,
        "deletions": 19
    }
}