{
    "author": "Cyrilvallez",
    "message": "[loading] Re-add and improve disk offloading support (#42242)\n\n* unskip tests\n\n* first shot\n\n* offload in safetensors format\n\n* remove hard-coded value\n\n* update error\n\n* typo\n\n* fix\n\n* update test\n\n* fix\n\n* return it\n\n* post rebase\n\n* improve var names\n\n* improve names\n\n* fix finally\n\n* comment\n\n* fix tests\n\n* fix\n\n* simplify\n\n* fix\n\n* doc\n\n* fix\n\n* remove additional tiying after rebase\n\n* update test function source\n\n* fix\n\n* post rebase\n\n* new renaming patterns\n\n* clear confusion about variable names\n\n* create cleaner function\n\n* better doc\n\n* other tests\n\n* remove skip\n\n* unskip other tests",
    "sha": "af6a36a34af2f463ad2be13987d3ddd9c0753887",
    "files": [
        {
            "sha": "fecaf6f39fd7c002a38682f46034ce1aab7d62ef",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 65,
            "deletions": 35,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -28,6 +28,7 @@\n \n import torch\n \n+from .integrations.accelerate import offload_weight\n from .integrations.tensor_parallel import ALL_PARALLEL_STYLES, DTensor, Replicate, TensorParallelLayer\n from .utils import is_torch_greater_or_equal, logging\n \n@@ -397,7 +398,7 @@ def dot_natural_key(s: str):\n \n @contextmanager\n def log_to_misc(\n-    layer_name: str,\n+    first_target_key: str,\n     misc: MutableMapping[str, str],\n     extras: Any = None,\n     op: Union[list[ConversionOps], ConversionOps, None] = None,\n@@ -421,22 +422,22 @@ def _format_op_name(curr_op: Union[list[ConversionOps], ConversionOps, None]) ->\n         if isinstance(extras, tuple) and len(extras) == 2:\n             values, target_keys = extras\n             descriptor = f\"{op_name} \" if op_name else \"\"\n-            misc[layer_name] = (\n+            misc[first_target_key] = (\n                 f\"{e}\\nError: {descriptor}on tensors destined for {target_keys}. Ckpt contains: {len(values)}\"\n             )\n         elif isinstance(extras, str):\n             suffix = f\" via {op_name}\" if op_name else \"\"\n-            misc[layer_name] = f\"{e}\\nError{suffix} when processing parameter {extras}\"\n+            misc[first_target_key] = f\"{e}\\nError{suffix} when processing parameter {extras}\"\n         elif extras is None and op_name:\n-            misc[layer_name] = f\"{op_name}: {e}\"\n+            misc[first_target_key] = f\"{op_name}: {e}\"\n         else:\n-            misc[layer_name] = f\"{extras} |Error: {e}\"\n+            misc[first_target_key] = f\"{extras} |Error: {e}\"\n         raise SkipLayer()\n \n \n def set_param_for_module(\n     model: PreTrainedModel,\n-    layer_name: str,\n+    target_name: str,\n     param_value: torch.Tensor,\n     mismatch_keys: MutableSet[tuple[str, torch.Size, torch.Size]],\n     missing_keys: MutableSet[str],\n@@ -445,17 +446,13 @@ def set_param_for_module(\n     distributed_operation: Optional[TensorParallelLayer],\n     hf_quantizer: HfQuantizer,\n ):\n-    with log_to_misc(layer_name, misc, layer_name):\n-        module_path, _, param_name = layer_name.rpartition(\".\")\n+    with log_to_misc(target_name, misc, target_name):\n+        module_path, _, param_name = target_name.rpartition(\".\")\n         module_obj = model.get_submodule(module_path) if module_path else model\n-        if isinstance(param_value, list):\n-            param_value = param_value[0]\n-        elif not isinstance(param_value, torch.nn.Parameter):\n-            param_value = param_value[...]\n \n         ref = getattr(module_obj, param_name)\n         if ref is None:\n-            unexpected_keys.add(layer_name)\n+            unexpected_keys.add(target_name)\n         else:\n             use_dtensor = hasattr(distributed_operation, \"use_dtensor\") and distributed_operation.use_dtensor\n             if not isinstance(param_value, torch.nn.Parameter):\n@@ -475,17 +472,35 @@ def set_param_for_module(\n                     param_value = torch.nn.Parameter(param_value, requires_grad=param_value.is_floating_point())\n \n             # Remove from missing keys (it's either mismatched, or all good)\n-            missing_keys.discard(layer_name)\n+            missing_keys.discard(target_name)\n             if ref is not None and ref.shape != param_value.shape and hf_quantizer is None:\n-                mismatch_keys.add((layer_name, param_value.shape, ref.shape))\n+                mismatch_keys.add((target_name, param_value.shape, ref.shape))\n                 module_obj.param_name._is_hf_initialized = False  # Needs to be initialized\n             else:\n-                param_value._is_hf_initialized = (\n-                    True  # super important otherwise _init_weight re-initi if bias is missing\n-                )\n+                # super important otherwise _init_weight will re-init the param\n+                param_value._is_hf_initialized = True\n                 setattr(module_obj, param_name, param_value)\n \n \n+def offload_and_maybe_resave_param(\n+    target_name: str,\n+    param: torch.Tensor,\n+    missing_keys: MutableSet[str],\n+    disk_offload_folder: str,\n+    disk_offload_index: dict,\n+    applied_ops: WeightConverter | WeightRenaming,\n+) -> dict:\n+    \"\"\"Takes care of correctly offloading `param`. If it's not already present in the `disk_offload_index`, or if any\n+    WeightConverter operations have been applied, it will resave the new parameter. Otherwise, it will use the original\n+    `disk_offload_index` for this given param.\"\"\"\n+    # We need to remove from missing keys\n+    missing_keys.discard(target_name)\n+    # If not already offloaded, or if we applied any special Operation except Renaming, we need to re-save\n+    if target_name not in disk_offload_index or isinstance(applied_ops, WeightConverter):\n+        disk_offload_index = offload_weight(param, target_name, disk_offload_folder, disk_offload_index)\n+    return disk_offload_index\n+\n+\n class SkipLayer(Exception):\n     \"\"\"Control-flow sentinel: abort processing of the current layer only.\"\"\"\n \n@@ -521,6 +536,8 @@ def convert_and_load_state_dict_in_model(\n     device_map: dict | None = None,\n     dtype_plan: dict | None = None,\n     device_mesh: torch.distributed.device_mesh.DeviceMesh | None = None,\n+    disk_offload_index: dict | None = None,\n+    disk_offload_folder: str | None = None,\n ):\n     r\"\"\"\n     We build a mapping from the keys obtained by renaming each of the checkpoint keys according to the weight_mapping rules.\n@@ -612,6 +629,7 @@ def convert_and_load_state_dict_in_model(\n     prefix = model.base_model_prefix\n     tp_plan = tp_plan or {}\n     device_map = device_map or {\"\": \"cpu\"}\n+    # Here, we first sort by number of submodules, then length of the full string, to make sure to match correctly\n     device_map_regex = re.compile(\n         \"|\".join(rf\"({k})\" for k in sorted(device_map.keys(), key=lambda x: (x.count(\".\"), len(x)), reverse=True))\n     )\n@@ -708,9 +726,11 @@ def convert_and_load_state_dict_in_model(\n                         shard_index,\n                     )\n \n-            if future is None:  # TODO handle disk offload\n+            if future is None:\n                 device_match = device_map_regex.match(renamed_key)\n                 param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+                # If disk, we need to materialize on cpu first\n+                param_device = \"cpu\" if param_device == \"disk\" else param_device\n                 future = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n \n             mapping.add_tensor(renamed_key, original_key, source_pattern, future)\n@@ -723,30 +743,40 @@ def convert_and_load_state_dict_in_model(\n \n     total_entries = len(param_name_to_load)\n     with logging.tqdm(total=total_entries, desc=\"Loading weights\") as pbar:\n-        for layer_name, mapping in param_name_to_load.items():\n+        for first_param_name, mapping in param_name_to_load.items():\n             pbar.update(1)\n-            pbar.set_postfix({\"Materializing param\": layer_name})\n+            pbar.set_postfix({\"Materializing param\": first_param_name})\n             pbar.refresh()\n             try:\n                 realized_value, misc = mapping.convert(\n-                    layer_name, config=model.config, quantizer=hf_quantizer, missing_keys=missing_keys\n+                    first_param_name, config=model.config, quantizer=hf_quantizer, missing_keys=missing_keys\n                 )\n-                for k, output_value in realized_value.items():\n-                    set_param_for_module(\n-                        model,\n-                        k,\n-                        output_value,\n-                        mismatch_keys,\n-                        missing_keys,\n-                        misc,\n-                        unexpected_keys,\n-                        mapping.distributed_operation,\n-                        hf_quantizer,\n-                    )\n+                for target_name, param in realized_value.items():\n+                    param = param[0] if isinstance(param, list) else param\n+                    device_match = device_map_regex.match(target_name)\n+                    param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+                    # Offloading support\n+                    if param_device == \"disk\":\n+                        disk_offload_index = offload_and_maybe_resave_param(\n+                            target_name, param, missing_keys, disk_offload_folder, disk_offload_index, mapping\n+                        )\n+                    else:\n+                        set_param_for_module(\n+                            model,\n+                            target_name,\n+                            param,\n+                            mismatch_keys,\n+                            missing_keys,\n+                            misc,\n+                            unexpected_keys,\n+                            mapping.distributed_operation,\n+                            hf_quantizer,\n+                        )\n             except SkipLayer:\n                 continue\n+\n     thread_pool.shutdown(wait=False)\n-    return missing_keys, unexpected_keys, mismatch_keys, misc\n+    return missing_keys, unexpected_keys, mismatch_keys, disk_offload_index, misc\n \n \n # TODO this is not done yet!"
        },
        {
            "sha": "f201ae3970bebc5db6a22a76be1d747283fd2a2e",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 77,
            "deletions": 41,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -19,10 +19,14 @@\n import copy\n import inspect\n import os\n+import re\n from collections import OrderedDict, defaultdict\n from contextlib import contextmanager\n from typing import TYPE_CHECKING, Optional, Union\n \n+from safetensors import safe_open\n+from safetensors.torch import save_file\n+\n from ..utils import (\n     is_accelerate_available,\n     is_torch_available,\n@@ -445,71 +449,103 @@ def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload\n         dispatch_model(model, **device_map_kwargs)\n \n \n-def get_disk_only_shard_files(device_map, weight_map):\n-    \"\"\"\n-    Returns the list of shard files containing only weights offloaded to disk.\n-    \"\"\"\n-    files_content = defaultdict(list)\n-    for weight_name, filename in weight_map.items():\n-        while len(weight_name) > 0 and weight_name not in device_map:\n-            weight_name = \".\".join(weight_name.split(\".\")[:-1])\n-        files_content[filename].append(device_map[weight_name])\n-\n-    return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n-\n-\n def expand_device_map(device_map, param_names):\n     \"\"\"\n     Expand a device map to return the correspondence parameter name to device.\n     \"\"\"\n+    # Here, we first sort by number of submodules, then length of the full string, to make sure to match correctly\n+    device_map_regex = re.compile(\n+        \"|\".join(rf\"({k})\" for k in sorted(device_map.keys(), key=lambda x: (x.count(\".\"), len(x)), reverse=True))\n+    )\n     new_device_map = {}\n-    for module, device in device_map.items():\n-        new_device_map.update(\n-            {p: device for p in param_names if p == module or p.startswith(f\"{module}.\") or module == \"\"}\n-        )\n+    for param in param_names:\n+        device_match = device_map_regex.match(param)\n+        new_device_map[param] = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+\n     return new_device_map\n \n \n def accelerate_disk_offload(\n-    disk_offload_folder,\n-    checkpoint_files,\n-    device_map,\n-    checkpoint_keys,\n-    sharded_metadata,\n-    dtype,\n+    disk_offload_folder: str | None,\n+    checkpoint_files: list[str] | None,\n+    device_map: dict,\n+    expected_keys: list[str],\n+    sharded_metadata: dict | None,\n+    dtype: torch.dtype | None,\n+    weight_mapping=None,\n ):\n-    disk_only_shard_files = []\n+    \"\"\"\n+    Prepare the `disk_offload_index` that will be used for reading offloaded parameters. If reading from a safetensors\n+    file, parameters which do not need any special WeightConverter operation during loading (i.e. they are used as-is, or only\n+    renamed) will be mapped to where they already reside on disk. Otherwise, the parameters will be resaved inside\n+    `disk_offload_folder` during loading.\n+    \"\"\"\n+    from ..core_model_loading import WeightRenaming, build_glob_alternation, repl\n+\n     if disk_offload_folder is not None:\n         os.makedirs(disk_offload_folder, exist_ok=True)\n     is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n-    if disk_offload_folder is None and not is_offloaded_safetensors:\n-        raise ValueError(\n-            \"The current `device_map` had weights offloaded to the disk. Please provide an `offload_folder`\"\n-            \" for them. Alternatively, make sure you have `safetensors` installed if the model you are using\"\n-            \" offers the weights in this format.\"\n-        )\n+\n+    rename = False\n+    if weight_mapping is not None:\n+        renamings = [entry for entry in weight_mapping if isinstance(entry, WeightRenaming)]\n+        if len(renamings) > 0:\n+            rename = True\n+            rename_alt, _, rename_by_group = build_glob_alternation(renamings)\n+\n+    # In this case, the offload index is simply the existing safetensors (except if using custom weight loading\n+    # Operation, e.g. the MoE models, where we need to resave the weights that were changed at loading time)\n     if is_offloaded_safetensors:\n-        param_device_map = expand_device_map(device_map, checkpoint_keys)\n+        param_device_map = expand_device_map(device_map, expected_keys)\n         str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n         if sharded_metadata is None:\n-            weight_map = dict.fromkeys(checkpoint_keys, checkpoint_files[0])\n+            weight_map = dict.fromkeys(safe_open(checkpoint_files[0], framework=\"pt\").keys(), checkpoint_files[0])\n         else:\n             folder = os.path.sep.join(checkpoint_files[0].split(os.path.sep)[:-1])\n-            weight_map = {k: os.path.join(folder, v) for k, v in weight_map.items()}\n-            # Find potential checkpoints containing only offloaded weights\n-            disk_only_shard_files = get_disk_only_shard_files(device_map, weight_map)\n+            weight_map = {k: os.path.join(folder, v) for k, v in sharded_metadata[\"weight_map\"].items()}\n+\n+        # Update the weight names according to the `weight_mapping`\n+        weight_renaming_map = {\n+            rename_alt.sub(lambda m: repl(m, rename_by_group), k).replace(\"\\\\\", \"\") if rename else k: k\n+            for k in weight_map\n+        }\n+\n+        # Prepare the index using existing safetensors files\n         disk_offload_index = {\n-            name: {\n-                \"safetensors_file\": file,\n-                \"weight_name\": name,\n+            target_name: {\n+                \"safetensors_file\": weight_map[source_name],\n+                \"weight_name\": source_name,\n                 \"dtype\": str_dtype,\n             }\n-            for name, file in weight_map.items()\n-            if param_device_map[name] == \"disk\"\n+            for target_name, source_name in weight_renaming_map.items()\n+            # Need to check if it's in the mapping in case of unexpected keys that would result in KeyError (we skip them)\n+            if target_name in param_device_map and param_device_map[target_name] == \"disk\"\n         }\n+    # In this case we will resave every offloaded weight\n     else:\n         disk_offload_index = {}\n-    return disk_offload_index, disk_only_shard_files, is_offloaded_safetensors\n+\n+    return disk_offload_index\n+\n+\n+def offload_weight(weight: torch.Tensor, weight_name: str, offload_folder: str | None, offload_index: dict) -> dict:\n+    \"\"\"Write `weight` to disk inside `offload_folder`, and update `offload_index` accordingly. Everything is\n+    saved in `safetensors` format.\"\"\"\n+\n+    if offload_folder is None:\n+        raise ValueError(\n+            \"The current `device_map` had weights offloaded to the disk, which needed to be re-saved. This is either \"\n+            \"because the weights are not in `safetensors` format, or because the model uses an internal weight format \"\n+            \"different than the one saved (i.e. most MoE models). Please provide an `offload_folder` for them in \"\n+            \"`from_pretrained`.\"\n+        )\n+    # Write the weight to disk\n+    safetensor_file = os.path.join(offload_folder, f\"{weight_name}.safetensors\")\n+    save_file({weight_name: weight}, safetensor_file)\n+    # Update the offloading index\n+    str_dtype = str(weight.dtype).replace(\"torch.\", \"\")\n+    offload_index[weight_name] = {\"safetensors_file\": safetensor_file, \"weight_name\": weight_name, \"dtype\": str_dtype}\n+    return offload_index\n \n \n def _init_infer_auto_device_map("
        },
        {
            "sha": "e1df5de3ae3575a68890000ac38256bc654bf030",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 31,
            "deletions": 14,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -59,6 +59,7 @@\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n from .integrations.accelerate import (\n     _get_device_map,\n+    accelerate_disk_offload,\n     accelerate_dispatch,\n     check_and_set_device_map,\n     expand_device_map,\n@@ -132,9 +133,7 @@\n \n if is_accelerate_available():\n     from accelerate.hooks import add_hook_to_module\n-    from accelerate.utils import (\n-        extract_model_from_parallel,\n-    )\n+    from accelerate.utils import extract_model_from_parallel\n     from accelerate.utils.modeling import get_state_dict_from_offload\n \n \n@@ -4073,6 +4072,20 @@ def _load_pretrained_model(\n         if logger.level >= logging.WARNING:\n             verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n \n+        # This offload index if for params explicitly on the \"disk\" in the device_map\n+        disk_offload_index = None\n+        # Prepare parameters offloading if needed\n+        if device_map is not None and \"disk\" in device_map.values():\n+            disk_offload_index = accelerate_disk_offload(\n+                disk_offload_folder,\n+                checkpoint_files,\n+                device_map,\n+                expected_keys,\n+                sharded_metadata,\n+                dtype,\n+                weight_mapping,\n+            )\n+\n         # Warmup cuda to load the weights much faster on devices\n         if device_map is not None and not is_hqq_or_quark:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n@@ -4111,16 +4124,20 @@ def _load_pretrained_model(\n             else:\n                 raise ValueError(\"Neither a state dict nor checkpoint files were found.\")\n \n-            missing_keys, unexpected_keys, mismatched_keys, misc = convert_and_load_state_dict_in_model(\n-                model,\n-                merged_state_dict,\n-                weight_mapping,\n-                tp_plan,\n-                hf_quantizer,\n-                dtype,\n-                device_map,\n-                model.dtype_plan,\n-                device_mesh,\n+            missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, misc = (\n+                convert_and_load_state_dict_in_model(\n+                    model,\n+                    merged_state_dict,\n+                    weight_mapping,\n+                    tp_plan,\n+                    hf_quantizer,\n+                    dtype,\n+                    device_map,\n+                    model.dtype_plan,\n+                    device_mesh,\n+                    disk_offload_index,\n+                    disk_offload_folder,\n+                )\n             )\n \n             # finally close all opened file pointers\n@@ -4184,7 +4201,7 @@ def _load_pretrained_model(\n             misc=misc,\n             ignore_mismatched_sizes=ignore_mismatched_sizes,\n         )\n-        disk_offload_index = None\n+\n         return model, missing_keys, unexpected_keys, mismatched_keys, disk_offload_index, error_msgs\n \n     def retrieve_modules_from_names(self, names, add_prefix=False, remove_prefix=False):"
        },
        {
            "sha": "4095e19c00d0c1b4dc7f006fe64fa375690adeca",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -167,6 +167,7 @@ class DeepseekVLHybridModelTest(ModelTesterMixin, GenerationTesterMixin, unittes\n         else {}\n     )\n     _is_composite = True\n+    model_split_percents = [0.5, 0.85, 0.9]  # it tries to offload everything with the default value\n \n     def setUp(self):\n         self.model_tester = DeepseekVLHybridModelTester(self)"
        },
        {
            "sha": "526e81e297664b2e5b211b951188023ee5aca4b8",
            "filename": "tests/models/glm4_moe/test_modeling_glm4_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4_moe%2Ftest_modeling_glm4_moe.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -61,6 +61,7 @@ class Glm4MoeModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = Glm4MoeModelTester\n     # used in `test_torch_compile_for_training`. Skip as \"Dynamic control flow in MoE\"\n     _torch_compile_train_cls = None\n+    model_split_percents = [0.5, 0.85, 0.9]  # it tries to offload everything with the default value\n \n \n @require_torch_accelerator"
        },
        {
            "sha": "e48717283f3ea63762710a93a8ca3013a56d2852",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -105,18 +105,13 @@\n     CONFIG_NAME,\n     GENERATION_CONFIG_NAME,\n     SAFE_WEIGHTS_NAME,\n-    is_accelerate_available,\n     is_torch_bf16_available_on_device,\n     is_torch_fp16_available_on_device,\n )\n \n from .generation.test_utils import GenerationTesterMixin\n \n \n-if is_accelerate_available():\n-    from accelerate.utils import compute_module_sizes\n-\n-\n if is_torch_available():\n     import torch\n     from safetensors import safe_open\n@@ -125,6 +120,7 @@\n     from torch import nn\n \n     from transformers import MODEL_MAPPING\n+    from transformers.integrations.accelerate import compute_module_sizes\n     from transformers.integrations.tensor_parallel import _get_parameter_tp_plan\n     from transformers.modeling_utils import load_state_dict\n     from transformers.pytorch_utils import id_tensor_storage\n@@ -2357,7 +2353,6 @@ def check_device_map_is_respected(self, model, device_map):\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_accelerator\n-    @unittest.skip(\"# TODO @CyrilVallez fix this in the other PR\")\n     def test_disk_offload_bin(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -2371,20 +2366,22 @@ def test_disk_offload_bin(self):\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n \n-            model_size = compute_module_sizes(model)[\"\"]\n+            model_size = compute_module_sizes(model)[0][\"\"]\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.cpu().save_pretrained(tmp_dir, safe_serialization=False)\n \n                 with self.assertRaises(ValueError):\n                     max_size = int(self.model_split_percents[0] * model_size)\n                     max_memory = {0: max_size, \"cpu\": max_size}\n                     # This errors out cause it's missing an offload folder\n-                    new_model = model_class.from_pretrained(tmp_dir, device_map=\"auto\", max_memory=max_memory)\n+                    new_model = model_class.from_pretrained(\n+                        tmp_dir, device_map=\"auto\", max_memory=max_memory, use_safetensors=False\n+                    )\n \n                 max_size = int(self.model_split_percents[1] * model_size)\n                 max_memory = {0: max_size, \"cpu\": max_size}\n                 new_model = model_class.from_pretrained(\n-                    tmp_dir, device_map=\"auto\", max_memory=max_memory, offload_folder=tmp_dir\n+                    tmp_dir, device_map=\"auto\", max_memory=max_memory, offload_folder=tmp_dir, use_safetensors=False\n                 )\n \n                 self.check_device_map_is_respected(new_model, new_model.hf_device_map)\n@@ -2402,7 +2399,6 @@ def test_disk_offload_bin(self):\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_accelerator\n-    @unittest.skip(\"# TODO @CyrilVallez fix this in the other PR\")\n     def test_disk_offload_safetensors(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -2416,7 +2412,7 @@ def test_disk_offload_safetensors(self):\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n \n-            model_size = compute_module_sizes(model)[\"\"]\n+            model_size = compute_module_sizes(model)[0][\"\"]\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.cpu().save_pretrained(tmp_dir)\n \n@@ -2441,7 +2437,6 @@ def test_disk_offload_safetensors(self):\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_accelerator\n-    @unittest.skip(\"# TODO @CyrilVallez fix this in the other PR\")\n     def test_cpu_offload(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -2456,7 +2451,7 @@ def test_cpu_offload(self):\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n \n-            model_size = compute_module_sizes(model)[\"\"]\n+            model_size = compute_module_sizes(model)[0][\"\"]\n             # We test several splits of sizes to make sure it works.\n             max_gpu_sizes = [int(p * model_size) for p in self.model_split_percents[1:]]\n             with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -2499,7 +2494,7 @@ def test_model_parallelism(self):\n             torch.manual_seed(0)\n             base_output = model(**inputs_dict_class)\n \n-            model_size = compute_module_sizes(model)[\"\"]\n+            model_size = compute_module_sizes(model)[0][\"\"]\n             # We test several splits of sizes to make sure it works.\n             max_gpu_sizes = [int(p * model_size) for p in self.model_split_percents[1:]]\n             with tempfile.TemporaryDirectory() as tmp_dir:"
        },
        {
            "sha": "8973e9900f0f7c0da25b2b3fc1dcf1fba9b34c9b",
            "filename": "tests/utils/test_core_model_loading.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Futils%2Ftest_core_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Futils%2Ftest_core_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_core_model_loading.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -250,7 +250,7 @@ def test_moe_and_qkv_conversion(self):\n             ),\n             WeightRenaming(\"mlp.w2.weight\", \"mlp.down_proj.weight\"),\n         ]\n-        missing, unexpected, mismatch, misc = convert_and_load_state_dict_in_model(\n+        missing, unexpected, mismatch, _, misc = convert_and_load_state_dict_in_model(\n             model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=None\n         )\n \n@@ -400,7 +400,7 @@ def __init__(self):\n             )\n         ]\n \n-        missing, unexpected, mismatch, misc = convert_and_load_state_dict_in_model(\n+        missing, unexpected, mismatch, _, misc = convert_and_load_state_dict_in_model(\n             model, state_dict, weight_mapping, tp_plan=None, hf_quantizer=quantizer\n         )\n "
        },
        {
            "sha": "69273e4c1ddd69554fa216a079d9a45eca706b6d",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6a36a34af2f463ad2be13987d3ddd9c0753887/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=af6a36a34af2f463ad2be13987d3ddd9c0753887",
            "patch": "@@ -1208,7 +1208,6 @@ def test_save_model_with_device_map_cpu(self):\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_accelerator\n-    @unittest.skip(\"TODO @cyrilvallez when saving\")\n     def test_save_offloaded_model(self):\n         device_map = {\n             \"transformer.wte\": f\"{torch_device}:0\",\n@@ -1246,7 +1245,6 @@ def test_save_offloaded_model(self):\n     @require_accelerate\n     @mark.accelerate_tests\n     @require_torch_accelerator\n-    @unittest.skip(\"TODO @cyrilvallez when saving\")\n     def test_save_offloaded_model_with_direct_params(self):\n         from accelerate import dispatch_model\n \n@@ -2057,7 +2055,6 @@ def test_ignore_missing_key_works(self):\n         for k, v in model.state_dict().items():\n             self.assertTrue(v.device.type == \"cpu\", f\"{k} is not on cpu!\")\n \n-    @unittest.skip(\"TODO fix offloaded in another PR @CyrilVallez\")\n     def test_device_map_works_with_unexpected_keys(self):\n         \"\"\"Test that if a parameter is specified in `_keys_to_ignore_on_load_unexpected` and is actually\n         present in the checkpoint, it will correctly be removed from the weights we load, especially those\n@@ -2081,7 +2078,6 @@ def test_device_map_works_with_unexpected_keys(self):\n         # Unexpected keys (mtp) should be removed from the state dict, therefore this should not error out.\n         BaseModelWithUnexpectedKeys.from_pretrained(temp.name, device_map={\"linear\": \"cpu\", \"linear_2\": \"disk\"})\n \n-    @unittest.skip(\"TODO fix offloaded in another PR @CyrilVallez\")\n     def test_device_map_works_with_unexpected_keys_sharded(self):\n         \"\"\"Test that if a parameter is specified in `_keys_to_ignore_on_load_unexpected` and is actually\n         present in the checkpoint, it will correctly be removed from the weights we load, especially those"
        }
    ],
    "stats": {
        "total": 296,
        "additions": 186,
        "deletions": 110
    }
}