{
    "author": "ahadnagy",
    "message": "Benchmarking improvements (#39768)\n\n* Start revamping benchmarking\n\n* Start refactoring benchmarking\n\n* Use Pandas for CSV\n\n* import fix\n\n* Remove benchmark files\n\n* Remove sample data\n\n* Address review comments",
    "sha": "29e4e35927711da5381ccf5b12d11ed5e2d6d5e7",
    "files": [
        {
            "sha": "2f3040f513f24ba696b7d457130dfbf0cb876b53",
            "filename": "benchmark/.gitignore",
            "status": "added",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2F.gitignore",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2F.gitignore",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2F.gitignore?ref=29e4e35927711da5381ccf5b12d11ed5e2d6d5e7",
            "patch": "@@ -0,0 +1 @@\n+benchmark_results/\n\\ No newline at end of file"
        },
        {
            "sha": "8d7e60fbfe3ca3dc6706891e6d6eb219751af2e0",
            "filename": "benchmark/benches/llama.py",
            "status": "added",
            "additions": 345,
            "deletions": 0,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Fbenches%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Fbenches%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenches%2Fllama.py?ref=29e4e35927711da5381ccf5b12d11ed5e2d6d5e7",
            "patch": "@@ -0,0 +1,345 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from logging import Logger\n+import os\n+from threading import Event, Thread\n+from time import perf_counter, sleep\n+from typing import Optional\n+import sys\n+\n+# Add the parent directory to Python path to import benchmarks_entrypoint\n+sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))\n+from benchmarks_entrypoint import MetricsRecorder\n+\n+import gpustat\n+import psutil\n+import psycopg2\n+\n+# Optional heavy ML dependencies - only required when actually running the benchmark\n+try:\n+    import torch\n+    from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, StaticCache\n+    TRANSFORMERS_AVAILABLE = True\n+except ImportError:\n+    TRANSFORMERS_AVAILABLE = False\n+    torch = None\n+    AutoModelForCausalLM = None\n+    AutoTokenizer = None\n+    GenerationConfig = None\n+    StaticCache = None\n+\n+os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n+os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n+\n+# Only set torch precision if torch is available\n+if TRANSFORMERS_AVAILABLE:\n+    torch.set_float32_matmul_precision(\"high\")\n+\n+\n+def collect_metrics(benchmark_id, continue_metric_collection, metrics_recorder):\n+    p = psutil.Process(os.getpid())\n+    while not continue_metric_collection.is_set():\n+        with p.oneshot():\n+            cpu_util = p.cpu_percent()\n+            mem_megabytes = p.memory_info().rss / (1024 * 1024)\n+        gpu_stats = gpustat.GPUStatCollection.new_query()\n+        gpu_util = gpu_stats[0][\"utilization.gpu\"]\n+        gpu_mem_megabytes = gpu_stats[0][\"memory.used\"]\n+        metrics_recorder.collect_device_measurements(\n+            benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes\n+        )\n+        sleep(0.01)\n+\n+\n+def run_benchmark(\n+    logger: Logger, repository: str, branch: str, commit_id: str, commit_msg: str, metrics_recorder=None, num_tokens_to_generate=100\n+):\n+    # Check if required ML dependencies are available\n+    if not TRANSFORMERS_AVAILABLE:\n+        logger.error(\"Transformers and torch are required to run the LLaMA benchmark. Please install them with:\")\n+        logger.error(\"pip install torch transformers\")\n+        logger.error(\"Skipping LLaMA benchmark due to missing dependencies.\")\n+        return\n+    \n+    continue_metric_collection = Event()\n+    metrics_thread = None\n+    model_id = \"meta-llama/Llama-2-7b-hf\"\n+    \n+    # If no metrics_recorder is provided, create one for backward compatibility\n+    if metrics_recorder is None:\n+        try:\n+            metrics_recorder = MetricsRecorder(\n+                psycopg2.connect(\"dbname=metrics\"), logger, repository, branch, commit_id, commit_msg, True\n+            )\n+            should_close_recorder = True\n+        except Exception as e:\n+            logger.error(f\"Failed to create metrics recorder: {e}\")\n+            return\n+    else:\n+        should_close_recorder = False\n+    try:\n+        gpu_stats = gpustat.GPUStatCollection.new_query()\n+        gpu_name = gpu_stats[0][\"name\"]\n+        benchmark_id = metrics_recorder.initialise_benchmark({\"gpu_name\": gpu_name, \"model_id\": model_id})\n+        logger.info(f\"running benchmark #{benchmark_id} on {gpu_name} for {model_id}\")\n+        metrics_thread = Thread(\n+            target=collect_metrics,\n+            args=[benchmark_id, continue_metric_collection, metrics_recorder],\n+        )\n+        metrics_thread.start()\n+        logger.info(\"started background thread to fetch device metrics\")\n+\n+        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # silence warnings when compiling\n+\n+        device = \"cuda\"\n+\n+        logger.info(\"downloading weights\")\n+        # This is to avoid counting download in model load time measurement\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n+        gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n+        logger.info(\"loading model\")\n+        start = perf_counter()\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, torch_dtype=torch.float16, generation_config=gen_config\n+        ).eval()\n+        model.to(device)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        model_load_time = end - start\n+        logger.info(f\"loaded model in: {model_load_time}s\")\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id)\n+\n+        prompt = \"Why dogs are so cute?\"\n+        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n+\n+        # Specify the max length (including both the prompt and the response)\n+        # When calling `generate` with `cache_implementation=\"static\" later, this is also used to create a `StaticCache` object\n+        # with sequence length = `max_length`. The longer the more you will re-use it\n+        seq_length = inputs[\"input_ids\"].shape[1]\n+        model.generation_config.max_length = seq_length + num_tokens_to_generate\n+        batch_size = inputs[\"input_ids\"].shape[0]\n+\n+        # Copied from the gpt-fast repo\n+        def multinomial_sample_one_no_sync(probs_sort):  # Does multinomial sampling without a cuda synchronization\n+            q = torch.empty_like(probs_sort).exponential_(1)\n+            return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n+\n+        def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n+            logits = logits / max(temperature, 1e-5)\n+\n+            if top_k is not None:\n+                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n+                pivot = v.select(-1, -1).unsqueeze(-1)\n+                logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n+            probs = torch.nn.functional.softmax(logits, dim=-1)\n+            return probs\n+\n+        def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n+            probs = logits_to_probs(logits[0, -1], temperature, top_k)\n+            idx_next = multinomial_sample_one_no_sync(probs)\n+            return idx_next, probs\n+\n+        # First eager forward pass\n+        logger.info(\"running first eager forward pass\")\n+        start = perf_counter()\n+        outputs = model(**inputs)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        first_eager_fwd_pass_time = end - start\n+        logger.info(f\"completed first eager forward pass in: {first_eager_fwd_pass_time}s\")\n+\n+        # Second eager forward pass (should be faster)\n+        logger.info(\"running second eager forward pass\")\n+        start = perf_counter()\n+        outputs = model(**inputs)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        second_eager_fwd_pass_time = end - start\n+        logger.info(f\"completed second eager forward pass in: {second_eager_fwd_pass_time}s\")\n+\n+        # First eager generation\n+        logger.info(\"running first eager generation\")\n+        start = perf_counter()\n+        output = model.generate(**inputs)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        first_eager_generate_time = end - start\n+        logger.info(f\"completed first eager generation in: {first_eager_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        # Second eager generation (should be faster)\n+        logger.info(\"running second eager generation\")\n+        start = perf_counter()\n+        output = model.generate(**inputs)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        second_eager_generate_time = end - start\n+        logger.info(f\"completed second eager generation in: {second_eager_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        logger.info(\"running generation timing loop\")\n+\n+        input_pos = torch.arange(0, seq_length, device=device)\n+        inputs = inputs[\"input_ids\"]\n+\n+        start = perf_counter()\n+        with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.MATH):\n+            logits = model(inputs, position_ids=input_pos).logits\n+        next_token, probs = sample(logits, temperature=0.6, top_k=5)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        time_to_first_token = end - start\n+\n+        input_pos = torch.tensor([seq_length], device=device, dtype=torch.int)\n+        next_token = next_token.clone()\n+        start = perf_counter()\n+        with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.MATH):\n+            logits = model(next_token, position_ids=input_pos).logits\n+        next_token, probs = sample(logits, temperature=0.6, top_k=5)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        time_to_second_token = end - start\n+\n+        input_pos = torch.tensor([seq_length + 1], device=device, dtype=torch.int)\n+        next_token = next_token.clone()\n+        start = perf_counter()\n+        with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.MATH):\n+            logits = model(next_token, position_ids=input_pos).logits\n+        next_token, probs = sample(logits, temperature=0.6, top_k=5)\n+        torch.cuda.synchronize()\n+        end = perf_counter()\n+        time_to_third_token = end - start\n+\n+        logger.info(\"running longer generation timing loop\")\n+\n+        total_time = 0\n+        for i in range(20):\n+            input_pos = torch.tensor([seq_length + 2 + i], device=device, dtype=torch.int)\n+            next_token = next_token.clone()\n+            start = perf_counter()\n+            with torch.nn.attention.sdpa_kernel(torch.nn.attention.SDPBackend.MATH):\n+                logits = model(next_token, position_ids=input_pos).logits\n+            next_token, probs = sample(logits, temperature=0.6, top_k=5)\n+            torch.cuda.synchronize()\n+            end = perf_counter()\n+            total_time += end - start\n+\n+        mean_time_to_next_token = total_time / 20\n+\n+        logger.info(\"running compilation benchmarks\")\n+\n+        # Now compile the model\n+        model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n+\n+        # StaticCache for generation\n+        with torch.device(device):\n+            model.setup_caches(max_batch_size=batch_size, max_seq_len=seq_length + num_tokens_to_generate)\n+\n+        input_pos = torch.arange(0, seq_length, device=device)\n+        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)[\"input_ids\"]\n+\n+        logger.info(\"compiling model\")\n+\n+        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, generation_config=gen_config)\n+        model.to(device)\n+        model = torch.compile(model, mode=\"max-autotune\", fullgraph=True)\n+\n+        past_key_values = StaticCache(\n+            model.config,\n+            max_batch_size=batch_size,\n+            device=device,\n+            dtype=torch.float16,\n+            max_cache_len=seq_length + 128,\n+        )\n+        # 1st call\n+        start = perf_counter()\n+        output = model.generate(**inputs, past_key_values=past_key_values)\n+        end = perf_counter()\n+        first_compile_generate_time = end - start\n+        logger.info(f\"completed first compile generation in: {first_compile_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        past_key_values = StaticCache(\n+            model.config,\n+            max_batch_size=batch_size,\n+            device=device,\n+            dtype=torch.float16,\n+            max_cache_len=seq_length + 128,\n+        )\n+        # 2nd call\n+        start = perf_counter()\n+        output = model.generate(**inputs, past_key_values=past_key_values)\n+        end = perf_counter()\n+        second_compile_generate_time = end - start\n+        logger.info(f\"completed second compile generation in: {second_compile_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        past_key_values = StaticCache(\n+            model.config,\n+            max_batch_size=batch_size,\n+            device=device,\n+            dtype=torch.float16,\n+            max_cache_len=seq_length + 128,\n+        )\n+        # 3rd call\n+        start = perf_counter()\n+        output = model.generate(**inputs, past_key_values=past_key_values)\n+        end = perf_counter()\n+        third_compile_generate_time = end - start\n+        logger.info(f\"completed third compile generation in: {third_compile_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        past_key_values = StaticCache(\n+            model.config,\n+            max_batch_size=batch_size,\n+            device=device,\n+            dtype=torch.float16,\n+            max_cache_len=seq_length + 128,\n+        )\n+        # 4th call\n+        start = perf_counter()\n+        output = model.generate(**inputs, past_key_values=past_key_values)\n+        end = perf_counter()\n+        fourth_compile_generate_time = end - start\n+        logger.info(f\"completed fourth compile generation in: {fourth_compile_generate_time}s\")\n+        logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n+\n+        metrics_recorder.collect_model_measurements(\n+            benchmark_id,\n+            {\n+                \"model_load_time\": model_load_time,\n+                \"first_eager_forward_pass_time_secs\": first_eager_fwd_pass_time,\n+                \"second_eager_forward_pass_time_secs\": second_eager_fwd_pass_time,\n+                \"first_eager_generate_time_secs\": first_eager_generate_time,\n+                \"second_eager_generate_time_secs\": second_eager_generate_time,\n+                \"time_to_first_token_secs\": time_to_first_token,\n+                \"time_to_second_token_secs\": time_to_second_token,\n+                \"time_to_third_token_secs\": time_to_third_token,\n+                \"time_to_next_token_mean_secs\": mean_time_to_next_token,\n+                \"first_compile_generate_time_secs\": first_compile_generate_time,\n+                \"second_compile_generate_time_secs\": second_compile_generate_time,\n+                \"third_compile_generate_time_secs\": third_compile_generate_time,\n+                \"fourth_compile_generate_time_secs\": fourth_compile_generate_time,\n+            },\n+        )\n+    except Exception as e:\n+        logger.error(f\"Caught exception: {e}\")\n+    continue_metric_collection.set()\n+    if metrics_thread is not None:\n+        metrics_thread.join()\n+    \n+    # Only close the recorder if we created it locally\n+    if should_close_recorder:\n+        metrics_recorder.close() \n\\ No newline at end of file"
        },
        {
            "sha": "929fe64288d8fdd0cc10ba175dc14a2f27bdd53d",
            "filename": "benchmark/benchmarks_entrypoint.py",
            "status": "modified",
            "additions": 371,
            "deletions": 54,
            "changes": 425,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Fbenchmarks_entrypoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Fbenchmarks_entrypoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmarks_entrypoint.py?ref=29e4e35927711da5381ccf5b12d11ed5e2d6d5e7",
            "patch": "@@ -1,15 +1,35 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n import argparse\n import importlib.util\n import logging\n import os\n import sys\n-from typing import Dict, Tuple\n+import json\n+import uuid\n+from datetime import datetime\n+from typing import Dict, Tuple, Optional, List\n \n-from psycopg2.extensions import register_adapter\n-from psycopg2.extras import Json\n+import pandas as pd\n \n-\n-register_adapter(dict, Json)\n+try:\n+    from psycopg2.extensions import register_adapter\n+    from psycopg2.extras import Json\n+    register_adapter(dict, Json)\n+    PSYCOPG2_AVAILABLE = True\n+except ImportError:\n+    PSYCOPG2_AVAILABLE = False\n \n \n class ImportModuleException(Exception):\n@@ -18,61 +38,239 @@ class ImportModuleException(Exception):\n \n class MetricsRecorder:\n     def __init__(\n-        self, connection, logger: logging.Logger, repository: str, branch: str, commit_id: str, commit_msg: str\n+        self, connection, logger: logging.Logger, repository: str, branch: str, commit_id: str, commit_msg: str, \n+        collect_csv_data: bool = True\n     ):\n         self.conn = connection\n-        self.conn.autocommit = True\n+        self.use_database = connection is not None\n+        if self.use_database:\n+            self.conn.autocommit = True\n         self.logger = logger\n         self.repository = repository\n         self.branch = branch\n         self.commit_id = commit_id\n         self.commit_msg = commit_msg\n+        self.collect_csv_data = collect_csv_data\n+        \n+        # For CSV export - store all data in pandas DataFrames (only if CSV collection is enabled)\n+        if self.collect_csv_data:\n+            # Initialize empty DataFrames with proper schemas\n+            self.benchmarks_df = pd.DataFrame(columns=[\n+                'benchmark_id', 'repository', 'branch', 'commit_id', 'commit_message', \n+                'metadata', 'created_at'\n+            ])\n+            self.device_measurements_df = pd.DataFrame(columns=[\n+                'benchmark_id', 'cpu_util', 'mem_megabytes', 'gpu_util', \n+                'gpu_mem_megabytes', 'time'\n+            ])\n+            self.model_measurements_df = pd.DataFrame(columns=[\n+                'benchmark_id', 'time', 'model_load_time', 'first_eager_forward_pass_time_secs',\n+                'second_eager_forward_pass_time_secs', 'first_eager_generate_time_secs',\n+                'second_eager_generate_time_secs', 'time_to_first_token_secs',\n+                'time_to_second_token_secs', 'time_to_third_token_secs',\n+                'time_to_next_token_mean_secs', 'first_compile_generate_time_secs',\n+                'second_compile_generate_time_secs', 'third_compile_generate_time_secs',\n+                'fourth_compile_generate_time_secs'\n+            ])\n+        else:\n+            self.benchmarks_df = None\n+            self.device_measurements_df = None\n+            self.model_measurements_df = None\n \n-    def initialise_benchmark(self, metadata: dict[str, str]) -> int:\n+    def initialise_benchmark(self, metadata: dict[str, str]) -> str:\n         \"\"\"\n-        Creates a new benchmark, returns the benchmark id\n+        Creates a new benchmark, returns the benchmark id (UUID)\n         \"\"\"\n-        # gpu_name: str, model_id: str\n-        with self.conn.cursor() as cur:\n-            cur.execute(\n-                \"INSERT INTO benchmarks (repository, branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s, %s) RETURNING benchmark_id\",\n-                (self.repository, self.branch, self.commit_id, self.commit_msg, metadata),\n-            )\n-            benchmark_id = cur.fetchone()[0]\n-            logger.debug(f\"initialised benchmark #{benchmark_id}\")\n-            return benchmark_id\n-\n-    def collect_device_measurements(self, benchmark_id: int, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes):\n+        # Generate a unique UUID for this benchmark\n+        benchmark_id = str(uuid.uuid4())\n+        \n+        if self.use_database:\n+            with self.conn.cursor() as cur:\n+                cur.execute(\n+                    \"INSERT INTO benchmarks (benchmark_id, repository, branch, commit_id, commit_message, metadata) VALUES (%s, %s, %s, %s, %s, %s)\",\n+                    (benchmark_id, self.repository, self.branch, self.commit_id, self.commit_msg, metadata),\n+                )\n+                self.logger.debug(f\"initialised benchmark #{benchmark_id}\")\n+        \n+        # Store benchmark data for CSV export (if enabled)\n+        if self.collect_csv_data:\n+            # Add row to pandas DataFrame\n+            new_row = pd.DataFrame([{\n+                'benchmark_id': benchmark_id,\n+                'repository': self.repository,\n+                'branch': self.branch,\n+                'commit_id': self.commit_id,\n+                'commit_message': self.commit_msg,\n+                'metadata': json.dumps(metadata),\n+                'created_at': datetime.utcnow().isoformat()\n+            }])\n+            self.benchmarks_df = pd.concat([self.benchmarks_df, new_row], ignore_index=True)\n+            \n+        mode_info = []\n+        if self.use_database:\n+            mode_info.append(\"database\")\n+        if self.collect_csv_data:\n+            mode_info.append(\"CSV\")\n+        mode_str = \" + \".join(mode_info) if mode_info else \"no storage\"\n+        \n+        self.logger.debug(f\"initialised benchmark #{benchmark_id} ({mode_str} mode)\")\n+        return benchmark_id\n+\n+    def collect_device_measurements(self, benchmark_id: str, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes):\n         \"\"\"\n         Collect device metrics, such as CPU & GPU usage. These are \"static\", as in you cannot pass arbitrary arguments to the function.\n         \"\"\"\n-        with self.conn.cursor() as cur:\n-            cur.execute(\n-                \"INSERT INTO device_measurements (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes) VALUES (%s, %s, %s, %s, %s)\",\n-                (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes),\n-            )\n+        # Store device measurements for CSV export (if enabled)\n+        if self.collect_csv_data:\n+            # Add row to pandas DataFrame\n+            new_row = pd.DataFrame([{\n+                'benchmark_id': benchmark_id,\n+                'cpu_util': cpu_util,\n+                'mem_megabytes': mem_megabytes,\n+                'gpu_util': gpu_util,\n+                'gpu_mem_megabytes': gpu_mem_megabytes,\n+                'time': datetime.utcnow().isoformat()\n+            }])\n+            self.device_measurements_df = pd.concat([self.device_measurements_df, new_row], ignore_index=True)\n+        \n+        # Store in database if available\n+        if self.use_database:\n+            with self.conn.cursor() as cur:\n+                cur.execute(\n+                    \"INSERT INTO device_measurements (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes) VALUES (%s, %s, %s, %s, %s)\",\n+                    (benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes),\n+                )\n+            \n         self.logger.debug(\n-            f\"inserted device measurements for benchmark #{benchmark_id} [CPU util: {cpu_util}, mem MBs: {mem_megabytes}, GPU util: {gpu_util}, GPU mem MBs: {gpu_mem_megabytes}]\"\n+            f\"collected device measurements for benchmark #{benchmark_id} [CPU util: {cpu_util}, mem MBs: {mem_megabytes}, GPU util: {gpu_util}, GPU mem MBs: {gpu_mem_megabytes}]\"\n         )\n \n-    def collect_model_measurements(self, benchmark_id: int, measurements: dict[str, float]):\n-        with self.conn.cursor() as cur:\n-            cur.execute(\n-                \"\"\"\n-                INSERT INTO model_measurements (\n-                    benchmark_id,\n-                    measurements\n-                ) VALUES (%s, %s)\n-                \"\"\",\n-                (\n-                    benchmark_id,\n-                    measurements,\n-                ),\n-            )\n-        self.logger.debug(f\"inserted model measurements for benchmark #{benchmark_id}: {measurements}\")\n+    def collect_model_measurements(self, benchmark_id: str, measurements: dict[str, float]):\n+        # Store model measurements for CSV export (if enabled)\n+        if self.collect_csv_data:\n+            # Add row to pandas DataFrame with flattened measurements\n+            row_data = {\n+                'benchmark_id': benchmark_id,\n+                'time': datetime.utcnow().isoformat()\n+            }\n+            # Flatten the measurements dict into the row\n+            row_data.update(measurements)\n+            \n+            new_row = pd.DataFrame([row_data])\n+            self.model_measurements_df = pd.concat([self.model_measurements_df, new_row], ignore_index=True)\n+        \n+        # Store in database if available\n+        if self.use_database:\n+            with self.conn.cursor() as cur:\n+                cur.execute(\n+                    \"\"\"\n+                    INSERT INTO model_measurements (\n+                        benchmark_id,\n+                        measurements\n+                    ) VALUES (%s, %s)\n+                    \"\"\",\n+                    (\n+                        benchmark_id,\n+                        measurements,\n+                    ),\n+                )\n+            \n+        self.logger.debug(f\"collected model measurements for benchmark #{benchmark_id}: {measurements}\")\n+\n+    def export_to_csv(self, output_dir: str = \"benchmark_results\"):\n+        \"\"\"\n+        Export all collected data to CSV files using pandas DataFrames\n+        \"\"\"\n+        if not self.collect_csv_data:\n+            self.logger.warning(\"CSV data collection is disabled - no CSV files will be generated\")\n+            return\n+            \n+        if not os.path.exists(output_dir):\n+            os.makedirs(output_dir)\n+            self.logger.info(f\"Created output directory: {output_dir}\")\n+            \n+        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n+        files_created = []\n+        \n+        # Export using pandas DataFrames\n+        self._export_pandas_data(output_dir, timestamp, files_created)\n+        \n+        self.logger.info(f\"CSV export complete! Created {len(files_created)} files in {output_dir}\")\n+    \n+    def _export_pandas_data(self, output_dir: str, timestamp: str, files_created: list):\n+        \"\"\"\n+        Export CSV files using pandas DataFrames\n+        \"\"\"\n+        # Export benchmarks\n+        benchmarks_file = os.path.join(output_dir, f\"benchmarks_{timestamp}.csv\")\n+        self.benchmarks_df.to_csv(benchmarks_file, index=False)\n+        files_created.append(benchmarks_file)\n+        self.logger.info(f\"Exported {len(self.benchmarks_df)} benchmark records to {benchmarks_file}\")\n+        \n+        # Export device measurements  \n+        device_file = os.path.join(output_dir, f\"device_measurements_{timestamp}.csv\")\n+        self.device_measurements_df.to_csv(device_file, index=False)\n+        files_created.append(device_file)\n+        self.logger.info(f\"Exported {len(self.device_measurements_df)} device measurement records to {device_file}\")\n+        \n+        # Export model measurements (already flattened)\n+        model_file = os.path.join(output_dir, f\"model_measurements_{timestamp}.csv\")\n+        self.model_measurements_df.to_csv(model_file, index=False)\n+        files_created.append(model_file)\n+        self.logger.info(f\"Exported {len(self.model_measurements_df)} model measurement records to {model_file}\")\n+        \n+        # Create comprehensive summary using pandas operations\n+        summary_file = os.path.join(output_dir, f\"benchmark_summary_{timestamp}.csv\")\n+        self._create_summary(summary_file)\n+        files_created.append(summary_file)\n+    \n+    def _create_summary(self, summary_file: str):\n+        \"\"\"\n+        Create a comprehensive summary CSV using pandas operations\n+        \"\"\"\n+        if len(self.benchmarks_df) == 0:\n+            # Create empty summary file\n+            summary_df = pd.DataFrame()\n+            summary_df.to_csv(summary_file, index=False)\n+            self.logger.info(f\"Created empty benchmark summary at {summary_file}\")\n+            return\n+        \n+        # Start with benchmarks as the base\n+        summary_df = self.benchmarks_df.copy()\n+        \n+        # Add model measurements (join on benchmark_id)\n+        if len(self.model_measurements_df) > 0:\n+            # Drop 'time' column from model measurements to avoid conflicts\n+            model_df = self.model_measurements_df.drop(columns=['time'], errors='ignore')\n+            summary_df = summary_df.merge(model_df, on='benchmark_id', how='left')\n+        \n+        # Calculate device measurement aggregates using pandas groupby\n+        if len(self.device_measurements_df) > 0:\n+            device_agg = self.device_measurements_df.groupby('benchmark_id').agg({\n+                'cpu_util': ['mean', 'max', 'std', 'count'],\n+                'mem_megabytes': ['mean', 'max', 'std'],\n+                'gpu_util': ['mean', 'max', 'std'],\n+                'gpu_mem_megabytes': ['mean', 'max', 'std']\n+            }).round(3)\n+            \n+            # Flatten column names\n+            device_agg.columns = [f\"{col[0]}_{col[1]}\" for col in device_agg.columns]\n+            device_agg = device_agg.reset_index()\n+            \n+            # Rename count column to be more descriptive\n+            if 'cpu_util_count' in device_agg.columns:\n+                device_agg = device_agg.rename(columns={'cpu_util_count': 'device_measurement_count'})\n+            \n+            # Merge with summary\n+            summary_df = summary_df.merge(device_agg, on='benchmark_id', how='left')\n+        \n+        # Export the comprehensive summary\n+        summary_df.to_csv(summary_file, index=False)\n+        self.logger.info(f\"Created comprehensive benchmark summary with {len(summary_df)} records at {summary_file}\")\n \n     def close(self):\n-        self.conn.close()\n+        if self.use_database and self.conn:\n+            self.conn.close()\n \n \n logger = logging.getLogger(__name__)\n@@ -85,7 +283,7 @@ def close(self):\n logger.addHandler(handler)\n \n \n-def parse_arguments() -> tuple[str, str, str, str]:\n+def parse_arguments() -> tuple[str, str, str, str, bool, str]:\n     \"\"\"\n     Parse command line arguments for the benchmarking CLI.\n     \"\"\"\n@@ -114,10 +312,27 @@ def parse_arguments() -> tuple[str, str, str, str]:\n         type=str,\n         help=\"The commit message associated with the commit, truncated to 70 characters.\",\n     )\n+    \n+    parser.add_argument(\n+        \"--csv\",\n+        action=\"store_true\",\n+        default=False,\n+        help=\"Enable CSV output files generation.\"\n+    )\n+    \n+    parser.add_argument(\n+        \"--csv-output-dir\",\n+        type=str,\n+        default=\"benchmark_results\",\n+        help=\"Directory for CSV output files (default: benchmark_results).\"\n+    )\n \n     args = parser.parse_args()\n+    \n+    # CSV is disabled by default, only enabled when --csv is used\n+    generate_csv = args.csv\n \n-    return args.repository, args.branch, args.commit_id, args.commit_msg\n+    return args.repository, args.branch, args.commit_id, args.commit_msg, generate_csv, args.csv_output_dir\n \n \n def import_from_path(module_name, file_path):\n@@ -131,22 +346,124 @@ def import_from_path(module_name, file_path):\n         raise ImportModuleException(f\"failed to load python module: {e}\")\n \n \n+def create_database_connection():\n+    \"\"\"\n+    Try to create a database connection. Returns None if connection fails.\n+    \"\"\"\n+    if not PSYCOPG2_AVAILABLE:\n+        logger.warning(\"psycopg2 not available - running in CSV-only mode\")\n+        return None\n+        \n+    try:\n+        import psycopg2\n+        conn = psycopg2.connect(\"dbname=metrics\")\n+        logger.info(\"Successfully connected to database\")\n+        return conn\n+    except Exception as e:\n+        logger.warning(f\"Failed to connect to database: {e}. Running in CSV-only mode\")\n+        return None\n+\n+\n+def create_global_metrics_recorder(repository: str, branch: str, commit_id: str, commit_msg: str, \n+                                   generate_csv: bool = False) -> MetricsRecorder:\n+    \"\"\"\n+    Create a global metrics recorder that will be used across all benchmarks.\n+    \"\"\"\n+    connection = create_database_connection()\n+    recorder = MetricsRecorder(connection, logger, repository, branch, commit_id, commit_msg, generate_csv)\n+    \n+    # Log the storage mode\n+    storage_modes = []\n+    if connection is not None:\n+        storage_modes.append(\"database\")\n+    if generate_csv:\n+        storage_modes.append(\"CSV\")\n+    \n+    if not storage_modes:\n+        logger.warning(\"Running benchmarks with NO data storage (no database connection, CSV disabled)\")\n+        logger.warning(\"Use --csv flag to enable CSV output when database is unavailable\")\n+    else:\n+        logger.info(f\"Running benchmarks with: {' + '.join(storage_modes)} storage\")\n+    \n+    return recorder\n+\n+\n if __name__ == \"__main__\":\n     benchmarks_folder_path = os.path.dirname(os.path.realpath(__file__))\n+    benches_folder_path = os.path.join(benchmarks_folder_path, \"benches\")\n \n-    repository, branch, commit_id, commit_msg = parse_arguments()\n-\n-    for entry in os.scandir(benchmarks_folder_path):\n-        try:\n+    repository, branch, commit_id, commit_msg, generate_csv, csv_output_dir = parse_arguments()\n+    \n+    # Create a global metrics recorder\n+    global_metrics_recorder = create_global_metrics_recorder(repository, branch, commit_id, commit_msg, generate_csv)\n+    \n+    successful_benchmarks = 0\n+    failed_benchmarks = 0\n+    \n+    # Automatically discover all benchmark modules in benches/ folder\n+    benchmark_modules = []\n+    \n+    if os.path.exists(benches_folder_path):\n+        logger.debug(f\"Scanning for benchmarks in: {benches_folder_path}\")\n+        for entry in os.scandir(benches_folder_path):\n             if not entry.name.endswith(\".py\"):\n                 continue\n-            if entry.path == __file__:\n+            if entry.name.startswith(\"__\"):  # Skip __init__.py, __pycache__, etc.\n                 continue\n-            logger.debug(f\"loading: {entry.name}\")\n-            module = import_from_path(entry.name.split(\".\")[0], entry.path)\n-            logger.info(f\"running benchmarks in: {entry.name}\")\n-            module.run_benchmark(logger, repository, branch, commit_id, commit_msg)\n+                \n+            # Check if the file has a run_benchmark function\n+            try:\n+                logger.debug(f\"checking if benches/{entry.name} has run_benchmark function\")\n+                module = import_from_path(entry.name.split(\".\")[0], entry.path)\n+                if hasattr(module, 'run_benchmark'):\n+                    benchmark_modules.append(entry.name)\n+                    logger.debug(f\"discovered benchmark: {entry.name}\")\n+                else:\n+                    logger.debug(f\"skipping {entry.name} - no run_benchmark function found\")\n+            except Exception as e:\n+                logger.debug(f\"failed to check benches/{entry.name}: {e}\")\n+    else:\n+        logger.warning(f\"Benches directory not found: {benches_folder_path}\")\n+\n+    if benchmark_modules:\n+        logger.info(f\"Discovered {len(benchmark_modules)} benchmark(s): {benchmark_modules}\")\n+    else:\n+        logger.warning(\"No benchmark modules found in benches/ directory\")\n+\n+    for module_name in benchmark_modules:\n+        module_path = os.path.join(benches_folder_path, module_name)\n+        try:\n+            logger.debug(f\"loading: {module_name}\")\n+            module = import_from_path(module_name.split(\".\")[0], module_path)\n+            logger.info(f\"running benchmarks in: {module_name}\")\n+            \n+            # Check if the module has an updated run_benchmark function that accepts metrics_recorder\n+            try:\n+                # Try the new signature first\n+                module.run_benchmark(logger, repository, branch, commit_id, commit_msg, global_metrics_recorder)\n+            except TypeError:\n+                # Fall back to the old signature for backward compatibility\n+                logger.warning(f\"Module {module_name} using old run_benchmark signature - database connection will be created per module\")\n+                module.run_benchmark(logger, repository, branch, commit_id, commit_msg)\n+            \n+            successful_benchmarks += 1\n         except ImportModuleException as e:\n             logger.error(e)\n+            failed_benchmarks += 1\n         except Exception as e:\n-            logger.error(f\"error running benchmarks for {entry.name}: {e}\")\n+            logger.error(f\"error running benchmarks for {module_name}: {e}\")\n+            failed_benchmarks += 1\n+\n+    # Export CSV results at the end (if enabled)\n+    try:\n+        if generate_csv:\n+            global_metrics_recorder.export_to_csv(csv_output_dir)\n+            logger.info(f\"CSV reports have been generated and saved to the {csv_output_dir} directory\")\n+        else:\n+            logger.info(\"CSV generation disabled - no CSV files created (use --csv to enable)\")\n+        \n+        logger.info(f\"Benchmark run completed. Successful: {successful_benchmarks}, Failed: {failed_benchmarks}\")\n+    except Exception as e:\n+        logger.error(f\"Failed to export CSV results: {e}\")\n+    finally:\n+        global_metrics_recorder.close()"
        },
        {
            "sha": "9a575177d72dc82dbbea73d6b3a7cdaec11de26e",
            "filename": "benchmark/init_db.sql",
            "status": "removed",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/de437d0d7ac30beb44b196e1413544122df64152/benchmark%2Finit_db.sql",
            "raw_url": "https://github.com/huggingface/transformers/raw/de437d0d7ac30beb44b196e1413544122df64152/benchmark%2Finit_db.sql",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Finit_db.sql?ref=de437d0d7ac30beb44b196e1413544122df64152",
            "patch": "@@ -1,34 +0,0 @@\n-CREATE TABLE IF NOT EXISTS benchmarks (\n-  benchmark_id SERIAL PRIMARY KEY,\n-  repository VARCHAR(255),\n-  branch VARCHAR(255),\n-  commit_id VARCHAR(72),\n-  commit_message VARCHAR(70),\n-  metadata jsonb,\n-  created_at timestamp without time zone NOT NULL DEFAULT (current_timestamp AT TIME ZONE 'UTC')\n-);\n-\n-CREATE INDEX IF NOT EXISTS benchmarks_benchmark_id_idx ON benchmarks (benchmark_id);\n-\n-CREATE INDEX IF NOT EXISTS benchmarks_branch_idx ON benchmarks (branch);\n-\n-CREATE TABLE IF NOT EXISTS device_measurements (\n-  measurement_id SERIAL PRIMARY KEY,\n-  benchmark_id int REFERENCES benchmarks (benchmark_id),\n-  cpu_util double precision,\n-  mem_megabytes double precision,\n-  gpu_util double precision,\n-  gpu_mem_megabytes double precision,\n-  time timestamp without time zone NOT NULL DEFAULT (current_timestamp AT TIME ZONE 'UTC')\n-);\n-\n-CREATE INDEX IF NOT EXISTS device_measurements_branch_idx ON device_measurements (benchmark_id);\n-\n-CREATE TABLE IF NOT EXISTS model_measurements (\n-  measurement_id SERIAL PRIMARY KEY,\n-  benchmark_id int REFERENCES benchmarks (benchmark_id),\n-  measurements jsonb,\n-  time timestamp without time zone NOT NULL DEFAULT (current_timestamp AT TIME ZONE 'UTC')\n-);\n-\n-CREATE INDEX IF NOT EXISTS model_measurements_branch_idx ON model_measurements (benchmark_id);"
        },
        {
            "sha": "bc60454e0af34e7bc4735e1f4bab8248c0d9f493",
            "filename": "benchmark/llama.py",
            "status": "removed",
            "additions": 0,
            "deletions": 346,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/de437d0d7ac30beb44b196e1413544122df64152/benchmark%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de437d0d7ac30beb44b196e1413544122df64152/benchmark%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fllama.py?ref=de437d0d7ac30beb44b196e1413544122df64152",
            "patch": "@@ -1,346 +0,0 @@\n-from logging import Logger\n-import os\n-from threading import Event, Thread\n-from time import perf_counter, sleep\n-from typing import Optional\n-from benchmarks_entrypoint import MetricsRecorder\n-import gpustat\n-import psutil\n-import psycopg2\n-import torch\n-\n-from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig, StaticCache\n-\n-\n-os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n-\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"1\"\n-torch.set_float32_matmul_precision(\"high\")\n-\n-\n-def collect_metrics(benchmark_id, continue_metric_collection, metrics_recorder):\n-    p = psutil.Process(os.getpid())\n-    while not continue_metric_collection.is_set():\n-        with p.oneshot():\n-            cpu_util = p.cpu_percent()\n-            mem_megabytes = p.memory_info().rss / (1024 * 1024)\n-        gpu_stats = gpustat.GPUStatCollection.new_query()\n-        gpu_util = gpu_stats[0][\"utilization.gpu\"]\n-        gpu_mem_megabytes = gpu_stats[0][\"memory.used\"]\n-        metrics_recorder.collect_device_measurements(\n-            benchmark_id, cpu_util, mem_megabytes, gpu_util, gpu_mem_megabytes\n-        )\n-        sleep(0.01)\n-\n-\n-def run_benchmark(\n-    logger: Logger, repository: str, branch: str, commit_id: str, commit_msg: str, num_tokens_to_generate=100\n-):\n-    continue_metric_collection = Event()\n-    metrics_thread = None\n-    model_id = \"meta-llama/Llama-2-7b-hf\"\n-    metrics_recorder = MetricsRecorder(\n-        psycopg2.connect(\"dbname=metrics\"), logger, repository, branch, commit_id, commit_msg\n-    )\n-    try:\n-        gpu_stats = gpustat.GPUStatCollection.new_query()\n-        gpu_name = gpu_stats[0][\"name\"]\n-        benchmark_id = metrics_recorder.initialise_benchmark({\"gpu_name\": gpu_name, \"model_id\": model_id})\n-        logger.info(f\"running benchmark #{benchmark_id} on {gpu_name} for {model_id}\")\n-        metrics_thread = Thread(\n-            target=collect_metrics,\n-            args=[benchmark_id, continue_metric_collection, metrics_recorder],\n-        )\n-        metrics_thread.start()\n-        logger.info(\"started background thread to fetch device metrics\")\n-\n-        os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # silence warnings when compiling\n-\n-        device = \"cuda\"\n-\n-        logger.info(\"downloading weights\")\n-        # This is to avoid counting download in model load time measurement\n-        model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n-        gen_config = GenerationConfig(do_sample=False, top_p=1, temperature=1)\n-        logger.info(\"loading model\")\n-        start = perf_counter()\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, torch_dtype=torch.float16, generation_config=gen_config\n-        ).eval()\n-        model.to(device)\n-        torch.cuda.synchronize()\n-        end = perf_counter()\n-        model_load_time = end - start\n-        logger.info(f\"loaded model in: {model_load_time}s\")\n-\n-        tokenizer = AutoTokenizer.from_pretrained(model_id)\n-\n-        prompt = \"Why dogs are so cute?\"\n-        inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n-\n-        # Specify the max length (including both the prompt and the response)\n-        # When calling `generate` with `cache_implementation=\"static\" later, this is also used to create a `StaticCache` object\n-        # with sequence length = `max_length`. The longer the more you will re-use it\n-        seq_length = inputs[\"input_ids\"].shape[1]\n-        model.generation_config.max_length = seq_length + num_tokens_to_generate\n-        batch_size = inputs[\"input_ids\"].shape[0]\n-\n-        # Copied from the gpt-fast repo\n-        def multinomial_sample_one_no_sync(probs_sort):  # Does multinomial sampling without a cuda synchronization\n-            q = torch.empty_like(probs_sort).exponential_(1)\n-            return torch.argmax(probs_sort / q, dim=-1, keepdim=True).to(dtype=torch.int)\n-\n-        def logits_to_probs(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n-            logits = logits / max(temperature, 1e-5)\n-\n-            if top_k is not None:\n-                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n-                pivot = v.select(-1, -1).unsqueeze(-1)\n-                logits = torch.where(logits < pivot, -float(\"Inf\"), logits)\n-            probs = torch.nn.functional.softmax(logits, dim=-1)\n-            return probs\n-\n-        def sample(logits, temperature: float = 1.0, top_k: Optional[int] = None):\n-            probs = logits_to_probs(logits[:, -1], temperature, top_k)\n-            idx_next = multinomial_sample_one_no_sync(probs)\n-            return idx_next, probs\n-\n-        def decode_one_token(model, cur_token, cache_position, past_key_values):\n-            logits = model(\n-                cur_token,\n-                cache_position=cache_position,\n-                past_key_values=past_key_values,\n-                return_dict=False,\n-                use_cache=True,\n-            )[0]\n-            new_token = sample(logits, temperature=0.6, top_k=5)[0]\n-            return new_token\n-\n-        #########\n-        # Eager #\n-        #########\n-        with torch.no_grad():\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + num_tokens_to_generate,\n-            )\n-            cache_position = torch.arange(seq_length, device=device)\n-            start = perf_counter()\n-            model(\n-                **inputs,\n-                cache_position=cache_position,\n-                past_key_values=past_key_values,\n-                return_dict=False,\n-                use_cache=True,\n-            )\n-            end = perf_counter()\n-            first_eager_fwd_pass_time = end - start\n-            logger.info(f\"completed first eager fwd pass in: {first_eager_fwd_pass_time}s\")\n-            start = perf_counter()\n-            output = model.generate(**inputs, do_sample=False)\n-            end = perf_counter()\n-            first_eager_generate_time = end - start\n-            logger.info(f\"completed first eager generation in: {first_eager_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + num_tokens_to_generate,\n-            )\n-            cache_position = torch.arange(seq_length, device=device)\n-            start = perf_counter()\n-            model(\n-                **inputs,\n-                cache_position=cache_position,\n-                past_key_values=past_key_values,\n-                return_dict=False,\n-                use_cache=True,\n-            )\n-            end = perf_counter()\n-            second_eager_fwd_pass_time = end - start\n-            logger.info(f\"completed second eager fwd pass in: {second_eager_fwd_pass_time}s\")\n-            start = perf_counter()\n-            model.generate(**inputs, do_sample=False)\n-            end = perf_counter()\n-            second_eager_generate_time = end - start\n-            logger.info(f\"completed second eager generation in: {second_eager_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-            torch.compiler.reset()\n-\n-            ################\n-            # Forward pass #\n-            ################\n-\n-            # `torch.compile(model, ...)` is not recommended as you compile callbacks\n-            # and full generate. We recommend compiling only the forward for now.\n-            # \"reduce-overhead\" will use cudagraphs.\n-            generated_ids = torch.zeros(\n-                (batch_size, num_tokens_to_generate + seq_length), dtype=torch.int, device=device\n-            )\n-\n-            generated_ids[:, :seq_length] = inputs[\"input_ids\"]\n-            decode_one_token = torch.compile(decode_one_token, mode=\"reduce-overhead\", fullgraph=True)\n-            # model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n-            # TODO use  decode_one_token(model, input_id.clone(), cache_position) for verification\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + num_tokens_to_generate + 10,\n-            )\n-            cache_position = torch.arange(seq_length, device=device)\n-            all_generated_tokens = []\n-            ### First compile, prefill\n-            start = perf_counter()\n-            next_token = decode_one_token(\n-                model, inputs[\"input_ids\"], cache_position=cache_position, past_key_values=past_key_values\n-            )\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            time_to_first_token = end - start\n-            logger.info(f\"completed first compile generation in: {time_to_first_token}s\")\n-            cache_position += 1\n-            all_generated_tokens += next_token.tolist()\n-\n-            cache_position = torch.tensor([seq_length], device=device)\n-            ### First compile, decoding\n-            start = perf_counter()\n-            next_token = decode_one_token(\n-                model, next_token.clone(), cache_position=cache_position, past_key_values=past_key_values\n-            )\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            time_to_second_token = end - start\n-            logger.info(f\"completed second compile generation in: {time_to_second_token}s\")\n-            cache_position += 1\n-            all_generated_tokens += next_token.tolist()\n-\n-            ### Second compile, decoding\n-            start = perf_counter()\n-            next_token = decode_one_token(\n-                model, next_token.clone(), cache_position=cache_position, past_key_values=past_key_values\n-            )\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            time_to_third_token = end - start\n-            logger.info(f\"completed third compile forward in: {time_to_third_token}s\")\n-            cache_position += 1\n-            all_generated_tokens += next_token.tolist()\n-\n-            ### Using cuda graphs decoding\n-\n-            start = perf_counter()\n-            for _ in range(1, num_tokens_to_generate):\n-                all_generated_tokens += next_token.tolist()\n-                next_token = decode_one_token(\n-                    model, next_token.clone(), cache_position=cache_position, past_key_values=past_key_values\n-                )\n-                cache_position += 1\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            mean_time_to_next_token = (end - start) / num_tokens_to_generate\n-            logger.info(f\"completed next compile generation in: {mean_time_to_next_token}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(all_generated_tokens)}\")\n-\n-            ####################\n-            # Generate compile #\n-            ####################\n-            torch.compiler.reset()\n-            # we will not compile full generate as it' s to intensive, tho we measure full forward!\n-\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + 128,\n-            )\n-\n-            # 1st call\n-            start = perf_counter()\n-            output = model.generate(**inputs, past_key_values=past_key_values)\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            first_compile_generate_time = end - start\n-            logger.info(f\"completed first compile generation in: {first_compile_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + 128,\n-            )\n-            # 2nd call\n-            start = perf_counter()\n-            output = model.generate(**inputs, past_key_values=past_key_values)\n-            torch.cuda.synchronize()\n-            end = perf_counter()\n-            second_compile_generate_time = end - start\n-            logger.info(f\"completed second compile generation in: {second_compile_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + 128,\n-            )\n-\n-            # 3rd call\n-            start = perf_counter()\n-            output = model.generate(**inputs, past_key_values=past_key_values)\n-            end = perf_counter()\n-            third_compile_generate_time = end - start\n-            logger.info(f\"completed third compile generation in: {third_compile_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-            past_key_values = StaticCache(\n-                model.config,\n-                max_batch_size=batch_size,\n-                device=device,\n-                dtype=torch.float16,\n-                max_cache_len=seq_length + 128,\n-            )\n-            # 4th call\n-            start = perf_counter()\n-            output = model.generate(**inputs, past_key_values=past_key_values)\n-            end = perf_counter()\n-            fourth_compile_generate_time = end - start\n-            logger.info(f\"completed fourth compile generation in: {fourth_compile_generate_time}s\")\n-            logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n-\n-        metrics_recorder.collect_model_measurements(\n-            benchmark_id,\n-            {\n-                \"model_load_time\": model_load_time,\n-                \"first_eager_forward_pass_time_secs\": first_eager_fwd_pass_time,\n-                \"second_eager_forward_pass_time_secs\": second_eager_fwd_pass_time,\n-                \"first_eager_generate_time_secs\": first_eager_generate_time,\n-                \"second_eager_generate_time_secs\": second_eager_generate_time,\n-                \"time_to_first_token_secs\": time_to_first_token,\n-                \"time_to_second_token_secs\": time_to_second_token,\n-                \"time_to_third_token_secs\": time_to_third_token,\n-                \"time_to_next_token_mean_secs\": mean_time_to_next_token,\n-                \"first_compile_generate_time_secs\": first_compile_generate_time,\n-                \"second_compile_generate_time_secs\": second_compile_generate_time,\n-                \"third_compile_generate_time_secs\": third_compile_generate_time,\n-                \"fourth_compile_generate_time_secs\": fourth_compile_generate_time,\n-            },\n-        )\n-    except Exception as e:\n-        logger.error(f\"Caught exception: {e}\")\n-    continue_metric_collection.set()\n-    if metrics_thread is not None:\n-        metrics_thread.join()\n-    metrics_recorder.close()"
        },
        {
            "sha": "5b0d7e406993cc0cc8c06bd3b41ba3f9d8ddd610",
            "filename": "benchmark/requirements.txt",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Frequirements.txt?ref=29e4e35927711da5381ccf5b12d11ed5e2d6d5e7",
            "patch": "@@ -2,4 +2,5 @@ gpustat==1.1.1\n psutil==6.0.0\n psycopg2==2.9.9\n torch>=2.4.0\n-hf_transfer\n\\ No newline at end of file\n+hf_transfer\n+pandas>=1.5.0\n\\ No newline at end of file"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "benchmark/utils/init_db.sql",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Futils%2Finit_db.sql",
            "raw_url": "https://github.com/huggingface/transformers/raw/29e4e35927711da5381ccf5b12d11ed5e2d6d5e7/benchmark%2Futils%2Finit_db.sql",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Futils%2Finit_db.sql?ref=29e4e35927711da5381ccf5b12d11ed5e2d6d5e7"
        }
    ],
    "stats": {
        "total": 1154,
        "additions": 719,
        "deletions": 435
    }
}