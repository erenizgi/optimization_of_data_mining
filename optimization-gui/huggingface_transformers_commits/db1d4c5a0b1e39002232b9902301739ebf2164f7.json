{
    "author": "Cyrilvallez",
    "message": "Loading optimizations (#36742)\n\n* improvements\n\n* Update modeling_utils.py\n\n* add some doc about loading\n\n* Update modeling_utils.py",
    "sha": "db1d4c5a0b1e39002232b9902301739ebf2164f7",
    "files": [
        {
            "sha": "d4c9815bd342a0c9bec026590ab391e359e36f51",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 23,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/db1d4c5a0b1e39002232b9902301739ebf2164f7/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/db1d4c5a0b1e39002232b9902301739ebf2164f7/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=db1d4c5a0b1e39002232b9902301739ebf2164f7",
            "patch": "@@ -4824,11 +4824,10 @@ def _load_pretrained_model(\n         # Warmup cuda to load the weights much faster on devices\n         if device_map is not None and hf_quantizer is None:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n-            caching_allocator_warmup(model_to_load, expanded_device_map, dtype)\n+            caching_allocator_warmup(model_to_load, expanded_device_map)\n \n         error_msgs = []\n         mismatched_keys = []\n-        has_multiple_shards = len(checkpoint_files) > 1\n         # Iterate on all the shards to load the weights\n         for shard_file in checkpoint_files:\n             # Skip the load for shards that only contain disk-offloaded weights\n@@ -4865,7 +4864,7 @@ def _load_pretrained_model(\n                 prefix if loading_base_model_from_task_state_dict else \"\",\n             )\n \n-            if low_cpu_mem_usage and shard_file is not None:\n+            if low_cpu_mem_usage:\n                 # Skip it with fsdp on ranks other than 0\n                 if not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n                     disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n@@ -4893,10 +4892,8 @@ def _load_pretrained_model(\n                 else:\n                     model_to_load.load_state_dict(state_dict, strict=False, assign=assign_params)\n \n+            # force memory release if loading multiple shards, to avoid having 2 state dicts in memory in next loop\n             del state_dict\n-            # force memory release if loading multiple shards\n-            if has_multiple_shards:\n-                gc.collect()\n \n         # Adjust offloaded weights name and save if needed\n         if disk_offload_index is not None and len(disk_offload_index) > 0:\n@@ -5789,11 +5786,24 @@ def expand_device_map(device_map, param_names):\n     return new_device_map\n \n \n-def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict, dtype: torch.dtype) -> Dict:\n+def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict):\n     \"\"\"This function warm-ups the caching allocator based on the size of the model tensors that will reside on each\n     device. It allows to have one large call to Malloc, instead of recursively calling it later when loading\n     the model, which is actually the loading speed botteneck.\n     Calling this function allows to cut the model loading time by a very large margin.\n+\n+    A few facts related to loading speed (taking into account the use of this function):\n+    - When loading a model the first time, it is usually slower than the subsequent times, because the OS is very likely\n+    to cache the different state dicts (if enough ressources/RAM are available)\n+    - Trying to force the OS to cache the files in advance (by e.g. accessing a small portion of them) is really hard,\n+    and not a good idea in general as this is low level OS optimizations that depend on ressource usage anyway\n+    - As of 18/03/2025, loading a Llama 70B model with TP takes ~1 min without file cache, and ~13s with full file cache.\n+    The baseline, i.e. only loading the tensor shards on device and adjusting dtype (i.e. copying them) is ~5s with full cache.\n+    These numbers are reported for TP on 4 H100 GPUs.\n+    - It is useless to pre-allocate more than the model size in this function (i.e. using an `allocation_factor` > 1) as\n+    cudaMalloc is not a bottleneck at all anymore\n+    - Loading speed bottleneck is now almost only tensor copy (i.e. changing the dtype) and moving the tensors to the devices.\n+    However, we cannot really improve on those aspects obviously, as the data needs to be moved/copied in the end.\n     \"\"\"\n     # Remove disk and cpu devices, and cast to proper torch.device\n     accelerator_device_map = {\n@@ -5808,31 +5818,26 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: Dict,\n         else None\n     )\n \n-    parameter_count = defaultdict(lambda: 0)\n-    allocation_factor = 1\n-    if torch.distributed.is_initialized() or len(set(accelerator_device_map.values())) >= 2:\n-        allocation_factor = 2\n-\n+    total_byte_count = defaultdict(lambda: 0)\n     for param_name, device in accelerator_device_map.items():\n         param = model.get_parameter_or_buffer(param_name)\n-        param_size = int(math.prod(param.shape) * allocation_factor)\n+        # The dtype of different parameters may be different with composite models or `keep_in_fp32_modules`\n+        param_byte_count = math.prod(param.shape) * dtype_byte_size(param.dtype)\n \n         if tp_plan_regex is not None:\n             generic_name = re.sub(r\"\\.\\d+\\.\", \".*.\", param_name)\n-            param_size //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n-\n-        parameter_count[device] += param_size\n+            param_byte_count //= torch.distributed.get_world_size() if tp_plan_regex.search(generic_name) else 1\n \n-    dtype = dtype if dtype is not None else torch.float32\n+        total_byte_count[device] += param_byte_count\n \n     # This will kick off the caching allocator to avoid having to Malloc afterwards\n-    for device, param_count in parameter_count.items():\n-        max_memory_device = None\n+    for device, byte_count in total_byte_count.items():\n         if device.type == \"cuda\":\n-            max_memory_device = torch.cuda.mem_get_info(device.index)[0]\n-        # allocate only if we have enough memory\n-        if max_memory_device is None or max_memory_device > param_count * dtype_byte_size(dtype):\n-            _ = torch.empty(param_count, dtype=dtype, device=device, requires_grad=False)\n+            device_memory = torch.cuda.mem_get_info(device)[0]\n+            # Allow up to 95% of max device memory\n+            byte_count = min(byte_count, int(0.95 * device_memory))\n+        # Allocate memory\n+        _ = torch.empty(byte_count // 2, dtype=torch.float16, device=device, requires_grad=False)\n \n \n def get_disk_only_shard_files(device_map, weight_map):"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 28,
        "deletions": 23
    }
}