{
    "author": "yonigozlan",
    "message": "Uniformize OwlViT and Owlv2 processors (#35700)\n\n* uniformize owlvit processor\r\n\r\n* uniformize owlv2\r\n\r\n* nit\r\n\r\n* add positional arg test owlvit\r\n\r\n* run-slow: owlvit, owlv2\r\n\r\n* run-slow: owlvit, owlv2\r\n\r\n* remove one letter variable",
    "sha": "336dc69d63d56f232a183a3e7f52790429b871ef",
    "files": [
        {
            "sha": "664c63ffee0ecce2adaf04ab364e21c8679a37b7",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 76,
            "deletions": 34,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/336dc69d63d56f232a183a3e7f52790429b871ef/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=336dc69d63d56f232a183a3e7f52790429b871ef",
            "patch": "@@ -21,15 +21,40 @@\n \n import numpy as np\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n \n \n if TYPE_CHECKING:\n     from .modeling_owlv2 import Owlv2ImageGuidedObjectDetectionOutput, Owlv2ObjectDetectionOutput\n \n \n+class Owlv2ImagesKwargs(ImagesKwargs, total=False):\n+    query_images: Optional[ImageInput]\n+\n+\n+class Owlv2ProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Owlv2ImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": \"max_length\",\n+        },\n+        \"images_kwargs\": {},\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"np\",\n+        },\n+    }\n+\n+\n class Owlv2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs an Owlv2 processor which wraps [`Owlv2ImageProcessor`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`] into\n@@ -46,12 +71,27 @@ class Owlv2Processor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"Owlv2ImageProcessor\"\n     tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n+    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n+    optional_call_args = [\"query_images\"]\n \n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OwlViT->Owlv2\n-    def __call__(self, text=None, images=None, query_images=None, padding=\"max_length\", return_tensors=\"np\", **kwargs):\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        # The following is to capture `query_images` argument that may be passed as a positional argument.\n+        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n+        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n+        # This behavior is only needed for backward compatibility and will be removed in future versions.\n+        #\n+        *args,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Owlv2ProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n@@ -60,14 +100,14 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`,\n             `List[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             query_images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The query image to be prepared, one query image is expected per target image to be queried. Each image\n                 can be a PIL image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image\n@@ -78,36 +118,49 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return NumPy `np.ndarray` objects.\n                 - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n         Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n               `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **query_pixel_values** -- Pixel values of the query images to be fed to a model. Returned when `query_images` is not `None`.\n         \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            Owlv2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+            **self.prepare_and_validate_optional_call_args(*args),\n+        )\n+        query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n+        return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n \n         if text is None and query_images is None and images is None:\n             raise ValueError(\n                 \"You have to specify at least one text or query image or image. All three cannot be none.\"\n             )\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n \n+        data = {}\n         if text is not None:\n             if isinstance(text, str) or (isinstance(text, List) and not isinstance(text[0], List)):\n-                encodings = [self.tokenizer(text, padding=padding, return_tensors=return_tensors, **kwargs)]\n+                encodings = [self.tokenizer(text, **output_kwargs[\"text_kwargs\"])]\n \n             elif isinstance(text, List) and isinstance(text[0], List):\n                 encodings = []\n \n                 # Maximum number of queries across batch\n-                max_num_queries = max([len(t) for t in text])\n+                max_num_queries = max([len(text_single) for text_single in text])\n \n                 # Pad all batch samples to max number of text queries\n-                for t in text:\n-                    if len(t) != max_num_queries:\n-                        t = t + [\" \"] * (max_num_queries - len(t))\n+                for text_single in text:\n+                    if len(text_single) != max_num_queries:\n+                        text_single = text_single + [\" \"] * (max_num_queries - len(text_single))\n \n-                    encoding = self.tokenizer(t, padding=padding, return_tensors=return_tensors, **kwargs)\n+                    encoding = self.tokenizer(text_single, **output_kwargs[\"text_kwargs\"])\n                     encodings.append(encoding)\n             else:\n                 raise TypeError(\"Input text should be a string, a list of strings or a nested list of strings\")\n@@ -137,30 +190,19 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n             else:\n                 raise ValueError(\"Target return tensor type could not be returned\")\n \n-            encoding = BatchEncoding()\n-            encoding[\"input_ids\"] = input_ids\n-            encoding[\"attention_mask\"] = attention_mask\n+            data[\"input_ids\"] = input_ids\n+            data[\"attention_mask\"] = attention_mask\n \n         if query_images is not None:\n-            encoding = BatchEncoding()\n-            query_pixel_values = self.image_processor(\n-                query_images, return_tensors=return_tensors, **kwargs\n-            ).pixel_values\n-            encoding[\"query_pixel_values\"] = query_pixel_values\n+            query_pixel_values = self.image_processor(query_images, **output_kwargs[\"images_kwargs\"]).pixel_values\n+            # Query images always override the text prompt\n+            data = {\"query_pixel_values\": query_pixel_values}\n \n         if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif query_images is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None or query_images is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            data[\"pixel_values\"] = image_features.pixel_values\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.post_process_object_detection with OwlViT->Owlv2\n     def post_process_object_detection(self, *args, **kwargs):"
        },
        {
            "sha": "98c24747b468d0b0db1d822ae454115fdc29a96c",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 76,
            "deletions": 34,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/336dc69d63d56f232a183a3e7f52790429b871ef/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=336dc69d63d56f232a183a3e7f52790429b871ef",
            "patch": "@@ -21,15 +21,40 @@\n \n import numpy as np\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from ...image_processing_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import (\n+    ImagesKwargs,\n+    ProcessingKwargs,\n+    ProcessorMixin,\n+    Unpack,\n+    _validate_images_text_input_order,\n+)\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n \n \n if TYPE_CHECKING:\n     from .modeling_owlvit import OwlViTImageGuidedObjectDetectionOutput, OwlViTObjectDetectionOutput\n \n \n+class OwlViTImagesKwargs(ImagesKwargs, total=False):\n+    query_images: Optional[ImageInput]\n+\n+\n+class OwlViTProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: OwlViTImagesKwargs\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": \"max_length\",\n+        },\n+        \"images_kwargs\": {},\n+        \"common_kwargs\": {\n+            \"return_tensors\": \"np\",\n+        },\n+    }\n+\n+\n class OwlViTProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs an OWL-ViT processor which wraps [`OwlViTImageProcessor`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`]\n@@ -46,6 +71,8 @@ class OwlViTProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"OwlViTImageProcessor\"\n     tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n+    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n+    optional_call_args = [\"query_images\"]\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None\n@@ -65,7 +92,20 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n \n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(self, text=None, images=None, query_images=None, padding=\"max_length\", return_tensors=\"np\", **kwargs):\n+    def __call__(\n+        self,\n+        images: Optional[ImageInput] = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        # The following is to capture `query_images` argument that may be passed as a positional argument.\n+        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n+        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n+        # This behavior is only needed for backward compatibility and will be removed in future versions.\n+        #\n+        *args,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[OwlViTProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n         `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n@@ -74,14 +114,14 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`,\n             `List[torch.Tensor]`):\n                 The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n                 tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             query_images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The query image to be prepared, one query image is expected per target image to be queried. Each image\n                 can be a PIL image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image\n@@ -92,36 +132,49 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n                 - `'pt'`: Return PyTorch `torch.Tensor` objects.\n                 - `'np'`: Return NumPy `np.ndarray` objects.\n                 - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n         Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n               `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **query_pixel_values** -- Pixel values of the query images to be fed to a model. Returned when `query_images` is not `None`.\n         \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            OwlViTProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+            **self.prepare_and_validate_optional_call_args(*args),\n+        )\n+        query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n+        return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n \n         if text is None and query_images is None and images is None:\n             raise ValueError(\n                 \"You have to specify at least one text or query image or image. All three cannot be none.\"\n             )\n+        # check if images and text inputs are reversed for BC\n+        images, text = _validate_images_text_input_order(images, text)\n \n+        data = {}\n         if text is not None:\n             if isinstance(text, str) or (isinstance(text, List) and not isinstance(text[0], List)):\n-                encodings = [self.tokenizer(text, padding=padding, return_tensors=return_tensors, **kwargs)]\n+                encodings = [self.tokenizer(text, **output_kwargs[\"text_kwargs\"])]\n \n             elif isinstance(text, List) and isinstance(text[0], List):\n                 encodings = []\n \n                 # Maximum number of queries across batch\n-                max_num_queries = max([len(t) for t in text])\n+                max_num_queries = max([len(text_single) for text_single in text])\n \n                 # Pad all batch samples to max number of text queries\n-                for t in text:\n-                    if len(t) != max_num_queries:\n-                        t = t + [\" \"] * (max_num_queries - len(t))\n+                for text_single in text:\n+                    if len(text_single) != max_num_queries:\n+                        text_single = text_single + [\" \"] * (max_num_queries - len(text_single))\n \n-                    encoding = self.tokenizer(t, padding=padding, return_tensors=return_tensors, **kwargs)\n+                    encoding = self.tokenizer(text_single, **output_kwargs[\"text_kwargs\"])\n                     encodings.append(encoding)\n             else:\n                 raise TypeError(\"Input text should be a string, a list of strings or a nested list of strings\")\n@@ -151,30 +204,19 @@ def __call__(self, text=None, images=None, query_images=None, padding=\"max_lengt\n             else:\n                 raise ValueError(\"Target return tensor type could not be returned\")\n \n-            encoding = BatchEncoding()\n-            encoding[\"input_ids\"] = input_ids\n-            encoding[\"attention_mask\"] = attention_mask\n+            data[\"input_ids\"] = input_ids\n+            data[\"attention_mask\"] = attention_mask\n \n         if query_images is not None:\n-            encoding = BatchEncoding()\n-            query_pixel_values = self.image_processor(\n-                query_images, return_tensors=return_tensors, **kwargs\n-            ).pixel_values\n-            encoding[\"query_pixel_values\"] = query_pixel_values\n+            query_pixel_values = self.image_processor(query_images, **output_kwargs[\"images_kwargs\"]).pixel_values\n+            # Query images always override the text prompt\n+            data = {\"query_pixel_values\": query_pixel_values}\n \n         if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n-\n-        if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif query_images is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n-            return encoding\n-        elif text is not None or query_images is not None:\n-            return encoding\n-        else:\n-            return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            data[\"pixel_values\"] = image_features.pixel_values\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n \n     def post_process(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "7a7543dbb3609462b471a26dd24c2aa8383ad958",
            "filename": "tests/models/owlv2/test_processor_owlv2.py",
            "status": "added",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/336dc69d63d56f232a183a3e7f52790429b871ef/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py?ref=336dc69d63d56f232a183a3e7f52790429b871ef",
            "patch": "@@ -0,0 +1,38 @@\n+import shutil\n+import tempfile\n+import unittest\n+\n+import pytest\n+\n+from transformers import Owlv2Processor\n+from transformers.testing_utils import require_scipy\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+@require_scipy\n+class Owlv2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Owlv2Processor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+        processor = self.processor_class.from_pretrained(\"google/owlv2-base-patch16-ensemble\")\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def test_processor_query_images_positional(self):\n+        processor_components = self.prepare_components()\n+        processor = Owlv2Processor(**processor_components)\n+\n+        image_input = self.prepare_image_inputs()\n+        query_images = self.prepare_image_inputs()\n+\n+        inputs = processor(None, image_input, query_images)\n+\n+        self.assertListEqual(list(inputs.keys()), [\"query_pixel_values\", \"pixel_values\"])\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()"
        },
        {
            "sha": "5f99a2275f1a5e90596ec983062acbee342a4da5",
            "filename": "tests/models/owlvit/test_processor_owlvit.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/336dc69d63d56f232a183a3e7f52790429b871ef/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/336dc69d63d56f232a183a3e7f52790429b871ef/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py?ref=336dc69d63d56f232a183a3e7f52790429b871ef",
            "patch": "@@ -232,6 +232,21 @@ def test_processor_case2(self):\n         with pytest.raises(ValueError):\n             processor()\n \n+    def test_processor_query_images_positional(self):\n+        processor_components = self.prepare_components()\n+        processor = OwlViTProcessor(**processor_components)\n+\n+        image_input = self.prepare_image_inputs()\n+        query_images = self.prepare_image_inputs()\n+\n+        inputs = processor(None, image_input, query_images)\n+\n+        self.assertListEqual(list(inputs.keys()), [\"query_pixel_values\", \"pixel_values\"])\n+\n+        # test if it raises when no input is passed\n+        with pytest.raises(ValueError):\n+            processor()\n+\n     def test_tokenizer_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()"
        }
    ],
    "stats": {
        "total": 273,
        "additions": 205,
        "deletions": 68
    }
}