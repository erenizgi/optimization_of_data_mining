{
    "author": "garrett361",
    "message": "Add padding-free to bamba (#35861)\n\n* add seq_idx and fa kwargs\n\n* update tests\n\n* docs and grad ckpt support\n\n* fmt\n\n* better names\n\n* test_raise_missing_padding_free_kwarg_errs\n\n* + seq_idx in doc strings\n\n* padding free training docs\n\n* add link to pr plots\n\n* raise err on attn_mask with padding free\n\n* rm raising missing padding free err test\n\n* BambaFlashAttentionKwargs\n\n* run modular util for modular_granitemoehybrid.py",
    "sha": "390f153469dfdc793e7a9c7eb4822ea76f4f796a",
    "files": [
        {
            "sha": "b776a5732a0f84132476c8fa763ac6a7d5c55e30",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/390f153469dfdc793e7a9c7eb4822ea76f4f796a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/390f153469dfdc793e7a9c7eb4822ea76f4f796a/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=390f153469dfdc793e7a9c7eb4822ea76f4f796a",
            "patch": "@@ -39,7 +39,7 @@ Checkout all Bamba-9B model checkpoints [here](https://github.com/foundation-mod\n <!---\n ## Usage Tips\n \n-Tips: \n+Tips:\n \n - The architecture is based on Mamba-2 models.\n \n@@ -63,7 +63,35 @@ response = model.generate(**inputs, max_new_tokens=64)\n print(tokenizer.batch_decode(response, skip_special_tokens=True)[0])\n ```\n \n+\n+## Padding-Free Training\n+\n+Bamba supports padding-free training in which distinct training examples can be concatenated\n+together while nevertheless processing the inputs as though they belonged to separate batches. When\n+the examples are of varying lengths, padding-free training can provide significant speed ups and\n+memory savings compared to batching the examples together and using padding, as the unnecessary\n+compute and memory due to padding is avoided entirely. The performance gains depend on factors such\n+as the model and the data distribution, but throughput gains up to [~2x are commonly\n+seen](https://github.com/huggingface/transformers/pull/35861#issue-2807873129).\n+\n+Using padding-free training with Bamba requires the `flash-attn`, `mamba-ssm`, and `causal-conv1d`\n+packages, and the following arguments must be passed to the model in addition to `input_ids` and\n+`labels`:\n+* `position_ids: torch.LongTensor`: the position index of each token in each sequence.\n+* `seq_idx: torch.IntTensor`: the index of each sequence in the batch.\n+* Each of the [`FlashAttentionKwargs`]\n+    * `cu_seq_lens_q: torch.LongTensor`: The cumulative sequence lengths of all queries.\n+    * `cu_seq_lens_k: torch.LongTensor`: The cumulative sequence lengths of all keys.\n+    * `max_length_q: int`: the longest query length in the batch.\n+    * `max_length_k: int`: the longest key length in the batch.\n+\n+The `attention_mask` inputs should not be provided. The [`DataCollatorWithFlattening`] can be used\n+to programmatically generate the above set of additional arguments using `return_seq_idx=True` and\n+`return_flash_attn_kwargs=True`. See [this blog post](https://huggingface.co/blog/packing-with-FA2)\n+for additional information.\n+\n+\n [[autodoc]] BambaForCausalLM\n     - forward\n \n-This HF implementation is contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim). \n+This HF implementation is contributed by [ani300](https://github.com/ani300) and [fabianlim](https://github.com/fabianlim)."
        },
        {
            "sha": "11742b1a3216a9b29e236612b189db2f6d77387e",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 45,
            "deletions": 9,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=390f153469dfdc793e7a9c7eb4822ea76f4f796a",
            "patch": "@@ -24,7 +24,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from typing import Callable, Optional, Tuple, Union\n+from functools import partial\n+from typing import Callable, Optional, Tuple, TypedDict, Union\n \n import torch\n from torch import nn\n@@ -61,6 +62,31 @@\n logger = logging.get_logger(__name__)\n \n \n+class BambaFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n     \"\"\"\n@@ -487,6 +513,7 @@ def cuda_kernels_forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n     ):\n         # 1. Gated MLP's linear projection\n         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n@@ -569,7 +596,7 @@ def cuda_kernels_forward(\n                     A,\n                     D=self.D,\n                     chunk_size=self.chunk_size,\n-                    seq_idx=None,  # was seq_idx\n+                    seq_idx=seq_idx,\n                     activation=self.activation,\n                     rmsnorm_weight=self.norm.weight,\n                     rmsnorm_eps=self.norm.variance_epsilon,\n@@ -610,6 +637,7 @@ def cuda_kernels_forward(\n                         weight=self.conv1d.weight.squeeze(1),\n                         bias=self.conv1d.bias,\n                         activation=self.activation,\n+                        seq_idx=seq_idx,\n                     ).transpose(1, 2)\n \n                 hidden_states_B_C = apply_mask_to_padding_states(hidden_states_B_C, attention_mask)\n@@ -629,7 +657,7 @@ def cuda_kernels_forward(\n                     chunk_size=self.chunk_size,\n                     D=self.D,\n                     z=None,\n-                    seq_idx=None,\n+                    seq_idx=seq_idx,\n                     return_final_states=True,\n                     dt_bias=self.dt_bias,\n                     dt_softplus=True,\n@@ -863,9 +891,15 @@ def forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n+        **kwargs,\n     ):\n         if is_fast_path_available and \"cuda\" in self.in_proj.weight.device.type:\n-            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n+            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask, seq_idx)\n+        if seq_idx is not None:\n+            raise NotImplementedError(\n+                \"`seq_idx` support requires fast path support. Please install `mamba_ssm` and `causal_conv1d`\"\n+            )\n         dtype = hidden_states.dtype\n         if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n             # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n@@ -939,7 +973,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -959,8 +993,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs. Can be used to provide `BambaFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n \n         residual = hidden_states\n@@ -974,6 +1008,7 @@ def forward(\n                 cache_params=past_key_value,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n+                **kwargs,\n             )\n             self_attn_weights = None\n         elif self.layer_type == \"attention\":\n@@ -1076,7 +1111,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwargs, for now\n+        **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1128,7 +1163,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **kwargs),\n                     hidden_states,\n                     layer_mask,\n                     position_ids,\n@@ -1148,6 +1183,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "7e0090b3945da77b228f41569c6a0c2304cc8193",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 51,
            "deletions": 10,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=390f153469dfdc793e7a9c7eb4822ea76f4f796a",
            "patch": "@@ -19,7 +19,8 @@\n # limitations under the License.\n \"\"\"PyTorch Bamba model.\"\"\"\n \n-from typing import Optional, Tuple, Union\n+from functools import partial\n+from typing import Optional, Tuple, TypedDict, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -46,7 +47,12 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, logging\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    auto_docstring,\n+    can_return_tuple,\n+    logging,\n+)\n from ...utils.import_utils import is_causal_conv1d_available, is_flash_attn_2_available, is_mamba_2_ssm_available\n from .configuration_bamba import BambaConfig\n \n@@ -71,6 +77,31 @@\n logger = logging.get_logger(__name__)\n \n \n+class BambaFlashAttentionKwargs(TypedDict, total=False):\n+    \"\"\"\n+    Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n+    Use cases include padding-free training and fewer `torch.compile` graph breaks.\n+\n+    Attributes:\n+        cu_seq_lens_q (`torch.LongTensor`)\n+            Gets cumulative sequence length for query state.\n+        cu_seq_lens_k (`torch.LongTensor`)\n+            Gets cumulative sequence length for key state.\n+        max_length_q (`int`):\n+            Maximum sequence length for query state.\n+        max_length_k (`int`):\n+            Maximum sequence length for key state.\n+        seq_idx (`torch.IntTensor):\n+            Index of each packed sequence.\n+    \"\"\"\n+\n+    cu_seq_lens_q: torch.LongTensor\n+    cu_seq_lens_k: torch.LongTensor\n+    max_length_q: int\n+    max_length_k: int\n+    seq_idx: torch.IntTensor\n+\n+\n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n class HybridMambaAttentionDynamicCache(modeling_jamba.HybridMambaAttentionDynamicCache):\n     \"\"\"\n@@ -278,6 +309,7 @@ def cuda_kernels_forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n     ):\n         # 1. Gated MLP's linear projection\n         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n@@ -360,7 +392,7 @@ def cuda_kernels_forward(\n                     A,\n                     D=self.D,\n                     chunk_size=self.chunk_size,\n-                    seq_idx=None,  # was seq_idx\n+                    seq_idx=seq_idx,\n                     activation=self.activation,\n                     rmsnorm_weight=self.norm.weight,\n                     rmsnorm_eps=self.norm.variance_epsilon,\n@@ -401,6 +433,7 @@ def cuda_kernels_forward(\n                         weight=self.conv1d.weight.squeeze(1),\n                         bias=self.conv1d.bias,\n                         activation=self.activation,\n+                        seq_idx=seq_idx,\n                     ).transpose(1, 2)\n \n                 hidden_states_B_C = apply_mask_to_padding_states(hidden_states_B_C, attention_mask)\n@@ -420,7 +453,7 @@ def cuda_kernels_forward(\n                     chunk_size=self.chunk_size,\n                     D=self.D,\n                     z=None,\n-                    seq_idx=None,\n+                    seq_idx=seq_idx,\n                     return_final_states=True,\n                     dt_bias=self.dt_bias,\n                     dt_softplus=True,\n@@ -654,9 +687,15 @@ def forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n+        **kwargs,\n     ):\n         if is_fast_path_available and \"cuda\" in self.in_proj.weight.device.type:\n-            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n+            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask, seq_idx)\n+        if seq_idx is not None:\n+            raise NotImplementedError(\n+                \"`seq_idx` support requires fast path support. Please install `mamba_ssm` and `causal_conv1d`\"\n+            )\n         dtype = hidden_states.dtype\n         if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n             # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66\n@@ -701,7 +740,7 @@ def forward(\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -721,8 +760,8 @@ def forward(\n                 Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n                 with `head_dim` being the embedding dimension of each attention head.\n             kwargs (`dict`, *optional*):\n-                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n-                into the model\n+                Arbitrary kwargs. Can be used to provide `BambaFlashAttentionKwargs` for\n+                padding-free training and/or improve torch.compile performance.\n         \"\"\"\n \n         residual = hidden_states\n@@ -736,6 +775,7 @@ def forward(\n                 cache_params=past_key_value,\n                 cache_position=cache_position,\n                 attention_mask=attention_mask,\n+                **kwargs,\n             )\n             self_attn_weights = None\n         elif self.layer_type == \"attention\":\n@@ -838,7 +878,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,  # NOOP kwargs, for now\n+        **kwargs: Unpack[BambaFlashAttentionKwargs],\n     ) -> BaseModelOutputWithPast:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -890,7 +930,7 @@ def forward(\n \n             if self.gradient_checkpointing and self.training:\n                 layer_outputs = self._gradient_checkpointing_func(\n-                    decoder_layer.__call__,\n+                    partial(decoder_layer.__call__, **kwargs),\n                     hidden_states,\n                     layer_mask,\n                     position_ids,\n@@ -910,6 +950,7 @@ def forward(\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n                     position_embeddings=position_embeddings,\n+                    **kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]"
        },
        {
            "sha": "48e1dc0020f01f1e906577fbc3fa6d4c7cbfa856",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/390f153469dfdc793e7a9c7eb4822ea76f4f796a/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=390f153469dfdc793e7a9c7eb4822ea76f4f796a",
            "patch": "@@ -439,6 +439,7 @@ def cuda_kernels_forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n     ):\n         # 1. Gated MLP's linear projection\n         hidden_states = apply_mask_to_padding_states(hidden_states, attention_mask)\n@@ -521,7 +522,7 @@ def cuda_kernels_forward(\n                     A,\n                     D=self.D,\n                     chunk_size=self.chunk_size,\n-                    seq_idx=None,  # was seq_idx\n+                    seq_idx=seq_idx,\n                     activation=self.activation,\n                     rmsnorm_weight=self.norm.weight,\n                     rmsnorm_eps=self.norm.variance_epsilon,\n@@ -562,6 +563,7 @@ def cuda_kernels_forward(\n                         weight=self.conv1d.weight.squeeze(1),\n                         bias=self.conv1d.bias,\n                         activation=self.activation,\n+                        seq_idx=seq_idx,\n                     ).transpose(1, 2)\n \n                 hidden_states_B_C = apply_mask_to_padding_states(hidden_states_B_C, attention_mask)\n@@ -581,7 +583,7 @@ def cuda_kernels_forward(\n                     chunk_size=self.chunk_size,\n                     D=self.D,\n                     z=None,\n-                    seq_idx=None,\n+                    seq_idx=seq_idx,\n                     return_final_states=True,\n                     dt_bias=self.dt_bias,\n                     dt_softplus=True,\n@@ -815,9 +817,15 @@ def forward(\n         cache_params: Optional[HybridMambaAttentionDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        seq_idx: Optional[torch.IntTensor] = None,\n+        **kwargs,\n     ):\n         if is_fast_path_available and \"cuda\" in self.in_proj.weight.device.type:\n-            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask)\n+            return self.cuda_kernels_forward(hidden_states, cache_params, cache_position, attention_mask, seq_idx)\n+        if seq_idx is not None:\n+            raise NotImplementedError(\n+                \"`seq_idx` support requires fast path support. Please install `mamba_ssm` and `causal_conv1d`\"\n+            )\n         dtype = hidden_states.dtype\n         if attention_mask is not None and attention_mask.shape[1] > 1 and attention_mask.shape[0] > 1:\n             # tune out hidden states for pad tokens, see https://github.com/state-spaces/mamba/issues/66"
        },
        {
            "sha": "bacbfa0aead86aefd4d0bded604a175cbadbb857",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 96,
            "deletions": 1,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/390f153469dfdc793e7a9c7eb4822ea76f4f796a/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/390f153469dfdc793e7a9c7eb4822ea76f4f796a/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=390f153469dfdc793e7a9c7eb4822ea76f4f796a",
            "patch": "@@ -14,16 +14,25 @@\n \"\"\"Testing suite for the PyTorch Bamba model.\"\"\"\n \n import inspect\n+import tempfile\n import unittest\n \n import pytest\n+from pytest import mark\n \n-from transformers import AutoTokenizer, BambaConfig, is_torch_available\n+from transformers import (\n+    AutoTokenizer,\n+    BambaConfig,\n+    DataCollatorWithFlattening,\n+    is_torch_available,\n+)\n from transformers.testing_utils import (\n     Expectations,\n     require_deterministic_for_xpu,\n+    require_flash_attn,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -489,6 +498,92 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n+    @unittest.skip(\n+        \"Bamba requires additionally specifying position_ids, seq_idx, and FlashAttentionKwargs for padding-free training.\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n+        pass\n+\n+    @unittest.skip(\n+        \"Bamba requires additionally specifying position_ids, seq_idx, and FlashAttentionKwargs for padding-free training.\"\n+    )\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        pass\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_seq_idx_and_fa_kwargs(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        max_new_tokens = 30\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n+                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n+                dummy_input = dummy_input.to(torch.float16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # ensure left padding, to adapt for some models\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.float16,\n+                        attn_implementation=\"flash_attention_2\",\n+                        low_cpu_mem_usage=True,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                features = [\n+                    {\"input_ids\": i[a.bool()].tolist()}\n+                    for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n+                ]\n+\n+                # add position_ids + fa_kwargs + seq_idx\n+                data_collator = DataCollatorWithFlattening(\n+                    return_tensors=\"pt\", return_seq_idx=True, return_flash_attn_kwargs=True\n+                )\n+                batch = data_collator(features)\n+                batch_cuda = {k: t.cuda() if torch.is_tensor(t) else t for k, t in batch.items()}\n+\n+                res_padded = model(**inputs_dict)\n+                res_padfree = model(**batch_cuda)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.float16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 258,
        "additions": 233,
        "deletions": 25
    }
}