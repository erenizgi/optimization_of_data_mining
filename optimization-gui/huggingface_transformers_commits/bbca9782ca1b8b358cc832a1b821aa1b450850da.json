{
    "author": "dross20",
    "message": "Update LED model card (#39233)\n\n* Update LED model card\n\n* Remove extra arguments\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "bbca9782ca1b8b358cc832a1b821aa1b450850da",
    "files": [
        {
            "sha": "7ac5e44b432e488461d5db422a65a1400e47aba4",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 124,
            "deletions": 51,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/bbca9782ca1b8b358cc832a1b821aa1b450850da/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bbca9782ca1b8b358cc832a1b821aa1b450850da/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=bbca9782ca1b8b358cc832a1b821aa1b450850da",
            "patch": "@@ -14,62 +14,135 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# LED\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+            <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The LED model was proposed in [Longformer: The Long-Document Transformer](https://huggingface.co/papers/2004.05150) by Iz\n-Beltagy, Matthew E. Peters, Arman Cohan.\n-\n-The abstract from the paper is the following:\n-\n-*Transformer-based models are unable to process long sequences due to their self-attention operation, which scales\n-quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention\n-mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or\n-longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local\n-windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we\n-evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In\n-contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our\n-pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on\n-WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting\n-long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization\n-dataset.*\n-\n-## Usage tips\n-\n-- [`LEDForConditionalGeneration`] is an extension of\n-  [`BartForConditionalGeneration`] exchanging the traditional *self-attention* layer with\n-  *Longformer*'s *chunked self-attention* layer. [`LEDTokenizer`] is an alias of\n-  [`BartTokenizer`].\n-- LED works very well on long-range *sequence-to-sequence* tasks where the `input_ids` largely exceed a length of\n-  1024 tokens.\n-- LED pads the `input_ids` to be a multiple of `config.attention_window` if required. Therefore a small speed-up is\n-  gained, when [`LEDTokenizer`] is used with the `pad_to_multiple_of` argument.\n-- LED makes use of *global attention* by means of the `global_attention_mask` (see\n-  [`LongformerModel`]). For summarization, it is advised to put *global attention* only on the first\n-  `<s>` token. For question answering, it is advised to put *global attention* on all tokens of the question.\n-- To fine-tune LED on all 16384, *gradient checkpointing* can be enabled in case training leads to out-of-memory (OOM)\n-  errors. This can be done by executing `model.gradient_checkpointing_enable()`. \n- Moreover, the `use_cache=False`\n-  flag can be used to disable the caching mechanism to save memory.\n-- LED is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than\n-  the left.\n-\n-This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n+# LED\n+\n+[Longformer-Encoder-Decoder (LED)](https://huggingface.co/papers/2004.05150) is an encoder-decoder  transformer model for sequence-to-sequence tasks like summarization. It extends [Longformer](.longformer), an encoder-only model designed to handle long inputs, by adding a decoder layer. The decoder uses full self-attention on the encoded tokens and previously decoded locations. Because of Longformer's linear self-attention mechanism, LED is more efficient than standard encoder-decoder models when processing long sequences.\n+\n+You can find all the original [LED] checkpoints under the [Ai2](https://huggingface.co/allenai/models?search=led) organization.\n+\n+> [!TIP]\n+> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n+>\n+> Click on the LED models in the right sidebar for more examples of how to apply LED to different language tasks.\n+\n+The example below demonstrates how to summarize text with [`Pipeline`], [`AutoModel`], and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"summarization\",\n+    model=\"allenai/led-base-16384\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\")\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```python\n+import torch\n+from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/led-base-16384\"\n+)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"allenai/led-base-16384\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\"\n+)\n+\n+input_text = \"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# Place global attention on the first token\n+global_attention_mask = torch.zeros_like(input_ids.input_ids).to(\"cuda\")\n+global_attention_mask[:, 0] = 1\n+\n+output = model.generate(**input_ids, global_attention_mask=global_attention_mask, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers-cli\">\n+\n+```bash\n+!echo -e \"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet. Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts.\" | transformers-cli run --task summarization --model allenai/led-base-16384 --device 0\n+```\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n+\n+```python\n+import torch\n+from transformers import BitsAndBytesConfig, AutoModelForSeq2SeqLM, AutoTokenizer\n+\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_quant_type=\"nf4\"\n+)\n+model = AutoModelForSeq2SeqLM.from_pretrained(\n+    \"allenai/led-large-16384\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/led-large-16384\"\n+)\n+\n+input_text = \"\"\"Plants are among the most remarkable and essential life forms on Earth, possessing a unique ability to produce their own food through a process known as photosynthesis. This complex biochemical process is fundamental not only to plant life but to virtually all life on the planet.\n+Through photosynthesis, plants capture energy from sunlight using a green pigment called chlorophyll, which is located in specialized cell structures called chloroplasts. In the presence of light, plants absorb carbon dioxide from the atmosphere through small pores in their leaves called stomata, and take in water from the soil through their root systems.\n+These ingredients are then transformed into glucose, a type of sugar that serves as a source of chemical energy, and oxygen, which is released as a byproduct into the atmosphere. The glucose produced during photosynthesis is not just used immediately; plants also store it as starch or convert it into other organic compounds like cellulose, which is essential for building their cellular structure.\n+This energy reserve allows them to grow, develop leaves, produce flowers, bear fruit, and carry out various physiological processes throughout their lifecycle.\"\"\"\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+\n+# Place global attention on the first token\n+global_attention_mask = torch.zeros_like(input_ids.input_ids).to(\"cuda\")\n+global_attention_mask[:, 0] = 1\n+\n+output = model.generate(**input_ids, global_attention_mask=global_attention_mask, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+## Notes\n+\n+- [`LEDForConditionalGeneration`] is an extension of [`BartForConditionalGeneration`] exchanging the traditional self-attention layer with Longformer's chunked self-attention layer. [`LEDTokenizer`] is an alias of [`BartTokenizer`].\n+- LED pads the `input_ids` to be a multiple of `config.attention_window` if required. A small speedup is gained when [`LEDTokenizer`] is used with the `pad_to_multiple_of` argument.\n+- LED works best on long-range sequence-to-sequence tasks where the `input_ids` are significantly longer than 1024 tokens.\n+- LED uses global attention by means of the `global_attention_mask` (see [`LongformerModel`]). For summarization, it is advised to put global attention only on the first `<s>` token. For question answering, it is advised to put global attention on all tokens of the question.\n+- To fine-tune LED on all 16384 parameters, gradient checkpointing can be enabled in case training leads to out-of-memory (OOM) errors. Enable gradient checkpointing by adding `model.gradient_checkpointing_enable()` and setting `use_cache=False` to disable the caching mechanism to save memory.\n+- Inputs should be padded on the right because LED uses absolute position embeddings.\n \n ## Resources\n \n-- [A notebook showing how to evaluate LED](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing).\n-- [A notebook showing how to fine-tune LED](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing).\n-- [Text classification task guide](../tasks/sequence_classification)\n-- [Question answering task guide](../tasks/question_answering)\n-- [Translation task guide](../tasks/translation)\n-- [Summarization task guide](../tasks/summarization)\n+- Read the [LED on Arxiv notebook](https://colab.research.google.com/drive/12INTTR6n64TzS4RrXZxMSXfrOd9Xzamo?usp=sharing) to see how LED can achieve state-of-the-art performance on Arxiv article summarization.\n+- Read the [Fine-tune LED notebook](https://colab.research.google.com/drive/12LjJazBl7Gam0XBPy_y0CTOJZeZ34c2v?usp=sharing) to learn how to fine-tune LED on PubMed articles.\n \n ## LEDConfig\n "
        }
    ],
    "stats": {
        "total": 175,
        "additions": 124,
        "deletions": 51
    }
}