{
    "author": "cyyever",
    "message": "Apply several ruff SIM rules   (#37283)\n\n* Apply ruff SIM118 fix\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Apply ruff SIM910 fix\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Apply ruff SIM101 fix\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Format code\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "95faabf0a6cd845f4c5548697e288a79e424b096",
    "files": [
        {
            "sha": "414e1c62e305e7824b2ac333b775c4cd97339db2",
            "filename": "examples/flax/image-captioning/run_image_captioning_flax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fimage-captioning%2Frun_image_captioning_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -906,7 +906,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "2ddadcdf99a5b09f1372a05048d9d06e5b1e9603",
            "filename": "examples/flax/language-modeling/run_bart_dlm_flax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_bart_dlm_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -530,7 +530,7 @@ def main():\n             trust_remote_code=data_args.trust_remote_code,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -567,7 +567,7 @@ def main():\n             num_proc=data_args.preprocessing_num_workers,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -671,7 +671,7 @@ def tokenize_function(examples):\n     # max_seq_length.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n         # customize this part to your needs.\n@@ -777,7 +777,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "fc1367fb6c16b72106b1b73d30fa216ec180acba",
            "filename": "examples/flax/language-modeling/run_clm_flax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_clm_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -407,7 +407,7 @@ def main():\n             trust_remote_code=model_args.trust_remote_code,\n         )\n \n-        if \"validation\" not in dataset.keys():\n+        if \"validation\" not in dataset:\n             dataset[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -447,7 +447,7 @@ def main():\n             num_proc=data_args.preprocessing_num_workers,\n         )\n \n-        if \"validation\" not in dataset.keys():\n+        if \"validation\" not in dataset:\n             dataset[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -580,7 +580,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n         # customize this part to your needs.\n@@ -674,7 +674,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "0d80b7b0bf1568ca98866d7c8f9dd47e9dd5d80e",
            "filename": "examples/flax/language-modeling/run_mlm_flax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_mlm_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -448,7 +448,7 @@ def main():\n             trust_remote_code=model_args.trust_remote_code,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -485,7 +485,7 @@ def main():\n             num_proc=data_args.preprocessing_num_workers,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -603,7 +603,7 @@ def tokenize_function(examples):\n         # max_seq_length.\n         def group_texts(examples):\n             # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n             total_length = len(concatenated_examples[list(examples.keys())[0]])\n             # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n             # customize this part to your needs.\n@@ -707,7 +707,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "af3394cccbff633baeb7eafb698e24e3cfbae13b",
            "filename": "examples/flax/language-modeling/run_t5_mlm_flax.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Flanguage-modeling%2Frun_t5_mlm_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -572,7 +572,7 @@ def main():\n             trust_remote_code=data_args.trust_remote_code,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -609,7 +609,7 @@ def main():\n             num_proc=data_args.preprocessing_num_workers,\n         )\n \n-        if \"validation\" not in datasets.keys():\n+        if \"validation\" not in datasets:\n             datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -703,7 +703,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of expanded_inputs_length.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n         # customize this part to your needs.\n@@ -814,7 +814,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "d17304c7021a1f65dc1ee9d80ec056ea6eba8d77",
            "filename": "examples/flax/question-answering/run_qa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fquestion-answering%2Frun_qa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fquestion-answering%2Frun_qa.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -345,7 +345,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "40267a95c64eb3326ca0f28ba8fe38a3af8dd250",
            "filename": "examples/flax/speech-recognition/run_flax_speech_recognition_seq2seq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fspeech-recognition%2Frun_flax_speech_recognition_seq2seq.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -668,7 +668,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "5240db323f1cad675fd034e5c4b8f42a74339b46",
            "filename": "examples/flax/summarization/run_summarization_flax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Fsummarization%2Frun_summarization_flax.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -768,7 +768,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "ade6bc0e499790bd4260231a490d8a15ca814021",
            "filename": "examples/flax/text-classification/run_flax_glue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftext-classification%2Frun_flax_glue.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -249,7 +249,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "93130cc52ca8dd5e4ac70e8dbf22d4be6b315b88",
            "filename": "examples/flax/token-classification/run_flax_ner.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fflax%2Ftoken-classification%2Frun_flax_ner.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -310,7 +310,7 @@ def decay_mask_fn(params):\n         layer_norm_named_params = {\n             layer[-2:]\n             for layer_norm_name in layer_norm_candidates\n-            for layer in flat_params.keys()\n+            for layer in flat_params\n             if layer_norm_name in \"\".join(layer).lower()\n         }\n         flat_mask = {path: (path[-1] != \"bias\" and path[-2:] not in layer_norm_named_params) for path in flat_params}"
        },
        {
            "sha": "228fc87ca3928d8a8da8a9c4309cd94f638e2a7a",
            "filename": "examples/legacy/pytorch-lightning/lightning_base.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fpytorch-lightning%2Flightning_base.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -379,8 +379,8 @@ def generic_train(\n         train_params[\"distributed_backend\"] = \"ddp\"\n \n     train_params[\"accumulate_grad_batches\"] = args.accumulate_grad_batches\n-    train_params[\"accelerator\"] = extra_train_kwargs.get(\"accelerator\", None)\n-    train_params[\"profiler\"] = extra_train_kwargs.get(\"profiler\", None)\n+    train_params[\"accelerator\"] = extra_train_kwargs.get(\"accelerator\")\n+    train_params[\"profiler\"] = extra_train_kwargs.get(\"profiler\")\n \n     trainer = pl.Trainer.from_argparse_args(\n         args,"
        },
        {
            "sha": "2cff6f59920ab701c464fbb7372fc59e3c7519a3",
            "filename": "examples/legacy/seq2seq/download_wmt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Fdownload_wmt.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -44,7 +44,7 @@ def download_wmt_dataset(src_lang=\"ro\", tgt_lang=\"en\", dataset=\"wmt16\", save_dir\n     save_dir = Path(save_dir)\n     save_dir.mkdir(exist_ok=True)\n \n-    for split in ds.keys():\n+    for split in ds:\n         print(f\"Splitting {split} with {ds[split].num_rows} records\")\n \n         # to save to val.source, val.target like summary datasets"
        },
        {
            "sha": "10baf5f8a03bf67613577e1eafcbb3f9ee171b9f",
            "filename": "examples/pytorch/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -288,7 +288,7 @@ def collate_fn(examples):\n         return {\"pixel_values\": pixel_values, \"labels\": labels}\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in dataset.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "e27605b8ed8939f048275d0dfc4e3f672072fa5b",
            "filename": "examples/pytorch/image-classification/run_image_classification_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-classification%2Frun_image_classification_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -324,7 +324,7 @@ def main():\n         )\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n+    args.train_val_split = None if \"validation\" in dataset else args.train_val_split\n     if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(args.train_val_split)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "1ae581b223ea88701213c5ce1d24b2766fd9c3bc",
            "filename": "examples/pytorch/image-pretraining/run_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mae.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -247,7 +247,7 @@ def main():\n     )\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in ds.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in ds else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = ds[\"train\"].train_test_split(data_args.train_val_split)\n         ds[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "f3d47bfff33f77d97956444a3edb8c5a781940a5",
            "filename": "examples/pytorch/image-pretraining/run_mim.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -311,7 +311,7 @@ def main():\n     )\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in ds.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in ds else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = ds[\"train\"].train_test_split(data_args.train_val_split)\n         ds[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "581a6101371ea304da25189984b10a77c332c93c",
            "filename": "examples/pytorch/image-pretraining/run_mim_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fimage-pretraining%2Frun_mim_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -452,7 +452,7 @@ def main():\n     )\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    args.train_val_split = None if \"validation\" in ds.keys() else args.train_val_split\n+    args.train_val_split = None if \"validation\" in ds else args.train_val_split\n     if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n         split = ds[\"train\"].train_test_split(args.train_val_split)\n         ds[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "0c5829818d463553a165bf87f2334e33e043b2e1",
            "filename": "examples/pytorch/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -358,7 +358,7 @@ def main():\n             streaming=data_args.streaming,\n             trust_remote_code=model_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             if data_args.streaming:\n                 dataset_stream = load_dataset(\n                     data_args.dataset_name,\n@@ -412,7 +412,7 @@ def main():\n             **dataset_args,\n         )\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             if data_args.streaming:\n                 dataset_stream = load_dataset(\n                     extension,\n@@ -579,7 +579,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n         # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "0c397bc28cc8ef6fb5aeb149d0f424b50acd7f72",
            "filename": "examples/pytorch/language-modeling/run_clm_no_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_clm_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -335,7 +335,7 @@ def main():\n         raw_datasets = load_dataset(\n             args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 args.dataset_name,\n                 args.dataset_config_name,\n@@ -362,7 +362,7 @@ def main():\n             dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n         raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -465,7 +465,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n         # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "bce3adabfc5f36eb4df55702064bfa275ebe5cca",
            "filename": "examples/pytorch/language-modeling/run_fim.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -388,7 +388,7 @@ def main():\n             streaming=data_args.streaming,\n             trust_remote_code=model_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -430,7 +430,7 @@ def main():\n             **dataset_args,\n         )\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -652,7 +652,7 @@ def tokenize_function(examples):\n     # Data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n         # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "6e5cc427f483e09e8afa759b972b326f18356f98",
            "filename": "examples/pytorch/language-modeling/run_fim_no_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_fim_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -401,7 +401,7 @@ def main():\n         raw_datasets = load_dataset(\n             args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 args.dataset_name,\n                 args.dataset_config_name,\n@@ -427,7 +427,7 @@ def main():\n             dataset_args[\"keep_linebreaks\"] = not args.no_keep_linebreaks\n         raw_datasets = load_dataset(extension, data_files=data_files, **dataset_args)\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -599,7 +599,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n         # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "4573d343a7b3ad0a677290f33b05cec3509d304d",
            "filename": "examples/pytorch/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -331,7 +331,7 @@ def main():\n             streaming=data_args.streaming,\n             trust_remote_code=model_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -368,7 +368,7 @@ def main():\n         )\n \n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -541,7 +541,7 @@ def tokenize_function(examples):\n         # max_seq_length.\n         def group_texts(examples):\n             # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n             total_length = len(concatenated_examples[list(examples.keys())[0]])\n             # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\n             # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "d975e1acf6ed819ff87454c02c2f4c6d864e5df0",
            "filename": "examples/pytorch/language-modeling/run_mlm_no_trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_mlm_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -342,7 +342,7 @@ def main():\n         raw_datasets = load_dataset(\n             args.dataset_name, args.dataset_config_name, trust_remote_code=args.trust_remote_code\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 args.dataset_name,\n                 args.dataset_config_name,\n@@ -367,7 +367,7 @@ def main():\n             extension = \"text\"\n         raw_datasets = load_dataset(extension, data_files=data_files)\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -496,7 +496,7 @@ def tokenize_function(examples):\n         # max_seq_length.\n         def group_texts(examples):\n             # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n             total_length = len(concatenated_examples[list(examples.keys())[0]])\n             # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\n             # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "d7bb35d595277245debbd50dfec47a0f24b25be6",
            "filename": "examples/pytorch/language-modeling/run_plm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Flanguage-modeling%2Frun_plm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -309,7 +309,7 @@ def main():\n             token=model_args.token,\n             trust_remote_code=data_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -338,7 +338,7 @@ def main():\n             extension = \"text\"\n         raw_datasets = load_dataset(extension, data_files=data_files, cache_dir=model_args.cache_dir)\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -466,7 +466,7 @@ def tokenize_function(examples):\n         # max_seq_length.\n         def group_texts(examples):\n             # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n             total_length = len(concatenated_examples[list(examples.keys())[0]])\n             # We drop the small remainder, and if the total_length < max_seq_length  we exclude this batch and return an empty dict.\n             # We could add padding if the model supported it instead of this drop, you can customize this part to your needs."
        },
        {
            "sha": "d99cd869f0ca20bee0eca67fc135075f2617c8aa",
            "filename": "examples/pytorch/multiple-choice/run_swag_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fmultiple-choice%2Frun_swag_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -316,7 +316,7 @@ def main():\n         raw_datasets = load_dataset(extension, data_files=data_files)\n     # Trim a number of training examples\n     if args.debug:\n-        for split in raw_datasets.keys():\n+        for split in raw_datasets:\n             raw_datasets[split] = raw_datasets[split].select(range(100))\n     # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n     # https://huggingface.co/docs/datasets/loading_datasets."
        },
        {
            "sha": "71fff54ccad1f884b66ed8af30fe7c2ca9a23c05",
            "filename": "examples/pytorch/object-detection/run_object_detection.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -403,7 +403,7 @@ def main():\n     )\n \n     # If we don't have a validation split, split off a percentage of train as validation\n-    data_args.train_val_split = None if \"validation\" in dataset.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(data_args.train_val_split, seed=training_args.seed)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "aaf54138f09cf37ce60e548a2c0c6aea9d4ca01c",
            "filename": "examples/pytorch/object-detection/run_object_detection_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fobject-detection%2Frun_object_detection_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -465,7 +465,7 @@ def main():\n     dataset = load_dataset(args.dataset_name, cache_dir=args.cache_dir, trust_remote_code=args.trust_remote_code)\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n+    args.train_val_split = None if \"validation\" in dataset else args.train_val_split\n     if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(args.train_val_split, seed=args.seed)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "3facad307e54c3693e7d846b85f6f81db0587e33",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -255,7 +255,7 @@ def main():\n         dataset = dataset.rename_columns({\"annotation\": \"label\"})\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in dataset.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "3d2caf88bf6f1834e7290120feea9f7f24176548",
            "filename": "examples/pytorch/semantic-segmentation/run_semantic_segmentation_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsemantic-segmentation%2Frun_semantic_segmentation_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -314,7 +314,7 @@ def main():\n         dataset = dataset.rename_columns({\"annotation\": \"label\"})\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    args.train_val_split = None if \"validation\" in dataset.keys() else args.train_val_split\n+    args.train_val_split = None if \"validation\" in dataset else args.train_val_split\n     if isinstance(args.train_val_split, float) and args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(args.train_val_split)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "80c616ee7481eb73b0ffa5d665a1e23835e58b26",
            "filename": "examples/pytorch/text-classification/run_classification.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -378,7 +378,7 @@ def main():\n             else:\n                 raise ValueError(\"Need either a dataset name or a test file for `do_predict`.\")\n \n-        for key in data_files.keys():\n+        for key in data_files:\n             logger.info(f\"load a local file for {key}: {data_files[key]}\")\n \n         if data_args.train_file.endswith(\".csv\"):\n@@ -422,13 +422,13 @@ def main():\n         raw_datasets.pop(data_args.test_split_name)\n \n     if data_args.remove_columns is not None:\n-        for split in raw_datasets.keys():\n+        for split in raw_datasets:\n             for column in data_args.remove_columns.split(\",\"):\n                 logger.info(f\"removing column {column} from split {split}\")\n                 raw_datasets[split] = raw_datasets[split].remove_columns(column)\n \n     if data_args.label_column_name is not None and data_args.label_column_name != \"label\":\n-        for key in raw_datasets.keys():\n+        for key in raw_datasets:\n             raw_datasets[key] = raw_datasets[key].rename_column(data_args.label_column_name, \"label\")\n \n     # Trying to have good defaults here, don't hesitate to tweak to your needs.\n@@ -444,7 +444,7 @@ def main():\n         label_list = None\n         num_labels = 1\n         # regression requires float as label type, let's cast it if needed\n-        for split in raw_datasets.keys():\n+        for split in raw_datasets:\n             if raw_datasets[split].features[\"label\"].dtype not in [\"float32\", \"float64\"]:\n                 logger.warning(\n                     f\"Label type for {split} set to float32, was {raw_datasets[split].features['label'].dtype}\""
        },
        {
            "sha": "63b3b9ab8ff639dbaf3ea25cb64e50f253a807ad",
            "filename": "examples/pytorch/text-classification/run_glue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-classification%2Frun_glue.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -161,7 +161,7 @@ class DataTrainingArguments:\n     def __post_init__(self):\n         if self.task_name is not None:\n             self.task_name = self.task_name.lower()\n-            if self.task_name not in task_to_keys.keys():\n+            if self.task_name not in task_to_keys:\n                 raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n         elif self.dataset_name is not None:\n             pass\n@@ -335,7 +335,7 @@ def main():\n             else:\n                 raise ValueError(\"Need either a GLUE task or a test file for `do_predict`.\")\n \n-        for key in data_files.keys():\n+        for key in data_files:\n             logger.info(f\"load a local file for {key}: {data_files[key]}\")\n \n         if data_args.train_file.endswith(\".csv\"):"
        },
        {
            "sha": "f89ca96eefd7f740d97b952543fd3b6c07122489",
            "filename": "examples/pytorch/text-generation/run_generation.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftext-generation%2Frun_generation.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -370,7 +370,7 @@ def main():\n     prompt_text = args.prompt if args.prompt else input(\"Model prompt >>> \")\n \n     # Different models need different input formatting and/or extra arguments\n-    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS.keys()\n+    requires_preprocessing = args.model_type in PREPROCESSING_FUNCTIONS\n     if requires_preprocessing:\n         prepare_input = PREPROCESSING_FUNCTIONS.get(args.model_type)\n         preprocessed_prompt_text = prepare_input(args, model, tokenizer, prompt_text)"
        },
        {
            "sha": "841337d6766af594617edf9ea86b07d443ad966c",
            "filename": "examples/pytorch/token-classification/run_ner_no_trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Ftoken-classification%2Frun_ner_no_trainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -357,7 +357,7 @@ def main():\n         raw_datasets = load_dataset(extension, data_files=data_files)\n     # Trim a number of training examples\n     if args.debug:\n-        for split in raw_datasets.keys():\n+        for split in raw_datasets:\n             raw_datasets[split] = raw_datasets[split].select(range(100))\n     # See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n     # https://huggingface.co/docs/datasets/loading_datasets."
        },
        {
            "sha": "097ef4c67ddac01283bc2e98d879eaa46866b61d",
            "filename": "examples/tensorflow/image-classification/run_image_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Fimage-classification%2Frun_image_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -327,7 +327,7 @@ def main():\n     )\n \n     # If we don't have a validation split, split off a percentage of train as validation.\n-    data_args.train_val_split = None if \"validation\" in dataset.keys() else data_args.train_val_split\n+    data_args.train_val_split = None if \"validation\" in dataset else data_args.train_val_split\n     if isinstance(data_args.train_val_split, float) and data_args.train_val_split > 0.0:\n         split = dataset[\"train\"].train_test_split(data_args.train_val_split)\n         dataset[\"train\"] = split[\"train\"]"
        },
        {
            "sha": "a839c79a5c9bf0ed7984092489040373f0994f57",
            "filename": "examples/tensorflow/language-modeling-tpu/prepare_tfrecord_shards.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling-tpu%2Fprepare_tfrecord_shards.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -151,7 +151,7 @@ def main(args):\n \n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n+        concatenated_examples = {k: sum(examples[k], []) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, though you could add padding instead if the model supports it\n         # In this, as in all things, we advise you to follow your heart "
        },
        {
            "sha": "28a955734b71d2934f0096c9a6cbfdf2f507a402",
            "filename": "examples/tensorflow/language-modeling/run_clm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_clm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -299,7 +299,7 @@ def main():\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -339,7 +339,7 @@ def main():\n             **dataset_args,\n         )\n         # If no validation data is there, validation_split_percentage will be used to divide the dataset.\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 extension,\n                 data_files=data_files,\n@@ -429,7 +429,7 @@ def tokenize_function(examples):\n     # Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n     def group_texts(examples):\n         # Concatenate all texts.\n-        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+        concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n         total_length = len(concatenated_examples[list(examples.keys())[0]])\n         # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n         # customize this part to your needs."
        },
        {
            "sha": "ef2c43c69eff5403be0d3925ef88121985936206",
            "filename": "examples/tensorflow/language-modeling/run_mlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Flanguage-modeling%2Frun_mlm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -308,7 +308,7 @@ def main():\n             token=model_args.token,\n             trust_remote_code=model_args.trust_remote_code,\n         )\n-        if \"validation\" not in raw_datasets.keys():\n+        if \"validation\" not in raw_datasets:\n             raw_datasets[\"validation\"] = load_dataset(\n                 data_args.dataset_name,\n                 data_args.dataset_config_name,\n@@ -446,7 +446,7 @@ def tokenize_function(examples):\n         # max_seq_length.\n         def group_texts(examples):\n             # Concatenate all texts.\n-            concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n+            concatenated_examples = {k: list(chain(*examples[k])) for k in examples}\n             total_length = len(concatenated_examples[list(examples.keys())[0]])\n             # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n             # customize this part to your needs."
        },
        {
            "sha": "da09963028d3153200cf34716e3d7d72b59784be",
            "filename": "examples/tensorflow/text-classification/run_glue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_glue.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -132,7 +132,7 @@ class DataTrainingArguments:\n \n     def __post_init__(self):\n         self.task_name = self.task_name.lower()\n-        if self.task_name not in task_to_keys.keys():\n+        if self.task_name not in task_to_keys:\n             raise ValueError(\"Unknown task, you should pick one in \" + \",\".join(task_to_keys.keys()))\n \n \n@@ -268,7 +268,7 @@ def main():\n \n         data_files = {\"data\": data_args.predict_file}\n \n-        for key in data_files.keys():\n+        for key in data_files:\n             logger.info(f\"Loading a local file for {key}: {data_files[key]}\")\n \n         if data_args.predict_file.endswith(\".csv\"):\n@@ -406,7 +406,7 @@ def compute_metrics(preds, label_ids):\n             \"test_mismatched\": data_args.max_predict_samples,\n             \"user_data\": None,\n         }\n-        for key in datasets.keys():\n+        for key in datasets:\n             if key == \"train\" or key.startswith(\"validation\"):\n                 assert \"label\" in datasets[key].features, f\"Missing labels from {key} data!\"\n             if key == \"train\":"
        },
        {
            "sha": "c87c9040080d4cc9b3ce855a8f3a0b862e7d0af7",
            "filename": "examples/tensorflow/text-classification/run_text_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Ftensorflow%2Ftext-classification%2Frun_text_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -272,7 +272,7 @@ def main():\n     data_files = {\"train\": data_args.train_file, \"validation\": data_args.validation_file, \"test\": data_args.test_file}\n     data_files = {key: file for key, file in data_files.items() if file is not None}\n \n-    for key in data_files.keys():\n+    for key in data_files:\n         logger.info(f\"Loading a local file for {key}: {data_files[key]}\")\n \n     if data_args.input_file_extension == \"csv\":"
        },
        {
            "sha": "70f9979a8f14412040b1d4bce9b02df1aba90f75",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -678,7 +678,7 @@ def _get_config_dict(\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n \n-        gguf_file = kwargs.get(\"gguf_file\", None)\n+        gguf_file = kwargs.get(\"gguf_file\")\n \n         if trust_remote_code is True:\n             logger.warning(\n@@ -1033,7 +1033,7 @@ def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\", None) is not None:\n+        if d.get(\"torch_dtype\") is not None:\n             if isinstance(d[\"torch_dtype\"], dict):\n                 d[\"torch_dtype\"] = {k: str(v).split(\".\")[-1] for k, v in d[\"torch_dtype\"].items()}\n             elif not isinstance(d[\"torch_dtype\"], str):"
        },
        {
            "sha": "24e1f8506490bae3e37d807edc27d31493d07ffd",
            "filename": "src/transformers/convert_pytorch_checkpoint_to_tf2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_pytorch_checkpoint_to_tf2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -267,7 +267,7 @@ def convert_pt_checkpoint_to_tf(\n     tf_model = model_class(config)\n \n     # Load weights from tf checkpoint\n-    if pytorch_checkpoint_path in aws_config_map.keys():\n+    if pytorch_checkpoint_path in aws_config_map:\n         pytorch_checkpoint_path = cached_file(\n             pytorch_checkpoint_path, WEIGHTS_NAME, force_download=not use_cached_models\n         )"
        },
        {
            "sha": "18f2980a38626d86d9b1f2f66f0a8450efc8a020",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -326,8 +326,8 @@ class DataCollatorForTokenClassification(DataCollatorMixin):\n     def torch_call(self, features):\n         import torch\n \n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n+        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n+        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n \n         no_labels_features = [{k: v for k, v in feature.items() if k != label_name} for feature in features]\n \n@@ -366,8 +366,8 @@ def to_list(tensor_or_iterable):\n     def tf_call(self, features):\n         import tensorflow as tf\n \n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n+        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n+        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n         batch = pad_without_fast_tokenizer_warning(\n             self.tokenizer,\n             features,\n@@ -396,8 +396,8 @@ def tf_call(self, features):\n         return batch\n \n     def numpy_call(self, features):\n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n+        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n+        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n         batch = pad_without_fast_tokenizer_warning(\n             self.tokenizer,\n             features,\n@@ -573,7 +573,7 @@ def torch_call(self, examples: list[dict[str, Any]]):  # Refactored implementati\n         import torch\n \n         # Take labels out of the examples beforehand, because they aren't nested.\n-        label_name = \"label\" if \"label\" in examples[0].keys() else \"labels\"\n+        label_name = \"label\" if \"label\" in examples[0] else \"labels\"\n         labels = [example.pop(label_name) for example in examples]\n \n         batch_size = len(examples)\n@@ -602,7 +602,7 @@ def torch_call(self, examples: list[dict[str, Any]]):  # Refactored implementati\n     def tf_call(self, features):  # Implementation taken from the docs.\n         import tensorflow as tf\n \n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n+        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n         labels = [feature.pop(label_name) for feature in features]\n         batch_size = len(features)\n         num_choices = len(features[0][\"input_ids\"])\n@@ -671,8 +671,8 @@ def __call__(self, features, return_tensors=None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n \n-        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-        labels = [feature[label_name] for feature in features] if label_name in features[0].keys() else None\n+        label_name = \"label\" if \"label\" in features[0] else \"labels\"\n+        labels = [feature[label_name] for feature in features] if label_name in features[0] else None\n         # reconvert list[None] to None if necessary\n         # this might occur when we pass {..., \"labels\": None}\n         if labels is not None and all(label is None for label in labels):"
        },
        {
            "sha": "e0be17bd7d28cf61c70c177af4d025d0321617d7",
            "filename": "src/transformers/feature_extraction_sequence_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_sequence_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -124,7 +124,7 @@ def pad(\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(processed_features, (list, tuple)) and isinstance(processed_features[0], (dict, BatchFeature)):\n             processed_features = {\n-                key: [example[key] for example in processed_features] for key in processed_features[0].keys()\n+                key: [example[key] for example in processed_features] for key in processed_features[0]\n             }\n \n         # The model's main input name, usually `input_values`, has be passed for padding"
        },
        {
            "sha": "59e1a401917fa76ae79fe5b8b20daf4fcdaf4ea4",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -405,7 +405,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )"
        },
        {
            "sha": "38a15f3dae9c142121ecf5eadf0fcdfa3f23fa47",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -132,7 +132,7 @@ def __init__(\n                 )\n \n         # Remove potential default \"logits_to_keep\" key\n-        if \"logits_to_keep\" in assistant_kwargs.keys() and not assistant_model._supports_logits_to_keep():\n+        if \"logits_to_keep\" in assistant_kwargs and not assistant_model._supports_logits_to_keep():\n             del assistant_kwargs[\"logits_to_keep\"]\n \n         # If the assistant is an encoder-decoder model, assume the encoder is different on the assistant."
        },
        {
            "sha": "77ce12d969eb2d55c1046e927b50d6086ecca069",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -614,10 +614,7 @@ def validate(self, strict=False):\n             )\n         # 1.4. Watermarking attributes\n         if self.watermarking_config is not None:\n-            if not (\n-                isinstance(self.watermarking_config, WatermarkingConfig)\n-                or isinstance(self.watermarking_config, SynthIDTextWatermarkingConfig)\n-            ):\n+            if not (isinstance(self.watermarking_config, (WatermarkingConfig, SynthIDTextWatermarkingConfig))):\n                 minor_issues[\"watermarking_config\"] = (\n                     \"`watermarking_config` as a dict is deprecated and will be removed in v4.54.0. Please construct \"\n                     \"`watermarking_config` object with `WatermarkingConfig` or `SynthIDTextWatermarkingConfig` class.\"\n@@ -847,7 +844,7 @@ def save_pretrained(\n                 \"Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -1110,7 +1107,7 @@ def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\", None) is not None and not isinstance(d[\"torch_dtype\"], str):\n+        if d.get(\"torch_dtype\") is not None and not isinstance(d[\"torch_dtype\"], str):\n             d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n         for value in d.values():\n             if isinstance(value, dict):\n@@ -1247,7 +1244,7 @@ def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\"\n         if decoder_config is not model_config:\n             default_generation_config = GenerationConfig()\n             decoder_config_dict = decoder_config.to_dict()\n-            for attr in generation_config.to_dict().keys():\n+            for attr in generation_config.to_dict():\n                 is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n                 if attr in decoder_config_dict and is_unset:\n                     setattr(generation_config, attr, decoder_config_dict[attr])"
        },
        {
            "sha": "c7b6c7a39d07129a823217a08ae590d01809fb7a",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1241,13 +1241,13 @@ def _validate_arguments(self):\n                 f\"`sequence_bias` has to be a non-empty dictionary, or non-empty list of lists but is {sequence_bias}.\"\n             )\n         if isinstance(sequence_bias, dict) and any(\n-            not isinstance(sequence_ids, tuple) for sequence_ids in sequence_bias.keys()\n+            not isinstance(sequence_ids, tuple) for sequence_ids in sequence_bias\n         ):\n             raise ValueError(f\"`sequence_bias` has to be a dict with tuples as keys, but is {sequence_bias}.\")\n         if isinstance(sequence_bias, dict) and any(\n             any((not isinstance(token_id, (int, np.integer)) or token_id < 0) for token_id in sequence_ids)\n             or len(sequence_ids) == 0\n-            for sequence_ids in sequence_bias.keys()\n+            for sequence_ids in sequence_bias\n         ):\n             raise ValueError(\n                 f\"Each key in `sequence_bias` has to be a non-empty tuple of positive integers, but is \""
        },
        {
            "sha": "acd53a20b79c9e937e31122c10b8a9506a048b6c",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1867,7 +1867,7 @@ def _get_layer_device_map_for_cache_init(self) -> Optional[dict[int, Union[str,\n                 )\n \n             decoder_mapped_modules = [\n-                module_name for module_name in execution_device_map.keys() if decoder_name in module_name\n+                module_name for module_name in execution_device_map if decoder_name in module_name\n             ]\n             # The decoder name may be present in `execution_device_map` in two forms:\n             # a) each layer has a device mapping\n@@ -5275,7 +5275,7 @@ def _concat(data):\n     # Use a dictionary comprehension to gather attributes from all objects and concatenate them\n     concatenated_data = {\n         k: _concat([getattr(model_output, k) for model_output in model_outputs])\n-        for k in model_output_cls.__dataclass_fields__.keys()\n+        for k in model_output_cls.__dataclass_fields__\n     }\n \n     # Return a new object of the inferred class with the concatenated attributes"
        },
        {
            "sha": "0b0743f99ea0237069ac1427fb0195b5000515cb",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -161,7 +161,7 @@ def _score_ngrams_in_passage(self, input_ids: torch.LongTensor):\n         for batch_idx in range(ngram_tensors.shape[0]):\n             frequencies_table = collections.Counter(ngram_tensors[batch_idx])\n             ngram_to_watermark_lookup = {}\n-            for ngram_example in frequencies_table.keys():\n+            for ngram_example in frequencies_table:\n                 prefix = ngram_example if selfhash else ngram_example[:-1]\n                 target = ngram_example[-1]\n                 ngram_to_watermark_lookup[ngram_example] = self._get_ngram_score_cached(prefix, target)"
        },
        {
            "sha": "4d708efb7c2ae12809cb3c55e79bdb3e33c397bc",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -229,7 +229,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )"
        },
        {
            "sha": "c575770a65f20bbf3f5615df6b2516ec6e05a726",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -225,7 +225,7 @@ def __init__(\n         )\n         crop_size = kwargs.pop(\"crop_size\", self.crop_size)\n         self.crop_size = get_size_dict(crop_size, param_name=\"crop_size\") if crop_size is not None else None\n-        for key in self.valid_kwargs.__annotations__.keys():\n+        for key in self.valid_kwargs.__annotations__:\n             kwarg = kwargs.pop(key, None)\n             if kwarg is not None:\n                 setattr(self, key, kwarg)"
        },
        {
            "sha": "a0bcff7a188e7d29700a2571c1dcd7451cc15477",
            "filename": "src/transformers/image_transforms.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_transforms.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fimage_transforms.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_transforms.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -860,14 +860,14 @@ def _group_images_by_shape(nested_images, is_nested: bool = False):\n def _reconstruct_nested_structure(indices, processed_images):\n     \"\"\"Helper function to reconstruct a single level nested structure.\"\"\"\n     # Find the maximum outer index\n-    max_outer_idx = max(idx[0] for idx in indices.keys())\n+    max_outer_idx = max(idx[0] for idx in indices)\n \n     # Create the outer list\n     result = [None] * (max_outer_idx + 1)\n \n     # Group indices by outer index\n     nested_indices = defaultdict(list)\n-    for i, j in indices.keys():\n+    for i, j in indices:\n         nested_indices[i].append(j)\n \n     for i in range(max_outer_idx + 1):"
        },
        {
            "sha": "9464a4a67530965bfda4c75a81f2de7a338525f1",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -130,7 +130,7 @@ def wrapper(*args, **kwargs):\n         nn.Module.register_parameter = register_empty_parameter\n         if include_buffers:\n             nn.Module.register_buffer = register_empty_buffer\n-        for torch_function_name in tensor_constructors_to_patch.keys():\n+        for torch_function_name in tensor_constructors_to_patch:\n             setattr(torch, torch_function_name, patch_tensor_constructor(getattr(torch, torch_function_name)))\n         yield\n     finally:"
        },
        {
            "sha": "1dd32b42687e8279cef4a6fe5a22a650226d75ff",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -164,7 +164,7 @@ def _replace_with_bnb_linear(\n             current_key_name = []\n         current_key_name.append(name)\n \n-        if (isinstance(module, nn.Linear) or isinstance(module, Conv1D)) and name not in modules_to_not_convert:\n+        if (isinstance(module, (nn.Linear, Conv1D))) and name not in modules_to_not_convert:\n             # Check if the current key is not in the `modules_to_not_convert`\n             current_key_name_str = \".\".join(current_key_name)\n             if not any(\n@@ -382,7 +382,7 @@ def _create_accelerate_new_hook(old_hook):\n     old_hook_attr = old_hook.__dict__\n     filtered_old_hook_attr = {}\n     old_hook_init_signature = inspect.signature(old_hook_cls.__init__)\n-    for k in old_hook_attr.keys():\n+    for k in old_hook_attr:\n         if k in old_hook_init_signature.parameters:\n             filtered_old_hook_attr[k] = old_hook_attr[k]\n     new_hook = old_hook_cls(**filtered_old_hook_attr)"
        },
        {
            "sha": "24e8546a50f1aecc6cf22cb95fd6134a3eadf4a5",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -333,7 +333,7 @@ def load(module: nn.Module, state_dict, prefix=\"\", assign_to_params_buffers=Fals\n             # In sharded models, each shard has only part of the full state_dict, so only gather\n             # parameters that are in the current state_dict.\n             named_parameters = dict(module.named_parameters(prefix=prefix[:-1], recurse=False))\n-            params_to_gather = [named_parameters[k] for k in state_dict.keys() if k in named_parameters]\n+            params_to_gather = [named_parameters[k] for k in state_dict if k in named_parameters]\n             if len(params_to_gather) > 0:\n                 # because zero3 puts placeholders in model params, this context\n                 # manager gathers (unpartitions) the params of the current layer, then loads from"
        },
        {
            "sha": "9af6eba11f663268bf922c17915565f64a1259a4",
            "filename": "src/transformers/integrations/flash_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fflash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_attention.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -23,7 +23,7 @@ def flash_attention_forward(\n     softcap: Optional[float] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n-    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n+    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\") is not None:\n         logger.warning_once(\n             \"`flash_attention_2` does not support `output_attentions=True` or `head_mask`.\"\n             \" Please set your attention to `eager` if you want any of these features.\""
        },
        {
            "sha": "e310ff8ac5dbf0065865c2487684bcbe323892dc",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -282,7 +282,7 @@ def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n         value = repeat_kv(value, query.shape[1] // value.shape[1])\n         enable_gqa = False\n \n-    kernel_options = kwargs.get(\"kernel_options\", None)\n+    kernel_options = kwargs.get(\"kernel_options\")\n     attn_output, attention_weights = compile_friendly_flex_attention(\n         query,\n         key,"
        },
        {
            "sha": "083ec53a2fd327cafae536b94dda1c05fea39e76",
            "filename": "src/transformers/integrations/hqq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fhqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhqq.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -104,7 +104,7 @@ def prepare_for_hqq_linear(model, quantization_config=None, modules_to_not_conve\n     quant_config = quantization_config.quant_config\n     linear_tags = list(set(linear_tags) - set(skip_modules) - set(modules_to_not_convert))\n \n-    if any(key in linear_tags for key in quant_config.keys()):\n+    if any(key in linear_tags for key in quant_config):\n         # If the user doesn't specify a key from get_linear_tags, the layer is not quantized via (key, None)\n         patch_params = dict.fromkeys(linear_tags)\n         patch_params.update(quant_config)"
        },
        {
            "sha": "f46c6f0f70b4b69c0b3dd1bc5a45dc5e47741cb5",
            "filename": "src/transformers/integrations/sdpa_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_attention.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -51,7 +51,7 @@ def sdpa_attention_forward(\n     is_causal: Optional[bool] = None,\n     **kwargs,\n ) -> tuple[torch.Tensor, None]:\n-    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\", None) is not None:\n+    if kwargs.get(\"output_attentions\", False) or kwargs.get(\"head_mask\") is not None:\n         logger.warning_once(\n             \"`sdpa` attention does not support `output_attentions=True` or `head_mask`.\"\n             \" Please set your attention to `eager` if you want any of these features.\""
        },
        {
            "sha": "b6669a7b45a8d1e0a683423bd8feb548e9814f0c",
            "filename": "src/transformers/keras_callbacks.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fkeras_callbacks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fkeras_callbacks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fkeras_callbacks.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -162,7 +162,7 @@ def _concatenate_batches(batches, padding_index=-100):\n     def _postprocess_predictions_or_labels(self, inputs):\n         if isinstance(inputs[0], dict):\n             outputs = {}\n-            for key in inputs[0].keys():\n+            for key in inputs[0]:\n                 outputs[key] = self._concatenate_batches([batch[key] for batch in inputs])\n             # If it's a dict with only one key, just return the array\n             if len(outputs) == 1:\n@@ -242,7 +242,7 @@ def generation_function(inputs, attention_mask):\n                 labels = {key: batch[key].numpy() for key in self.label_cols}\n             elif isinstance(labels, dict):\n                 labels = {key: array.numpy() for key, array in labels.items()}\n-            elif isinstance(labels, list) or isinstance(labels, tuple):\n+            elif isinstance(labels, (list, tuple)):\n                 labels = [array.numpy() for array in labels]\n             elif isinstance(labels, tf.Tensor):\n                 labels = labels.numpy()"
        },
        {
            "sha": "37a93a319d3229c33d8d743c1bbf35129e1d2351",
            "filename": "src/transformers/loss/loss_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_deformable_detr.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -136,7 +136,7 @@ def DeformableDetrForSegmentationLoss(\n             aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n         weight_dict.update(aux_weight_dict)\n \n-    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n     return loss, loss_dict, auxiliary_outputs\n \n \n@@ -174,5 +174,5 @@ def DeformableDetrForObjectDetectionLoss(\n         for i in range(config.decoder_layers - 1):\n             aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n         weight_dict.update(aux_weight_dict)\n-    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n     return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "a6b2f9646e5954692dac962a19eb99d60f61c4a8",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -522,7 +522,7 @@ def ForSegmentationLoss(\n         for i in range(config.decoder_layers - 1):\n             aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n         weight_dict.update(aux_weight_dict)\n-    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n     return loss, loss_dict, auxiliary_outputs\n \n \n@@ -558,5 +558,5 @@ def ForObjectDetectionLoss(\n         for i in range(config.decoder_layers - 1):\n             aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n         weight_dict.update(aux_weight_dict)\n-    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n     return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "3e18fac8667ff6dc7feac8c0d3d90aa12eee6504",
            "filename": "src/transformers/loss/loss_grounding_dino.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_grounding_dino.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -267,5 +267,5 @@ def GroundingDinoForObjectDetectionLoss(\n             aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n         weight_dict.update(aux_weight_dict)\n \n-    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n     return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "8c68d8b8af102aeceaf86e2b87c09f0e4d44fe0a",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -322,7 +322,7 @@ def infer_metric_tags_from_eval_results(eval_results):\n     if eval_results is None:\n         return {}\n     result = {}\n-    for key in eval_results.keys():\n+    for key in eval_results:\n         if key.lower().replace(\" \", \"_\") in METRIC_TAGS:\n             result[key.lower().replace(\" \", \"_\")] = key\n         elif key.lower() == \"rouge1\":\n@@ -839,7 +839,7 @@ def make_markdown_table(lines):\n     \"\"\"\n     if lines is None or len(lines) == 0:\n         return \"\"\n-    col_widths = {key: len(str(key)) for key in lines[0].keys()}\n+    col_widths = {key: len(str(key)) for key in lines[0]}\n     for line in lines:\n         for key, value in line.items():\n             if col_widths[key] < len(_maybe_round(value)):"
        },
        {
            "sha": "07d83d5e4aa64e822203ef99393ed22b5e5b98e3",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -179,10 +179,10 @@ def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model):\n     flax_state_dict = {}\n \n     load_model_with_head_into_base_model = (model_prefix not in flax_model_params) and (\n-        model_prefix in {k.split(\".\")[0] for k in pt_state_dict.keys()}\n+        model_prefix in {k.split(\".\")[0] for k in pt_state_dict}\n     )\n     load_base_model_into_model_with_head = (model_prefix in flax_model_params) and (\n-        model_prefix not in {k.split(\".\")[0] for k in pt_state_dict.keys()}\n+        model_prefix not in {k.split(\".\")[0] for k in pt_state_dict}\n     )\n \n     # Need to change some parameters name to match Flax names\n@@ -267,10 +267,10 @@ def convert_pytorch_sharded_state_dict_to_flax(shard_filenames, flax_model):\n             random_flax_state_dict = flatten_dict(flax_model_params)\n \n         load_model_with_head_into_base_model = (model_prefix not in flax_model_params) and (\n-            model_prefix in {k.split(\".\")[0] for k in pt_state_dict.keys()}\n+            model_prefix in {k.split(\".\")[0] for k in pt_state_dict}\n         )\n         load_base_model_into_model_with_head = (model_prefix in flax_model_params) and (\n-            model_prefix not in {k.split(\".\")[0] for k in pt_state_dict.keys()}\n+            model_prefix not in {k.split(\".\")[0] for k in pt_state_dict}\n         )\n         # Need to change some parameters name to match Flax names\n         for pt_key, pt_tensor in pt_state_dict.items():\n@@ -381,10 +381,10 @@ def load_flax_weights_in_pytorch_model(pt_model, flax_state):\n     pt_model_dict = pt_model.state_dict()\n \n     load_model_with_head_into_base_model = (pt_model.base_model_prefix in flax_state) and (\n-        pt_model.base_model_prefix not in {k.split(\".\")[0] for k in pt_model_dict.keys()}\n+        pt_model.base_model_prefix not in {k.split(\".\")[0] for k in pt_model_dict}\n     )\n     load_base_model_into_model_with_head = (pt_model.base_model_prefix not in flax_state) and (\n-        pt_model.base_model_prefix in {k.split(\".\")[0] for k in pt_model_dict.keys()}\n+        pt_model.base_model_prefix in {k.split(\".\")[0] for k in pt_model_dict}\n     )\n \n     # keep track of unexpected & missing keys"
        },
        {
            "sha": "bc9a4d473f36f95bb13d6de17dc0bfa7cfae2279",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -138,7 +138,7 @@ def flax_shard_checkpoint(params, max_shard_size=\"10GB\"):\n     for idx, shard in enumerate(sharded_state_dicts):\n         shard_file = FLAX_WEIGHTS_NAME.replace(\".msgpack\", f\"-{idx + 1:05d}-of-{len(sharded_state_dicts):05d}.msgpack\")\n         shards[shard_file] = shard\n-        for weight_name in shard.keys():\n+        for weight_name in shard:\n             weight_map[weight_name] = shard_file\n \n     # Add the metadata\n@@ -963,7 +963,7 @@ def from_pretrained(\n         # Mismatched keys contains tuples key/shape1/shape2 of weights in the checkpoint that have a shape not\n         # matching the weights in the model.\n         mismatched_keys = []\n-        for key in state.keys():\n+        for key in state:\n             if key in random_state and state[key].shape != random_state[key].shape:\n                 if ignore_mismatched_sizes:\n                     mismatched_keys.append((key, state[key].shape, random_state[key].shape))\n@@ -1169,11 +1169,7 @@ def save_pretrained(\n         for filename in os.listdir(save_directory):\n             full_filename = os.path.join(save_directory, filename)\n             weights_no_suffix = weights_name.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n-            if (\n-                filename.startswith(weights_no_suffix)\n-                and os.path.isfile(full_filename)\n-                and filename not in shards.keys()\n-            ):\n+            if filename.startswith(weights_no_suffix) and os.path.isfile(full_filename) and filename not in shards:\n                 os.remove(full_filename)\n \n         if index is None:"
        },
        {
            "sha": "feb6c6c3914f2cd730b33efcca7fd9a597b35469",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -315,7 +315,7 @@ def get_gguf_hf_weights_map(\n     # hf => gguf and gguf => hf mappings are reversed\n     gguf_to_hf_name_map = {}\n     state_dict = hf_model.state_dict()\n-    for hf_name in state_dict.keys():\n+    for hf_name in state_dict:\n         # An exception for qwen2moe model, where the expert layers are packed\n         if model_type == \"qwen2moe\" and \"mlp.experts.\" in hf_name:\n             hf_name = re.sub(r\"mlp.experts.\\d+.\", \"mlp.experts.\", hf_name)"
        },
        {
            "sha": "8f688af7be36439311465d019de36c20c6aaae77",
            "filename": "src/transformers/modeling_tf_pytorch_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_pytorch_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -330,7 +330,7 @@ def load_pytorch_state_dict_in_tf2_model(\n             tf_model(tf_inputs, training=False)  # Make sure model is built\n     # Convert old format to new format if needed from a PyTorch state_dict\n     tf_keys_to_pt_keys = {}\n-    for key in pt_state_dict.keys():\n+    for key in pt_state_dict:\n         new_key = None\n         if \"gamma\" in key:\n             new_key = key.replace(\"gamma\", \"weight\")\n@@ -361,7 +361,7 @@ def load_pytorch_state_dict_in_tf2_model(\n     # and there is no MainLayer class. This means that TF base classes have one\n     # extra layer in their weight names, corresponding to the MainLayer class. This code block compensates for that.\n     start_prefix_to_remove = \"\"\n-    if not any(s.startswith(tf_model.base_model_prefix) for s in tf_keys_to_pt_keys.keys()):\n+    if not any(s.startswith(tf_model.base_model_prefix) for s in tf_keys_to_pt_keys):\n         start_prefix_to_remove = tf_model.base_model_prefix + \".\"\n \n     symbolic_weights = tf_model.trainable_weights + tf_model.non_trainable_weights\n@@ -573,7 +573,7 @@ def load_tf2_state_dict_in_pytorch_model(pt_model, tf_state_dict, allow_missing_\n     # Make sure we are able to load PyTorch base models as well as derived models (with heads)\n     # TF models always have a prefix, some of PyTorch models (base ones) don't\n     start_prefix_to_remove = \"\"\n-    if not any(s.startswith(pt_model.base_model_prefix) for s in current_pt_params_dict.keys()):\n+    if not any(s.startswith(pt_model.base_model_prefix) for s in current_pt_params_dict):\n         start_prefix_to_remove = pt_model.base_model_prefix + \".\"\n \n     # Build a map from potential PyTorch weight names to TF 2.0 Variables"
        },
        {
            "sha": "c7bb80656d1b8596e9f3313b6f768c6d2251099d",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 13,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -991,13 +991,13 @@ def load_tf_weights_from_h5(model, resolved_archive_file, ignore_mismatched_size\n                     # here we check if the current weight is among the weights from the H5 file\n                     # If yes, get the weight_value of the corresponding weight from the H5 file\n                     # If not, make the value to None\n-                    saved_weight_value = saved_weights.get(symbolic_weight_name, None)\n+                    saved_weight_value = saved_weights.get(symbolic_weight_name)\n \n                     # Retrocompatibility patch: some embeddings are stored with the weights name (e.g. Bart's\n                     # `model.shared/embeddings:0` are stored as `model.shared/weights:0`)\n                     if saved_weight_value is None and symbolic_weight_name.endswith(\"embeddings:0\"):\n                         symbolic_weight_name = symbolic_weight_name[:-12] + \"weight:0\"\n-                        saved_weight_value = saved_weights.get(symbolic_weight_name, None)\n+                        saved_weight_value = saved_weights.get(symbolic_weight_name)\n \n                     # Add the updated name to the final list for computing missing/unexpected values\n                     symbolic_weights_names.add(symbolic_weight_name)\n@@ -1637,7 +1637,7 @@ def train_step(self, data):\n                 for key, val in y.items():\n                     if key in arg_names and key not in x:\n                         x[key] = val\n-                    elif output_to_label.get(key, None) in arg_names and key not in x:\n+                    elif output_to_label.get(key) in arg_names and key not in x:\n                         x[output_to_label[key]] = val\n         if y is None:\n             y = {key: val for key, val in x.items() if key in label_kwargs}\n@@ -1662,7 +1662,7 @@ def train_step(self, data):\n             # This next block matches outputs to label keys. Tensorflow's standard method for doing this\n             # can get very confused if any of the keys contain nested values (e.g. lists/tuples of Tensors)\n             if isinstance(y, dict) and len(y) == 1:\n-                if list(y.keys())[0] in y_pred.keys():\n+                if list(y.keys())[0] in y_pred:\n                     y_pred = y_pred[list(y.keys())[0]]\n                 elif list(y_pred.keys())[0] == \"loss\":\n                     y_pred = y_pred[1]\n@@ -1672,7 +1672,7 @@ def train_step(self, data):\n             elif isinstance(y, dict):\n                 # If the labels are a dict, match keys from the output by name\n                 y_pred = {key: val for key, val in y_pred.items() if key in y}\n-            elif isinstance(y, tuple) or isinstance(y, list):\n+            elif isinstance(y, (tuple, list)):\n                 # If the labels are a tuple/list, match keys to the output by order, skipping the loss.\n                 if list(y_pred.keys())[0] == \"loss\":\n                     y_pred = y_pred.to_tuple()[1:]\n@@ -1745,7 +1745,7 @@ def test_step(self, data):\n                 for key, val in y.items():\n                     if key in arg_names and key not in x:\n                         x[key] = val\n-                    elif output_to_label.get(key, None) in arg_names and key not in x:\n+                    elif output_to_label.get(key) in arg_names and key not in x:\n                         x[output_to_label[key]] = val\n         if y is None:\n             y = {key: val for key, val in x.items() if key in label_kwargs}\n@@ -1769,7 +1769,7 @@ def test_step(self, data):\n         # This next block matches outputs to label keys. Tensorflow's standard method for doing this\n         # can get very confused if any of the keys contain nested values (e.g. lists/tuples of Tensors)\n         if isinstance(y, dict) and len(y) == 1:\n-            if list(y.keys())[0] in y_pred.keys():\n+            if list(y.keys())[0] in y_pred:\n                 y_pred = y_pred[list(y.keys())[0]]\n             elif list(y_pred.keys())[0] == \"loss\":\n                 y_pred = y_pred[1]\n@@ -1779,7 +1779,7 @@ def test_step(self, data):\n         elif isinstance(y, dict):\n             # If the labels are a dict, match keys from the output by name\n             y_pred = {key: val for key, val in y_pred.items() if key in y}\n-        elif isinstance(y, tuple) or isinstance(y, list):\n+        elif isinstance(y, (tuple, list)):\n             # If the labels are a tuple/list, match keys to the output by order, skipping the loss.\n             if list(y_pred.keys())[0] == \"loss\":\n                 y_pred = y_pred.to_tuple()[1:]\n@@ -2464,11 +2464,7 @@ def save_pretrained(\n             # If we have a shard file that is not going to be replaced, we delete it, but only from the main process\n             # in distributed settings to avoid race conditions.\n             weights_no_suffix = weights_name.replace(\".bin\", \"\").replace(\".safetensors\", \"\")\n-            if (\n-                filename.startswith(weights_no_suffix)\n-                and os.path.isfile(full_filename)\n-                and filename not in shards.keys()\n-            ):\n+            if filename.startswith(weights_no_suffix) and os.path.isfile(full_filename) and filename not in shards:\n                 os.remove(full_filename)\n \n         if index is None:"
        },
        {
            "sha": "5a7c6eb0871f1552fc95a240f2a43928be97112f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1357,12 +1357,12 @@ def _get_torch_dtype(\n             elif hasattr(torch, torch_dtype):\n                 torch_dtype = getattr(torch, torch_dtype)\n                 config.torch_dtype = torch_dtype\n-                for sub_config_key in config.sub_configs.keys():\n+                for sub_config_key in config.sub_configs:\n                     sub_config = getattr(config, sub_config_key)\n                     sub_config.torch_dtype = torch_dtype\n         elif isinstance(torch_dtype, torch.dtype):\n             config.torch_dtype = torch_dtype\n-            for sub_config_key in config.sub_configs.keys():\n+            for sub_config_key in config.sub_configs:\n                 sub_config = getattr(config, sub_config_key)\n                 sub_config.torch_dtype = torch_dtype\n         elif isinstance(torch_dtype, dict):\n@@ -1388,7 +1388,7 @@ def _get_torch_dtype(\n         # set fp32 as the default dtype for BC\n         default_dtype = torch.get_default_dtype()\n         config.torch_dtype = default_dtype\n-        for key in config.sub_configs.keys():\n+        for key in config.sub_configs:\n             value = getattr(config, key)\n             value.torch_dtype = default_dtype\n \n@@ -1446,7 +1446,7 @@ def _get_device_map(\n \n         # `inferred_max_memory` contains non-reserved memory. There may be *unused* reserved memory in the GPU,\n         # which we can use to allocate parameters.\n-        for device_name in inferred_max_memory.keys():\n+        for device_name in inferred_max_memory:\n             if isinstance(device_name, int):  # it's a GPU device\n                 if is_torch_xpu_available():\n                     unused_memory = torch.xpu.memory_reserved(device_name) - torch.xpu.memory_allocated(device_name)\n@@ -3002,9 +3002,9 @@ def tie_encoder_to_decoder_recursively(\n                     f\"Encoder module {encoder_pointer} does not match decoder module {decoder_pointer}\"\n                 )\n \n-                all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules.keys()}\n+                all_encoder_weights = {module_name + \"/\" + sub_name for sub_name in encoder_modules}\n                 encoder_layer_pos = 0\n-                for name in decoder_modules.keys():\n+                for name in decoder_modules:\n                     if name.isdigit():\n                         encoder_name = str(int(name) + encoder_layer_pos)\n                         decoder_name = name\n@@ -3942,7 +3942,7 @@ def save_pretrained(\n         # Handle the case where some state_dict keys shouldn't be saved\n         if self._keys_to_ignore_on_save is not None:\n             for ignore_key in self._keys_to_ignore_on_save:\n-                if ignore_key in state_dict.keys():\n+                if ignore_key in state_dict:\n                     del state_dict[ignore_key]\n \n         # Rename state_dict keys before saving to file. Do nothing unless overridden in a particular model.\n@@ -4057,7 +4057,7 @@ def save_pretrained(\n             if (\n                 filename.startswith(weights_no_suffix)\n                 and os.path.isfile(full_filename)\n-                and filename not in state_dict_split.filename_to_tensors.keys()\n+                and filename not in state_dict_split.filename_to_tensors\n                 and is_main_process\n                 and reg.fullmatch(filename_no_suffix) is not None\n             ):\n@@ -5334,7 +5334,7 @@ def _load_pretrained_model(\n             if device_map is not None:\n                 device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n             # small sanity check: the base model should not contain task-specific head keys\n-            task_specific_expected_keys = [s for s in model.state_dict().keys() if not s.startswith(_prefix)]\n+            task_specific_expected_keys = [s for s in model.state_dict() if not s.startswith(_prefix)]\n             base_model_expected_keys = list(model_to_load.state_dict().keys())\n             if any(\n                 key in task_specific_expected_keys and key not in base_model_expected_keys for key in checkpoint_keys"
        },
        {
            "sha": "9e6125720d9f2378e62e9e372562352b2539e8e7",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -191,7 +191,7 @@ def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tens\n         \"\"\"\n         batch_size, num_patches = key_value_states.shape[0], key_value_states.shape[1]\n \n-        if num_patches not in self.patch_to_query_dict.keys():\n+        if num_patches not in self.patch_to_query_dict:\n             raise KeyError(\n                 f\"Number of patches {num_patches} not found in patch_to_query_dict amongst possible values {self.patch_to_query_dict.keys()}.\"\n             )"
        },
        {
            "sha": "c27916a0df2d2f05c3d84be438e058074498b315",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -438,7 +438,7 @@ def forward(self, key_value_states: torch.Tensor, attn_mask: Optional[torch.Tens\n         \"\"\"\n         batch_size, num_patches = key_value_states.shape[0], key_value_states.shape[1]\n \n-        if num_patches not in self.patch_to_query_dict.keys():\n+        if num_patches not in self.patch_to_query_dict:\n             raise KeyError(\n                 f\"Number of patches {num_patches} not found in patch_to_query_dict amongst possible values {self.patch_to_query_dict.keys()}.\"\n             )"
        },
        {
            "sha": "325e0f65b47cad8116615b78c4579e48e63c0408",
            "filename": "src/transformers/models/audio_spectrogram_transformer/convert_audio_spectrogram_transformer_original_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fconvert_audio_spectrogram_transformer_original_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -105,7 +105,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "8439affd6d264af6743d540078c8be9e3e7f27c4",
            "filename": "src/transformers/models/auto/auto_factory.py",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fauto_factory.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -425,7 +425,7 @@ def __init__(self, *args, **kwargs) -> None:\n     def from_config(cls, config, **kwargs):\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n-        has_local_code = type(config) in cls._model_mapping.keys()\n+        has_local_code = type(config) in cls._model_mapping\n         if has_remote_code:\n             class_ref = config.auto_map[cls.__name__]\n             if \"--\" in class_ref:\n@@ -451,13 +451,13 @@ def from_config(cls, config, **kwargs):\n             _ = kwargs.pop(\"code_revision\", None)\n             model_class = add_generation_mixin_to_remote_model(model_class)\n             return model_class._from_config(config, **kwargs)\n-        elif type(config) in cls._model_mapping.keys():\n+        elif type(config) in cls._model_mapping:\n             model_class = _get_model_class(config, cls._model_mapping)\n             return model_class._from_config(config, **kwargs)\n \n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n-            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n+            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping)}.\"\n         )\n \n     @classmethod\n@@ -468,7 +468,7 @@ def _prepare_config_for_auto_class(cls, config: PretrainedConfig) -> PretrainedC\n     @classmethod\n     def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[str]], *model_args, **kwargs):\n         config = kwargs.pop(\"config\", None)\n-        trust_remote_code = kwargs.get(\"trust_remote_code\", None)\n+        trust_remote_code = kwargs.get(\"trust_remote_code\")\n         kwargs[\"_from_auto\"] = True\n         hub_kwargs_names = [\n             \"cache_dir\",\n@@ -538,10 +538,10 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             kwargs_orig = copy.deepcopy(kwargs)\n             # ensure not to pollute the config object with torch_dtype=\"auto\" - since it's\n             # meaningless in the context of the config object - torch.dtype values are acceptable\n-            if kwargs.get(\"torch_dtype\", None) == \"auto\":\n+            if kwargs.get(\"torch_dtype\") == \"auto\":\n                 _ = kwargs.pop(\"torch_dtype\")\n             # to not overwrite the quantization_config if config has a quantization_config\n-            if kwargs.get(\"quantization_config\", None) is not None:\n+            if kwargs.get(\"quantization_config\") is not None:\n                 _ = kwargs.pop(\"quantization_config\")\n \n             config, kwargs = AutoConfig.from_pretrained(\n@@ -560,7 +560,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n                 kwargs[\"quantization_config\"] = kwargs_orig[\"quantization_config\"]\n \n         has_remote_code = hasattr(config, \"auto_map\") and cls.__name__ in config.auto_map\n-        has_local_code = type(config) in cls._model_mapping.keys()\n+        has_local_code = type(config) in cls._model_mapping\n         upstream_repo = None\n         if has_remote_code:\n             class_ref = config.auto_map[cls.__name__]\n@@ -593,7 +593,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             return model_class.from_pretrained(\n                 pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n             )\n-        elif type(config) in cls._model_mapping.keys():\n+        elif type(config) in cls._model_mapping:\n             model_class = _get_model_class(config, cls._model_mapping)\n             if model_class.config_class == config.sub_configs.get(\"text_config\", None):\n                 config = config.get_text_config()\n@@ -602,7 +602,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n             )\n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\n-            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping.keys())}.\"\n+            f\"Model type should be one of {', '.join(c.__name__ for c in cls._model_mapping)}.\"\n         )\n \n     @classmethod\n@@ -636,7 +636,7 @@ def _load_timm_backbone_from_pretrained(cls, pretrained_model_name_or_path, *mod\n \n         config = kwargs.pop(\"config\", TimmBackboneConfig())\n \n-        if kwargs.get(\"out_features\", None) is not None:\n+        if kwargs.get(\"out_features\") is not None:\n             raise ValueError(\"Cannot specify `out_features` for timm backbones\")\n \n         if kwargs.get(\"output_loading_info\", False):\n@@ -820,7 +820,7 @@ def keys(self) -> list[type[PretrainedConfig]]:\n         mapping_keys = [\n             self._load_attr_from_module(key, name)\n             for key, name in self._config_mapping.items()\n-            if key in self._model_mapping.keys()\n+            if key in self._model_mapping\n         ]\n         return mapping_keys + list(self._extra_content.keys())\n \n@@ -837,7 +837,7 @@ def values(self) -> list[_LazyAutoMappingValue]:\n         mapping_values = [\n             self._load_attr_from_module(key, name)\n             for key, name in self._model_mapping.items()\n-            if key in self._config_mapping.keys()\n+            if key in self._config_mapping\n         ]\n         return mapping_values + list(self._extra_content.values())\n \n@@ -847,8 +847,8 @@ def items(self) -> list[tuple[type[PretrainedConfig], _LazyAutoMappingValue]]:\n                 self._load_attr_from_module(key, self._config_mapping[key]),\n                 self._load_attr_from_module(key, self._model_mapping[key]),\n             )\n-            for key in self._model_mapping.keys()\n-            if key in self._config_mapping.keys()\n+            for key in self._model_mapping\n+            if key in self._config_mapping\n         ]\n         return mapping_items + list(self._extra_content.items())\n \n@@ -869,7 +869,7 @@ def register(self, key: type[PretrainedConfig], value: _LazyAutoMappingValue, ex\n         \"\"\"\n         if hasattr(key, \"__name__\") and key.__name__ in self._reverse_config_mapping:\n             model_type = self._reverse_config_mapping[key.__name__]\n-            if model_type in self._model_mapping.keys() and not exist_ok:\n+            if model_type in self._model_mapping and not exist_ok:\n                 raise ValueError(f\"'{key}' is already used by a Transformers model.\")\n \n         self._extra_content[key] = value"
        },
        {
            "sha": "99ba9349860f805af3f491773f24142da0590ef2",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -974,10 +974,10 @@ def keys(self) -> list[str]:\n         return list(self._mapping.keys()) + list(self._extra_content.keys())\n \n     def values(self) -> list[type[PretrainedConfig]]:\n-        return [self[k] for k in self._mapping.keys()] + list(self._extra_content.values())\n+        return [self[k] for k in self._mapping] + list(self._extra_content.values())\n \n     def items(self) -> list[tuple[str, type[PretrainedConfig]]]:\n-        return [(k, self[k]) for k in self._mapping.keys()] + list(self._extra_content.items())\n+        return [(k, self[k]) for k in self._mapping] + list(self._extra_content.items())\n \n     def __iter__(self) -> Iterator[str]:\n         return iter(list(self._mapping.keys()) + list(self._extra_content.keys()))\n@@ -989,7 +989,7 @@ def register(self, key: str, value: type[PretrainedConfig], exist_ok=False) -> N\n         \"\"\"\n         Register a new configuration in this mapping.\n         \"\"\"\n-        if key in self._mapping.keys() and not exist_ok:\n+        if key in self._mapping and not exist_ok:\n             raise ValueError(f\"'{key}' is already used by a Transformers config, pick another name.\")\n         self._extra_content[key] = value\n \n@@ -1230,7 +1230,7 @@ def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike[s\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )"
        },
        {
            "sha": "7816e739c6a88d76e2f64796b424c247219d4247",
            "filename": "src/transformers/models/auto/feature_extraction_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ffeature_extraction_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -342,7 +342,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -400,7 +400,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n         raise ValueError(\n             f\"Unrecognized feature extractor in {pretrained_model_name_or_path}. Should have a \"\n             f\"`feature_extractor_type` key in its {FEATURE_EXTRACTOR_NAME} of {CONFIG_NAME}, or one of the following \"\n-            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in FEATURE_EXTRACTOR_MAPPING_NAMES.keys())}\"\n+            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in FEATURE_EXTRACTOR_MAPPING_NAMES)}\"\n         )\n \n     @staticmethod"
        },
        {
            "sha": "cefa1335ebaf83af8248d3349ffea0931a21b1e6",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -451,7 +451,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -625,7 +625,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         raise ValueError(\n             f\"Unrecognized image processor in {pretrained_model_name_or_path}. Should have a \"\n             f\"`image_processor_type` key in its {IMAGE_PROCESSOR_NAME} of {CONFIG_NAME}, or one of the following \"\n-            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in IMAGE_PROCESSOR_MAPPING_NAMES.keys())}\"\n+            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in IMAGE_PROCESSOR_MAPPING_NAMES)}\"\n         )\n \n     @staticmethod"
        },
        {
            "sha": "0d711cee0669ab1da7cc205b507451fa25f06845",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -261,7 +261,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -276,9 +276,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n         # First, let's see if we have a processor or preprocessor config.\n         # Filter the kwargs for `cached_file`.\n-        cached_file_kwargs = {\n-            key: kwargs[key] for key in inspect.signature(cached_file).parameters.keys() if key in kwargs\n-        }\n+        cached_file_kwargs = {key: kwargs[key] for key in inspect.signature(cached_file).parameters if key in kwargs}\n         # We don't want to raise\n         cached_file_kwargs.update(\n             {"
        },
        {
            "sha": "f9832df525be496d84c6ced0945f55faf4cff0a7",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -875,7 +875,7 @@ def get_tokenizer_config(\n             raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n         token = use_auth_token\n \n-    commit_hash = kwargs.get(\"_commit_hash\", None)\n+    commit_hash = kwargs.get(\"_commit_hash\")\n     resolved_config_file = cached_file(\n         pretrained_model_name_or_path,\n         TOKENIZER_CONFIG_FILE,\n@@ -1000,7 +1000,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -1012,7 +1012,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         use_fast = kwargs.pop(\"use_fast\", True)\n         tokenizer_type = kwargs.pop(\"tokenizer_type\", None)\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n-        gguf_file = kwargs.get(\"gguf_file\", None)\n+        gguf_file = kwargs.get(\"gguf_file\")\n \n         # First, let's see whether the tokenizer_type is passed so that we can leverage it\n         if tokenizer_type is not None:\n@@ -1022,7 +1022,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n             if tokenizer_class_tuple is None:\n                 raise ValueError(\n                     f\"Passed `tokenizer_type` {tokenizer_type} does not exist. `tokenizer_type` should be one of \"\n-                    f\"{', '.join(c for c in TOKENIZER_MAPPING_NAMES.keys())}.\"\n+                    f\"{', '.join(c for c in TOKENIZER_MAPPING_NAMES)}.\"\n                 )\n \n             tokenizer_class_name, tokenizer_fast_class_name = tokenizer_class_tuple\n@@ -1142,7 +1142,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n \n         raise ValueError(\n             f\"Unrecognized configuration class {config.__class__} to build an AutoTokenizer.\\n\"\n-            f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING.keys())}.\"\n+            f\"Model type should be one of {', '.join(c.__name__ for c in TOKENIZER_MAPPING)}.\"\n         )\n \n     @staticmethod"
        },
        {
            "sha": "545fcc4d92e3273626c92ac262080ad281c91172",
            "filename": "src/transformers/models/auto/video_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fvideo_processing_auto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -291,7 +291,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -364,7 +364,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         raise ValueError(\n             f\"Unrecognized video processor in {pretrained_model_name_or_path}. Should have a \"\n             f\"`video_processor_type` key in its {VIDEO_PROCESSOR_NAME} of {CONFIG_NAME}, or one of the following \"\n-            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in VIDEO_PROCESSOR_MAPPING_NAMES.keys())}\"\n+            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in VIDEO_PROCESSOR_MAPPING_NAMES)}\"\n         )\n \n     @staticmethod"
        },
        {
            "sha": "814db3ca4faa490de06284a5da21512eea791368",
            "filename": "src/transformers/models/bit/convert_bit_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbit%2Fconvert_bit_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -94,7 +94,7 @@ def convert_bit_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub=Fal\n \n     # load state_dict of original model\n     state_dict = timm_model.state_dict()\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val.squeeze() if \"head\" in key else val\n "
        },
        {
            "sha": "267b0ffcb0ca46868795f3607be77673a7338432",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -443,7 +443,7 @@ class BlipPreTrainedModel(PreTrainedModel):\n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n         factor = self.config.initializer_range\n-        if isinstance(module, nn.Conv2d) or isinstance(module, nn.Embedding) or isinstance(module, nn.Linear):\n+        if isinstance(module, (nn.Conv2d, nn.Embedding, nn.Linear)):\n             module.weight.data.normal_(mean=0.0, std=factor)\n             if hasattr(module, \"bias\") and module.bias is not None:\n                 module.bias.data.zero_()"
        },
        {
            "sha": "7dae1126e03b12b39b17b8fdc00b6050f9681fcd",
            "filename": "src/transformers/models/blip/modeling_tf_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_tf_blip_text.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1096,8 +1096,8 @@ def prepare_inputs_for_generation(self, input_ids, past_key_values=None, attenti\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"past_key_values\": past_key_values,\n-            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\", None),\n-            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\", None),\n+            \"encoder_hidden_states\": model_kwargs.get(\"encoder_hidden_states\"),\n+            \"encoder_attention_mask\": model_kwargs.get(\"encoder_attention_mask\"),\n             \"is_decoder\": True,\n         }\n "
        },
        {
            "sha": "26be31dcbb4f6726e903e1b108ed439ed9d5da59",
            "filename": "src/transformers/models/bloom/convert_bloom_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fconvert_bloom_original_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -114,7 +114,7 @@ def convert_bloom_checkpoint_to_pytorch(\n                 if tensors is None:\n                     tensors = temp\n                 else:\n-                    for key in tensors.keys():\n+                    for key in tensors:\n                         if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n                             # We average (sum and then divide) some weights across TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n                             tensors[key] += temp[key]\n@@ -125,7 +125,7 @@ def convert_bloom_checkpoint_to_pytorch(\n                             tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n \n             # Divide by the number of TP the weights we want to average\n-            for key in tensors.keys():\n+            for key in tensors:\n                 if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n                     tensors[key] = tensors[key] / pretraining_tp\n             torch.save(\n@@ -136,7 +136,7 @@ def convert_bloom_checkpoint_to_pytorch(\n                 ),\n             )\n \n-            for key in tensors.keys():\n+            for key in tensors:\n                 value = tensors[key]\n                 total_size += value.numel() * get_dtype_size(value.dtype)\n                 if key not in index_dict[\"weight_map\"]:\n@@ -174,7 +174,7 @@ def convert_bloom_checkpoint_to_pytorch(\n                 if tensors is None:\n                     tensors = temp\n                 else:\n-                    for key in tensors.keys():\n+                    for key in tensors:\n                         # We average (sum and then divide) some weights across TP ranks (see https://github.com/bigscience-workshop/Megatron-DeepSpeed/blob/olruwase/sync_layer_norms/megatron/training.py#L425)\n                         if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n                             tensors[key] += temp[key]\n@@ -185,7 +185,7 @@ def convert_bloom_checkpoint_to_pytorch(\n                             tensors[key] = torch.cat([tensors[key], temp[key]], dim=cat_dim)\n \n             # Divide by the number of TP the weights we want to average\n-            for key in tensors.keys():\n+            for key in tensors:\n                 if any(key.endswith(end) for end in WEIGHTS_TO_AVERAGE_ENDSWITH):\n                     tensors[key] = tensors[key] / pretraining_tp\n "
        },
        {
            "sha": "35c89a88da6940fe71b384de8205b63f413e8d67",
            "filename": "src/transformers/models/bros/convert_bros_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fconvert_bros_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -55,7 +55,7 @@ def rename_key(name):\n \n def convert_state_dict(orig_state_dict, model):\n     # rename keys\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n         orig_state_dict[rename_key(key)] = val\n "
        },
        {
            "sha": "86b766eabf5af92037088038307480f3d5e10ff0",
            "filename": "src/transformers/models/clipseg/convert_clipseg_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconvert_clipseg_original_pytorch_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -112,7 +112,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if key.startswith(\"clip_model\") and \"attn.in_proj\" in key:\n@@ -172,7 +172,7 @@ def convert_clipseg_checkpoint(model_name, checkpoint_path, pytorch_dump_folder_\n     state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n \n     # remove some keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if key.startswith(\"model\"):\n             state_dict.pop(key, None)\n "
        },
        {
            "sha": "4b0b285561c5475a624c4e33de429ec204951ae3",
            "filename": "src/transformers/models/clvp/tokenization_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Ftokenization_clvp.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -300,7 +300,7 @@ def _tokenize(self, text):\n \n             # if the token is \"\" we replace it with \"[SPACE]\" (if \"[SPACE]\" is present in the vocab), otherwise we keep the \"\".\n             bpe_tokens.extend(\n-                \"[SPACE]\" if bpe_token == \"\\u0120\" and \"[SPACE]\" in self.encoder.keys() else bpe_token\n+                \"[SPACE]\" if bpe_token == \"\\u0120\" and \"[SPACE]\" in self.encoder else bpe_token\n                 for bpe_token in self.bpe(token).split(\" \")\n             )\n "
        },
        {
            "sha": "22658419eb744c7663f6fa87234ca328c2b00592",
            "filename": "src/transformers/models/conditional_detr/convert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconvert_conditional_detr_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -267,7 +267,7 @@ def convert_conditional_detr_checkpoint(model_name, pytorch_dump_folder_path):\n     read_in_q_k_v(state_dict, is_panoptic=is_panoptic)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"conditional_detr.model.\" if is_panoptic else \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if is_panoptic:\n             if (\n                 key.startswith(\"conditional_detr\")"
        },
        {
            "sha": "f48af23ccfd44b213df476e017aafc65d5b3e068",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -327,8 +327,8 @@ def __init__(self, **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs]) ->\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "426ed98b883bbbccb7b7833b050162f5687563c5",
            "filename": "src/transformers/models/convnext/convert_convnext_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fconvert_convnext_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -128,11 +128,11 @@ def convert_convnext_checkpoint(checkpoint_url, pytorch_dump_folder_path):\n     # load original state_dict from URL\n     state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)[\"model\"]\n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n     # add prefix to all keys expect classifier head\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         if not key.startswith(\"classifier\"):\n             key = \"convnext.\" + key"
        },
        {
            "sha": "d23f248816e2cf5694ba2debd145220ba42bd04e",
            "filename": "src/transformers/models/convnextv2/convert_convnextv2_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconvert_convnextv2_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconvert_convnextv2_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnextv2%2Fconvert_convnextv2_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -153,11 +153,11 @@ def convert_convnextv2_checkpoint(checkpoint_url, pytorch_dump_folder_path, save\n \n     print(\"Converting model parameters...\")\n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n     # add prefix to all keys expect classifier head\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         if not key.startswith(\"classifier\"):\n             key = \"convnextv2.\" + key"
        },
        {
            "sha": "1dce90147bd854b5072410eb6d6e74f73c0a9fd0",
            "filename": "src/transformers/models/ctrl/modeling_tf_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_tf_ctrl.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -686,15 +686,15 @@ def set_bias(self, value):\n \n     # Copied from transformers.models.gpt2.modeling_tf_gpt2.TFGPT2LMHeadModel.prepare_inputs_for_generation\n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n-        token_type_ids = kwargs.get(\"token_type_ids\", None)\n+        token_type_ids = kwargs.get(\"token_type_ids\")\n         # only last token for inputs_ids if past is defined in kwargs\n         if past_key_values:\n             inputs = tf.expand_dims(inputs[:, -1], -1)\n             if token_type_ids is not None:\n                 token_type_ids = tf.expand_dims(token_type_ids[:, -1], -1)\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n+        attention_mask = kwargs.get(\"attention_mask\")\n \n         if attention_mask is not None and position_ids is None:\n             position_ids = tf.math.cumsum(attention_mask, axis=-1, exclusive=True)"
        },
        {
            "sha": "0b77ee35578ee0b5a1d6bd0de71b3d02fa45c7f6",
            "filename": "src/transformers/models/d_fine/convert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconvert_d_fine_original_pytorch_checkpoint_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -158,7 +158,7 @@ def load_original_state_dict(repo_id, model_name):\n \n     original_state_dict = {}\n     model = torch.load(directory_path, map_location=\"cpu\")[\"model\"]\n-    for key in model.keys():\n+    for key in model:\n         original_state_dict[key] = model[key]\n \n     return original_state_dict\n@@ -406,7 +406,7 @@ def convert_d_fine_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub,\n     # query, key and value matrices need special treatment\n     read_in_q_k_v(state_dict, config, model_name)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if key.endswith(\"num_batches_tracked\"):\n             del state_dict[key]\n         # for two_stage"
        },
        {
            "sha": "efaac368f64b2f3548cdf33c08ac45a15966f564",
            "filename": "src/transformers/models/dab_detr/convert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconvert_dab_detr_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -180,7 +180,7 @@ def write_model(model_name, pretrained_model_weights_path, pytorch_dump_folder_p\n     gc.collect()\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if not key.startswith(\"class_embed\") and not key.startswith(\"bbox_predictor\"):\n             val = state_dict.pop(key)\n             state_dict[prefix + key] = val"
        },
        {
            "sha": "dbd7fa3f4d233dc6e8f3b539397dbb39e9fbc29b",
            "filename": "src/transformers/models/deformable_detr/convert_deformable_detr_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconvert_deformable_detr_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -127,14 +127,14 @@ def convert_deformable_detr_checkpoint(\n     # load original state dict\n     state_dict = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n     # query, key and value matrices need special treatment\n     read_in_q_k_v(state_dict)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if not key.startswith(\"class_embed\") and not key.startswith(\"bbox_embed\"):\n             val = state_dict.pop(key)\n             state_dict[prefix + key] = val"
        },
        {
            "sha": "b78ae6ee66db79b03b4dc18a9d92c5082f9bfe2d",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -318,8 +318,8 @@ def __init__(self, **kwargs: Unpack[DeformableDetrFastImageProcessorKwargs]) ->\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "2a38bc05ccac26b30fd89dec41438b2696083bb9",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_resnet_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -238,7 +238,7 @@ def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n     read_in_decoder_q_k_v(state_dict, config)\n \n     # fix some prefixes\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if \"transformer.decoder.class_embed\" in key or \"transformer.decoder.bbox_embed\" in key:\n             val = state_dict.pop(key)\n             state_dict[key.replace(\"transformer.decoder\", \"model.decoder\")] = val"
        },
        {
            "sha": "a72c8c54221c1220fed143de95ad1e1eaef8d99a",
            "filename": "src/transformers/models/deprecated/deta/convert_deta_swin_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconvert_deta_swin_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -244,7 +244,7 @@ def convert_deta_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub):\n     read_in_decoder_q_k_v(state_dict, config)\n \n     # fix some prefixes\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if \"transformer.decoder.class_embed\" in key or \"transformer.decoder.bbox_embed\" in key:\n             val = state_dict.pop(key)\n             state_dict[key.replace(\"transformer.decoder\", \"model.decoder\")] = val"
        },
        {
            "sha": "edc0f2598afe33baa98c2ef7bf7bc7bcca00965d",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -2042,7 +2042,7 @@ def forward(\n                     aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n                 aux_weight_dict.update({k + \"_enc\": v for k, v in weight_dict.items()})\n                 weight_dict.update(aux_weight_dict)\n-            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+            loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict if k in weight_dict)\n \n         if not return_dict:\n             if auxiliary_outputs is not None:"
        },
        {
            "sha": "7b1a4aa5f207a38e9d4992a2182ff333ab399e03",
            "filename": "src/transformers/models/deprecated/efficientformer/convert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fefficientformer%2Fconvert_efficientformer_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -105,7 +105,7 @@ def rename_key(old_name, num_meta4D_last_stage):\n \n \n def convert_torch_checkpoint(checkpoint, num_meta4D_last_stage):\n-    for key in checkpoint.copy().keys():\n+    for key in checkpoint.copy():\n         val = checkpoint.pop(key)\n         checkpoint[rename_key(key, num_meta4D_last_stage)] = val\n "
        },
        {
            "sha": "76b9c9cf328cdc61b9a3df893386b5be1874f920",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/convert_gptsan_tf_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fconvert_gptsan_tf_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -38,7 +38,7 @@ def convert_tf_gptsan_to_pt(args):\n     with tf.device(\"/CPU:0\"):\n         reader = tf.train.load_checkpoint(args.tf_model_dir)\n         shapes = reader.get_variable_to_shape_map()\n-        for key_name in shapes.keys():\n+        for key_name in shapes:\n             vnp = reader.get_tensor(key_name).astype(np.float16)\n             if key_name.endswith(\"/adam_m\") or key_name.endswith(\"/adam_v\"):\n                 continue"
        },
        {
            "sha": "c67b27f64fa16c8b0c69f2cb0c31f7d4d5688437",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/tokenization_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Ftokenization_gptsan_japanese.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -398,7 +398,7 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n         self.vocab = vocab  # same as swe\n         self.ids_to_tokens = ids_to_tokens  # same as bpe\n         self.emoji = emoji\n-        self.maxlen = np.max([len(w) for w in self.vocab.keys()])\n+        self.maxlen = np.max([len(w) for w in self.vocab])\n         self.content_repatter1 = re.compile(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+$,%#]+)\")\n         self.content_repatter2 = re.compile(r\"[A-Za-z0-9\\._+]*@[\\-_0-9A-Za-z]+(\\.[A-Za-z]+)*\")\n         self.content_repatter3 = re.compile(r\"[\\(]{0,1}[0-9]{2,4}[\\)\\-\\(]{0,1}[0-9]{2,4}[\\)\\-]{0,1}[0-9]{3,4}\")"
        },
        {
            "sha": "19bcaac3f572c75fa247936eac5852e2d8d418ce",
            "filename": "src/transformers/models/deprecated/graphormer/collating_graphormer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -27,12 +27,12 @@ def convert_to_single_emb(x, offset: int = 512):\n def preprocess_item(item, keep_features=True):\n     requires_backends(preprocess_item, [\"cython\"])\n \n-    if keep_features and \"edge_attr\" in item.keys():  # edge_attr\n+    if keep_features and \"edge_attr\" in item:  # edge_attr\n         edge_attr = np.asarray(item[\"edge_attr\"], dtype=np.int64)\n     else:\n         edge_attr = np.ones((len(item[\"edge_index\"][0]), 1), dtype=np.int64)  # same embedding for all\n \n-    if keep_features and \"node_feat\" in item.keys():  # input_nodes\n+    if keep_features and \"node_feat\" in item:  # input_nodes\n         node_feature = np.asarray(item[\"node_feat\"], dtype=np.int64)\n     else:\n         node_feature = np.ones((item[\"num_nodes\"], 1), dtype=np.int64)  # same embedding for all"
        },
        {
            "sha": "29763daaa30a145b5b2156cc95e515a38a291cb5",
            "filename": "src/transformers/models/deprecated/jukebox/convert_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fconvert_jukebox.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -231,7 +231,7 @@ def convert_openai_checkpoint(model_name=None, pytorch_dump_folder_path=None):\n         old_dic = torch.load(f\"{pytorch_dump_folder_path}/{dict_name.split('/')[-1]}\", weights_only=True)[\"model\"]\n \n         new_dic = {}\n-        for k in old_dic.keys():\n+        for k in old_dic:\n             if k.endswith(\".b\"):\n                 new_dic[k.replace(\"b\", \"bias\")] = old_dic[k]\n             elif k.endswith(\".w\"):"
        },
        {
            "sha": "f928d49cf5f73156c1352e1a5cb8695a57a08c1c",
            "filename": "src/transformers/models/deprecated/jukebox/modeling_jukebox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fjukebox%2Fmodeling_jukebox.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -2269,7 +2269,7 @@ class JukeboxPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = False\n \n     def _init_weights(self, module):\n-        if isinstance(module, JukeboxPrior) or isinstance(module, JukeboxVQVAE):\n+        if isinstance(module, (JukeboxPrior, JukeboxVQVAE)):\n             module.apply(module._init_weights)\n \n     def __init__(self, *inputs, **kwargs):"
        },
        {
            "sha": "6ac5dd4df11e41e234199fcb106e4a9d2bdd104c",
            "filename": "src/transformers/models/deprecated/mega/convert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fconvert_mega_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -188,7 +188,7 @@ def convert_checkpoint_to_huggingface(pretrained_checkpoint_path, output_path, i\n     # also renaming previously confusing parameter names\n     original_state_dict = original_mlm.mega.encoders.state_dict()\n     updated_keys = {}\n-    for module_name in original_state_dict.keys():\n+    for module_name in original_state_dict:\n         new_module_name = None\n         # have to handle gamma, beta, and alpha differently due to their use\n         # in multiple modules within the original repository;"
        },
        {
            "sha": "235cb81717d1244a437aa8eca9113a828d9cdc7d",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -787,7 +787,7 @@ def prepare_inputs_for_generation(\n \n             input_ids = input_ids[:, remove_prefix_length:]\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n         if attention_mask is not None and position_ids is None:\n             # create position_ids on the fly for batch generation\n             position_ids = attention_mask.long().cumsum(-1) - 1"
        },
        {
            "sha": "b32383ddd497828b6a429d39c5d4069a21154b9f",
            "filename": "src/transformers/models/deprecated/tapex/tokenization_tapex.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftapex%2Ftokenization_tapex.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1354,12 +1354,12 @@ def truncate_table_cells(self, table_content: dict, question: str, answer: list)\n         # modify the answer list\n         if answer is not None:\n             for i, case in enumerate(answer):\n-                if case in cell_mapping.keys():\n+                if case in cell_mapping:\n                     answer[i] = cell_mapping[case]\n \n     def truncate_cell(self, cell_value):\n         # do not process on these cases\n-        if isinstance(cell_value, int) or isinstance(cell_value, float):\n+        if isinstance(cell_value, (int, float)):\n             return cell_value\n         if cell_value.strip() != \"\":\n             try_tokens = self.tokenize(cell_value)"
        },
        {
            "sha": "ec43af68d76cea109cc12132f8f2d08865719976",
            "filename": "src/transformers/models/deprecated/van/convert_van_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvan%2Fconvert_van_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -45,7 +45,7 @@ class Tracker:\n     handles: list = field(default_factory=list)\n \n     def _forward_hook(self, m, inputs: Tensor, outputs: Tensor):\n-        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d)\n+        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, (nn.Conv2d, nn.BatchNorm2d))\n         if has_not_submodules:\n             if not isinstance(m, VanLayerScaling):\n                 self.traced.append(m)"
        },
        {
            "sha": "6bc14a0e154f479097e5cf6869c7145374b3e5d9",
            "filename": "src/transformers/models/depth_pro/configuration_depth_pro.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fconfiguration_depth_pro.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -158,7 +158,7 @@ def __init__(\n         self.patch_model_config = patch_model_config\n         self.fov_model_config = fov_model_config\n \n-        for sub_config_key in self.sub_configs.keys():\n+        for sub_config_key in self.sub_configs:\n             sub_config = getattr(self, sub_config_key)\n \n             if sub_config is None:"
        },
        {
            "sha": "8a7a2e0e0af83f8cab1e4d714b5829eb00ddaa97",
            "filename": "src/transformers/models/detr/convert_detr_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -224,7 +224,7 @@ def convert_detr_checkpoint(model_name, pytorch_dump_folder_path):\n     read_in_q_k_v(state_dict, is_panoptic=is_panoptic)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"detr.model.\" if is_panoptic else \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if is_panoptic:\n             if (\n                 key.startswith(\"detr\")"
        },
        {
            "sha": "ffc755074d50dfe7e15e8f96b6ec1df14c6f23f7",
            "filename": "src/transformers/models/detr/convert_detr_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconvert_detr_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -310,7 +310,7 @@ def convert_detr_checkpoint(model_name, pytorch_dump_folder_path=None, push_to_h\n     read_in_q_k_v(state_dict, is_panoptic=is_panoptic)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"detr.model.\" if is_panoptic else \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if is_panoptic:\n             if (\n                 key.startswith(\"detr\")"
        },
        {
            "sha": "70ae52fef4b384e7eaea8eb382dfdaf600f7deff",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -339,8 +339,8 @@ def __init__(self, **kwargs: Unpack[DetrFastImageProcessorKwargs]) -> None:\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "5111e77644b39e2fe30f9ea1abcd19b95d0a5436",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -421,7 +421,7 @@ def generate(\n         **kwargs,\n     ) -> Union[GenerateOutput, torch.LongTensor]:\n         # We expect the initial input ids to be the complete mask (delayed input)\n-        delay_mask = kwargs.get(\"decoder_input_ids\", None)\n+        delay_mask = kwargs.get(\"decoder_input_ids\")\n         if delay_mask is not None:\n             delay_mask = delay_mask.clone()\n "
        },
        {
            "sha": "d58cdd6224798d7064cb93dbc7f2241f89462c37",
            "filename": "src/transformers/models/donut/convert_donut_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fconvert_donut_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -93,7 +93,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, model):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "ce53018a76273a3f41d1776bdb336a197c10cbd1",
            "filename": "src/transformers/models/dpt/convert_dpt_hybrid_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_hybrid_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -230,7 +230,7 @@ def convert_dpt_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub\n     # remove certain keys\n     remove_ignore_keys_(state_dict)\n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n     # read in qkv matrices"
        },
        {
            "sha": "1341f8908bcd0a2707450096ebd94a4475eb0e26",
            "filename": "src/transformers/models/dpt/convert_dpt_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconvert_dpt_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -197,7 +197,7 @@ def convert_dpt_checkpoint(checkpoint_url, pytorch_dump_folder_path, push_to_hub\n     # remove certain keys\n     remove_ignore_keys_(state_dict)\n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n     # read in qkv matrices"
        },
        {
            "sha": "b065266795740ddf016b6cde64210b4b1c8fce64",
            "filename": "src/transformers/models/encoder_decoder/modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -373,9 +373,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "67fa50ac361b4b4fecb8619ead9dec60304d95aa",
            "filename": "src/transformers/models/encoder_decoder/modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_flax_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -818,9 +818,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "da0dde6915e55963567ad07dbe069a059ddcc13b",
            "filename": "src/transformers/models/encoder_decoder/modeling_tf_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencoder_decoder%2Fmodeling_tf_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -374,9 +374,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "37b9b11103ad16ced2cfbd8ee361d52f6da71e51",
            "filename": "src/transformers/models/eomt/image_processing_eomt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fimage_processing_eomt.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -239,7 +239,7 @@ def compute_segments(\n def get_target_size(size_dict: dict[str, int]) -> tuple[int, int]:\n     \"\"\"Returns the height and width from a size dict.\"\"\"\n     target_height = size_dict[\"shortest_edge\"]\n-    target_width = size_dict.get(\"longest_edge\", None) or target_height\n+    target_width = size_dict.get(\"longest_edge\") or target_height\n \n     return target_height, target_width\n "
        },
        {
            "sha": "a6b73b80cee598f93854e9ce10572d3a5b8080cb",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -650,7 +650,7 @@ def __init__(self, *inputs, **kwargs):\n \n     def _init_weights(self, module: nn.Module):\n         \"\"\"Initialize the weights.\"\"\"\n-        if isinstance(module, nn.Linear) or isinstance(module, FalconLinear):\n+        if isinstance(module, (nn.Linear, FalconLinear)):\n             # Slightly different from the TF version which uses truncated_normal for initialization\n             # cf https://github.com/pytorch/pytorch/pull/5617\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "d6af117e567541bbaf704eb29a4dfff1b05916da",
            "filename": "src/transformers/models/focalnet/convert_focalnet_to_hf_format.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffocalnet%2Fconvert_focalnet_to_hf_format.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -141,7 +141,7 @@ def convert_focalnet_checkpoint(model_name, pytorch_dump_folder_path, push_to_hu\n     state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=\"cpu\")[\"model\"]\n \n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         state_dict[rename_key(key)] = val\n "
        },
        {
            "sha": "35e826585049c023381e37872dd317b70856decd",
            "filename": "src/transformers/models/fsmt/convert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconvert_fsmt_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -134,7 +134,7 @@ def convert_fsmt_checkpoint_to_pytorch(fsmt_checkpoint_path, pytorch_dump_folder\n     # detect whether this is a do_lower_case situation, which can be derived by checking whether we\n     # have at least one uppercase letter in the source vocab\n     do_lower_case = True\n-    for k in src_vocab.keys():\n+    for k in src_vocab:\n         if not k.islower():\n             do_lower_case = False\n             break"
        },
        {
            "sha": "54e5054efb2522ab3d60ecbfd4851444822212b8",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -607,7 +607,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         vision_data = {}\n         if image_sizes is not None:\n-            size = kwargs.get(\"size\", None) or self.image_processor.size\n+            size = kwargs.get(\"size\") or self.image_processor.size\n             padded_height, padded_width = size[\"height\"], size[\"width\"]\n \n             num_image_tokens = []"
        },
        {
            "sha": "978df567c79f2ffe919916c59947bc48d62f5b1d",
            "filename": "src/transformers/models/gemma3n/configuration_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fconfiguration_gemma3n.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -509,7 +509,7 @@ def __init__(\n \n     @classmethod\n     def from_dict(cls, config_dict: dict[str, Any], **kwargs):\n-        label_names = config_dict.get(\"label_names\", None)\n+        label_names = config_dict.get(\"label_names\")\n         is_custom_model = \"num_labels\" in kwargs or \"id2label\" in kwargs\n \n         # if no labels added to config, use imagenet labeller in timm"
        },
        {
            "sha": "5a195b06ecdbc104b5a45c5ffce5f5dfb593451d",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1451,7 +1451,7 @@ def prepare_inputs_for_generation(\n         return {\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"pixel_values\": kwargs.get(\"pixel_values\", None),\n+            \"pixel_values\": kwargs.get(\"pixel_values\"),\n             \"past_key_values\": past_key_values,\n             \"use_cache\": use_cache,\n         }"
        },
        {
            "sha": "a9398805e9ef84f958d65e920d2cdffff77ceb9b",
            "filename": "src/transformers/models/glm4v/convert_glm4v_mgt_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fconvert_glm4v_mgt_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -232,7 +232,7 @@ def save_sharded_model(state_dict, output_path, max_shard_size_gb=5, num_layers=\n         shard_filename = f\"model-{i + 1:05d}-of-{len(shards):05d}.safetensors\"\n         shard_path = os.path.join(output_path, shard_filename)\n \n-        for param_name in shard.keys():\n+        for param_name in shard:\n             index_dict[\"weight_map\"][param_name] = shard_filename\n \n         save_file(shard, shard_path, metadata={\"format\": \"pt\"})"
        },
        {
            "sha": "7e1100d64ed042252faa9eb964e8b81fca938f1b",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -718,7 +718,7 @@ def parallelize(self, device_map=None):\n         )\n         assert_device_map(self.device_map, len(self.h))\n         self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n+        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n         self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n         self.wte = self.wte.to(self.first_device)\n         self.wpe = self.wpe.to(self.first_device)"
        },
        {
            "sha": "42e23fc290151f09d47a30efca1cb7f4e4a3d669",
            "filename": "src/transformers/models/gpt2/modeling_tf_gpt2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_tf_gpt2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -845,15 +845,15 @@ def set_output_embeddings(self, value):\n         self.set_input_embeddings(value)\n \n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n-        token_type_ids = kwargs.get(\"token_type_ids\", None)\n+        token_type_ids = kwargs.get(\"token_type_ids\")\n         # only last token for inputs_ids if past is defined in kwargs\n         if past_key_values:\n             inputs = tf.expand_dims(inputs[:, -1], -1)\n             if token_type_ids is not None:\n                 token_type_ids = tf.expand_dims(token_type_ids[:, -1], -1)\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n+        attention_mask = kwargs.get(\"attention_mask\")\n \n         if attention_mask is not None and position_ids is None:\n             position_ids = tf.math.cumsum(attention_mask, axis=-1, exclusive=True)"
        },
        {
            "sha": "145a45da0db6d36f75f5cec6091027e36541184e",
            "filename": "src/transformers/models/gpt2/tokenization_gpt2_tf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Ftokenization_gpt2_tf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -61,7 +61,7 @@ def from_tokenizer(cls, tokenizer: GPT2Tokenizer, *args, **kwargs):\n         tf_tokenizer = TFGPT2Tokenizer.from_tokenizer(tokenizer)\n         ```\n         \"\"\"\n-        merges = [\" \".join(m) for m in tokenizer.bpe_ranks.keys()]\n+        merges = [\" \".join(m) for m in tokenizer.bpe_ranks]\n         vocab = tokenizer.get_vocab()\n         return cls(vocab, merges, *args, **kwargs)\n "
        },
        {
            "sha": "891f77ece304f8749fcd42d33661fa04bab6ebaa",
            "filename": "src/transformers/models/gpt_neox_japanese/tokenization_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Ftokenization_gpt_neox_japanese.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -221,7 +221,7 @@ def __init__(self, vocab, ids_to_tokens, emoji):\n         self.vocab = vocab  # same as swe\n         self.ids_to_tokens = ids_to_tokens  # same as bpe\n         self.emoji = emoji\n-        self.maxlen = np.max([len(w) for w in self.vocab.keys()])\n+        self.maxlen = np.max([len(w) for w in self.vocab])\n         self.content_repatter1 = re.compile(r\"(https?|ftp)(:\\/\\/[-_\\.!~*\\'()a-zA-Z0-9;\\/?:\\@&=\\+$,%#]+)\")\n         self.content_repatter2 = re.compile(r\"[A-Za-z0-9\\._+]*@[\\-_0-9A-Za-z]+(\\.[A-Za-z]+)*\")\n         self.content_repatter3 = re.compile(r\"[\\(]{0,1}[0-9]{2,4}[\\)\\-\\(]{0,1}[0-9]{2,4}[\\)\\-]{0,1}[0-9]{3,4}\")"
        },
        {
            "sha": "27ec2f20d89f8955b188ccde02a49ed580ac33dd",
            "filename": "src/transformers/models/gpt_sw3/convert_megatron_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Fconvert_megatron_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Fconvert_megatron_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_sw3%2Fconvert_megatron_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -34,7 +34,7 @@ def recursive_print(name, val, spaces=0):\n     if isinstance(val, dict):\n         if msg is not None:\n             print(msg)\n-        for k in val.keys():\n+        for k in val:\n             recursive_print(k, val[k], spaces + 2)\n     elif isinstance(val, torch.Tensor):\n         print(msg, \":\", val.size())"
        },
        {
            "sha": "d95d83d2c9b3c435fcb8565db08a0be2dc889374",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -581,7 +581,7 @@ def parallelize(self, device_map=None):\n         )\n         assert_device_map(self.device_map, len(self.h))\n         self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n+        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n         self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n         self.wte = self.wte.to(self.first_device)\n         # Load onto devices"
        },
        {
            "sha": "0ec32258223cea695705b1d07840919ef84c9984",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -752,15 +752,15 @@ def __init__(self, config, *inputs, **kwargs):\n         self.config = config\n \n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n-        token_type_ids = kwargs.get(\"token_type_ids\", None)\n+        token_type_ids = kwargs.get(\"token_type_ids\")\n         # only last token for inputs_ids if past is defined in kwargs\n         if past_key_values:\n             inputs = tf.expand_dims(inputs[:, -1], -1)\n             if token_type_ids is not None:\n                 token_type_ids = tf.expand_dims(token_type_ids[:, -1], -1)\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n+        attention_mask = kwargs.get(\"attention_mask\")\n \n         if attention_mask is not None and position_ids is None:\n             position_ids = tf.math.cumsum(attention_mask, axis=-1, exclusive=True)"
        },
        {
            "sha": "b7358e2a015f2b7d3ab9f6fbff5aabfa618f870a",
            "filename": "src/transformers/models/grounding_dino/convert_grounding_dino_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconvert_grounding_dino_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -239,7 +239,7 @@ def create_rename_keys(state_dict, config):\n     ########################################## DECODER - END\n \n     ########################################## Additional - START\n-    for layer_name in state_dict.keys():\n+    for layer_name in state_dict:\n         #### TEXT BACKBONE\n         if \"bert\" in layer_name:\n             rename_keys.append((layer_name, layer_name.replace(\"bert\", \"model.text_backbone\")))"
        },
        {
            "sha": "ff291b3d67fdb360ea960ad537e5e35ebf27c288",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -349,8 +349,8 @@ def __init__(self, **kwargs: Unpack[GroundingDinoFastImageProcessorKwargs]) -> N\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "ac6844bd34c6fa9385aa9f6aa807200a5e0f9e3b",
            "filename": "src/transformers/models/groupvit/convert_groupvit_nvlab_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconvert_groupvit_nvlab_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -85,7 +85,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "d3b462692a593f487fcc6e1b3f6b67cf91df050c",
            "filename": "src/transformers/models/hubert/modeling_tf_hubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhubert%2Fmodeling_tf_hubert.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1276,7 +1276,7 @@ def call(\n \n         hidden_states = self.feature_projection(hidden_states, training=training)\n \n-        mask_time_indices = kwargs.get(\"mask_time_indices\", None)\n+        mask_time_indices = kwargs.get(\"mask_time_indices\")\n         if training:\n             hidden_states = self._mask_hidden_states(hidden_states, mask_time_indices=mask_time_indices)\n "
        },
        {
            "sha": "e47ddbe27370560dafba729fe6eb5fdfe1f07919",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -132,10 +132,10 @@ def expand_inputs_for_generation(\n         torch.arange(input_ids.shape[0]).view(-1, 1).repeat(1, expand_size).view(-1).to(input_ids.device)\n     )\n     input_ids = input_ids.index_select(0, expanded_return_idx)\n-    model_kwargs[\"pixel_values\"] = model_kwargs.get(\"pixel_values\", None)\n-    model_kwargs[\"image_encoder_embeddings\"] = model_kwargs.get(\"image_encoder_embeddings\", None)\n-    model_kwargs[\"perceiver_embeddings\"] = model_kwargs.get(\"perceiver_embeddings\", None)\n-    model_kwargs[\"image_attention_mask\"] = model_kwargs.get(\"image_attention_mask\", None)\n+    model_kwargs[\"pixel_values\"] = model_kwargs.get(\"pixel_values\")\n+    model_kwargs[\"image_encoder_embeddings\"] = model_kwargs.get(\"image_encoder_embeddings\")\n+    model_kwargs[\"perceiver_embeddings\"] = model_kwargs.get(\"perceiver_embeddings\")\n+    model_kwargs[\"image_attention_mask\"] = model_kwargs.get(\"image_attention_mask\")\n \n     if \"token_type_ids\" in model_kwargs:\n         token_type_ids = model_kwargs[\"token_type_ids\"]"
        },
        {
            "sha": "28bffb7921a8c52d56e7fb7855a91ac684da3ae2",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -149,10 +149,10 @@ def expand_inputs_for_generation(\n ):\n     expanded_return_idx = tf.reshape(tf.repeat(tf.range(tf.shape(input_ids)[0]), expand_size), [-1])\n     input_ids = tf.gather(input_ids, expanded_return_idx)\n-    model_kwargs[\"pixel_values\"] = model_kwargs.get(\"pixel_values\", None)\n-    model_kwargs[\"image_encoder_embeddings\"] = model_kwargs.get(\"image_encoder_embeddings\", None)\n-    model_kwargs[\"perceiver_embeddings\"] = model_kwargs.get(\"perceiver_embeddings\", None)\n-    model_kwargs[\"image_attention_mask\"] = model_kwargs.get(\"image_attention_mask\", None)\n+    model_kwargs[\"pixel_values\"] = model_kwargs.get(\"pixel_values\")\n+    model_kwargs[\"image_encoder_embeddings\"] = model_kwargs.get(\"image_encoder_embeddings\")\n+    model_kwargs[\"perceiver_embeddings\"] = model_kwargs.get(\"perceiver_embeddings\")\n+    model_kwargs[\"image_attention_mask\"] = model_kwargs.get(\"image_attention_mask\")\n \n     if \"token_type_ids\" in model_kwargs:\n         token_type_ids = model_kwargs[\"token_type_ids\"]\n@@ -208,15 +208,15 @@ def update_model_kwargs_for_generation(outputs, model_kwargs):\n \n \n def prepare_inputs_for_generation(input_ids, past_key_values=None, **kwargs):\n-    token_type_ids = kwargs.get(\"token_type_ids\", None)\n+    token_type_ids = kwargs.get(\"token_type_ids\")\n     # only last token for inputs_ids if past is defined in kwargs\n     if past_key_values is not None:\n         input_ids = input_ids[:, -1:]\n         if token_type_ids is not None:\n             token_type_ids = token_type_ids[:, -1:]\n \n-    attention_mask = kwargs.get(\"attention_mask\", None)\n-    position_ids = kwargs.get(\"position_ids\", None)\n+    attention_mask = kwargs.get(\"attention_mask\")\n+    position_ids = kwargs.get(\"position_ids\")\n \n     if attention_mask is not None and position_ids is None:\n         # create position_ids on the fly for batch generation\n@@ -225,10 +225,10 @@ def prepare_inputs_for_generation(input_ids, past_key_values=None, **kwargs):\n         if past_key_values is not None:\n             position_ids = position_ids[:, -1:]\n \n-    pixel_values = kwargs.get(\"pixel_values\", None)\n-    image_encoder_embeddings = kwargs.get(\"image_encoder_embeddings\", None)\n-    perceiver_embeddings = kwargs.get(\"perceiver_embeddings\", None)\n-    image_attention_mask = kwargs.get(\"image_attention_mask\", None)\n+    pixel_values = kwargs.get(\"pixel_values\")\n+    image_encoder_embeddings = kwargs.get(\"image_encoder_embeddings\")\n+    perceiver_embeddings = kwargs.get(\"perceiver_embeddings\")\n+    image_attention_mask = kwargs.get(\"image_attention_mask\")\n     interpolate_pos_encoding = kwargs.get(\"interpolate_pos_encoding\", False)\n \n     return {"
        },
        {
            "sha": "e20fcf4f36fb0a65c4201baf84e9c73fb1e45d25",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -128,7 +128,7 @@ def get_lm_type(path: str) -> Literal[\"qwen2\", \"llama\"]:\n     \"\"\"\n     Determine the type of language model (either 'qwen2' or 'llama') based on a given model path.\n     \"\"\"\n-    if path not in LM_TYPE_CORRESPONDENCE.keys():\n+    if path not in LM_TYPE_CORRESPONDENCE:\n         base_config = AutoModel.from_pretrained(path, trust_remote_code=True).config\n \n         lm_arch = base_config.llm_config.architectures[0]"
        },
        {
            "sha": "8e324ee0b8fe971b06cd102eaa9930990a1f5b99",
            "filename": "src/transformers/models/layoutlmv2/tokenization_layoutlmv2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Ftokenization_layoutlmv2_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -519,7 +519,7 @@ def _batch_encode_plus(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "d0407638595d8a127cef02e5bf687b34663e9bfa",
            "filename": "src/transformers/models/layoutlmv3/tokenization_layoutlmv3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Ftokenization_layoutlmv3_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -566,7 +566,7 @@ def _batch_encode_plus(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "6710c6c8cb66ed08da2df391c28ad1db2e6cf81d",
            "filename": "src/transformers/models/layoutxlm/tokenization_layoutxlm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Ftokenization_layoutxlm_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -503,7 +503,7 @@ def _batch_encode_plus(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "efbc757e86307c7b70a4d36d7d6930f5557a43e7",
            "filename": "src/transformers/models/luke/tokenization_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Ftokenization_luke.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1450,7 +1450,7 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n \n         # The model's main input name, usually `input_ids`, has be passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "4033ef319ff8cbec11f6c29d075e59b741710513",
            "filename": "src/transformers/models/markuplm/tokenization_markuplm_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Ftokenization_markuplm_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -625,7 +625,7 @@ def _batch_encode_plus(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "33cba259eed479df7e84660013219ca15f093b26",
            "filename": "src/transformers/models/mask2former/convert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconvert_mask2former_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -804,7 +804,7 @@ def convert(self, mask2former: Mask2FormerModel) -> Mask2FormerModel:\n         logger.info(f\"Not copied keys are {pformat(src_state_dict.keys())}\")\n         logger.info(\" Done\")\n \n-        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track.keys()}\n+        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track}\n         mask2former.load_state_dict(state_dict)\n         return mask2former\n \n@@ -816,7 +816,7 @@ def convert_universal_segmentation(\n \n         self.replace_universal_segmentation_module(dst_state_dict, src_state_dict)\n \n-        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track.keys()}\n+        state_dict = {key: dst_state_dict[key] for key in dst_state_dict.to_track}\n         mask2former.load_state_dict(state_dict)\n \n         return mask2former"
        },
        {
            "sha": "69ebed5aa8e0f1a5d908654938fc85623e27cace",
            "filename": "src/transformers/models/megatron_bert/convert_megatron_bert_checkpoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconvert_megatron_bert_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconvert_megatron_bert_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fconvert_megatron_bert_checkpoint.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -57,7 +57,7 @@ def recursive_print(name, val, spaces=0):\n     if isinstance(val, dict):\n         if msg is not None:\n             print(msg)\n-        for k in val.keys():\n+        for k in val:\n             recursive_print(k, val[k], spaces + 2)\n     elif isinstance(val, torch.Tensor):\n         print(msg, \":\", val.size())\n@@ -116,7 +116,7 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n     # The hidden_size per head.\n     hidden_size_per_head = config.hidden_size // heads\n     # Megatron-LM checkpoint version\n-    if \"checkpoint_version\" in input_state_dict.keys():\n+    if \"checkpoint_version\" in input_state_dict:\n         checkpoint_version = input_state_dict[\"checkpoint_version\"]\n     else:\n         checkpoint_version = 0.0\n@@ -147,7 +147,7 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n     output_state_dict[\"bert.embeddings.token_type_embeddings.weight\"] = tokentype_embeddings\n \n     # The transformer.\n-    transformer = lm[\"transformer\"] if \"transformer\" in lm.keys() else lm[\"encoder\"]\n+    transformer = lm[\"transformer\"] if \"transformer\" in lm else lm[\"encoder\"]\n \n     # The regex to extract layer names.\n     layer_re = re.compile(r\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z]+)\")"
        },
        {
            "sha": "ab866834a2c974469f6eed9693732f0526b8e84d",
            "filename": "src/transformers/models/megatron_gpt2/checkpoint_reshaping_and_interoperability.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fcheckpoint_reshaping_and_interoperability.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -189,7 +189,7 @@ def recursive_print(name, val, spaces=0):\n     if isinstance(val, dict):\n         if msg is not None:\n             print(msg)\n-        for k in val.keys():\n+        for k in val:\n             recursive_print(k, val[k], spaces + 2)\n     elif isinstance(val, torch.Tensor):\n         print(msg, \":\", val.size())\n@@ -448,7 +448,7 @@ def convert_checkpoint_from_megatron_to_transformers(args):\n         # The transformer.\n         path = (\n             \"model.language_model.transformer\"\n-            if \"transformer\" in get_element_from_dict_by_path(tp_state_dicts[0], \"model.language_model\").keys()\n+            if \"transformer\" in get_element_from_dict_by_path(tp_state_dicts[0], \"model.language_model\")\n             else \"model.language_model.encoder\"\n         )\n         # Extract the layers.\n@@ -793,9 +793,7 @@ def convert_checkpoint_from_transformers_to_megatron(args):\n         for layer in range(num_layers):\n             pp_layer_id = layer + layer_offset\n             layers_to_copy = [\n-                layer_name\n-                for layer_name in state_dict.keys()\n-                if layer_name.startswith(f\"transformer.h.{pp_layer_id}.\")\n+                layer_name for layer_name in state_dict if layer_name.startswith(f\"transformer.h.{pp_layer_id}.\")\n             ]\n \n             for layer_name in layers_to_copy:\n@@ -844,15 +842,15 @@ def convert_checkpoint_from_transformers_to_megatron(args):\n \n                 # handle attention and mlp weights\n                 elif weight_or_bias == \"weight\":\n-                    out_name = transformers_to_megatron.get(op_name, None)\n+                    out_name = transformers_to_megatron.get(op_name)\n                     if out_name is None:\n                         continue\n                     params = params.transpose(0, 1)\n                     layer_name = f\"layers.{layer}.{out_name}.{weight_or_bias}\"\n \n                 # handle attention and mlp bias\n                 elif weight_or_bias == \"bias\":\n-                    out_name = transformers_to_megatron.get(op_name, None)\n+                    out_name = transformers_to_megatron.get(op_name)\n                     if out_name is None:\n                         continue\n                     layer_name = f\"layers.{layer}.{out_name}.{weight_or_bias}\""
        },
        {
            "sha": "d1953f50baed91d8f3f09edcea9356865fa6091f",
            "filename": "src/transformers/models/megatron_gpt2/convert_megatron_gpt2_checkpoint.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_gpt2%2Fconvert_megatron_gpt2_checkpoint.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -57,7 +57,7 @@ def recursive_print(name, val, spaces=0):\n     if isinstance(val, dict):\n         if msg is not None:\n             print(msg)\n-        for k in val.keys():\n+        for k in val:\n             recursive_print(k, val[k], spaces + 2)\n     elif isinstance(val, torch.Tensor):\n         print(msg, \":\", val.size())\n@@ -115,7 +115,7 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n     # The hidden_size per head.\n     hidden_size_per_head = config.n_embd // config.n_head\n     # Megatron-LM checkpoint version\n-    if \"checkpoint_version\" in input_state_dict.keys():\n+    if \"checkpoint_version\" in input_state_dict:\n         checkpoint_version = input_state_dict[\"checkpoint_version\"]\n     else:\n         checkpoint_version = 0.0\n@@ -145,7 +145,7 @@ def convert_megatron_checkpoint(args, input_state_dict, config):\n     output_state_dict[\"transformer.wpe.weight\"] = pos_embeddings\n \n     # The transformer.\n-    transformer = lm[\"transformer\"] if \"transformer\" in lm.keys() else lm[\"encoder\"]\n+    transformer = lm[\"transformer\"] if \"transformer\" in lm else lm[\"encoder\"]\n \n     # The regex to extract layer names.\n     layer_re = re.compile(r\"layers\\.(\\d+)\\.([a-z0-9_.]+)\\.([a-z0-9_]+)\")"
        },
        {
            "sha": "a790fed81d1bdf33948a55305f7bfa0e8f5deae6",
            "filename": "src/transformers/models/mistral/convert_mistral_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconvert_mistral_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -176,7 +176,7 @@ def convert_config(original_config: dict, max_position_embeddings: int = 32768):\n     new_config_kwargs.update({k: v for k, v in original_config.items() if k in similar_keys_to_keep})\n \n     # These are not always defined depending on `params.json`\n-    new_config_kwargs[\"sliding_window\"] = original_config.get(\"sliding_window\", None)\n+    new_config_kwargs[\"sliding_window\"] = original_config.get(\"sliding_window\")\n     new_config_kwargs[\"num_key_value_heads\"] = original_config.get(\n         \"n_kv_heads\", new_config_kwargs[\"num_attention_heads\"]\n     )"
        },
        {
            "sha": "30ca8d5769be56fdb733270f24b8e40f79921e83",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -880,7 +880,7 @@ def prepare_inputs_for_generation(\n         if past_key_values:\n             input_ids = tf.expand_dims(input_ids[:, -1], -1)\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n         if attention_mask is not None and position_ids is None:\n             position_ids = tf.math.cumsum(attention_mask, axis=-1, exclusive=True)\n             if past_key_values:"
        },
        {
            "sha": "be0f52a70ebc9ee3fb76a1231ca3bd9e9d0bc795",
            "filename": "src/transformers/models/mluke/convert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Fconvert_mluke_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -100,7 +100,7 @@ def convert_luke_checkpoint(checkpoint_path, metadata_path, entity_vocab_path, p\n     state_dict.pop(\"lm_head.decoder.weight\")\n     state_dict.pop(\"lm_head.decoder.bias\")\n     state_dict_for_hugging_face = OrderedDict()\n-    for key in state_dict.keys():\n+    for key in state_dict:\n         if not (key.startswith(\"lm_head\") or key.startswith(\"entity_predictions\")):\n             state_dict_for_hugging_face[f\"luke.{key}\"] = state_dict[key]\n         else:"
        },
        {
            "sha": "15f4db53287a7c64d8b156578c63abd9b5feba3e",
            "filename": "src/transformers/models/mluke/tokenization_mluke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmluke%2Ftokenization_mluke.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1288,7 +1288,7 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n \n         # The model's main input name, usually `input_ids`, has be passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "a8159b446f1e290040c9e3878ccf33203a439e9e",
            "filename": "src/transformers/models/mobilevit/convert_mlcvnets_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevit%2Fconvert_mlcvnets_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -155,7 +155,7 @@ def convert_state_dict(orig_state_dict, model, base_model=False):\n     else:\n         model_prefix = \"mobilevit.\"\n \n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if key[:8] == \"encoder.\":"
        },
        {
            "sha": "8d462c7dd49f58e77aac0232b0c36d54ca942731",
            "filename": "src/transformers/models/mobilevitv2/convert_mlcvnets_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmobilevitv2%2Fconvert_mlcvnets_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -134,7 +134,7 @@ def create_rename_keys(state_dict, base_model=False):\n         model_prefix = \"mobilevitv2.\"\n \n     rename_keys = []\n-    for k in state_dict.keys():\n+    for k in state_dict:\n         if k[:8] == \"encoder.\":\n             k_new = k[8:]\n         else:\n@@ -216,7 +216,7 @@ def create_rename_keys(state_dict, base_model=False):\n def remove_unused_keys(state_dict):\n     \"\"\"remove unused keys (e.g.: seg_head.aux_head)\"\"\"\n     keys_to_ignore = []\n-    for k in state_dict.keys():\n+    for k in state_dict:\n         if k.startswith(\"seg_head.aux_head.\"):\n             keys_to_ignore.append(k)\n     for k in keys_to_ignore:"
        },
        {
            "sha": "f29da8c8e21665484ce78ee6797ea5853047c945",
            "filename": "src/transformers/models/moonshine/convert_usefulsensors_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fconvert_usefulsensors_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -33,7 +33,7 @@ def _get_weights(model_name):\n \n \n def _read_h5_weights(group, current_key=\"\", weights={}):\n-    for key in group.keys():\n+    for key in group:\n         full_key = f\"{current_key}.{key}\" if current_key else key\n         if isinstance(group[key], h5py.Dataset):\n             w = np.array(group[key])"
        },
        {
            "sha": "6330e2fe9292bd18627e35b37b77d360b65f4af9",
            "filename": "src/transformers/models/mra/convert_mra_pytorch_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmra%2Fconvert_mra_pytorch_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -62,7 +62,7 @@ def rename_key(orig_key):\n \n \n def convert_checkpoint_helper(max_position_embeddings, orig_state_dict):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if (\"pooler\" in key) or (\"sen_class\" in key):"
        },
        {
            "sha": "0a414cdf11b2ce62c085b60740744ec096f2b803",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -898,7 +898,7 @@ def parallelize(self, device_map=None):\n         )\n         assert_device_map(self.device_map, len(self.block))\n         self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n+        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n         self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n         # Load onto devices\n         for k, v in self.device_map.items():"
        },
        {
            "sha": "bc0dc7122b590d12e25afcc73ad86744fdc6f62d",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1589,11 +1589,11 @@ def from_sub_models_pretrained(\n         }\n \n         # remove text encoder, audio encoder and decoder kwargs from kwargs\n-        for key in kwargs_text_encoder.keys():\n+        for key in kwargs_text_encoder:\n             del kwargs[\"text_encoder_\" + key]\n-        for key in kwargs_audio_encoder.keys():\n+        for key in kwargs_audio_encoder:\n             del kwargs[\"audio_encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "aca49d8e83a8961217444074129c29dbec964cac",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1472,11 +1472,11 @@ def from_sub_models_pretrained(\n         }\n \n         # remove text encoder, audio encoder and decoder kwargs from kwargs\n-        for key in kwargs_text_encoder.keys():\n+        for key in kwargs_text_encoder:\n             del kwargs[\"text_encoder_\" + key]\n-        for key in kwargs_audio_encoder.keys():\n+        for key in kwargs_audio_encoder:\n             del kwargs[\"audio_encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "ef2e3d0d90dd1e78afd9122646a08d1b86f831eb",
            "filename": "src/transformers/models/nllb_moe/convert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fconvert_nllb_moe_sharded_original_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -46,7 +46,7 @@ def make_linear_from_emb(emb):\n \n def rename_fairseq_keys(state_dict, expert_idx=None):\n     new_dict = {}\n-    for old_key in state_dict.keys():\n+    for old_key in state_dict:\n         key = old_key\n         if \"moe_layer.experts.\" in key:\n             if expert_idx is not None:"
        },
        {
            "sha": "d8096ad864a8ea56c246b47506b78cdc7eb28b1e",
            "filename": "src/transformers/models/nougat/convert_nougat_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fconvert_nougat_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -100,7 +100,7 @@ def rename_key(name):\n \n # Copied from transformers.models.donut.convert_donut_to_pytorch.convert_state_dict\n def convert_state_dict(orig_state_dict, model):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "934a23e0103ba250af6e3da18ad6c7dc763b335a",
            "filename": "src/transformers/models/nystromformer/convert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnystromformer%2Fconvert_nystromformer_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -61,7 +61,7 @@ def rename_key(orig_key):\n \n \n def convert_checkpoint_helper(config, orig_state_dict):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if (\"pooler\" in key) or (\"sen_class\" in key) or (\"conv.bias\" in key):"
        },
        {
            "sha": "da358d7119c339b59bc2d9a904263aad614523a6",
            "filename": "src/transformers/models/omdet_turbo/convert_omdet_turbo_to_hf.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconvert_omdet_turbo_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -66,7 +66,7 @@ def create_rename_keys_vision(state_dict, config):\n     rename_keys = []\n     # fmt: off\n     ########################################## VISION BACKBONE - START\n-    for layer_name in state_dict.keys():\n+    for layer_name in state_dict:\n         if layer_name.startswith(\"backbone\") and not layer_name.startswith(\"backbone.norm\"):\n             if config.use_timm_backbone:\n                 layer_name_replace = layer_name.replace(\"backbone\", \"vision_backbone.vision_backbone._backbone\")\n@@ -100,7 +100,7 @@ def create_rename_keys_vision(state_dict, config):\n     ########################################## VISION BACKBONE - END\n \n     ########################################## ENCODER - START\n-    for layer_name in state_dict.keys():\n+    for layer_name in state_dict:\n         if \"neck\" in layer_name:\n             layer_name_replace = layer_name.replace(\"neck\", \"encoder\")\n             layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")\n@@ -117,7 +117,7 @@ def create_rename_keys_vision(state_dict, config):\n     ########################################## ENCODER - END\n \n     ########################################## DECODER - START\n-    for layer_name in state_dict.keys():\n+    for layer_name in state_dict:\n         if layer_name.startswith(\"decoder\"):\n             layer_name_replace = layer_name.replace(\"decoder.decoder.layers\", \"decoder.layers\")\n             layer_name_replace = layer_name_replace.replace(\"input_proj\", \"channel_projection_layers\")\n@@ -136,7 +136,7 @@ def create_rename_keys_vision(state_dict, config):\n def create_rename_keys_language(state_dict):\n     rename_keys = []\n     # fmt: off\n-    for layer_name in state_dict.keys():\n+    for layer_name in state_dict:\n         if layer_name.startswith(\"language_backbone\") and not layer_name.startswith(\"language_backbone.text_projection\"):\n             layer_name_replace = layer_name.replace(\"language_backbone\", \"language_backbone.model.text_model\")\n             layer_name_replace = layer_name_replace.replace(\"transformer.resblocks\", \"encoder.layers\")"
        },
        {
            "sha": "98731ed2120391958b3aba2c9abcc1e226afaaf9",
            "filename": "src/transformers/models/opt/convert_opt_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fopt%2Fconvert_opt_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fopt%2Fconvert_opt_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fconvert_opt_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -30,7 +30,7 @@\n def load_checkpoint(checkpoint_path):\n     \"\"\"Checkpoint path should end in model.pt\"\"\"\n     sd = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)\n-    if \"model\" in sd.keys():\n+    if \"model\" in sd:\n         sd = torch.load(checkpoint_path, map_location=\"cpu\", weights_only=True)[\"model\"]\n \n     # pop unnecessary weights"
        },
        {
            "sha": "f996256063c073fcb03c48e8b00fc17a8f4a0047",
            "filename": "src/transformers/models/opt/modeling_tf_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -934,7 +934,7 @@ def get_output_embeddings(self):\n         return self.model.get_input_embeddings()\n \n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n+        attention_mask = kwargs.get(\"attention_mask\")\n \n         # only last token for inputs_ids if past is defined in kwargs\n         if past_key_values:"
        },
        {
            "sha": "ea766c366f34216b24f9ced8641f2ef32259e615",
            "filename": "src/transformers/models/owlvit/convert_owlvit_original_flax_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fconvert_owlvit_original_flax_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -220,7 +220,7 @@ def copy_class_box_heads(hf_model, flax_params):\n \n     # Copy flax params to PyTorch params\n     for name, param in new_params.items():\n-        if name in pt_params.keys():\n+        if name in pt_params:\n             pt_params[name].copy_(param)\n \n \n@@ -313,7 +313,7 @@ def convert_clip_backbone(flax_params, torch_config):\n \n     # Copy flax CLIP backbone params to PyTorch params\n     for name, param in new_torch_params.items():\n-        if name in torch_clip_params.keys():\n+        if name in torch_clip_params:\n             new_param = torch.from_numpy(param)\n             torch_clip_params[name].copy_(new_param)\n         else:"
        },
        {
            "sha": "630ff7b0b089ba5c9463a091b88a1e65da377d7a",
            "filename": "src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -263,7 +263,7 @@ def slice_state_dict(state_dict, config):\n         state_dict[f\"language_model.model.layers.{i}.post_feedforward_layernorm.weight\"] = llm_post_feedforward_layernorm[i]\n     state_dict[\"language_model.model.norm.weight\"] = state_dict.pop(\"llm/final_norm/scale\")\n     state_dict[\"language_model.lm_head.weight\"] = embedding_vector # weights are tied.\n-    [k for k in state_dict.keys() if not k.startswith('vision') and not k.startswith('language')]\n+    [k for k in state_dict if not k.startswith('vision') and not k.startswith('language')]\n     # fmt: on\n     for key, value in state_dict.items():\n         if not isinstance(value, torch.Tensor):"
        },
        {
            "sha": "d07fa49ddc5f8a44fc80686c0f30f09452cac19a",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -577,7 +577,7 @@ def _init_weights(self, module):\n         elif hasattr(module, \"position_embeddings\") and isinstance(module, PerceiverTrainablePositionEncoding):\n             module.position_embeddings.data.normal_(mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, nn.ParameterDict):\n-            for modality in module.keys():\n+            for modality in module:\n                 module[modality].data.normal_(mean=0.0, std=self.config.initializer_range)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)"
        },
        {
            "sha": "919a1203128e154122e7a9622ab7f504fba538cd",
            "filename": "src/transformers/models/perception_lm/image_processing_perception_lm_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fimage_processing_perception_lm_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -218,15 +218,15 @@ def _find_closest_aspect_ratio(self, img_width: int, img_height: int, tile_size:\n         closest_aspect_ratio = None\n         if target_aspect_ratio >= 1:\n             closest_aspect_ratio = min(\n-                [k for k in asp_dict.keys() if k <= target_aspect_ratio],\n+                [k for k in asp_dict if k <= target_aspect_ratio],\n                 key=lambda x: abs(x - target_aspect_ratio),\n             )\n             tiles_given_aspect_ratio = asp_dict[closest_aspect_ratio]\n             # select largest width\n             return max(tiles_given_aspect_ratio, key=lambda x: x[0])\n         else:\n             closest_aspect_ratio = min(\n-                [k for k in asp_dict.keys() if k > target_aspect_ratio],\n+                [k for k in asp_dict if k > target_aspect_ratio],\n                 key=lambda x: abs(1 / x - 1 / target_aspect_ratio),\n             )\n             tiles_given_aspect_ratio = asp_dict[closest_aspect_ratio]"
        },
        {
            "sha": "bbaa9b4c0c3db2482776e2d63df603b15c6e0be6",
            "filename": "src/transformers/models/phi/convert_phi_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconvert_phi_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -105,7 +105,7 @@ def _download(url: str, root: str):\n def convert_phi_weights(\n     model_name, checkpoint_path, pytorch_dump_folder_path, use_cuda, save_weights_directly, _MODELS\n ):\n-    _MODELS = _MODELS if model_name not in _MODELS.keys() else {model_name: _MODELS.get(model_name)}\n+    _MODELS = _MODELS if model_name not in _MODELS else {model_name: _MODELS.get(model_name)}\n     device = \"cuda\" if torch.cuda.is_available() and use_cuda else \"cpu\"\n     for model_name, model_url in _MODELS.items():\n         converted_checkpoint = {}"
        },
        {
            "sha": "bf3bf2b7237a6d72b08c926b1800ea51d442f78d",
            "filename": "src/transformers/models/pix2struct/convert_pix2struct_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconvert_pix2struct_original_pytorch_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -67,7 +67,7 @@ def rename_and_convert_flax_params(flax_dict):\n         \"decoder.logits_dense.weight\": \"decoder.lm_head.weight\",\n     }\n \n-    for key in flax_dict.keys():\n+    for key in flax_dict:\n         if \"target\" in key:\n             # remove the first prefix from the key\n             new_key = \".\".join(key[1:])"
        },
        {
            "sha": "06dadb8a9fc00654791c319a70b35618bcca754d",
            "filename": "src/transformers/models/pix2struct/image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fimage_processing_pix2struct.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -404,7 +404,7 @@ def preprocess(\n         max_patches = max_patches if max_patches is not None else self.max_patches\n         is_vqa = self.is_vqa\n \n-        if kwargs.get(\"data_format\", None) is not None:\n+        if kwargs.get(\"data_format\") is not None:\n             raise ValueError(\"data_format is not an accepted input as the outputs are \")\n \n         images = make_list_of_images(images)"
        },
        {
            "sha": "821c4c81724f764aa3c7cc014cd2d9912a618a47",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -425,7 +425,7 @@ def set_tgt_lang_special_tokens(self, lang: str) -> None:\n \n     def _convert_lang_code_special_format(self, lang: str) -> str:\n         \"\"\"Convert Language Codes to format tokenizer uses if required\"\"\"\n-        lang = FAIRSEQ_LANGUAGE_CODES_MAP[lang] if lang in FAIRSEQ_LANGUAGE_CODES_MAP.keys() else lang\n+        lang = FAIRSEQ_LANGUAGE_CODES_MAP[lang] if lang in FAIRSEQ_LANGUAGE_CODES_MAP else lang\n         return lang\n \n "
        },
        {
            "sha": "9676c3945544cf730564f47de03c957d5d162a7e",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1072,7 +1072,7 @@ def get_mel_conditioner_outputs(\n                 - 0 for tokens that are **padded**.\n         \"\"\"\n         composer_to_feature_token = generation_config.composer_to_feature_token\n-        if composer not in composer_to_feature_token.keys():\n+        if composer not in composer_to_feature_token:\n             raise ValueError(\n                 f\"Please choose a composer from {list(composer_to_feature_token.keys())}. Composer received - {composer}\"\n             )"
        },
        {
            "sha": "4356b5d93f168edf3d94c35847c414af0fb2e14a",
            "filename": "src/transformers/models/pop2piano/tokenization_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Ftokenization_pop2piano.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -265,7 +265,7 @@ def relative_tokens_ids_to_notes(\n \n         current_idx = start_idx\n         current_velocity = 0\n-        note_onsets_ready = [None for i in range(sum([k.endswith(\"NOTE\") for k in self.encoder.keys()]) + 1)]\n+        note_onsets_ready = [None for i in range(sum([k.endswith(\"NOTE\") for k in self.encoder]) + 1)]\n         notes = []\n         for token_type, number in words:\n             if token_type == \"TOKEN_SPECIAL\":"
        },
        {
            "sha": "a10a0955f84cc47912debddbbb8b42d4ce56006a",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -3872,7 +3872,7 @@ def generate(\n \n         # 2. Generate speech tokens from talker module\n         embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(input_ids.device)\n-        if thinker_kwargs.get(\"input_features\", None) is not None:\n+        if thinker_kwargs.get(\"input_features\") is not None:\n             audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n             audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             audio_mask_tensor = torch.zeros(\n@@ -3881,7 +3881,7 @@ def generate(\n                 device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n-        if thinker_kwargs.get(\"pixel_values\", None) is not None:\n+        if thinker_kwargs.get(\"pixel_values\") is not None:\n             image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n             image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             image_mask_tensor = torch.zeros(\n@@ -3890,7 +3890,7 @@ def generate(\n                 device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n-        if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n+        if thinker_kwargs.get(\"pixel_values_videos\") is not None:\n             video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n             video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             video_mask_tensor = torch.zeros("
        },
        {
            "sha": "56fdff57e579dbdd5cbceb4c2594b781fa21d486",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -4169,7 +4169,7 @@ def generate(\n \n         # 2. Generate speech tokens from talker module\n         embeds_to_talker = thinker_result.hidden_states[0][0].clone().to(input_ids.device)\n-        if thinker_kwargs.get(\"input_features\", None) is not None:\n+        if thinker_kwargs.get(\"input_features\") is not None:\n             audio_ids_mask = input_ids == self.config.thinker_config.audio_token_index\n             audio_mask = audio_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             audio_mask_tensor = torch.zeros(\n@@ -4178,7 +4178,7 @@ def generate(\n                 device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(audio_mask, audio_mask_tensor)\n-        if thinker_kwargs.get(\"pixel_values\", None) is not None:\n+        if thinker_kwargs.get(\"pixel_values\") is not None:\n             image_ids_mask = input_ids == self.config.thinker_config.image_token_index\n             image_mask = image_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             image_mask_tensor = torch.zeros(\n@@ -4187,7 +4187,7 @@ def generate(\n                 device=input_ids.device,\n             )\n             embeds_to_talker.masked_scatter_(image_mask, image_mask_tensor)\n-        if thinker_kwargs.get(\"pixel_values_videos\", None) is not None:\n+        if thinker_kwargs.get(\"pixel_values_videos\") is not None:\n             video_ids_mask = input_ids == self.config.thinker_config.video_token_index\n             video_mask = video_ids_mask.unsqueeze(-1).expand_as(embeds_to_talker)\n             video_mask_tensor = torch.zeros("
        },
        {
            "sha": "367b4dc4566c4b10dacb5142cce2772bfc0d7b92",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -318,9 +318,9 @@ def from_pretrained_question_encoder_generator(\n         }\n \n         # remove question_encoder, generator kwargs from kwargs\n-        for key in kwargs_question_encoder.keys():\n+        for key in kwargs_question_encoder:\n             del kwargs[\"question_encoder_\" + key]\n-        for key in kwargs_generator.keys():\n+        for key in kwargs_generator:\n             del kwargs[\"generator_\" + key]\n \n         # Load and initialize the question_encoder and generator\n@@ -370,7 +370,7 @@ def from_pretrained_question_encoder_generator(\n             )\n \n         # instantiate config with corresponding kwargs\n-        config = kwargs.get(\"config\", None)\n+        config = kwargs.get(\"config\")\n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n                 question_encoder.config, generator.config, **kwargs"
        },
        {
            "sha": "f430239e0d139e8350daaaf9e2adc6cf9cda09f7",
            "filename": "src/transformers/models/rag/modeling_tf_rag.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_tf_rag.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -322,9 +322,9 @@ def from_pretrained_question_encoder_generator(\n         }\n \n         # remove question_encoder, generator kwargs from kwargs\n-        for key in kwargs_question_encoder.keys():\n+        for key in kwargs_question_encoder:\n             del kwargs[\"question_encoder_\" + key]\n-        for key in kwargs_generator.keys():\n+        for key in kwargs_generator:\n             del kwargs[\"generator_\" + key]\n \n         # Load and initialize the question_encoder and generator\n@@ -376,7 +376,7 @@ def from_pretrained_question_encoder_generator(\n             )\n \n         # instantiate config with corresponding kwargs\n-        config = kwargs.get(\"config\", None)\n+        config = kwargs.get(\"config\")\n         if config is None:\n             config = RagConfig.from_question_encoder_generator_configs(\n                 question_encoder.config, generator.config, **kwargs"
        },
        {
            "sha": "ed4bc48035d05ebdc2df6ff1f725e2abf430043a",
            "filename": "src/transformers/models/regnet/convert_regnet_seer_10b_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_seer_10b_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -51,7 +51,7 @@ class Tracker:\n     name2module: dict[str, nn.Module] = field(default_factory=OrderedDict)\n \n     def _forward_hook(self, m, inputs: Tensor, outputs: Tensor, name: str):\n-        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d)\n+        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, (nn.Conv2d, nn.BatchNorm2d))\n         if has_not_submodules:\n             self.traced.append(m)\n             self.name2module[name] = m\n@@ -217,7 +217,7 @@ def load_using_classy_vision(checkpoint_url: str) -> tuple[dict, dict]:\n         not_used_keys = list(from_state_dict.keys())\n         regex = r\"\\.block.-part.\"\n         # this is \"interesting\", so the original checkpoints have `block[0,1]-part` in each key name, we remove it\n-        for key in from_state_dict.keys():\n+        for key in from_state_dict:\n             # remove the weird \"block[0,1]-part\" from the key\n             src_key = re.sub(regex, \"\", key)\n             # now src_key from the model checkpoints is the one we got from the original model after tracing, so use it to get the correct destination key"
        },
        {
            "sha": "9d6659d7685d1117021a706dbbe7d8d0100bc607",
            "filename": "src/transformers/models/regnet/convert_regnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fregnet%2Fconvert_regnet_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -44,7 +44,7 @@ class Tracker:\n     handles: list = field(default_factory=list)\n \n     def _forward_hook(self, m, inputs: Tensor, outputs: Tensor):\n-        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d)\n+        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, (nn.Conv2d, nn.BatchNorm2d))\n         if has_not_submodules:\n             self.traced.append(m)\n "
        },
        {
            "sha": "11b09c372c31aae7bcbba800180d970a0f560481",
            "filename": "src/transformers/models/resnet/convert_resnet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fresnet%2Fconvert_resnet_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -42,7 +42,7 @@ class Tracker:\n     handles: list = field(default_factory=list)\n \n     def _forward_hook(self, m, inputs: Tensor, outputs: Tensor):\n-        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, nn.Conv2d) or isinstance(m, nn.BatchNorm2d)\n+        has_not_submodules = len(list(m.modules())) == 1 or isinstance(m, (nn.Conv2d, nn.BatchNorm2d))\n         if has_not_submodules:\n             self.traced.append(m)\n "
        },
        {
            "sha": "8a76fa4b4d83f99d0d88dfd89fe9ca410685084d",
            "filename": "src/transformers/models/rt_detr/convert_rt_detr_original_pytorch_checkpoint_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconvert_rt_detr_original_pytorch_checkpoint_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconvert_rt_detr_original_pytorch_checkpoint_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconvert_rt_detr_original_pytorch_checkpoint_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -574,7 +574,7 @@ def convert_rt_detr_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub\n     # query, key and value matrices need special treatment\n     read_in_q_k_v(state_dict, config)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if key.endswith(\"num_batches_tracked\"):\n             del state_dict[key]\n         # for two_stage"
        },
        {
            "sha": "b6f52c28f844550dabff2cb9419133ea3cac55d3",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -160,8 +160,8 @@ class RTDetrImageProcessorFast(BaseImageProcessorFast):\n \n     def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "9a77a92b5a08632602e3c4ac9b0664539a04d4ea",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -123,8 +123,8 @@ class RTDetrImageProcessorFast(DetrImageProcessorFast, BaseImageProcessorFast):\n \n     def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "d2f9b200df9de23edaebdc7922b2f08de858ca76",
            "filename": "src/transformers/models/rt_detr_v2/convert_rt_detr_v2_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -239,13 +239,13 @@ def write_model_and_image_processor(model_name, output_dir, push_to_hub, repo_id\n     ][\"module\"]\n     # rename keys\n     state_dict = convert_old_keys_to_new_keys(state_dict)\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if key.endswith(\"num_batches_tracked\"):\n             del state_dict[key]\n     # query, key and value matrices need special treatment\n     read_in_q_k_v(state_dict, config)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if key.endswith(\"num_batches_tracked\"):\n             del state_dict[key]\n         # for two_stage"
        },
        {
            "sha": "619b6448b4cc3cbfa58e96f1d5bbc37327334187",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -3910,7 +3910,7 @@ def generate(\n               shape `(batch_size, sequence_length)` and `waveform_lengths` which gives the length of each sample.\n             - If `generate_speech=False`, it will returns `ModelOutput`.\n         \"\"\"\n-        if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\", None) is None:\n+        if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\") is None:\n             raise ValueError(\n                 \"`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.\"\n             )"
        },
        {
            "sha": "950c0d3b8f895be066c28672dd78e99ff4c5a7f6",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -4208,7 +4208,7 @@ def generate(\n               shape `(batch_size, sequence_length)` and `waveform_lengths` which gives the length of each sample.\n             - If `generate_speech=False`, it will returns `ModelOutput`.\n         \"\"\"\n-        if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\", None) is None:\n+        if input_ids is None and input_features is None and kwargs.get(\"inputs_embeds\") is None:\n             raise ValueError(\n                 \"`input_ids`,`input_features` and `inputs_embeds` are all empty. Make sure at least one of them is not.\"\n             )"
        },
        {
            "sha": "bc541262c74c5257e1ac7cd5abce574b75e35eb6",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_flax_speech_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -844,9 +844,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "f4e804dac26af1bcfa963e61c877d8197d3d8fed",
            "filename": "src/transformers/models/speech_encoder_decoder/modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_encoder_decoder%2Fmodeling_speech_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -242,9 +242,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "a2d8893b5d55886893c2dd7aaa9f60533f46fa08",
            "filename": "src/transformers/models/swiftformer/convert_swiftformer_original_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconvert_swiftformer_original_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconvert_swiftformer_original_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fconvert_swiftformer_original_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -65,7 +65,7 @@ def rename_key(dct, old, new):\n \n def create_rename_keys(state_dict):\n     rename_keys = []\n-    for k in state_dict.keys():\n+    for k in state_dict:\n         k_new = k\n         if \".pwconv\" in k:\n             k_new = k_new.replace(\".pwconv\", \".point_wise_conv\")"
        },
        {
            "sha": "dbaeeb31ef2b7e90e67a6cecb799efa21e8272b1",
            "filename": "src/transformers/models/swin/convert_swin_simmim_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_simmim_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -83,7 +83,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, model):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"attn_mask\" in key:"
        },
        {
            "sha": "9971da844aacaea8065e0291fcedb11675bf76b5",
            "filename": "src/transformers/models/swin/convert_swin_timm_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fconvert_swin_timm_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -90,7 +90,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, model):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"mask\" in key:"
        },
        {
            "sha": "e827070ed55d67957b058ce59f02a81a264c8466",
            "filename": "src/transformers/models/swin2sr/convert_swin2sr_original_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fconvert_swin2sr_original_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -124,7 +124,7 @@ def rename_key(name, config):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "60ea55edee5d329f660ec343a60dcc5f6001f128",
            "filename": "src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fconvert_swinv2_timm_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -130,7 +130,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, model):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"mask\" in key:"
        },
        {
            "sha": "e6ef99a3107587e377c04535ba1c7e8b0ae1371c",
            "filename": "src/transformers/models/switch_transformers/convert_big_switch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_big_switch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -80,7 +80,7 @@ def shard_on_the_fly(switch_checkpoint_path, dump_path, max_shard_size, dtype, w\n         checkpoint_info = flatten_dict(checkpoint_info, sep=\"/\")\n \n     all_layers = {}\n-    for layer in checkpoint_info.keys():\n+    for layer in checkpoint_info:\n         curr_real_layer_name, split_layer, content = get_key_and_tensorstore_dict(\n             layer, checkpoint_info, switch_checkpoint_path\n         )"
        },
        {
            "sha": "aeb8de042445a9206eb3f91052cf4811901ae340",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -911,7 +911,7 @@ def parallelize(self, device_map=None):\n         )\n         assert_device_map(self.device_map, len(self.block))\n         self.model_parallel = True\n-        self.first_device = \"cpu\" if \"cpu\" in self.device_map.keys() else \"cuda:\" + str(min(self.device_map.keys()))\n+        self.first_device = \"cpu\" if \"cpu\" in self.device_map else \"cuda:\" + str(min(self.device_map.keys()))\n         self.last_device = \"cuda:\" + str(max(self.device_map.keys()))\n         # Load onto devices\n         for k, v in self.device_map.items():"
        },
        {
            "sha": "aeba012ad04e8cd13ede63c3be6ae92b7cdc5cbe",
            "filename": "src/transformers/models/table_transformer/convert_table_transformer_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -203,7 +203,7 @@ def convert_table_transformer_checkpoint(checkpoint_url, pytorch_dump_folder_pat\n     read_in_q_k_v(state_dict)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if not key.startswith(\"class_labels_classifier\") and not key.startswith(\"bbox_predictor\"):\n             val = state_dict.pop(key)\n             state_dict[prefix + key] = val"
        },
        {
            "sha": "f9964369bfdc5b505b43f660ed9bd3e19d3adf8f",
            "filename": "src/transformers/models/table_transformer/convert_table_transformer_to_hf_no_timm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconvert_table_transformer_to_hf_no_timm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -335,7 +335,7 @@ def convert_table_transformer_checkpoint(checkpoint_url, pytorch_dump_folder_pat\n     read_in_q_k_v(state_dict)\n     # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n     prefix = \"model.\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if not key.startswith(\"class_labels_classifier\") and not key.startswith(\"bbox_predictor\"):\n             val = state_dict.pop(key)\n             state_dict[prefix + key] = val"
        },
        {
            "sha": "5db24e6367dc2a49176b927dbcdf4fb2bbf7d96d",
            "filename": "src/transformers/models/timesformer/convert_timesformer_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimesformer%2Fconvert_timesformer_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -101,7 +101,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if key.startswith(\"model.\"):"
        },
        {
            "sha": "5fa115a05431f4546386c7b0ca2424e4c85eac27",
            "filename": "src/transformers/models/timm_wrapper/configuration_timm_wrapper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftimm_wrapper%2Fconfiguration_timm_wrapper.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -77,7 +77,7 @@ def __init__(\n \n     @classmethod\n     def from_dict(cls, config_dict: dict[str, Any], **kwargs):\n-        label_names = config_dict.get(\"label_names\", None)\n+        label_names = config_dict.get(\"label_names\")\n         is_custom_model = \"num_labels\" in kwargs or \"id2label\" in kwargs\n \n         # if no labels added to config, use imagenet labeller in timm"
        },
        {
            "sha": "a8878b9b514c27a7caa3653d771e868a6cfc3356",
            "filename": "src/transformers/models/udop/tokenization_udop_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Ftokenization_udop_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -591,7 +591,7 @@ def _batch_encode_plus_boxes(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "17d110c57722129132eaee85857c65d559671472",
            "filename": "src/transformers/models/upernet/convert_convnext_upernet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_convnext_upernet_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -134,7 +134,7 @@ def convert_upernet_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub\n     model.eval()\n \n     # replace \"bn\" => \"batch_norm\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         if \"bn\" in key:\n             key = key.replace(\"bn\", \"batch_norm\")"
        },
        {
            "sha": "edf0e142da09ecf5289d35376c86c7b87f99ae5b",
            "filename": "src/transformers/models/upernet/convert_swin_upernet_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconvert_swin_upernet_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -208,7 +208,7 @@ def convert_upernet_checkpoint(model_name, pytorch_dump_folder_path, push_to_hub\n     model.eval()\n \n     # replace \"bn\" => \"batch_norm\"\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         if \"bn\" in key:\n             key = key.replace(\"bn\", \"batch_norm\")"
        },
        {
            "sha": "2a1ab62c6acdf8101a39b920117dedd2fc23ae03",
            "filename": "src/transformers/models/videomae/convert_videomae_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fconvert_videomae_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -136,7 +136,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if key.startswith(\"encoder.\"):"
        },
        {
            "sha": "4c7869184ed331b4391021cf331c39ca222c48e9",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_flax_vision_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -785,9 +785,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "5818a7427179c24a58e9af07b28929a97c136ec1",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_tf_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_tf_vision_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -375,9 +375,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "541c8bc9b98831f00463792a6bdb1a7fbbb2db8c",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -257,7 +257,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *model_args, **kwargs):\n                 del tf_model\n                 gc.collect()\n \n-                attn_implementation = kwargs.get(\"attn_implementation\", None)\n+                attn_implementation = kwargs.get(\"attn_implementation\")\n                 kwargs_encoder_decoder = {}\n                 if attn_implementation:\n                     kwargs_encoder_decoder = {\n@@ -360,9 +360,9 @@ def from_encoder_decoder_pretrained(\n         }\n \n         # remove encoder, decoder kwargs from kwargs\n-        for key in kwargs_encoder.keys():\n+        for key in kwargs_encoder:\n             del kwargs[\"encoder_\" + key]\n-        for key in kwargs_decoder.keys():\n+        for key in kwargs_decoder:\n             del kwargs[\"decoder_\" + key]\n \n         # Load and initialize the encoder and decoder"
        },
        {
            "sha": "663d6944c67e002b27822435edfc58866582d9ac",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_flax_vision_text_dual_encoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -480,9 +480,9 @@ def from_vision_text_pretrained(\n         }\n \n         # remove text, vision kwargs from kwargs\n-        for key in kwargs_vision.keys():\n+        for key in kwargs_vision:\n             del kwargs[\"vision_\" + key]\n-        for key in kwargs_text.keys():\n+        for key in kwargs_text:\n             del kwargs[\"text_\" + key]\n \n         # Load and initialize the text and vision model"
        },
        {
            "sha": "42ff0be7a9e8548ce2fbd4dc75235aae9aa8aefc",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_tf_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_tf_vision_text_dual_encoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -525,9 +525,9 @@ def from_vision_text_pretrained(\n         }\n \n         # remove vision, text kwargs from kwargs\n-        for key in kwargs_vision.keys():\n+        for key in kwargs_vision:\n             del kwargs[\"vision_\" + key]\n-        for key in kwargs_text.keys():\n+        for key in kwargs_text:\n             del kwargs[\"text_\" + key]\n \n         # Load and initialize the vision and text model"
        },
        {
            "sha": "fd36c7afafb85b910e6174d292e885e02b451047",
            "filename": "src/transformers/models/vision_text_dual_encoder/modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fmodeling_vision_text_dual_encoder.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -366,9 +366,9 @@ def from_vision_text_pretrained(\n         }\n \n         # remove vision, text kwargs from kwargs\n-        for key in kwargs_vision.keys():\n+        for key in kwargs_vision:\n             del kwargs[\"vision_\" + key]\n-        for key in kwargs_text.keys():\n+        for key in kwargs_text:\n             del kwargs[\"text_\" + key]\n \n         # Load and initialize the vision and text model"
        },
        {
            "sha": "c7e4a7dc3bda8a2758e9309e205d5d1431f6e922",
            "filename": "src/transformers/models/vit_mae/convert_vit_mae_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fconvert_vit_mae_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fconvert_vit_mae_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fconvert_vit_mae_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -67,7 +67,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "e9b171876a9c0ead7f2e298959d7eb3ac0582a9e",
            "filename": "src/transformers/models/vitmatte/convert_vitmatte_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconvert_vitmatte_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -85,7 +85,7 @@ def convert_vitmatte_checkpoint(model_name, pytorch_dump_folder_path, push_to_hu\n     state_dict = torch.load(filepath, map_location=\"cpu\", weights_only=True)\n \n     # rename keys\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         val = state_dict.pop(key)\n         if \"backbone.blocks\" in key:\n             key = key.replace(\"backbone.blocks\", \"backbone.encoder.layer\")"
        },
        {
            "sha": "bf6aa8e4a36b4150c0c51d82b2c59aabc841f411",
            "filename": "src/transformers/models/vivit/convert_vivit_flax_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fconvert_vivit_flax_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -129,7 +129,7 @@ def transform_state_encoder_block(state_dict, i):\n \n \n def get_n_layers(state_dict):\n-    return sum([1 if \"encoderblock_\" in k else 0 for k in state_dict[\"optimizer\"][\"target\"][\"Transformer\"].keys()])\n+    return sum([1 if \"encoderblock_\" in k else 0 for k in state_dict[\"optimizer\"][\"target\"][\"Transformer\"]])\n \n \n def transform_state(state_dict, classification_head=False):"
        },
        {
            "sha": "786782732219eebd1dcb470518f16ce1ef95b334",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -166,7 +166,7 @@ def apply_chat_template(\n         }\n \n         for kwarg_type in processed_kwargs:\n-            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__.keys():\n+            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__:\n                 kwarg_type_defaults = AllKwargsForChatTemplate.__annotations__[kwarg_type]\n                 default_value = getattr(kwarg_type_defaults, key, None)\n                 value = kwargs.pop(key, default_value)"
        },
        {
            "sha": "361684319c3e15900c231e6bce3f1118e5a77801",
            "filename": "src/transformers/models/wav2vec2/convert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fconvert_wav2vec2_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -89,7 +89,7 @@ def set_recursively(key, value, full_name, weight_type, hf_pointer):\n         hf_pointer = getattr(hf_pointer, attribute)\n \n     hf_param_name = None\n-    for param_key in PARAM_MAPPING.keys():\n+    for param_key in PARAM_MAPPING:\n         if full_name.endswith(param_key):\n             hf_param_name = PARAM_MAPPING[full_name.split(\".\")[-1]]\n             weight_type = \"param\"\n@@ -148,7 +148,7 @@ def set_recursively(key, value, full_name, weight_type, hf_pointer):\n \n def rename_dict(key, value, full_name, weight_type, hf_dict):\n     hf_param_name = None\n-    for param_key in PARAM_MAPPING.keys():\n+    for param_key in PARAM_MAPPING:\n         if full_name.endswith(param_key):\n             hf_param_name = PARAM_MAPPING[full_name.split(\".\")[-1]]\n             weight_type = \"param\""
        },
        {
            "sha": "427cd7261f44b0affce0864a8c2c391644cae504",
            "filename": "src/transformers/models/wav2vec2/modeling_tf_wav2vec2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fmodeling_tf_wav2vec2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1300,7 +1300,7 @@ def call(\n \n         hidden_states, extract_features = self.feature_projection(extract_features, training=training)\n \n-        mask_time_indices = kwargs.get(\"mask_time_indices\", None)\n+        mask_time_indices = kwargs.get(\"mask_time_indices\")\n         if training:\n             hidden_states = self._mask_hidden_states(hidden_states, mask_time_indices=mask_time_indices)\n "
        },
        {
            "sha": "7fab737064214b7b76a5591a7449bf22935f41e5",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -191,7 +191,7 @@ def __init__(\n \n         # make sure that tokens made of several\n         # characters are not split at tokenization\n-        for token in self.encoder.keys():\n+        for token in self.encoder:\n             if len(token) > 1:\n                 self.add_tokens(AddedToken(token, rstrip=True, lstrip=True, normalized=False))\n \n@@ -212,7 +212,7 @@ def set_target_lang(self, target_lang: str):\n \n         # make sure that tokens made of several\n         # characters are not split at tokenization\n-        for token in self.encoder.keys():\n+        for token in self.encoder:\n             if len(token) > 1:\n                 self.add_tokens(AddedToken(token, rstrip=True, lstrip=True, normalized=False))\n "
        },
        {
            "sha": "a5cbcd95341b522ad2d54f96dc60e102663c4b27",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1181,7 +1181,7 @@ def split_by_batch_index(values, key, batch_idx, is_shortform, beam_indices=None\n     def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n         # Stack back seek_outputs tensors after splitting them with the split_by_batch_index method\n         outputs = {}\n-        for key in seek_outputs[0].keys():\n+        for key in seek_outputs[0]:\n             if key in [\"sequences\", \"beam_indices\", \"token_timestamps\"]:\n                 outputs[key] = torch.stack([v[key] for v in seek_outputs], dim=0).to(device)\n             elif key in [\"scores\", \"encoder_attentions\", \"encoder_hidden_states\", \"logits\"]:\n@@ -1212,7 +1212,7 @@ def _stack_split_outputs(self, seek_outputs, model_output_type, device, kwargs):\n                 else:\n                     outputs[key] = None\n \n-        token_timestamps = outputs.get(\"token_timestamps\", None)\n+        token_timestamps = outputs.get(\"token_timestamps\")\n         if token_timestamps is not None:\n             model_output_type = dict\n \n@@ -1442,9 +1442,9 @@ def replace_or_add(lst: list[int], num: int, itr: Iterator[int]):\n \n         def language_to_id(language: str) -> int:\n             language = language.lower()\n-            if language in generation_config.lang_to_id.keys():\n+            if language in generation_config.lang_to_id:\n                 language_token = language\n-            elif language in TO_LANGUAGE_CODE.keys():\n+            elif language in TO_LANGUAGE_CODE:\n                 language_token = f\"<|{TO_LANGUAGE_CODE[language]}|>\"\n             elif language in TO_LANGUAGE_CODE.values():\n                 language_token = f\"<|{language}|>\""
        },
        {
            "sha": "b11df907b3ddc879c57c7b8abe0632c327d0aeec",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1592,14 +1592,14 @@ def generate(\n         ):\n             forced_decoder_ids = self.generation_config.forced_decoder_ids\n         else:\n-            forced_decoder_ids = kwargs.get(\"forced_decoder_ids\", None)\n+            forced_decoder_ids = kwargs.get(\"forced_decoder_ids\")\n \n         if task is not None or language is not None or (forced_decoder_ids is None and prompt_ids is not None):\n             forced_decoder_ids = []\n             if hasattr(generation_config, \"language\"):\n-                if generation_config.language in generation_config.lang_to_id.keys():\n+                if generation_config.language in generation_config.lang_to_id:\n                     language_token = generation_config.language\n-                elif generation_config.language in TO_LANGUAGE_CODE.keys():\n+                elif generation_config.language in TO_LANGUAGE_CODE:\n                     language_token = f\"<|{TO_LANGUAGE_CODE[generation_config.language]}|>\"\n                 elif generation_config.language in TO_LANGUAGE_CODE.values():\n                     language_token = f\"<|{generation_config.language}|>\""
        },
        {
            "sha": "4147f14d86bd2ec95228ab5334098abf9e5920b6",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1007,7 +1007,7 @@ def new_chunk():\n                 text = tokenizer.decode([token])\n                 # Removing outer shell <|XX|>\n                 text = text[2:-2]\n-                language = LANGUAGES.get(text, None)\n+                language = LANGUAGES.get(text)\n                 if language is not None:\n                     # 1/ Indeed some language\n                     # TODO Handle when language is different from the previous"
        },
        {
            "sha": "fbd2762cef85ff80f3fed60cda79bca204147056",
            "filename": "src/transformers/models/x_clip/convert_x_clip_original_pytorch_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconvert_x_clip_original_pytorch_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -118,7 +118,7 @@ def rename_key(name):\n \n \n def convert_state_dict(orig_state_dict, config):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"attn.in_proj\" in key:"
        },
        {
            "sha": "d799ced792080d6df30a14e787f187c4484c62c8",
            "filename": "src/transformers/models/xglm/modeling_tf_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -890,8 +890,8 @@ def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=\n         if past_key_values:\n             inputs = tf.expand_dims(inputs[:, -1], -1)\n \n-        position_ids = kwargs.get(\"position_ids\", None)\n-        attention_mask = kwargs.get(\"attention_mask\", None)\n+        position_ids = kwargs.get(\"position_ids\")\n+        attention_mask = kwargs.get(\"attention_mask\")\n \n         if attention_mask is not None and position_ids is None:\n             position_ids = tf.math.cumsum(attention_mask, axis=-1, exclusive=True)"
        },
        {
            "sha": "c110c005afb9c1011a9ff77f3579355ae98c0f41",
            "filename": "src/transformers/models/xmod/convert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fconvert_xmod_original_pytorch_checkpoint_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -144,7 +144,7 @@ def convert_xmod_checkpoint_to_pytorch(\n \n         if sorted(bert_output.adapter_modules.keys()) != sorted(xmod_layer.adapter_modules.keys()):\n             raise AssertionError(\"Lists of language adapters do not match.\")\n-        for lang_code in xmod_layer.adapter_modules.keys():\n+        for lang_code in xmod_layer.adapter_modules:\n             to_adapter = bert_output.adapter_modules[lang_code]\n             from_adapter = xmod_layer.adapter_modules[lang_code]\n             to_adapter.dense1.weight = from_adapter.fc1.weight"
        },
        {
            "sha": "54fbd18e0633835df7098868e5efee723c5ab02b",
            "filename": "src/transformers/models/yolos/convert_yolos_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fconvert_yolos_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -123,7 +123,7 @@ def rename_key(name: str) -> str:\n \n \n def convert_state_dict(orig_state_dict: dict, model: YolosForObjectDetection) -> dict:\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if \"qkv\" in key:"
        },
        {
            "sha": "d7efb99e464579566f3bdd9361ea378fed0b8ff4",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -366,8 +366,8 @@ def __init__(self, **kwargs: Unpack[YolosFastImageProcessorKwargs]) -> None:\n         self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n \n         # Backwards compatibility\n-        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n-        do_normalize = kwargs.get(\"do_normalize\", None)\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\")\n+        do_normalize = kwargs.get(\"do_normalize\")\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n "
        },
        {
            "sha": "b1d3e96859820897a27dc03f8815e2fad869ea76",
            "filename": "src/transformers/models/yoso/convert_yoso_pytorch_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyoso%2Fconvert_yoso_pytorch_to_pytorch.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -60,7 +60,7 @@ def rename_key(orig_key):\n \n \n def convert_checkpoint_helper(max_position_embeddings, orig_state_dict):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         if (\"pooler\" in key) or (\"sen_class\" in key):"
        },
        {
            "sha": "03f2145418e0fc5bbace44d8014cae4c9d202b22",
            "filename": "src/transformers/models/zoedepth/convert_zoedepth_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconvert_zoedepth_to_hf.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -255,7 +255,7 @@ def read_in_q_k_v_metric_head(state_dict):\n \n \n def convert_state_dict(orig_state_dict):\n-    for key in orig_state_dict.copy().keys():\n+    for key in orig_state_dict.copy():\n         val = orig_state_dict.pop(key)\n \n         # rename key\n@@ -266,7 +266,7 @@ def convert_state_dict(orig_state_dict):\n \n \n def remove_ignore_keys(state_dict):\n-    for key in state_dict.copy().keys():\n+    for key in state_dict.copy():\n         if (\n             \"fc_norm\" in key\n             or \"relative_position_index\" in key"
        },
        {
            "sha": "152bc98d6deea502a0de4e62df7b21c21c82ee8f",
            "filename": "src/transformers/onnx/convert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fonnx%2Fconvert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fonnx%2Fconvert.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -457,5 +457,5 @@ def ensure_model_and_config_inputs_match(\n \n     # Make sure the input order match (VERY IMPORTANT !!!!)\n     matching_inputs = forward_inputs_set.intersection(model_inputs_set)\n-    ordered_inputs = [parameter for parameter in forward_parameters.keys() if parameter in matching_inputs]\n+    ordered_inputs = [parameter for parameter in forward_parameters if parameter in matching_inputs]\n     return is_ok, ordered_inputs"
        },
        {
            "sha": "688d0f8db56f393910dc05bb35b53213b46ced2b",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -625,7 +625,7 @@ def get_scheduler(\n         optimizer_dict = optimizer.optimizer_dict\n         scheduler_dict = {}\n \n-        for param in optimizer_dict.keys():\n+        for param in optimizer_dict:\n             scheduler_dict[param] = get_scheduler(\n                 name,\n                 optimizer=optimizer_dict[param],\n@@ -639,7 +639,7 @@ def scheduler_hook(param):\n             # attach the scheduler hook, the gradients have been zeroed here\n             scheduler_dict[param].step()\n \n-        for param in optimizer_dict.keys():\n+        for param in optimizer_dict:\n             if param.requires_grad:\n                 param.register_post_accumulate_grad_hook(scheduler_hook)\n "
        },
        {
            "sha": "3e739800951b62d4e8676f12ce13c8090915c1c6",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1141,7 +1141,7 @@ def save_pretrained(\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )"
        },
        {
            "sha": "d87f6928c98afafad370d920a1740f2d021848d7",
            "filename": "src/transformers/pipelines/image_to_image.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fimage_to_image.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -136,7 +136,7 @@ def preprocess(self, image, timeout=None):\n \n     def postprocess(self, model_outputs):\n         images = []\n-        if \"reconstruction\" in model_outputs.keys():\n+        if \"reconstruction\" in model_outputs:\n             outputs = model_outputs.reconstruction\n         for output in outputs:\n             output = output.data.squeeze().float().cpu().clamp_(0, 1).numpy()"
        },
        {
            "sha": "ed21e3908490a4200df6e08399e57b27fea7bbf1",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -524,7 +524,7 @@ def _forward(self, inputs):\n         model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}\n         # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\n         model_forward = self.model.forward if self.framework == \"pt\" else self.model.call\n-        if \"use_cache\" in inspect.signature(model_forward).parameters.keys():\n+        if \"use_cache\" in inspect.signature(model_forward).parameters:\n             model_inputs[\"use_cache\"] = False\n         output = self.model(**model_inputs)\n         if isinstance(output, dict):"
        },
        {
            "sha": "6f11f3bc97418be6106b183a1925245a273cb085",
            "filename": "src/transformers/pipelines/text_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Ftext_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Ftext_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftext_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -194,7 +194,7 @@ def preprocess(self, inputs, **tokenizer_kwargs) -> dict[str, GenericTensor]:\n     def _forward(self, model_inputs):\n         # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\n         model_forward = self.model.forward if self.framework == \"pt\" else self.model.call\n-        if \"use_cache\" in inspect.signature(model_forward).parameters.keys():\n+        if \"use_cache\" in inspect.signature(model_forward).parameters:\n             model_inputs[\"use_cache\"] = False\n         return self.model(**model_inputs)\n "
        },
        {
            "sha": "a9d14674820bdb801a9097574978df2ab1cb07ee",
            "filename": "src/transformers/pipelines/token_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Ftoken_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -31,7 +31,7 @@ class TokenClassificationArgumentHandler(ArgumentHandler):\n \n     def __call__(self, inputs: Union[str, list[str]], **kwargs):\n         is_split_into_words = kwargs.get(\"is_split_into_words\", False)\n-        delimiter = kwargs.get(\"delimiter\", None)\n+        delimiter = kwargs.get(\"delimiter\")\n \n         if inputs is not None and isinstance(inputs, (list, tuple)) and len(inputs) > 0:\n             inputs = list(inputs)"
        },
        {
            "sha": "20675d4a2928430385cb6d7d4bc61aa48ff21ea6",
            "filename": "src/transformers/pipelines/zero_shot_classification.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_classification.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -146,7 +146,7 @@ def _parse_and_tokenize(\n         return inputs\n \n     def _sanitize_parameters(self, **kwargs):\n-        if kwargs.get(\"multi_class\", None) is not None:\n+        if kwargs.get(\"multi_class\") is not None:\n             kwargs[\"multi_label\"] = kwargs[\"multi_class\"]\n             logger.warning(\n                 \"The `multi_class` argument has been deprecated and renamed to `multi_label`. \"\n@@ -227,7 +227,7 @@ def _forward(self, inputs):\n         model_inputs = {k: inputs[k] for k in self.tokenizer.model_input_names}\n         # `XXXForSequenceClassification` models should not use `use_cache=True` even if it's supported\n         model_forward = self.model.forward if self.framework == \"pt\" else self.model.call\n-        if \"use_cache\" in inspect.signature(model_forward).parameters.keys():\n+        if \"use_cache\" in inspect.signature(model_forward).parameters:\n             model_inputs[\"use_cache\"] = False\n         outputs = self.model(**model_inputs)\n "
        },
        {
            "sha": "7f77daa2977a7702bf02d5c6f1dbb7daa2ad8dd3",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -678,7 +678,7 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -1187,7 +1187,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         for modality in default_kwargs:  # noqa: PLC0206\n             default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n             # update defaults with arguments from tokenizer init\n-            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n+            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__:\n                 # init with tokenizer init kwargs if necessary\n                 if tokenizer_init_kwargs is not None and modality_key in tokenizer_init_kwargs:\n                     value = (\n@@ -1203,7 +1203,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         # update modality kwargs with passed kwargs\n         non_modality_kwargs = set(kwargs) - set(output_kwargs)\n         for modality, output_kwarg in output_kwargs.items():\n-            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__.keys():\n+            for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__:\n                 # check if we received a structured kwarg dict or not to handle it correctly\n                 if modality in kwargs:\n                     kwarg_value = kwargs[modality].pop(modality_key, \"__empty__\")\n@@ -1236,7 +1236,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n             # kwargs is a flat dictionary\n             for key, kwarg in kwargs.items():\n                 if key not in used_keys:\n-                    if key in ModelProcessorKwargs.__annotations__[\"common_kwargs\"].__annotations__.keys():\n+                    if key in ModelProcessorKwargs.__annotations__[\"common_kwargs\"].__annotations__:\n                         output_kwargs[\"common_kwargs\"][key] = kwarg\n                     elif key not in possible_modality_keywords:\n                         logger.warning_once(\n@@ -1350,7 +1350,7 @@ def _get_arguments_from_pretrained(cls, pretrained_model_name_or_path, **kwargs)\n                 classes = tuple(cls.get_possibly_dynamic_module(n) if n is not None else None for n in class_name)\n                 if attribute_name == \"image_processor\":\n                     # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n-                    use_fast = kwargs.get(\"use_fast\", None)\n+                    use_fast = kwargs.get(\"use_fast\")\n                     if use_fast is None:\n                         logger.warning_once(\n                             \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n@@ -1495,7 +1495,7 @@ def apply_chat_template(\n         }\n \n         for kwarg_type in processed_kwargs:\n-            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__.keys():\n+            for key in AllKwargsForChatTemplate.__annotations__[kwarg_type].__annotations__:\n                 kwarg_type_defaults = AllKwargsForChatTemplate.__annotations__[kwarg_type]\n                 default_value = getattr(kwarg_type_defaults, key, None)\n                 value = kwargs.pop(key, default_value)\n@@ -1641,7 +1641,7 @@ def apply_chat_template(\n                                 current_mask[token_id] = 1\n                         assistant_masks.append(current_mask)\n                     out[\"assistant_masks\"] = assistant_masks\n-                    out.convert_to_tensors(tensor_type=kwargs.get(\"return_tensors\", None))\n+                    out.convert_to_tensors(tensor_type=kwargs.get(\"return_tensors\"))\n                 return out\n             else:\n                 return out[\"input_ids\"]"
        },
        {
            "sha": "161951d3409f737aaaeea80b3bfc1bc791fc762c",
            "filename": "src/transformers/quantizers/auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fauto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fauto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -116,7 +116,7 @@ class AutoQuantizationConfig:\n \n     @classmethod\n     def from_dict(cls, quantization_config_dict: dict):\n-        quant_method = quantization_config_dict.get(\"quant_method\", None)\n+        quant_method = quantization_config_dict.get(\"quant_method\")\n         # We need a special care for bnb models to make sure everything is BC ..\n         if quantization_config_dict.get(\"load_in_8bit\", False) or quantization_config_dict.get(\"load_in_4bit\", False):\n             suffix = \"_4bit\" if quantization_config_dict.get(\"load_in_4bit\", False) else \"_8bit\"\n@@ -126,7 +126,7 @@ def from_dict(cls, quantization_config_dict: dict):\n                 \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n             )\n \n-        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING.keys():\n+        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n             raise ValueError(\n                 f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                 f\" {list(AUTO_QUANTIZER_MAPPING.keys())}\"\n@@ -171,7 +171,7 @@ def from_config(cls, quantization_config: Union[QuantizationConfigMixin, dict],\n             else:\n                 quant_method += \"_4bit\"\n \n-        if quant_method not in AUTO_QUANTIZER_MAPPING.keys():\n+        if quant_method not in AUTO_QUANTIZER_MAPPING:\n             raise ValueError(\n                 f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                 f\" {list(AUTO_QUANTIZER_MAPPING.keys())}\"\n@@ -238,7 +238,7 @@ def supports_quant_method(quantization_config_dict):\n                 \"The model's quantization config from the arguments has no `quant_method` attribute. Make sure that the model has been correctly quantized\"\n             )\n \n-        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING.keys():\n+        if quant_method not in AUTO_QUANTIZATION_CONFIG_MAPPING:\n             logger.warning(\n                 f\"Unknown quantization type, got {quant_method} - supported types are:\"\n                 f\" {list(AUTO_QUANTIZER_MAPPING.keys())}. Hence, we will skip the quantization. \""
        },
        {
            "sha": "fb53c4c0f6de7f4ee236e393f59c2cafc92f649d",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -319,7 +319,7 @@ def _convert_model_for_quantization(self, model):\n \n         for name, module in model.named_modules():\n             module_class_name = module.__class__.__name__\n-            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION.keys() and (\n+            if module_class_name in MODULES_TO_PATCH_FOR_QUANTIZATION and (\n                 self.quantization_config.quant_method\n                 in MODULES_TO_PATCH_FOR_QUANTIZATION[module_class_name][\"quantization_methods\"]\n             ):"
        },
        {
            "sha": "2f73567860450e0d345386274bdd129043ba91e0",
            "filename": "src/transformers/quantizers/quantizer_auto_round.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_auto_round.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -42,7 +42,7 @@ def __init__(self, quantization_config: QuantizationConfigMixin, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n     def validate_environment(self, *args, **kwargs):\n-        self.device_map = kwargs.get(\"device_map\", None)\n+        self.device_map = kwargs.get(\"device_map\")\n         if not is_auto_round_available():\n             raise ImportError(\n                 \"Loading an AutoRound quantized model requires auto-round library (`pip install 'auto-round>=0.5'`)\""
        },
        {
            "sha": "a57e732b9823a6f41528acd7b3d2485c454f5fd9",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -62,7 +62,7 @@ def validate_environment(self, *args, **kwargs):\n             )\n             return\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n                 \"You have loaded a BitNet model on CPU and have a CUDA device available, make sure to set \""
        },
        {
            "sha": "fb268777cdd4325a7fb26b94ece7ee97f32367b9",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -102,14 +102,14 @@ def validate_environment(self, *args, **kwargs):\n                 \" sure the weights are in PyTorch format.\"\n             )\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if (\n             device_map is not None\n             and isinstance(device_map, dict)\n             and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n         ):\n             device_map_without_lm_head = {\n-                key: device_map[key] for key in device_map.keys() if key not in self.modules_to_not_convert\n+                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n             }\n             if set(device_map.values()) == {\"cpu\"} and bnb_multibackend_is_enabled:\n                 pass"
        },
        {
            "sha": "3dc759dfd2610d67c7def655266b7fc45f47c0dd",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -99,14 +99,14 @@ def validate_environment(self, *args, **kwargs):\n                 \" sure the weights are in PyTorch format.\"\n             )\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if (\n             device_map is not None\n             and isinstance(device_map, dict)\n             and not self.quantization_config.llm_int8_enable_fp32_cpu_offload\n         ):\n             device_map_without_lm_head = {\n-                key: device_map[key] for key in device_map.keys() if key not in self.modules_to_not_convert\n+                key: device_map[key] for key in device_map if key not in self.modules_to_not_convert\n             }\n             if set(device_map.values()) == {\"cpu\"} and bnb_multibackend_is_enabled:\n                 pass\n@@ -177,7 +177,7 @@ def check_quantized_param(\n         module, tensor_name = get_module_from_name(model, param_name)\n         if isinstance(module._parameters.get(tensor_name, None), bnb.nn.Int8Params):\n             if self.pre_quantized:\n-                if param_name.replace(\"weight\", \"SCB\") not in state_dict.keys():\n+                if param_name.replace(\"weight\", \"SCB\") not in state_dict:\n                     raise ValueError(\"Missing quantization component `SCB`\")\n                 if param_value.dtype != torch.int8:\n                     raise ValueError(\n@@ -204,8 +204,8 @@ def create_quantized_param(\n         fp16_statistics_key = param_name.replace(\"weight\", \"SCB\")\n         fp16_weights_format_key = param_name.replace(\"weight\", \"weight_format\")\n \n-        fp16_statistics = state_dict.get(fp16_statistics_key, None)\n-        fp16_weights_format = state_dict.get(fp16_weights_format_key, None)\n+        fp16_statistics = state_dict.get(fp16_statistics_key)\n+        fp16_weights_format = state_dict.get(fp16_weights_format_key)\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         if tensor_name not in module._parameters:"
        },
        {
            "sha": "0e94d87f329b7e131b6dac96d6aabcd8a3e81cc5",
            "filename": "src/transformers/quantizers/quantizer_compressed_tensors.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_compressed_tensors.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -130,7 +130,7 @@ def _process_model_after_weight_loading(self, model, **kwargs):\n         if (\n             self.quantization_config.is_quantization_compressed and not self.run_compressed\n         ) or self.quantization_config.is_sparsification_compressed:\n-            config = kwargs.get(\"config\", None)\n+            config = kwargs.get(\"config\")\n             cache_path = config._name_or_path\n \n             if not os.path.exists(cache_path):"
        },
        {
            "sha": "bb7a78c536c5dbebbfac9aa69d1f88b5bd937715",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -79,7 +79,7 @@ def validate_environment(self, *args, **kwargs):\n         if not torch.cuda.is_available():\n             raise RuntimeError(\"No GPU found. A GPU is needed for quantization.\")\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n                 \"You have loaded an EETQ model on CPU and have a CUDA device available, make sure to set \""
        },
        {
            "sha": "c00a141d6727e2caa0ee3c7162277525277f6d96",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -71,7 +71,7 @@ def validate_environment(self, *args, **kwargs):\n                 \"FP8 quantized models is only supported on GPUs with compute capability >= 9.0 (e.g H100)\"\n             )\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n                 \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \"\n@@ -231,7 +231,7 @@ def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> li\n \n         not_missing_keys = []\n         for name, module in model.named_modules():\n-            if isinstance(module, FbgemmFp8Linear) or isinstance(module, FbgemmFp8Llama4TextExperts):\n+            if isinstance(module, (FbgemmFp8Linear, FbgemmFp8Llama4TextExperts)):\n                 for missing in missing_keys:\n                     if (\n                         (name in missing or name in f\"{prefix}.{missing}\")"
        },
        {
            "sha": "58c747effaee4f477ec1e3ef1fe2efc35abb3c80",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -56,7 +56,7 @@ def validate_environment(self, *args, **kwargs):\n                     f\", actual = `{major}.{minor}`\"\n                 )\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n                 \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \""
        },
        {
            "sha": "f52ac36a8dbf4b14096b7d970dd11476f01b61c4",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -78,7 +78,7 @@ def validate_environment(self, *args, **kwargs):\n                 self.torch_dtype = torch.float32\n                 logger.info(\"Setting torch_dtype to torch.float32 as the default value since it was not specified.\")\n \n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if isinstance(device_map, dict):\n             if \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                 raise ValueError(\n@@ -171,7 +171,7 @@ def check_quantized_param(\n         module, tensor_name = get_module_from_name(model, param_name)\n \n         if self.pre_quantized:\n-            return (isinstance(module, torch.nn.Linear) or isinstance(module, HQQLinear)) and tensor_name != \"weight\"\n+            return (isinstance(module, (torch.nn.Linear, HQQLinear))) and tensor_name != \"weight\"\n         else:\n             return (\n                 isinstance(module, torch.nn.Linear)"
        },
        {
            "sha": "d273ddda59552b4acca009e754f545a1b260e827",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -117,8 +117,8 @@ def check_quantized_param(\n         if is_optimum_quanto_available():\n             from optimum.quanto import QModuleMixin\n \n-        device_map = kwargs.get(\"device_map\", None)\n-        param_device = kwargs.get(\"param_device\", None)\n+        device_map = kwargs.get(\"device_map\")\n+        param_device = kwargs.get(\"param_device\")\n         # we don't quantize the model if the module is going to be offloaded to the cpu\n         if device_map is not None and param_device is not None:\n             device_map_values = set(device_map.values())"
        },
        {
            "sha": "23c9d7e327634e3f8ccc53e4d39c6adb79618614",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -98,7 +98,7 @@ def validate_environment(self, *args, **kwargs):\n             raise ImportError(\"Loading an torchao quantized model requires torchao library (`pip install torchao`)\")\n \n         self.offload = False\n-        device_map = kwargs.get(\"device_map\", None)\n+        device_map = kwargs.get(\"device_map\")\n         if isinstance(device_map, dict):\n             if \"cpu\" in device_map.values() or \"disk\" in device_map.values():\n                 if self.pre_quantized:\n@@ -109,7 +109,7 @@ def validate_environment(self, *args, **kwargs):\n                 else:\n                     self.offload = True\n         if self.pre_quantized:\n-            weights_only = kwargs.get(\"weights_only\", None)\n+            weights_only = kwargs.get(\"weights_only\")\n             if weights_only:\n                 torch_version = version.parse(importlib.metadata.version(\"torch\"))\n                 if torch_version < version.parse(\"2.5.0\"):"
        },
        {
            "sha": "a41ea8166e3a9ea0e7a625be8deb5ff6ca3c20ea",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -3304,7 +3304,7 @@ def compare_pipeline_output_to_hub_spec(output, hub_spec):\n     missing_keys = []\n     unexpected_keys = []\n     all_field_names = {field.name for field in fields(hub_spec)}\n-    matching_keys = sorted([key for key in output.keys() if key in all_field_names])\n+    matching_keys = sorted([key for key in output if key in all_field_names])\n \n     # Fields with a MISSING default are required and must be in the output\n     for field in fields(hub_spec):"
        },
        {
            "sha": "cd85a641cb6830c6244a77df4ba19b6da3637b24",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1203,7 +1203,7 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n \n         # The model's main input name, usually `input_ids`, has been passed for padding\n         if self.model_input_names[0] not in encoded_inputs:\n@@ -1449,7 +1449,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             if not isinstance(message, dict):\n                 return\n             maybe_list_content: Optional[Union[str, list[dict[str, Union[str, dict[str, Any]]]]]] = message.get(\n-                \"content\", None\n+                \"content\"\n             )\n             if not maybe_list_content or isinstance(maybe_list_content, str):\n                 return"
        },
        {
            "sha": "beb91a9472bcfd3c49450ce3584199b29dfddc72",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -271,7 +271,7 @@ def __getitem__(self, item: Union[int, str]) -> Union[Any, EncodingFast]:\n         elif self._encodings is not None:\n             return self._encodings[item]\n         elif isinstance(item, slice):\n-            return {key: self.data[key][item] for key in self.data.keys()}\n+            return {key: self.data[key][item] for key in self.data}\n         else:\n             raise KeyError(\n                 \"Invalid key. Only three types of key are available: \"\n@@ -1846,7 +1846,7 @@ def from_pretrained(\n         from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n         from_auto_class = kwargs.pop(\"_from_auto\", False)\n         commit_hash = kwargs.pop(\"_commit_hash\", None)\n-        gguf_file = kwargs.get(\"gguf_file\", None)\n+        gguf_file = kwargs.get(\"gguf_file\")\n \n         if use_auth_token is not None:\n             warnings.warn(\n@@ -2042,7 +2042,7 @@ def _from_pretrained(\n         # We instantiate fast tokenizers based on a slow tokenizer if we don't have access to the tokenizer.json\n         # file or if `from_slow` is set to True.\n         from_slow = kwargs.get(\"from_slow\", False)\n-        gguf_file = kwargs.get(\"gguf_file\", None)\n+        gguf_file = kwargs.get(\"gguf_file\")\n         has_tokenizer_file = resolved_vocab_files.get(\"tokenizer_file\", None) is not None\n \n         # If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\n@@ -2136,7 +2136,7 @@ def _from_pretrained(\n                 else:\n                     # Fallback: use pattern matching on the string.\n                     model_type = None\n-                    for pattern in TOKENIZER_MAPPING_NAMES.keys():\n+                    for pattern in TOKENIZER_MAPPING_NAMES:\n                         if pattern in str(pretrained_model_name_or_path):\n                             model_type = pattern\n                             break\n@@ -2415,7 +2415,7 @@ def save_pretrained(\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )\n@@ -2464,7 +2464,7 @@ def save_pretrained(\n \n         if len(self.init_inputs) > 0:\n             tokenizer_config[\"init_inputs\"] = copy.deepcopy(self.init_inputs)\n-        for file_id in self.vocab_files_names.keys():\n+        for file_id in self.vocab_files_names:\n             tokenizer_config.pop(file_id, None)\n \n         # no typefields, this way old fast and slow can load it\n@@ -3283,7 +3283,7 @@ def pad(\n         # If we have a list of dicts, let's convert it in a dict of lists\n         # We do this to allow using this method as a collate_fn function in PyTorch Dataloader\n         if isinstance(encoded_inputs, (list, tuple)) and isinstance(encoded_inputs[0], Mapping):\n-            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0].keys()}\n+            encoded_inputs = {key: [example[key] for example in encoded_inputs] for key in encoded_inputs[0]}\n \n         # The model's main input name, usually `input_ids`, has been passed for padding\n         if self.model_input_names[0] not in encoded_inputs:"
        },
        {
            "sha": "4ca5bed60511396bf71e3dd1ad25b1070d67c87c",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -134,7 +134,7 @@ def __init__(self, *args, **kwargs):\n             fast_tokenizer = convert_slow_tokenizer(slow_tokenizer)\n         elif not slow_tokenizer:\n             # We tried loading a slow_tokenizer with spm and failed, try to load with tiktoken\n-            self.vocab_file = kwargs.get(\"vocab_file\", None)\n+            self.vocab_file = kwargs.get(\"vocab_file\")\n             self.additional_special_tokens = kwargs.get(\"additional_special_tokens\", [])\n             fast_tokenizer = convert_slow_tokenizer(self, from_tiktoken=True)\n             slow_tokenizer = None\n@@ -583,7 +583,7 @@ def _batch_encode_plus(\n         # To match each overflowing sample with the original sample in the batch\n         # we add an overflow_to_sample_mapping array (see below)\n         sanitized_tokens = {}\n-        for key in tokens_and_encodings[0][0].keys():\n+        for key in tokens_and_encodings[0][0]:\n             stack = [e for item, _ in tokens_and_encodings for e in item[key]]\n             sanitized_tokens[key] = stack\n         sanitized_encodings = [e for _, item in tokens_and_encodings for e in item]"
        },
        {
            "sha": "c0d4c12db56c85aedd21f7857deb11f8d0438c58",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -438,7 +438,7 @@ def __init__(\n             logger.info(f\"No `TrainingArguments` passed, using `output_dir={output_dir}`.\")\n             args = TrainingArguments(output_dir=output_dir)\n         if args.batch_eval_metrics and compute_metrics is not None:\n-            if \"compute_result\" not in inspect.signature(compute_metrics).parameters.keys():\n+            if \"compute_result\" not in inspect.signature(compute_metrics).parameters:\n                 raise ValueError(\n                     \"When using `batch_eval_metrics`, your `compute_metrics` function must take a `compute_result`\"\n                     \" boolean argument which will be triggered after the last batch of the eval set to signal that the\"\n@@ -4614,7 +4614,7 @@ def prediction_step(\n         # For CLIP-like models capable of returning loss values.\n         # If `return_loss` is not specified or being `None` in `inputs`, we check if the default value of `return_loss`\n         # is `True` in `model.forward`.\n-        return_loss = inputs.get(\"return_loss\", None)\n+        return_loss = inputs.get(\"return_loss\")\n         if return_loss is None:\n             return_loss = self.can_return_loss\n         loss_without_labels = True if len(self.label_names) == 0 and return_loss else False\n@@ -5259,7 +5259,7 @@ def create_accelerator_and_postprocess(self):\n         # some Trainer classes need to use `gather` instead of `gather_for_metrics`, thus we store a flag\n         self.gather_function = self.accelerator.gather_for_metrics\n \n-        if \"use_gather_object\" in inspect.signature(self.gather_function).parameters.keys():\n+        if \"use_gather_object\" in inspect.signature(self.gather_function).parameters:\n             self.gather_function = functools.partial(\n                 self.gather_function, use_gather_object=self.args.eval_use_gather_object\n             )"
        },
        {
            "sha": "e93e27905277b568b98b2cb736a639d49478703f",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1047,7 +1047,7 @@ def log_metrics(self, split, metrics):\n \n     print(f\"***** {split} metrics *****\")\n     metrics_formatted = self.metrics_format(metrics)\n-    k_width = max(len(str(x)) for x in metrics_formatted.keys())\n+    k_width = max(len(str(x)) for x in metrics_formatted)\n     v_width = max(len(str(x)) for x in metrics_formatted.values())\n     for key in sorted(metrics_formatted.keys()):\n         print(f\"  {key: <{k_width}} = {metrics_formatted[key]:>{v_width}}\")\n@@ -1139,9 +1139,7 @@ def get_parameter_names(model, forbidden_layer_types, forbidden_layer_names=None\n         ]\n     # Add model specific parameters that are not in any child\n     result += [\n-        k\n-        for k in model._parameters.keys()\n-        if not any(pattern.search(k.lower()) for pattern in forbidden_layer_patterns)\n+        k for k in model._parameters if not any(pattern.search(k.lower()) for pattern in forbidden_layer_patterns)\n     ]\n \n     return result\n@@ -1333,7 +1331,7 @@ def from_json_file(cls, json_file):\n         with open_file(json_file, \"r\", encoding=\"utf-8\") as f:\n             config_dict = json.load(f)\n         # Check for keys and load sensible defaults\n-        extra_keys = sorted(key for key in config_dict.keys() if key not in cls.__dataclass_fields__.keys())\n+        extra_keys = sorted(key for key in config_dict if key not in cls.__dataclass_fields__)\n         if len(extra_keys) > 0:\n             raise ValueError(\n                 f\"The config file at {json_file} had unknown keys ({extra_keys}), please try upgrading your `transformers`\""
        },
        {
            "sha": "c435c1f6dfbae0ddb485db93c77f01c482202201",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -288,9 +288,7 @@ def default_compute_objective(metrics: dict[str, float]) -> float:\n     _ = metrics.pop(\"epoch\", None)\n     # Remove speed metrics\n     speed_metrics = [\n-        m\n-        for m in metrics.keys()\n-        if m.endswith(\"_runtime\") or m.endswith(\"_per_second\") or m.endswith(\"_compilation_time\")\n+        m for m in metrics if m.endswith(\"_runtime\") or m.endswith(\"_per_second\") or m.endswith(\"_compilation_time\")\n     ]\n     for sm in speed_metrics:\n         _ = metrics.pop(sm, None)"
        },
        {
            "sha": "a9bbebab9401c9d0bf9ceeba874683a15698d4fa",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -2532,7 +2532,7 @@ def _dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n         converts torch.dtype to a string of just the type. For example, `torch.float32` get converted into *\"float32\"*\n         string, which can then be stored in the json format.\n         \"\"\"\n-        if d.get(\"torch_dtype\", None) is not None and not isinstance(d[\"torch_dtype\"], str):\n+        if d.get(\"torch_dtype\") is not None and not isinstance(d[\"torch_dtype\"], str):\n             d[\"torch_dtype\"] = str(d[\"torch_dtype\"]).split(\".\")[1]\n         for value in d.values():\n             if isinstance(value, dict):"
        },
        {
            "sha": "8f5b9bc82705c389840551b96b022c80080ea58d",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -233,7 +233,7 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n             attention_mask,\n             img_token=self.image_token,\n             sliding_window=getattr(self.config, \"sliding_window\", None),\n-            token_type_ids=kwargs.get(\"token_type_ids\", None),\n+            token_type_ids=kwargs.get(\"token_type_ids\"),\n             image_seq_length=image_seq_length,\n         )\n         print(f_string)"
        },
        {
            "sha": "335a5d57176639039671cb037f9bff8ea40eb6d4",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -830,7 +830,7 @@ def wrapper(*args, **kwargs):\n                     invalid_kwargs[k] = v\n \n             if invalid_kwargs:\n-                invalid_kwargs_names = [f\"'{k}'\" for k in invalid_kwargs.keys()]\n+                invalid_kwargs_names = [f\"'{k}'\" for k in invalid_kwargs]\n                 invalid_kwargs_names = \", \".join(invalid_kwargs_names)\n \n                 # Get the class name for better warning message\n@@ -993,7 +993,7 @@ def check_model_inputs(func):\n \n     @wraps(func)\n     def wrapper(self, *args, **kwargs):\n-        use_cache = kwargs.get(\"use_cache\", None)\n+        use_cache = kwargs.get(\"use_cache\")\n         if use_cache is None:\n             use_cache = getattr(self.config, \"use_cache\", False)\n "
        },
        {
            "sha": "106e8abe11d2dde7a7d3e3f793507d2c81c2cd81",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -939,9 +939,9 @@ def patched_scatter(\n     original_compile = torch.compile\n \n     def hpu_backend_compile(*args, **kwargs):\n-        if kwargs.get(\"backend\", None) not in [\"hpu_backend\", \"eager\"]:\n+        if kwargs.get(\"backend\") not in [\"hpu_backend\", \"eager\"]:\n             logger.warning(\n-                f\"Calling torch.compile with backend={kwargs.get('backend', None)} on a Gaudi device is not supported. \"\n+                f\"Calling torch.compile with backend={kwargs.get('backend')} on a Gaudi device is not supported. \"\n                 \"We will override the backend with 'hpu_backend' to avoid errors.\"\n             )\n             kwargs[\"backend\"] = \"hpu_backend\"\n@@ -2149,7 +2149,7 @@ def __init__(\n         self._object_missing_backend = {}\n         self._explicit_import_shortcut = explicit_import_shortcut if explicit_import_shortcut else {}\n \n-        if any(isinstance(key, frozenset) for key in import_structure.keys()):\n+        if any(isinstance(key, frozenset) for key in import_structure):\n             self._modules = set()\n             self._class_to_module = {}\n             self.__all__ = []\n@@ -2247,7 +2247,7 @@ def __dir__(self):\n     def __getattr__(self, name: str) -> Any:\n         if name in self._objects:\n             return self._objects[name]\n-        if name in self._object_missing_backend.keys():\n+        if name in self._object_missing_backend:\n             missing_backends = self._object_missing_backend[name]\n \n             class Placeholder(metaclass=DummyObject):\n@@ -2271,7 +2271,7 @@ def call(self, *args, **kwargs):\n             Placeholder.__module__ = module_name\n \n             value = Placeholder\n-        elif name in self._class_to_module.keys():\n+        elif name in self._class_to_module:\n             try:\n                 module = self._get_module(self._class_to_module[name])\n                 value = getattr(module, name)\n@@ -2726,7 +2726,7 @@ def propagate_frozenset(unordered_import_structure):\n             if not isinstance(_value, dict):\n                 frozenset_first_import_structure[_key] = _value\n \n-            elif any(isinstance(v, frozenset) for v in _value.keys()):\n+            elif any(isinstance(v, frozenset) for v in _value):\n                 for k, v in _value.items():\n                     if isinstance(k, frozenset):\n                         # Here we want to switch around _key and k to propagate k upstream if it is a frozenset"
        },
        {
            "sha": "b270a1410565523bc2b8f0e7bdd8e420dc3c7198",
            "filename": "src/transformers/utils/notebook.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fnotebook.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Futils%2Fnotebook.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fnotebook.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -244,7 +244,7 @@ def write_line(self, values):\n             self.inner_table = [list(values.keys()), list(values.values())]\n         else:\n             columns = self.inner_table[0]\n-            for key in values.keys():\n+            for key in values:\n                 if key not in columns:\n                     columns.append(key)\n             self.inner_table[0] = columns\n@@ -258,7 +258,7 @@ def write_line(self, values):\n                     # update last line\n                     new_values = values\n                     for c in columns:\n-                        if c not in new_values.keys():\n+                        if c not in new_values:\n                             new_values[c] = last_values[columns.index(c)]\n                     self.inner_table[-1] = [new_values[c] for c in columns]\n             else:"
        },
        {
            "sha": "b59b3874858048423681e5057804c7d13a86f315",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -553,7 +553,7 @@ def save_pretrained(self, save_directory: Union[str, os.PathLike], push_to_hub:\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n                 FutureWarning,\n             )\n-            if kwargs.get(\"token\", None) is not None:\n+            if kwargs.get(\"token\") is not None:\n                 raise ValueError(\n                     \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n                 )"
        },
        {
            "sha": "a31c23adfe6ad726f77f98f2aede5eee526bcdad",
            "filename": "tests/extended/test_trainer_ext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fextended%2Ftest_trainer_ext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fextended%2Ftest_trainer_ext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fextended%2Ftest_trainer_ext.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -83,7 +83,7 @@ def run_seq2seq_quick(\n         if not do_eval:\n             self.skipTest(reason=\"do_eval is False\")\n \n-        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n+        eval_metrics = [log for log in logs if \"eval_loss\" in log]\n \n         first_step_stats = eval_metrics[0]\n         if predict_with_generate:\n@@ -168,7 +168,7 @@ def test_run_seq2seq(self):\n \n         # Check metrics\n         logs = TrainerState.load_from_json(os.path.join(output_dir, \"trainer_state.json\")).log_history\n-        eval_metrics = [log for log in logs if \"eval_loss\" in log.keys()]\n+        eval_metrics = [log for log in logs if \"eval_loss\" in log]\n         first_step_stats = eval_metrics[0]\n         last_step_stats = eval_metrics[-1]\n "
        },
        {
            "sha": "d73bfa2274493c69be96a471109960f8b42e4a94",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1440,7 +1440,7 @@ def test_left_padding_compatibility(self):\n         #   added support for it yet. We skip these models for now.\n         has_encoder_attributes = any(\n             attr_name\n-            for attr_name in config.to_dict().keys()\n+            for attr_name in config.to_dict()\n             if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n         )\n         if has_encoder_attributes:\n@@ -1644,7 +1644,7 @@ def test_generate_from_random_inputs_embeds(self):\n             config.is_decoder = True\n \n             model = model_class(config).to(torch_device).eval()\n-            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters:\n                 continue\n \n             #  No easy fix, let's skip the test for now\n@@ -1701,7 +1701,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n             # Skip models without explicit support\n             model = model_class(config).to(torch_device).eval()\n             set_model_for_less_flaky_test(model)\n-            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters:\n                 continue\n \n             # There are a few exception patterns in this test:\n@@ -1773,7 +1773,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             model = model_class(config).to(torch_device).eval()\n-            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters:\n                 self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n \n             input_ids = inputs_dict.pop(\"input_ids\")\n@@ -1929,7 +1929,7 @@ def test_generate_continue_from_inputs_embeds(self):\n \n             model = model_class(config).to(torch_device).eval()\n \n-            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters:\n                 self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n \n             # If \"past_key_values\" is not returned, skip the test (e.g. RWKV uses a different cache name and format)"
        },
        {
            "sha": "167cb1ff7c2e04e3c4f9d37d0b519e8319b2a124",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -562,8 +562,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "9205e5f4f21c7f386c18a5c6cb8ceaedd39481d3",
            "filename": "tests/models/align/test_processor_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Falign%2Ftest_processor_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Falign%2Ftest_processor_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_processor_align.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -135,7 +135,7 @@ def test_image_processor(self):\n         input_image_proc = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_image_proc.keys():\n+        for key in input_image_proc:\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -149,7 +149,7 @@ def test_tokenizer(self):\n         encoded_processor = processor(text=input_str)\n \n         encoded_tok = tokenizer(input_str, padding=\"max_length\", max_length=64)\n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_processor(self):"
        },
        {
            "sha": "2a36470051f88d852bf514c6bacaa2e0f97398eb",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -532,8 +532,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "245163d672c3211395fc979a5e099a0d7586cde8",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -448,7 +448,7 @@ def test_left_padding_compatibility(self):\n         #   added support for it yet. We skip these models for now.\n         has_encoder_attributes = any(\n             attr_name\n-            for attr_name in config.to_dict().keys()\n+            for attr_name in config.to_dict()\n             if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n         )\n         if has_encoder_attributes:"
        },
        {
            "sha": "447d38b956545904ea6e1eb7f090fa0c23dfd227",
            "filename": "tests/models/bark/test_processor_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbark%2Ftest_processor_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbark%2Ftest_processor_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_processor_bark.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -123,5 +123,5 @@ def test_tokenizer(self):\n             return_token_type_ids=False,\n         )\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key].squeeze().tolist())"
        },
        {
            "sha": "26f2053a93aa71c890599c2f060123aa4b4fb672",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -224,7 +224,7 @@ def create_and_check_forward_and_backwards(\n     def create_and_check_biogpt_weight_initialization(self, config, *args):\n         model = BioGptModel(config)\n         model_std = model.config.initializer_range / math.sqrt(2 * model.config.num_hidden_layers)\n-        for key in model.state_dict().keys():\n+        for key in model.state_dict():\n             if \"c_proj\" in key and \"weight\" in key:\n                 self.parent.assertLessEqual(abs(torch.std(model.state_dict()[key]) - model_std), 0.001)\n                 self.parent.assertLessEqual(abs(torch.mean(model.state_dict()[key]) - 0.0), 0.01)"
        },
        {
            "sha": "e8cdb26cc4220c3420edc974619e362e4af72c40",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -536,8 +536,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {\n@@ -1056,8 +1056,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {\n@@ -1274,8 +1274,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "254eede275ba3e12ae09584cf088553089798879",
            "filename": "tests/models/blip/test_processor_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -81,7 +81,7 @@ def test_image_processor(self):\n         input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -96,7 +96,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str, return_token_type_ids=False)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_processor(self):"
        },
        {
            "sha": "952595bbe60c125acda4625c5288da8d62e54130",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -627,7 +627,7 @@ def test_left_padding_compatibility(self):\n         #   added support for it yet. We skip these models for now.\n         has_encoder_attributes = any(\n             attr_name\n-            for attr_name in config.to_dict().keys()\n+            for attr_name in config.to_dict()\n             if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n         )\n         if has_encoder_attributes:"
        },
        {
            "sha": "a24ccc4aeb50ed48d52376e4e5672151ab592132",
            "filename": "tests/models/blip_2/test_processor_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -84,7 +84,7 @@ def test_image_processor(self):\n         input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_processor(self):"
        },
        {
            "sha": "0c685f053604c230266c46d5dee546969b260374",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -290,7 +290,7 @@ def create_and_check_forward_and_backwards(\n     def create_and_check_bloom_weight_initialization(self, config, *args):\n         model = BloomModel(config)\n         model_std = model.config.initializer_range / math.sqrt(2 * model.config.n_layer)\n-        for key in model.state_dict().keys():\n+        for key in model.state_dict():\n             if \"c_proj\" in key and \"weight\" in key:\n                 self.parent.assertLessEqual(abs(torch.std(model.state_dict()[key]) - model_std), 0.001)\n                 self.parent.assertLessEqual(abs(torch.mean(model.state_dict()[key]) - 0.0), 0.01)\n@@ -747,7 +747,7 @@ def test_embeddings(self):\n             output_dict[\"max\"][idx] = embeddings.max(dim=-1).values[0][i].item()\n             output_dict[\"mean\"][idx] = embeddings.mean(dim=-1)[0][i].item()\n \n-        for key in TEST_EMBEDDINGS[str(model.dtype)].keys():\n+        for key in TEST_EMBEDDINGS[str(model.dtype)]:\n             self.assertDictEqual(TEST_EMBEDDINGS[str(model.dtype)][key], output_dict[key])\n \n         output_dict_norm = {\"min\": {}, \"max\": {}, \"mean\": {}}"
        },
        {
            "sha": "dc8e9a145b08273b8b275a4925171a23163b7949",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -649,8 +649,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "83f68c0361bedf946c3ca8e275de6548d352b05e",
            "filename": "tests/models/chinese_clip/test_processor_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -155,7 +155,7 @@ def test_image_processor(self):\n         input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -170,7 +170,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_processor(self):"
        },
        {
            "sha": "0dab34123de4fe9496a1dd72e8cfa784482d51c8",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -594,8 +594,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "43192cee2efd3ae8bf9311165f75a7f452cbbf41",
            "filename": "tests/models/clap/test_processor_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclap%2Ftest_processor_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclap%2Ftest_processor_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_processor_clap.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -81,7 +81,7 @@ def test_feature_extractor(self):\n         input_feat_extract = feature_extractor(raw_speech, return_tensors=\"np\")\n         input_processor = processor(audios=raw_speech, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -96,7 +96,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_tokenizer_decode(self):"
        },
        {
            "sha": "ad7d817f962cf12793ed77a38aa128d71540c73d",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -641,8 +641,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "bb7fae4a861def9d59ba310381db0523d9118e67",
            "filename": "tests/models/clip/test_processor_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_processor_clip.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -137,7 +137,7 @@ def test_image_processor(self):\n         input_image_proc = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_image_proc.keys():\n+        for key in input_image_proc:\n             self.assertAlmostEqual(input_image_proc[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -152,7 +152,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_processor(self):"
        },
        {
            "sha": "08a21f9dcf3b7857465559af5cb593d39356a302",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -562,8 +562,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "f7255838caa8504806dac45d8a70503d3e787cfe",
            "filename": "tests/models/clipseg/test_processor_clipseg.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_processor_clipseg.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -128,7 +128,7 @@ def test_image_processor(self):\n         input_feat_extract = image_processor(image_input, return_tensors=\"np\")\n         input_processor = processor(images=image_input, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     def test_tokenizer(self):\n@@ -143,7 +143,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     def test_processor_text(self):"
        },
        {
            "sha": "817cbbff889867637cf196bd642e7337d89b7438",
            "filename": "tests/models/clvp/test_processor_clvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclvp%2Ftest_processor_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fclvp%2Ftest_processor_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_processor_clvp.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -71,7 +71,7 @@ def test_feature_extractor(self):\n         input_feat_extract = feature_extractor(raw_speech, return_tensors=\"np\")\n         input_processor = processor(raw_speech=raw_speech, return_tensors=\"np\")\n \n-        for key in input_feat_extract.keys():\n+        for key in input_feat_extract:\n             self.assertAlmostEqual(input_feat_extract[key].sum(), input_processor[key].sum(), delta=1e-2)\n \n     # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer with Whisper->Clvp\n@@ -87,7 +87,7 @@ def test_tokenizer(self):\n \n         encoded_tok = tokenizer(input_str)\n \n-        for key in encoded_tok.keys():\n+        for key in encoded_tok:\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n     # Copied from transformers.tests.models.whisper.test_processor_whisper.WhisperProcessorTest.test_tokenizer_decode with Whisper->Clvp"
        },
        {
            "sha": "910a0e7868fc20b91d7b80a29e8f1461743e5ced",
            "filename": "tests/models/csm/test_processor_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_processor_csm.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -57,7 +57,7 @@ def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())\n         # chat templates aren't serialized to json in processors\n-        self.assertFalse(\"chat_template\" in processor_dict_loaded.keys())\n+        self.assertFalse(\"chat_template\" in processor_dict_loaded)\n \n         # they have to be saved as separate file and loaded back from that file\n         # so we check if the same template is loaded"
        },
        {
            "sha": "94429260ec3e79e767f19da0fceb82cc6260ba25",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -647,7 +647,7 @@ def test_initialization(self):\n             # Skip the check for the backbone\n             for name, module in model.named_modules():\n                 if module.__class__.__name__ == \"DFineConvEncoder\":\n-                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict()]\n                     break\n \n             for name, param in model.named_parameters():"
        },
        {
            "sha": "bfd6e7416b33b6aab065ee1b31d0357aad436192",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -228,8 +228,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "df4ebd3614e201f8f499bb68e8abaa9a4ac2301c",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -770,7 +770,7 @@ def test_inference_object_detection_head_equivalence_cpu_accelerator(self):\n \n         # 3. assert equivalence\n         # (on A10, the differences get larger than on T4)\n-        for key in cpu_outputs.keys():\n+        for key in cpu_outputs:\n             torch.testing.assert_close(cpu_outputs[key], gpu_outputs[key].cpu(), atol=2e-2, rtol=2e-2)\n \n         expected_logits = torch.tensor("
        },
        {
            "sha": "b015e3c197bf5e1beaab235f893a1e803afd7e50",
            "filename": "tests/models/dia/test_processor_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_processor_dia.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -135,7 +135,7 @@ def test_tokenize(self):\n         input_tokenizer = tokenizer(random_text, padding=True, return_tensors=\"pt\")\n         input_processor = self.processor(random_text)\n \n-        for key in input_tokenizer.keys():\n+        for key in input_tokenizer:\n             self.assertTrue((input_tokenizer[key] == input_processor[key]).all())\n \n     def test_no_audio(self):"
        },
        {
            "sha": "8b47a18a71ea1ac5ed08ae121143c00358b9a428",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -268,7 +268,7 @@ def test_initialization(self):\n             backbone_params = []\n             for name, module in model.named_modules():\n                 if module.__class__.__name__ == \"DPTViTHybridEmbeddings\":\n-                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict()]\n                     break\n \n             for name, param in model.named_parameters():"
        },
        {
            "sha": "b9068631bd4f2797d53990445d84f7edcdcaa777",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -205,7 +205,7 @@ def test_initialization(self):\n             backbone_params = []\n             for name, module in model.named_modules():\n                 if module.__class__.__name__ == \"DPTViTHybridEmbeddings\":\n-                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict()]\n                     break\n \n             for name, param in model.named_parameters():"
        },
        {
            "sha": "e7a184c400a7d2e8ca013750a08852a25633bee9",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -281,7 +281,7 @@ def test_initialization(self):\n             backbone_params = []\n             for name, module in model.named_modules():\n                 if module.__class__.__name__ == \"DPTViTHybridEmbeddings\":\n-                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict()]\n                     break\n \n             for name, param in model.named_parameters():"
        },
        {
            "sha": "6dae1643d2af88a3e91516a0da5d9a116eb379d3",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -258,8 +258,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict = loaded_model.state_dict()\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        },
        {
            "sha": "37afc2cceba15426202cfddf54bf6fd9c791b85a",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -460,7 +460,7 @@ def test_left_padding_compatibility(self):\n         #   added support for it yet. We skip these models for now.\n         has_encoder_attributes = any(\n             attr_name\n-            for attr_name in config.to_dict().keys()\n+            for attr_name in config.to_dict()\n             if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n         )\n         if has_encoder_attributes:"
        },
        {
            "sha": "9686b1660fdb06e7c505c895a24e2548f3118292",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -292,7 +292,7 @@ def assertInterval(self, member, container, msg=None):\n         \"\"\"\n         if isinstance(member, torch.Tensor):\n             max_value, min_value = member.max().item(), member.min().item()\n-        elif isinstance(member, list) or isinstance(member, tuple):\n+        elif isinstance(member, (list, tuple)):\n             max_value, min_value = max(member), min(member)\n \n         if not isinstance(container, list):"
        },
        {
            "sha": "896ce256955a6233cc5f2acc947d805378bc79ef",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/95faabf0a6cd845f4c5548697e288a79e424b096/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=95faabf0a6cd845f4c5548697e288a79e424b096",
            "patch": "@@ -1001,8 +1001,8 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             loaded_model_state_dict.pop(\"text_model.embeddings.token_type_ids\", None)\n \n             non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n                     non_persistent_buffers[key] = loaded_model_state_dict[key]\n \n             loaded_model_state_dict = {"
        }
    ],
    "stats": {
        "total": 1550,
        "additions": 762,
        "deletions": 788
    }
}