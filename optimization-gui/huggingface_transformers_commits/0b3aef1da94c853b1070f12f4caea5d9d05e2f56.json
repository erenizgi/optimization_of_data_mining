{
    "author": "Cyrilvallez",
    "message": "ğŸš¨ Remove torchscript support (#41688)\n\n* remove a lot\n\n* remove the rest\n\n* doc",
    "sha": "0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
    "files": [
        {
            "sha": "35b4cfb1ebfa67726bb10225ac45f8d794e46398",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -58,7 +58,6 @@\n     \"test_model_get_set_embeddings\",\n     \"test_model_main_input_name\",\n     \"test_correct_missing_keys\",\n-    \"test_tie_model_weights\",\n     \"test_can_use_safetensors\",\n     \"test_load_save_without_tied_weights\",\n     \"test_tied_weights_keys\","
        },
        {
            "sha": "127028e670350c2a562088253860c4b9778a97cd",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -123,8 +123,6 @@\n     title: ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Amazon SageMaker\n   - local: serialization\n     title: Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ ONNX\n-  - local: torchscript\n-    title: Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ TorchScript\n   - local: notebooks\n     title: Ø¯ÙØ§ØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ø¹ Ø§Ù„Ø£Ù…Ø«Ù„Ø©\n   - local: community"
        },
        {
            "sha": "bf0bc0dde04b629932d32f85e15529a6c0128305",
            "filename": "docs/source/ar/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 154,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Far%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Far%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,154 +0,0 @@\n-# Ø§Ù„ØªØµØ¯ÙŠØ± Ø¥Ù„Ù‰ TorchScript\n-\n-<Tip>\n-\n-Ù‡Ø°Ù‡ Ù‡ÙŠ Ø¨Ø¯Ø§ÙŠØ© ØªØ¬Ø§Ø±Ø¨Ù†Ø§ Ù…Ø¹ TorchScript ÙˆÙ„Ø§ Ø²Ù„Ù†Ø§ Ù†Ø³ØªÙƒØ´Ù Ù‚Ø¯Ø±Ø§ØªÙ‡ Ù…Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…ØªØºÙŠØ±Ø© Ø§Ù„Ø­Ø¬Ù…. Ø¥Ù†Ù‡ Ù…Ø¬Ø§Ù„ Ø§Ù‡ØªÙ…Ø§Ù…Ù†Ø§ ÙˆØ³Ù†Ø¹Ù…Ù‚ ØªØ­Ù„ÙŠÙ„Ù†Ø§ ÙÙŠ Ø§Ù„Ø¥ØµØ¯Ø§Ø±Ø§Øª Ø§Ù„Ù‚Ø§Ø¯Ù…Ø©ØŒ Ù…Ø¹ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©ØŒ ÙˆØªÙ†ÙÙŠØ° Ø£ÙƒØ«Ø± Ù…Ø±ÙˆÙ†Ø©ØŒ ÙˆÙ…Ù‚Ø§ÙŠÙŠØ³ Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ÙŠÙ†  Ø§Ù„Ø£ÙƒÙˆØ§Ø¯ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¹Ù„Ù‰ Python Ù…Ø¹ Ø£ÙƒÙˆØ§Ø¯ TorchScript Ø§Ù„Ù…ÙØ¬Ù…Ù‘Ø¹Ø©.\n-\n-</Tip>\n-\n-ÙˆÙÙ‚Ù‹Ø§ Ù„Ù€ [ÙˆØ«Ø§Ø¦Ù‚ TorchScript](https://pytorch.org/docs/stable/jit.html):\n-\n-> TorchScript Ù‡ÙŠ Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…Ø§Ø°Ø¬ Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„ ÙˆØ§Ù„ØªØ­Ø³ÙŠÙ† Ù…Ù† ØªØ¹Ù„ÙŠÙ…Ø§Øª PyTorch Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ©.\n-\n-Ù‡Ù†Ø§Ùƒ ÙˆØ­Ø¯ØªØ§Ù† Ù…Ù† PyTorchØŒ [JIT and TRACE](https://pytorch.org/docs/stable/jit.html)ØŒ ØªØªÙŠØ­Ø§Ù† Ù„Ù„Ù…Ø·ÙˆØ±ÙŠÙ† ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬Ù‡Ù… Ù„Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø¨Ø±Ø§Ù…Ø¬ Ø£Ø®Ø±Ù‰ Ù…Ø«Ù„ Ø¨Ø±Ø§Ù…Ø¬ C++ Ø§Ù„Ù…ÙØ­Ø³Ù‘Ù†Ø© Ù„Ù„Ø£Ø¯Ø§Ø¡.\n-\n-Ù†Ù‚Ø¯Ù… ÙˆØ§Ø¬Ù‡Ø© ØªØªÙŠØ­ Ù„Ùƒ ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ ğŸ¤— Transformers Ø¥Ù„Ù‰ TorchScript Ø¨Ø­ÙŠØ« ÙŠÙ…ÙƒÙ† Ø¥Ø¹Ø§Ø¯Ø© Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ ÙÙŠ Ø¨ÙŠØ¦Ø© Ù…Ø®ØªÙ„ÙØ© Ø¹Ù† Ø¨Ø±Ø§Ù…Ø¬ Python Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ PyTorch. Ù‡Ù†Ø§ Ù†Ø´Ø±Ø­ ÙƒÙŠÙÙŠØ© ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬Ù†Ø§ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TorchScript.\n-\n-ÙŠØªØ·Ù„Ø¨ ØªØµØ¯ÙŠØ± Ù†Ù…ÙˆØ°Ø¬ Ø£Ù…Ø±ÙŠÙ†:\n-\n-- ØªÙ‡ÙŠØ¦Ø© Ù…Ø«ÙŠÙ„ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø© `torchscript`\n-- ØªÙ…Ø±ÙŠØ± Ù…ÙØ¯Ø®Ù„Ø§Øª ÙˆÙ‡Ù…ÙŠØ© (dummy inputs) Ø®Ù„Ø§Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n-\n-ØªÙ†Ø·ÙˆÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø¶Ø±ÙˆØ±Ø§Øª Ø¹Ù„Ù‰ Ø¹Ø¯Ø© Ø£Ù…ÙˆØ± ÙŠØ¬Ø¨ Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø·ÙˆØ±ÙŠÙ† ØªÙˆØ®ÙŠ Ø§Ù„Ø­Ø°Ø± Ø¨Ø´Ø£Ù†Ù‡Ø§ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙØµÙ„ Ø£Ø¯Ù†Ø§Ù‡.\n-\n-## Ø¹Ù„Ø§Ù…Ø© TorchScript ÙˆØ§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©\n-\n-Ø¹Ù„Ø§Ù…Ø© `torchscript` Ø¶Ø±ÙˆØ±ÙŠØ© Ù„Ø£Ù† Ù…Ø¹Ø¸Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© ğŸ¤— Transformers Ù„Ù‡Ø§ Ø£ÙˆØ²Ø§Ù† Ù…Ø±ØªØ¨Ø·Ø© Ø¨ÙŠÙ† Ø·Ø¨Ù‚Ø© `Embedding` ÙˆØ·Ø¨Ù‚Ø© `Decoding`. Ù„Ø§ ÙŠØ³Ù…Ø­ Ù„Ùƒ TorchScript Ø¨ØªØµØ¯ÙŠØ± Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø°Ø§Øª Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù…Ù† Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠ ÙØµÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† ÙˆÙ†Ø³Ø®Ù‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§.\n-\n-Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙÙ‡ÙŠØ£Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¹Ù„Ø§Ù…Ø© `torchscript` Ù„Ù‡Ø§ Ø·Ø¨Ù‚Ø© `Embedding` ÙˆØ·Ø¨Ù‚Ø©`Decoding` Ù…Ù†ÙØµÙ„ØªÙŠÙ†ØŒ Ù…Ù…Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ Ù„Ø§ ÙŠÙ†Ø¨ØºÙŠ ØªØ¯Ø±ÙŠØ¨Ù‡Ø§ Ù„Ø§Ø­Ù‚Ù‹Ø§. Ø³ÙŠØ¤Ø¯ÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ Ø¹Ø¯Ù… ØªØ²Ø§Ù…Ù† Ø§Ù„Ø·Ø¨Ù‚ØªÙŠÙ†ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ ØºÙŠØ± Ù…ØªÙˆÙ‚Ø¹Ø©.\n-\n-Ù‡Ø°Ø§ Ù„Ø§ ÙŠÙ†Ø·Ø¨Ù‚ Ø¹Ù„Ù‰ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ Ù„Ø§ ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø±Ø£Ø³ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ©ØŒ Ø­ÙŠØ« Ù„Ø§ ØªÙ…Ù„Ùƒ Ø£ÙˆØ²Ø§Ù†Ù‹Ø§ Ù…Ø±ØªØ¨Ø·Ø©. ÙŠÙ…ÙƒÙ† ØªØµØ¯ÙŠØ± Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø¨Ø£Ù…Ø§Ù† Ø¯ÙˆÙ† Ø¹Ù„Ø§Ù…Ø© `torchscript`.\n-\n-## Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ© ÙˆØ§Ù„Ø£Ø·ÙˆØ§Ù„ Ø§Ù„Ù‚ÙŠØ§Ø³ÙŠØ©\n-\n-ØªÙØ³ØªØ®Ø¯Ù… Ø§Ù„Ù…ÙØ¯Ø®Ù„Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ© Ù„ØªÙ…Ø±ÙŠØ± Ø£Ù…Ø§Ù…ÙŠ Ø®Ù„Ø§Ù„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ø£Ø«Ù†Ø§Ø¡ Ø§Ù†ØªØ´Ø§Ø± Ù‚ÙŠÙ… Ø§Ù„Ù…ÙØ¯Ø®Ù„Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø·Ø¨Ù‚Ø§ØªØŒ ÙŠØªØªØ¨Ø¹ PyTorch Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ© Ø§Ù„ØªÙŠ ÙŠØªÙ… ØªÙ†ÙÙŠØ°Ù‡Ø§ Ø¹Ù„Ù‰ ÙƒÙ„ Ù…ØµÙÙˆÙØ©(tensor). Ø«Ù… ÙŠØªÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ÙØ³Ø¬Ù„Ø© Ø¨Ø¹Ø¯ Ø°Ù„Ùƒ Ù„Ø¥Ù†Ø´Ø§Ø¡ *Ø£Ø«Ø±* Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.\n-\n-ÙŠØªÙ… Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØªØ¨Ø¹ Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ¯Ø®Ù„Ø§Øª. ÙˆØ¨Ø§Ù„ØªØ§Ù„ÙŠØŒ ÙÙ‡Ùˆ Ù…ÙÙ‚ÙŠÙ‘Ø¯ Ø¨Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ÙØ¯Ø®Ù„Ø§Øª Ø§Ù„ÙˆÙ‡Ù…ÙŠØ©ØŒ ÙˆÙ„Ù† ÙŠØ¹Ù…Ù„ Ù„Ø£ÙŠ Ø·ÙˆÙ„ ØªØ³Ù„Ø³Ù„ Ø£Ùˆ Ø­Ø¬Ù… Ø¯ÙØ¹Ø© Ù…Ø®ØªÙ„Ù. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¨Ø­Ø¬Ù… Ù…Ø®ØªÙ„ÙØŒ ÙŠØªÙ… Ø±ÙØ¹ Ø§Ù„Ø®Ø·Ø£ Ø§Ù„ØªØ§Ù„ÙŠ:\n-\n-```\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n-```\n-\n-Ù†ÙˆØµÙŠ Ø¨ØªØªØ¨Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø­Ø¬Ù… Ù…ÙØ¯Ø®Ù„Ø§Øª ÙˆÙ‡Ù…ÙŠØ© Ù„Ø§ ÙŠÙ‚Ù„ Ø¹Ù† Ø£ÙƒØ¨Ø± Ù…ÙØ¯Ø®Ù„ Ø³ÙŠØªÙ… ØªÙ‚Ø¯ÙŠÙ…Ù‡ Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„. ÙŠÙ…ÙƒÙ† Ø£Ù† ØªØ³Ø§Ø¹Ø¯ Ø§Ù„Ø­Ø´ÙˆØ©(padding) ÙÙŠ Ù…Ù„Ø¡ Ø§Ù„Ù‚ÙŠÙ… Ø§Ù„Ù…ÙÙ‚ÙˆØ¯Ø©. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ Ù†Ø¸Ø±Ù‹Ø§ Ù„ØªØªØ¨Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø­Ø¬Ù… Ù…ÙØ¯Ø®Ù„ Ø£ÙƒØ¨Ø±ØŒ Ø³ØªÙƒÙˆÙ† Ø£Ø¨Ø¹Ø§Ø¯ Ø§Ù„Ù…ØµÙÙˆÙØ© Ø³ØªÙƒÙˆÙ† ÙƒØ¨ÙŠØ±Ø© Ø£ÙŠØ¶Ù‹Ø§ØŒ Ù…Ù…Ø§ ÙŠØ¤Ø¯ÙŠ Ø¹Ù†Ù‡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ø­Ø³Ø§Ø¨Ø§Øª.\n-\n-Ø§Ù†ØªØ¨Ù‡ Ø¥Ù„Ù‰ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ù…ÙÙ†ÙØ°Ø© Ø¹Ù„Ù‰ ÙƒÙ„ Ù…ÙØ¯Ø®Ù„ ÙˆØªØ§Ø¨Ø¹ Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø¹Ù† ÙƒØ«Ø¨ Ø¹Ù†Ø¯ ØªØµØ¯ÙŠØ± Ù†Ù…Ø§Ø°Ø¬ Ù…ØªØºÙŠØ±Ø© Ø·ÙˆÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„.\n-\n-## Ø§Ø³ØªØ®Ø¯Ø§Ù… TorchScript ÙÙŠ Python\n-\n-ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ù‚Ø³Ù… ÙƒÙŠÙÙŠØ© Ø­ÙØ¸ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ ÙˆØªØ­Ù…ÙŠÙ„Ù‡Ø§ØŒ Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„ØªØªØ¨Ø¹ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n-\n-### Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬\n-\n-Ù„ØªØµØ¯ÙŠØ± `BertModel` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… TorchScriptØŒ Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ù€ `BertModel` Ù…Ù† ÙØ¦Ø© `BertConfig` Ø«Ù… Ø§Ø­ÙØ¸Ù‡ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ ØªØ­Øª Ø§Ø³Ù… Ø§Ù„Ù…Ù„Ù `traced_bert.pt`:\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# Tokenizing input text\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# Masking one of the input tokens\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# Creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# Initializing the model with the torchscript flag\n-# Flag set to True even though it is not necessary as this model does not have an LM Head.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# Instantiating the model\n-model = BertModel(config)\n-\n-# The model needs to be in evaluation mode\n-model.eval()\n-\n-# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# Creating the trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-### ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† ØªØ­Ù…ÙŠÙ„ `BertModel` Ø§Ù„Ù…ÙØ­ÙØ¸ Ø³Ø§Ø¨Ù‚Ù‹Ø§ØŒ `traced_bert.pt`ØŒ Ù…Ù† Ø§Ù„Ù‚Ø±Øµ ÙˆØ§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¹Ù„Ù‰ `dummy_input` Ø§Ù„Ù…ÙÙ‡ÙŠØ£ Ø³Ø§Ø¨Ù‚Ù‹Ø§:\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-### Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØªØªØ¨Ø¹ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØªØªØ¨Ø¹ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø£Ø³Ù„ÙˆØ¨ `__call__` Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡:\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-## Ù†Ø´Ø± Ù†Ù…Ø§Ø°Ø¬ Hugging Face TorchScript Ø¹Ù„Ù‰ AWS Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Neuron SDK\n-\n-Ù‚Ø¯Ù…Øª AWS Ø¹Ø§Ø¦Ù„Ø© [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) Ù…Ù† Ø§ï»·Ø¬Ù‡Ø²Ø© Ù„Ø®ÙØ¶ Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ£Ø¯Ø§Ø¡ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¢Ù„ÙŠ Ø¹Ø§Ù„ÙŠ Ø§Ù„Ø£Ø¯Ø§Ø¡ ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©. ØªØ¹Ù…Ù„ Ø£Ø¬Ù‡Ø²Ø© Inf1 Ø¨ÙˆØ§Ø³Ø·Ø© Ø´Ø±ÙŠØ­Ø© Inferentia Ù…Ù† AWSØŒ ÙˆÙ‡ÙŠ Ù…ÙØ³Ø±Ù‘Ø¹ Ø£Ø¬Ù‡Ø²Ø© Ù…ÙØ®ØµØµØŒ Ù…ØªØ®ØµØµ ÙÙŠ Ø£Ø¹Ø¨Ø§Ø¡ Ø¹Ù…Ù„ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) Ù‡ÙŠ SDK Ù„Ù€ Inferentia Ø§Ù„ØªÙŠ ØªØ¯Ø¹Ù… ØªØªØ¨Ø¹ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª ÙˆØªØ­Ø³ÙŠÙ†Ù‡Ø§ Ù„Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Inf1. ØªÙˆÙØ± Neuron SDK Ù…Ø§ ÙŠÙ„ÙŠ:\n-\n-1. ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø³Ù‡Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹ ØªØºÙŠÙŠØ± Ø³Ø·Ø± ÙˆØ§Ø­Ø¯ Ù…Ù† Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„ØªØªØ¨Ø¹ Ù†Ù…ÙˆØ°Ø¬ TorchScript ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ ÙÙŠ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„Ø³Ø­Ø§Ø¨ÙŠØ©.\n-2. ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¬Ø§Ù‡Ø²Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… [ØªØ­Ø³ÙŠÙ† Ø§Ù„ØªÙƒÙ„ÙØ© ÙˆØ§Ù„Ø£Ø¯Ø§Ø¡](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>).\n-3. Ø¯Ø¹Ù… Ù†Ù…Ø§Ø°Ø¬ Hugging Face Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ù…Ø¨Ù†ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¥Ù…Ø§ [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) Ø£Ùˆ [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).\n-\n-### Ø§Ù„Ø¢Ø«Ø§Ø± Ø§Ù„Ù…ØªØ±ØªØ¨Ø©\n-\n-ØªØ¹Ù…Ù„ Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªÙ†Ø¯Ø© Ø¥Ù„Ù‰ Ø¨Ù†ÙŠØ© [BERT (ØªÙ…Ø«ÙŠÙ„Ø§Øª Ø§Ù„ØªØ±Ù…ÙŠØ² Ø«Ù†Ø§Ø¦ÙŠØ© Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù…Ù† Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª)](https://huggingface.co/docs/transformers/main/model_doc/bert) Ø£Ùˆ Ù…ØªØºÙŠØ±Ø§ØªÙ‡Ø§ Ù…Ø«Ù„ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) Ùˆ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) Ø¨Ø´ÙƒÙ„ Ø£ÙØ¶Ù„ Ø¹Ù„Ù‰ Inf1 Ù„Ù„Ù…Ù‡Ø§Ù… ØºÙŠØ± Ø§Ù„ØªÙˆÙ„ÙŠØ¯ÙŠØ© Ù…Ø«Ù„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ÙŠØ©ØŒ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§ØªØŒ ÙˆØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ² (tokens). ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ† ØªÙƒÙŠÙŠÙ Ù…Ù‡Ø§Ù… ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ù„Ù„Ø¹Ù…Ù„ Ø¹Ù„Ù‰ Inf1 ÙˆÙÙ‚Ù‹Ø§ Ù„Ù‡Ø°Ø§ [Ø¨Ø±Ù†Ø§Ù…Ø¬ ØªØ¹Ù„ÙŠÙ…ÙŠ AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html). ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ† ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¬Ø§Ù‡Ø²Ø© Ø¹Ù„Ù‰ Inferentia ÙÙŠ Ù‚Ø³Ù… [Ù…Ù„Ø§Ø¡Ù…Ø© Ø¨Ù†ÙŠØ© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) Ù…Ù† ÙˆØ«Ø§Ø¦Ù‚ Neuron.\n-\n-### Ø§Ù„ØªØ¨Ø¹ÙŠØ§Øª (Dependencies)\n-\n-ÙŠØªØ·Ù„Ø¨ Ø§Ø³ØªØ®Ø¯Ø§Ù… AWS Neuron Ù„ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ [Ø¨ÙŠØ¦Ø© SDK Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) ÙˆØ§Ù„ØªÙŠ ØªØ£ØªÙŠ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¹Ù„Ù‰ [AMI Ù„Ù„ØªØ¹Ù„Ù… Ø§Ù„Ø¹Ù…ÙŠÙ‚ Ù…Ù† AWS](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).\n-\n-### ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ AWS Neuron\n-\n-Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ AWS NEURON Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù…Ù† [Ø§Ø³ØªØ®Ø¯Ø§Ù… TorchScript ÙÙŠ Python](torchscript#using-torchscript-in-python) Ù„ØªØªØ¨Ø¹ `BertModel`. Ù‚Ù… Ø¨Ø§Ø³ØªÙŠØ±Ø§Ø¯ Ø§Ù…ØªØ¯Ø§Ø¯ Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ `torch.neuron` Ù„Ù„ÙˆØµÙˆÙ„ Ø¥Ù„Ù‰ Ù…ÙƒÙˆÙ†Ø§Øª Neuron SDK Ù…Ù† Ø®Ù„Ø§Ù„ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Python:\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-\n-ÙƒÙ„ Ù…Ø§ Ø¹Ù„ÙŠÙƒ ÙØ¹Ù„Ù‡ Ù‡Ùˆ ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ø³Ø·Ø± Ø§Ù„ØªØ§Ù„ÙŠ:\n-\n-```diff\n-- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-ÙŠØªÙŠØ­ Ø°Ù„Ùƒ Ù„Ù€ Neuron SDK ØªØªØ¨Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªØ­Ø³ÙŠÙ†Ù‡ Ù„Ù…Ø«ÙŠÙ„Ø§Øª Inf1.\n-\n-Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ Ù…ÙŠØ²Ø§Øª AWS Neuron SDK ÙˆØ§Ù„Ø£Ø¯ÙˆØ§Øª ÙˆØ¯Ø±ÙˆØ³ Ø§Ù„Ø¨Ø±Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠØ© ÙˆØ§Ù„ØªØ­Ø¯ÙŠØ«Ø§Øª Ø§Ù„Ø£Ø®ÙŠØ±Ø©ØŒ ÙŠØ±Ø¬Ù‰ Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ [ÙˆØ«Ø§Ø¦Ù‚ AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
        },
        {
            "sha": "4481977dcffaab76d407729444924aec6f83a51b",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -227,8 +227,6 @@\n     title: ONNX\n   - local: executorch\n     title: ExecuTorch\n-  - local: torchscript\n-    title: TorchScript\n   title: Export to production\n - isExpanded: false\n   sections:"
        },
        {
            "sha": "fc854f94c2dfc7cedff563c7d20e6a83bab77923",
            "filename": "docs/source/en/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 138,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fen%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fen%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,138 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TorchScript\n-\n-[TorchScript](https://pytorch.org/docs/stable/jit.html) serializes PyTorch models into programs that can be executed in non-Python processes. This is especially advantageous in production environments where Python may not be the most performant choice.\n-\n-Transformers can export a model to TorchScript by:\n-\n-1. creating dummy inputs to create a *trace* of the model to serialize to TorchScript\n-2. enabling the `torchscript` parameter in either [`~PreTrainedConfig.torchscript`] for a randomly initialized model or [`~PreTrainedModel.from_pretrained`] for a pretrained model\n-\n-## Dummy inputs\n-\n-The dummy inputs are used in the forward pass, and as the input values are propagated through each layer, PyTorch tracks the different operations executed on each tensor. The recorded operations are used to create the model trace. Once it is recorded, it is serialized into a TorchScript program.\n-\n-```py\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = tokenizer.tokenize(text)\n-\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-```\n-\n-The trace is created based on the provided inputs dimensions and it can only handle inputs with the same shape as the provided input during tracing. An input with a different size raises the error message shown below.\n-\n-```bash\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`.\n-```\n-\n-Try to create a trace with a dummy input size at least as large as the largest expected input during inference. Padding can help fill missing values for larger inputs. It may be slower though since a larger input size requires more calculations. Be mindful of the total number of operations performed on each input and track the model performance when exporting models with variable sequence lengths.\n-\n-## Tied weights\n-\n-Weights between the `Embedding` and `Decoding` layers are tied in Transformers and TorchScript can't export models with tied weights. Instantiating a model with `torchscript=True`, separates the `Embedding` and `Decoding` layers and they aren't trained any further because it would throw the two layers out of sync which can lead to unexpected results.\n-\n-Models *without* a language model head don't have tied weights and can be safely exported without the `torchscript` parameter.\n-\n-<hfoptions id=\"torchscript\">\n-<hfoption id=\"randomly initialized model\">\n-\n-```py\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-model = BertModel(config)\n-model.eval()\n-```\n-\n-</hfoption>\n-<hfoption id=\"pretrained model\">\n-\n-```py\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-model.eval()\n-```\n-\n-</hfoption>\n-</hfoptions>\n-\n-## Export to TorchScript\n-\n-Create the Torchscript program with [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html), and save with [torch.jit.save](https://pytorch.org/docs/stable/generated/torch.jit.save.html).\n-\n-```py\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-Use [torch.jit.load](https://pytorch.org/docs/stable/generated/torch.jit.load.html) to load the traced model.\n-\n-```py\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-To use the traced model for inference, use the `__call__` dunder method.\n-\n-```py\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-## Deploy to AWS\n-\n-TorchScript programs serialized from Transformers can be deployed on [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) instances. The instance is powered by AWS Inferentia chips, a custom hardware accelerator designed for deep learning inference workloads. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) supports tracing Transformers models for deployment on Inf1 instances.\n-\n-> [!TIP]\n-> AWS Neuron requires a [Neuron SDK environment](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/inference-torch-neuron.html#inference-torch-neuron) which is preconfigured on [AWS DLAMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).\n-\n-Instead of [torch.jit.trace](https://pytorch.org/docs/stable/generated/torch.jit.trace.html), use [torch.neuron.trace](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/frameworks/torch/torch-neuron/api-compilation-python-api.html) to trace a model and optimize it for Inf1 instances.\n-\n-```py\n-import torch.neuron\n-\n-torch.neuron.trace(model, [tokens_tensor, segments_tensors])\n-```\n-\n-Refer to the [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) documentation for more information.\n-\n-### Model architectures\n-\n-BERT-based models - like [DistilBERT](./model_doc/distilbert) or [RoBERTa](./model_doc/roberta) - run best on Inf1 instances for non-generative tasks such as extractive question answering, and sequence or token classification.\n-\n-Text generation can be adapted to run on an Inf1 instance as shown in the [Transformers MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html) tutorial.\n-\n-Refer to the [Inference Samples/Tutorials (Inf1)](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/general/models/inference-inf1-samples.html#model-samples-inference-inf1) guide for more information about which models can be converted out of the box to run on Inf1 instances."
        },
        {
            "sha": "2a0eb91002dc57fa318eb2ab7af725c3f4f44ac0",
            "filename": "docs/source/es/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fes%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fes%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -64,8 +64,6 @@\n     title: Entrenador\n   - local: sagemaker\n     title: Ejecutar el entrenamiento en Amazon SageMaker\n-  - local: torchscript\n-    title: Exportar a TorchScript\n   - local: community\n     title: Los recursos de la comunidad\n   title: GuÃ­as para desarrolladores"
        },
        {
            "sha": "93873fadcae80043f57af5418768e561244501ef",
            "filename": "docs/source/es/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 167,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fes%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fes%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,167 +0,0 @@\n-<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Exportar a TorchScript\n-\n-<Tip>\n-Este es el comienzo de nuestros experimentos con TorchScript y todavÃ­a estamos explorando sus capacidades con modelos de variables de entrada. Es un tema de interÃ©s para nosotros y profundizaremos en nuestro anÃ¡lisis en las prÃ³ximas versiones, con mÃ¡s ejemplos de cÃ³digo, una implementaciÃ³n mÃ¡s flexible y comparativas de rendimiento comparando cÃ³digos basados en Python con TorchScript compilado.  \n-\n-</Tip>\n-\n-De acuerdo con la documentaciÃ³n de TorchScript: \n-\n-> \"TorchScript es una manera de crear modelos serializables y optimizables a partir del cÃ³digo PyTorch.\"\n-\n-Hay dos mÃ³dulos de PyTorch, [JIT y TRACE](https://pytorch.org/docs/stable/jit.html), que permiten a los desarrolladores exportar sus modelos para ser reusados en otros programas, como los programas de C++ orientados a la eficiencia.\n-\n-Nosotros proveemos una interface que te permite exportar los modelos ğŸ¤—Transformers a TorchScript para que puedan ser reusados en un entorno diferente al de los programas Python basados en PyTorch. AquÃ­ explicamos como exportar y usar nuestros modelos utilizando TorchScript.\n-\n-Exportar un modelo requiere de dos cosas:\n-\n-- La instanciaciÃ³n del modelo con la bandera TorchScript.\n-- Un paso hacia adelante con entradas ficticias.\n-\n-Estas necesidades implican varias cosas de las que los desarrolladores deben tener cuidado, como se detalla a continuaciÃ³n.\n-\n-## Bandera TorchScript y pesos atados.\n-\n-La bandera `torchscript` es necesaria porque la mayorÃ­a de los modelos de lenguaje de ğŸ¤—Transformers tienen pesos atados entre su `capa de incrustaciÃ³n` (`Embedding`) y su `capa de decodificaciÃ³n` (`Decoding`). TorchScript no te permite exportar modelos que tienen pesos atados, por lo que es necesario desatar y clonar los pesos de antemano.\n-\n-Los modelos instanciados con la bandera `torchscript` tienen su `capa de incrustaciÃ³n` (`Embedding`) y su `capa de decodificaciÃ³n` (`Decoding`) separadas, lo que significa que no deben ser entrenados mÃ¡s adelante. Entrenar desincronizarÃ­a las dos capas, lo que llevarÃ­a a resultados inesperados.\n-\n-Esto no es asÃ­ para los modelos que no tienen una cabeza de modelo de lenguaje, ya que esos modelos no tienen pesos atados. Estos modelos pueden ser exportados de manera segura sin la bandera `torchscript`.\n-\n-## Entradas ficticias y longitudes estÃ¡ndar\n-\n-Las entradas ficticias se utilizan para un paso del modelo hacia adelante. Mientras los valores de las entradas se propagan a travÃ©s de las capas, PyTorch realiza un seguimiento de las diferentes operaciones ejecutadas en cada tensor. Estas operaciones registradas se utilizan luego para crear *la traza* del modelo.\n-La traza se crea en relaciÃ³n con las dimensiones de las entradas. Por lo tanto, estÃ¡ limitada por las dimensiones de la entrada ficticia y no funcionarÃ¡ para ninguna otra longitud de secuencia o tamaÃ±o de lote. Cuando se intenta con un tamaÃ±o diferente, se genera el siguiente error:\n-\n-```\n-`El tamaÃ±o expandido del tensor (3) debe coincidir con el tamaÃ±o existente (7) en la dimensiÃ³n no singleton 2`.\n-```\n-\n-Recomendamos trazar el modelo con un tamaÃ±o de entrada ficticio al menos tan grande como la entrada mÃ¡s grande con la que se alimentarÃ¡ al modelo durante la inferencia. El relleno puede ayudar a completar los valores faltantes. Sin embargo, dado que el modelo se traza con un tamaÃ±o de entrada mÃ¡s grande, las dimensiones de la matriz tambiÃ©n serÃ¡n grandes, lo que resultarÃ¡ en mÃ¡s cÃ¡lculos.\n-\n-Ten cuidado con el nÃºmero total de operaciones realizadas en cada entrada y sigue de cerca el rendimiento al exportar modelos con longitudes de secuencia variables.\n-\n-## Usando TorchScript en Python\n-\n-Esta secciÃ³n demuestra cÃ³mo guardar y cargar modelos, asÃ­ como cÃ³mo usar la traza para la inferencia.\n-\n-### Guardando un modelo\n-\n-Para exportar un `BertModel` con TorchScript, instancia `BertModel` a partir de la clase `BertConfig` y luego guÃ¡rdalo en disco bajo el nombre de archivo `traced_bert.pt`:\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n-\n-# Tokenizing input text\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# Masking one of the input tokens\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# Creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# Initializing the model with the torchscript flag\n-# Flag set to True even though it is not necessary as this model does not have an LM Head.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# Instantiating the model\n-model = BertModel(config)\n-\n-# The model needs to be in evaluation mode\n-model.eval()\n-\n-# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag\n-model = BertModel.from_pretrained(\"bert-base-uncased\", torchscript=True)\n-\n-# Creating the trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-### Cargando un modelo\n-\n-Ahora puedes cargar el `BertModel` guardado anteriormente, `traced_bert.pt`, desde el disco y usarlo en la entrada ficticia (`dummy_input`) previamente inicializada:\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-## Usando un modelo trazado para inferencia\n-\n-Utiliza el modelo trazado para inferencia utilizando su mÃ©todo `_call_` dunder:\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-## Despliega modelos TorchScript de Hugging Face en AWS con el Neuron SDK\n-\n-AWS introdujo la familia de instancias [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) para inferencia de aprendizaje automÃ¡tico de alto rendimiento y bajo costo en la nube. Las instancias Inf1 estÃ¡n alimentadas por el chip AWS Inferentia, un acelerador de hardware personalizado que se especializa en cargas de trabajo de inferencia de aprendizaje profundo. [AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) es el SDK para Inferentia que admite el trazado y la optimizaciÃ³n de modelos de transformers para implementaciÃ³n en Inf1. El SDK Neuron proporciona:\n-\n-1. Una API fÃ¡cil de usar con un solo cambio de lÃ­nea de cÃ³digo para trazar y optimizar un modelo TorchScript para inferencia en la nube.\n-\n-2. Optimizaciones de rendimiento listas para usar [para mejorar el rendimiento y el costo](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>).\n-\n-3. Soporte para modelos de transformers de Hugging Face construidos tanto con [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) como con [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html).\n-\n-### Implicaciones\n-\n-Los modelos transformers basados en la arquitectura [BERT (Bidirectional Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert), o sus variantes como [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) y [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta), funcionan mejor en Inf1 para tareas no generativas como la respuesta a preguntas extractivas, la clasificaciÃ³n de secuencias y la clasificaciÃ³n de tokens. Sin embargo, las tareas de generaciÃ³n de texto aÃºn pueden adaptarse para ejecutarse en Inf1 segÃºn este [tutorial de AWS Neuron MarianMT](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html). Se puede encontrar mÃ¡s informaciÃ³n sobre los modelos que se pueden convertir fÃ¡cilmente para usar en Inferentia en la secciÃ³n de [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) de la documentaciÃ³n de Neuron.\n-\n-### Dependencias\n-\n-El uso de AWS Neuron para convertir modelos requiere un [entorno de Neuron SDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) que viene preconfigurado en [la AMI de AWS Deep Learning](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html).\n-\n-### Convertir un modelo para AWS Neuron\n-\n-Convierte un modelo para AWS NEURON utilizando el mismo cÃ³digo de [Uso de TorchScript en Python](torchscript#using-torchscript-in-python) para trazar un `BertModel`. Importa la extensiÃ³n del framework `torch.neuron` para acceder a los componentes del Neuron SDK a travÃ©s de una API de Python:\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-Solo necesitas la linea sigueda:\n-\n-```diff\n-- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-Esto permite que el Neuron SDK trace el modelo y lo optimice para las instancias Inf1.\n-\n-Para obtener mÃ¡s informaciÃ³n sobre las caracterÃ­sticas, herramientas, tutoriales de ejemplo y Ãºltimas actualizaciones del AWS Neuron SDK, consulta [la documentaciÃ³n de AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).\n\\ No newline at end of file"
        },
        {
            "sha": "19e71b6068bc15eaee39bca8a2fbc9b1644b624d",
            "filename": "docs/source/ja/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fja%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fja%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -109,8 +109,6 @@\n     title: ãƒãƒ£ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ\n   - local: serialization\n     title: ONNX ã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n-  - local: torchscript\n-    title: ãƒˆãƒ¼ãƒã‚¹ã‚¯ãƒªãƒ—ãƒˆã¸ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ\n   - local: community\n     title: ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ãƒªã‚½ãƒ¼ã‚¹\n   - local: troubleshooting"
        },
        {
            "sha": "27d64a625c8c4299e1be68f71541966604dc65c1",
            "filename": "docs/source/ja/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 177,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fja%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fja%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,177 +0,0 @@\n-<!--Copyright 2023 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Export to TorchScript\n-\n-<Tip>\n-\n-ã“ã‚Œã¯TorchScriptã‚’ä½¿ç”¨ã—ãŸå®Ÿé¨“ã®æœ€åˆã§ã‚ã‚Šã€å¯å¤‰å…¥åŠ›ã‚µã‚¤ã‚ºã®ãƒ¢ãƒ‡ãƒ«ã«å¯¾ã™ã‚‹ãã®èƒ½åŠ›ã‚’ã¾ã æ¢æ±‚ä¸­ã§ã™ã€‚ã“ã‚Œã¯ç§ãŸã¡ã®é–¢å¿ƒã®ç„¦ç‚¹ã§ã‚ã‚Šã€ä»Šå¾Œã®ãƒªãƒªãƒ¼ã‚¹ã§ã¯ã€ã‚ˆã‚ŠæŸ”è»Ÿãªå®Ÿè£…ã‚„ã€Pythonãƒ™ãƒ¼ã‚¹ã®ã‚³ãƒ¼ãƒ‰ã¨ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã•ã‚ŒãŸTorchScriptã‚’æ¯”è¼ƒã™ã‚‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’å«ã‚€ã€ã‚ˆã‚Šå¤šãã®ã‚³ãƒ¼ãƒ‰ä¾‹ã§è©³ç´°ãªåˆ†æã‚’è¡Œã„ã¾ã™ã€‚\n-\n-</Tip>\n-\n-[TorchScriptã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://pytorch.org/docs/stable/jit.html)ã«ã‚ˆã‚Œã°ï¼š\n-\n-> TorchScriptã¯ã€PyTorchã‚³ãƒ¼ãƒ‰ã‹ã‚‰ç›´åˆ—åŒ–ãŠã‚ˆã³æœ€é©åŒ–å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã§ã™ã€‚\n-\n-TorchScriptã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€åŠ¹ç‡å¿—å‘ã®C++ãƒ—ãƒ­ã‚°ãƒ©ãƒ ãªã©ã€ä»–ã®ãƒ—ãƒ­ã‚°ãƒ©ãƒ ã§ãƒ¢ãƒ‡ãƒ«ã‚’å†åˆ©ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚PyTorchãƒ™ãƒ¼ã‚¹ã®Pythonãƒ—ãƒ­ã‚°ãƒ©ãƒ ä»¥å¤–ã®ç’°å¢ƒã§ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ä½¿ç”¨ã™ã‚‹ãŸã‚ã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒ•ã‚§ãƒ¼ã‚¹ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚ã“ã“ã§ã¯ã€TorchScriptã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã€ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€æ¬¡ã®2ã¤ã®è¦ä»¶ãŒã‚ã‚Šã¾ã™ï¼š\n-\n-- `torchscript`ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–\n-- ãƒ€ãƒŸãƒ¼ã®å…¥åŠ›ã‚’ä½¿ç”¨ã—ãŸãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n-\n-ã“ã‚Œã‚‰ã®å¿…è¦æ¡ä»¶ã¯ã€ä»¥ä¸‹ã§è©³ç´°ã«èª¬æ˜ã•ã‚Œã¦ã„ã‚‹ã‚ˆã†ã«ã€é–‹ç™ºè€…ãŒæ³¨æ„ã™ã‚‹å¿…è¦ãŒã‚ã‚‹ã„ãã¤ã‹ã®ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n-\n-## TorchScript flag and tied weights\n-\n-`torchscript`ãƒ•ãƒ©ã‚°ã¯ã€ã»ã¨ã‚“ã©ã®ğŸ¤— Transformersè¨€èªãƒ¢ãƒ‡ãƒ«ã«ãŠã„ã¦ã€`Embedding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨`Decoding`ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“ã§é‡ã¿ãŒé€£çµã•ã‚Œã¦ã„ã‚‹ãŸã‚å¿…è¦ã§ã™ã€‚\n-TorchScriptã§ã¯ã€é‡ã¿ãŒé€£çµã•ã‚Œã¦ã„ã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã®ã§ã€äº‹å‰ã«é‡ã¿ã‚’åˆ‡ã‚Šé›¢ã—ã¦è¤‡è£½ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-`torchscript`ãƒ•ãƒ©ã‚°ã‚’ä½¿ç”¨ã—ã¦ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã¯ã€`Embedding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨`Decoding`ãƒ¬ã‚¤ãƒ¤ãƒ¼ãŒåˆ†é›¢ã•ã‚Œã¦ãŠã‚Šã€ãã®ãŸã‚å¾Œã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¦ã¯ã„ã‘ã¾ã›ã‚“ã€‚\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã€ã“ã‚Œã‚‰ã®2ã¤ã®ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’éåŒæœŸã«ã™ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã€äºˆæœŸã—ãªã„çµæœã‚’ã‚‚ãŸã‚‰ã™å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-è¨€èªãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’æŒãŸãªã„ãƒ¢ãƒ‡ãƒ«ã«ã¯è¨€åŠã—ã¾ã›ã‚“ãŒã€ã“ã‚Œã‚‰ã®ãƒ¢ãƒ‡ãƒ«ã«ã¯é€£çµã•ã‚ŒãŸé‡ã¿ãŒå­˜åœ¨ã—ãªã„ãŸã‚ã€`torchscript`ãƒ•ãƒ©ã‚°ãªã—ã§å®‰å…¨ã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã§ãã¾ã™ã€‚\n-\n-## Dummy inputs and standard lengths\n-\n-ãƒ€ãƒŸãƒ¼å…¥åŠ›ã¯ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚å…¥åŠ›ã®å€¤ã¯ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’é€šã˜ã¦ä¼æ’­ã•ã‚Œã‚‹é–“ã€PyTorchã¯å„ãƒ†ãƒ³ã‚½ãƒ«ã«å®Ÿè¡Œã•ã‚ŒãŸç•°ãªã‚‹æ“ä½œã‚’è¿½è·¡ã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®è¨˜éŒ²ã•ã‚ŒãŸæ“ä½œã¯ã€ãƒ¢ãƒ‡ãƒ«ã®*ãƒˆãƒ¬ãƒ¼ã‚¹*ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«ä½¿ç”¨ã•ã‚Œã¾ã™ã€‚\n-\n-ãƒˆãƒ¬ãƒ¼ã‚¹ã¯å…¥åŠ›ã®å¯¸æ³•ã«å¯¾ã—ã¦ä½œæˆã•ã‚Œã¾ã™ã€‚ãã®ãŸã‚ã€ãƒ€ãƒŸãƒ¼å…¥åŠ›ã®å¯¸æ³•ã«åˆ¶ç´„ã•ã‚Œã€ä»–ã®ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã‚„ãƒãƒƒãƒã‚µã‚¤ã‚ºã§ã¯å‹•ä½œã—ã¾ã›ã‚“ã€‚ç•°ãªã‚‹ã‚µã‚¤ã‚ºã§è©¦ã™ã¨ã€ä»¥ä¸‹ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã™ï¼š\n-\n-```\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n-```\n-\n-ãŠå‹§ã‚ã—ã¾ã™ã®ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ¨è«–ä¸­ã«ä¾›çµ¦ã•ã‚Œã‚‹æœ€å¤§ã®å…¥åŠ›ã¨åŒã˜å¤§ãã•ã®ãƒ€ãƒŸãƒ¼å…¥åŠ›ã‚µã‚¤ã‚ºã§ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã™ã‚‹ã“ã¨ã§ã™ã€‚ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ä¸è¶³å€¤ã‚’è£œå®Œã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ãŸã ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒã‚ˆã‚Šå¤§ããªå…¥åŠ›ã‚µã‚¤ã‚ºã§ãƒˆãƒ¬ãƒ¼ã‚¹ã•ã‚Œã‚‹ãŸã‚ã€è¡Œåˆ—ã®å¯¸æ³•ã‚‚å¤§ãããªã‚Šã€ã‚ˆã‚Šå¤šãã®è¨ˆç®—ãŒç™ºç”Ÿã—ã¾ã™ã€‚\n-\n-ç•°ãªã‚‹ã‚·ãƒ¼ã‚±ãƒ³ã‚¹é•·ã®ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹éš›ã«ã€å„å…¥åŠ›ã«å¯¾ã—ã¦å®Ÿè¡Œã•ã‚Œã‚‹æ¼”ç®—ã®ç·æ•°ã«æ³¨æ„ã—ã¦ã€ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’å¯†æ¥ã«ãƒ•ã‚©ãƒ­ãƒ¼ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-## Using TorchScript in Python\n-\n-ã“ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨èª­ã¿è¾¼ã¿ã€ãŠã‚ˆã³æ¨è«–ã«ãƒˆãƒ¬ãƒ¼ã‚¹ã‚’ä½¿ç”¨ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚\n-\n-### Saving a model\n-\n-TorchScriptã§`BertModel`ã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã™ã‚‹ã«ã¯ã€`BertConfig`ã‚¯ãƒ©ã‚¹ã‹ã‚‰`BertModel`ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã€ãã‚Œã‚’ãƒ•ã‚¡ã‚¤ãƒ«å`traced_bert.pt`ã§ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã—ã¾ã™ï¼š\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# Tokenizing input text\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# Masking one of the input tokens\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# Creating a dummy input\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# Initializing the model with the torchscript flag\n-# Flag set to True even though it is not necessary as this model does not have an LM Head.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# Instantiating the model\n-model = BertModel(config)\n-\n-# The model needs to be in evaluation mode\n-model.eval()\n-\n-# If you are instantiating the model with *from_pretrained* you can also easily set the TorchScript flag\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# Creating the trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-### Loading a model\n-\n-ä»¥å‰ã«ä¿å­˜ã—ãŸ `BertModel`ã€`traced_bert.pt` ã‚’ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰èª­ã¿è¾¼ã‚“ã§ã€ä»¥å‰ã«åˆæœŸåŒ–ã—ãŸ `dummy_input` ã§ä½¿ç”¨ã§ãã¾ã™ã€‚\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-\n-### Using a traced model for inference\n-\n-ãƒˆãƒ¬ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ã¦æ¨è«–ã‚’è¡Œã†ã«ã¯ã€ãã® `__call__` ãƒ€ãƒ³ãƒ€ãƒ¼ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-\n-## Deploy Hugging Face TorchScript models to AWS with the Neuron SDK\n-\n-AWSã¯ã‚¯ãƒ©ã‚¦ãƒ‰ã§ã®ä½ã‚³ã‚¹ãƒˆã§é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’æ¨è«–å‘ã‘ã« [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ãƒ•ã‚¡ãƒŸãƒªãƒ¼ã‚’å°å…¥ã—ã¾ã—ãŸã€‚Inf1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¯AWS Inferentiaãƒãƒƒãƒ—ã«ã‚ˆã£ã¦é§†å‹•ã•ã‚Œã€ãƒ‡ã‚£ãƒ¼ãƒ—ãƒ©ãƒ¼ãƒ‹ãƒ³ã‚°æ¨è«–ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«ç‰¹åŒ–ã—ãŸã‚«ã‚¹ã‚¿ãƒ ãƒ“ãƒ«ãƒ‰ã®ãƒãƒ¼ãƒ‰ã‚¦ã‚§ã‚¢ã‚¢ã‚¯ã‚»ãƒ©ãƒ¬ãƒ¼ã‚¿ã§ã™ã€‚[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) ã¯Inferentiaç”¨ã®SDKã§ã€ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦æœ€é©åŒ–ã—ã€Inf1ã«å±•é–‹ã™ã‚‹ãŸã‚ã®ã‚µãƒãƒ¼ãƒˆã‚’æä¾›ã—ã¾ã™ã€‚\n-\n-Neuron SDK ãŒæä¾›ã™ã‚‹ã‚‚ã®:\n-\n-1. ã‚¯ãƒ©ã‚¦ãƒ‰ã§ã®æ¨è«–ã®ãŸã‚ã«TorchScriptãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¦æœ€é©åŒ–ã™ã‚‹ãŸã‚ã®ã€1è¡Œã®ã‚³ãƒ¼ãƒ‰å¤‰æ›´ã§ä½¿ç”¨ã§ãã‚‹ç°¡å˜ãªAPIã€‚\n-2. [æ”¹å–„ã•ã‚ŒãŸã‚³ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/) ã®ãŸã‚ã®ãƒœãƒƒã‚¯ã‚¹å¤–ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã€‚\n-3. [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) ã¾ãŸã¯ [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html) ã§æ§‹ç¯‰ã•ã‚ŒãŸHugging Faceãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¸ã®ã‚µãƒãƒ¼ãƒˆã€‚\n-\n-### Implications\n-\n-BERTï¼ˆBidirectional Encoder Representations from Transformersï¼‰ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚„ãã®å¤‰ç¨®ï¼ˆ[distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) ã‚„ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) ãªã©ï¼‰ã«åŸºã¥ããƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒãƒ¼ãƒ¢ãƒ‡ãƒ«ã¯ã€éç”Ÿæˆã‚¿ã‚¹ã‚¯ï¼ˆæŠ½å‡ºå‹è³ªå•å¿œç­”ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ã€ãƒˆãƒ¼ã‚¯ãƒ³åˆ†é¡ãªã©ï¼‰ã«ãŠã„ã¦ã€Inf1ä¸Šã§æœ€é©ã«å‹•ä½œã—ã¾ã™ã€‚ãŸã ã—ã€ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆã‚¿ã‚¹ã‚¯ã‚‚ [AWS Neuron MarianMT ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html) ã«å¾“ã£ã¦Inf1ä¸Šã§å®Ÿè¡Œã§ãã¾ã™ã€‚Inferentiaã§ãƒœãƒƒã‚¯ã‚¹å¤–ã§å¤‰æ›ã§ãã‚‹ãƒ¢ãƒ‡ãƒ«ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã¯ã€Neuronãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ã® [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã«ã‚ã‚Šã¾ã™ã€‚\n-\n-### Dependencies\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’AWS Neuronã«å¤‰æ›ã™ã‚‹ã«ã¯ã€[Neuron SDK ç’°å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide) ãŒå¿…è¦ã§ã€[AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html) ã«äº‹å‰ã«æ§‹æˆã•ã‚Œã¦ã„ã¾ã™ã€‚\n-\n-### Converting a model for AWS Neuron\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’AWS NEURONç”¨ã«å¤‰æ›ã™ã‚‹ã«ã¯ã€[Pythonã§TorchScriptã‚’ä½¿ç”¨ã™ã‚‹](torchscript#using-torchscript-in-python) ã¨åŒã˜ã‚³ãƒ¼ãƒ‰ã‚’ä½¿ç”¨ã—ã¦ `BertModel` ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã¾ã™ã€‚Python APIã‚’ä»‹ã—ã¦Neuron SDKã®ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«ã‚¢ã‚¯ã‚»ã‚¹ã™ã‚‹ãŸã‚ã«ã€`torch.neuron` ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯æ‹¡å¼µã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-\n-æ¬¡ã®è¡Œã‚’å¤‰æ›´ã™ã‚‹ã ã‘ã§æ¸ˆã¿ã¾ã™ã€‚\n-\n-```diff\n-- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-ã“ã‚Œã«ã‚ˆã‚Šã€Neuron SDKã¯ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ã‚¹ã—ã€Inf1ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹å‘ã‘ã«æœ€é©åŒ–ã—ã¾ã™ã€‚\n-\n-AWS Neuron SDKã®æ©Ÿèƒ½ã€ãƒ„ãƒ¼ãƒ«ã€ã‚µãƒ³ãƒ—ãƒ«ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã€æœ€æ–°ã®ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆã«ã¤ã„ã¦è©³ã—ãçŸ¥ã‚ŠãŸã„å ´åˆã¯ã€[AWS NeuronSDK ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html) ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-\n-"
        },
        {
            "sha": "c1da17155e035a4698395cbb5b191346637b0bde",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -212,8 +212,6 @@\n     title: ONNXë¡œ ë‚´ë³´ë‚´ê¸°\n   - local: executorch\n     title: ExecuTorch\n-  - local: torchscript\n-    title: TorchScriptë¡œ ë‚´ë³´ë‚´ê¸°\n   title: ë°°í¬í™˜ê²½ì— ë‚´ë³´ë‚´ê¸°\n - isExpanded: false\n   sections:"
        },
        {
            "sha": "58c9b0cbb264e30d737149939c67945e3eb95f8e",
            "filename": "docs/source/ko/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fko%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_cpu.md?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -17,11 +17,6 @@ rendered properly in your Markdown viewer.\n \n ì´ ê°€ì´ë“œëŠ” CPUì—ì„œ ëŒ€ê·œëª¨ ëª¨ë¸ì„ íš¨ìœ¨ì ìœ¼ë¡œ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì— ì¤‘ì ì„ ë‘ê³  ìˆìŠµë‹ˆë‹¤.\n \n-## PyTorch JIT ëª¨ë“œ (TorchScript) [[pytorch-jitmode-torchscript]]\n-TorchScriptëŠ” PyTorch ì½”ë“œì—ì„œ ì§ë ¬í™”ì™€ ìµœì í™”ê°€ ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìƒì„±í• ë•Œ ì“°ì…ë‹ˆë‹¤. TorchScriptë¡œ ë§Œë“¤ì–´ì§„ í”„ë¡œê·¸ë¨ì€ ê¸°ì¡´ Python í”„ë¡œì„¸ìŠ¤ì—ì„œ ì €ì¥í•œ ë’¤, ì¢…ì†ì„±ì´ ì—†ëŠ” ìƒˆë¡œìš´ í”„ë¡œì„¸ìŠ¤ë¡œ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. PyTorchì˜ ê¸°ë³¸ ì„¤ì •ì¸ `eager` ëª¨ë“œì™€ ë¹„êµí–ˆì„ë•Œ, `jit` ëª¨ë“œëŠ” ì—°ì‚°ì ê²°í•©ê³¼ ê°™ì€ ìµœì í™” ë°©ë²•ë¡ ì„ í†µí•´ ëª¨ë¸ ì¶”ë¡ ì—ì„œ ëŒ€ë¶€ë¶„ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.\n-\n-TorchScriptì— ëŒ€í•œ ì¹œì ˆí•œ ì†Œê°œëŠ” [PyTorch TorchScript íŠœí† ë¦¬ì–¼](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html#tracing-modules)ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n ### JIT ëª¨ë“œì™€ í•¨ê»˜í•˜ëŠ” IPEX ê·¸ë˜í”„ ìµœì í™” [[ipex-graph-optimization-with-jitmode]]\n IntelÂ® Extension for PyTorch(IPEX)ëŠ” Transformers ê³„ì—´ ëª¨ë¸ì˜ jit ëª¨ë“œì—ì„œ ì¶”ê°€ì ì¸ ìµœì í™”ë¥¼ ì œê³µí•©ë‹ˆë‹¤. jit ëª¨ë“œì™€ ë”ë¶ˆì–´ IntelÂ® Extension for PyTorch(IPEX)ë¥¼ í™œìš©í•˜ì‹œê¸¸ ê°•ë ¥íˆ ê¶Œì¥ë“œë¦½ë‹ˆë‹¤. Transformers ëª¨ë¸ì—ì„œ ìì£¼ ì‚¬ìš©ë˜ëŠ” ì¼ë¶€ ì—°ì‚°ì íŒ¨í„´ì€ ì´ë¯¸ jit ëª¨ë“œ ì—°ì‚°ì ê²°í•©(operator fusion)ì˜ í˜•íƒœë¡œ IntelÂ® Extension for PyTorch(IPEX)ì—ì„œ ì§€ì›ë˜ê³  ìˆìŠµë‹ˆë‹¤. Multi-head-attention, Concat Linear, Linear+Add, Linear+Gelu, Add+LayerNorm ê²°í•© íŒ¨í„´ ë“±ì´ ì´ìš© ê°€ëŠ¥í•˜ë©° í™œìš©í–ˆì„ ë•Œ ì„±ëŠ¥ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤. ì—°ì‚°ì ê²°í•©ì˜ ì´ì ì€ ì‚¬ìš©ìì—ê²Œ ê³ ìŠ¤ë€íˆ ì „ë‹¬ë©ë‹ˆë‹¤. ë¶„ì„ì— ë”°ë¥´ë©´, ì§ˆì˜ ì‘ë‹µ, í…ìŠ¤íŠ¸ ë¶„ë¥˜ ë° í† í° ë¶„ë¥˜ì™€ ê°™ì€ ê°€ì¥ ì¸ê¸° ìˆëŠ” NLP íƒœìŠ¤í¬ ì¤‘ ì•½ 70%ê°€ ì´ëŸ¬í•œ ê²°í•© íŒ¨í„´ì„ ì‚¬ìš©í•˜ì—¬ Float32 ì •ë°€ë„ì™€ BFloat16 í˜¼í•© ì •ë°€ë„ ëª¨ë‘ì—ì„œ ì„±ëŠ¥ìƒì˜ ì´ì ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n "
        },
        {
            "sha": "28e198c5ec9306cf7f91fe2cf3a1b60485b47a26",
            "filename": "docs/source/ko/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 189,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fko%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fko%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,189 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# TorchScriptë¡œ ë‚´ë³´ë‚´ê¸°[[export-to-torchscript]]\n-\n-<Tip>\n-\n-TorchScriptë¥¼ í™œìš©í•œ ì‹¤í—˜ì€ ì•„ì§ ì´ˆê¸° ë‹¨ê³„ë¡œ, ê°€ë³€ì ì¸ ì…ë ¥ í¬ê¸° ëª¨ë¸ë“¤ì„ í†µí•´ ê·¸ ê¸°ëŠ¥ì„±ì„ ê³„ì† íƒêµ¬í•˜ê³  ìˆìŠµë‹ˆë‹¤. \n-ì´ ê¸°ëŠ¥ì€ ì €í¬ê°€ ê´€ì‹¬ì„ ë‘ê³  ìˆëŠ” ë¶„ì•¼ ì¤‘ í•˜ë‚˜ì´ë©°, \n-ì•ìœ¼ë¡œ ì¶œì‹œë  ë²„ì „ì—ì„œ ë” ë§ì€ ì½”ë“œ ì˜ˆì œ, ë” ìœ ì—°í•œ êµ¬í˜„, ê·¸ë¦¬ê³  Python ê¸°ë°˜ ì½”ë“œì™€ ì»´íŒŒì¼ëœ TorchScriptë¥¼ ë¹„êµí•˜ëŠ” ë²¤ì¹˜ë§ˆí¬ë¥¼ ë“±ì„ í†µí•´ ë¶„ì„ì„ ì‹¬í™”í•  ì˜ˆì •ì…ë‹ˆë‹¤.\n-\n-</Tip>\n-\n-[TorchScript ë¬¸ì„œ](https://pytorch.org/docs/stable/jit.html)ì—ì„œëŠ” ì´ë ‡ê²Œ ë§í•©ë‹ˆë‹¤.\n-\n-> TorchScriptëŠ” PyTorch ì½”ë“œì—ì„œ ì§ë ¬í™” ë° ìµœì í™” ê°€ëŠ¥í•œ ëª¨ë¸ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n-\n-[JITê³¼ TRACE](https://pytorch.org/docs/stable/jit.html)ëŠ” ê°œë°œìê°€ ëª¨ë¸ì„ ë‚´ë³´ë‚´ì„œ íš¨ìœ¨ ì§€í–¥ì ì¸ C++ í”„ë¡œê·¸ë¨ê³¼ ê°™ì€ ë‹¤ë¥¸ í”„ë¡œê·¸ë¨ì—ì„œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” PyTorch ëª¨ë“ˆì…ë‹ˆë‹¤.\n-\n-PyTorch ê¸°ë°˜ Python í”„ë¡œê·¸ë¨ê³¼ ë‹¤ë¥¸ í™˜ê²½ì—ì„œ ëª¨ë¸ì„ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡, ğŸ¤— Transformers ëª¨ë¸ì„ TorchScriptë¡œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆëŠ” ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤. \n-ì´ ë¬¸ì„œì—ì„œëŠ” TorchScriptë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë‚´ë³´ë‚´ê³  ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n-\n-ëª¨ë¸ì„ ë‚´ë³´ë‚´ë ¤ë©´ ë‘ ê°€ì§€ê°€ í•„ìš”í•©ë‹ˆë‹¤:\n-\n-- `torchscript` í”Œë˜ê·¸ë¡œ ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤í™”\n-- ë”ë¯¸ ì…ë ¥ì„ ì‚¬ìš©í•œ ìˆœì „íŒŒ(forward pass)\n-\n-ì´ í•„ìˆ˜ ì¡°ê±´ë“¤ì€ ì•„ë˜ì— ìì„¸íˆ ì„¤ëª…ëœ ê²ƒì²˜ëŸ¼ ê°œë°œìë“¤ì´ ì£¼ì˜í•´ì•¼ í•  ì—¬ëŸ¬ ì‚¬í•­ë“¤ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n-\n-## TorchScript í”Œë˜ê·¸ì™€ ë¬¶ì¸ ê°€ì¤‘ì¹˜(tied weights)[[torchscript-flag-and-tied-weights]]\n-\n-`torchscript` í”Œë˜ê·¸ê°€ í•„ìš”í•œ ì´ìœ ëŠ” ëŒ€ë¶€ë¶„ì˜ ğŸ¤— Transformers ì–¸ì–´ ëª¨ë¸ì—ì„œ `Embedding` ë ˆì´ì–´ì™€ `Decoding` ë ˆì´ì–´ ê°„ì˜ ë¬¶ì¸ ê°€ì¤‘ì¹˜(tied weights)ê°€ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n-TorchScriptëŠ” ë¬¶ì¸ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§„ ëª¨ë¸ì„ ë‚´ë³´ë‚¼ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ, ë¯¸ë¦¬ ê°€ì¤‘ì¹˜ë¥¼ í’€ê³  ë³µì œí•´ì•¼ í•©ë‹ˆë‹¤.\n-\n-`torchscript` í”Œë˜ê·¸ë¡œ ì¸ìŠ¤í„´ìŠ¤í™”ëœ ëª¨ë¸ì€ `Embedding` ë ˆì´ì–´ì™€ `Decoding` ë ˆì´ì–´ê°€ ë¶„ë¦¬ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´í›„ì— í›ˆë ¨í•´ì„œëŠ” ì•ˆ ë©ë‹ˆë‹¤.\n-í›ˆë ¨ì„ í•˜ê²Œ ë˜ë©´ ë‘ ë ˆì´ì–´ ê°„ ë™ê¸°í™”ê°€ í•´ì œë˜ì–´ ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-ì–¸ì–´ ëª¨ë¸ í—¤ë“œë¥¼ ê°–ì§€ ì•Šì€ ëª¨ë¸ì€ ê°€ì¤‘ì¹˜ê°€ ë¬¶ì—¬ ìˆì§€ ì•Šì•„ì„œ ì´ ë¬¸ì œê°€ ë°œìƒí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n-ì´ëŸ¬í•œ ëª¨ë¸ë“¤ì€ `torchscript` í”Œë˜ê·¸ ì—†ì´ ì•ˆì „í•˜ê²Œ ë‚´ë³´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-## ë”ë¯¸ ì…ë ¥ê³¼ í‘œì¤€ ê¸¸ì´[[dummy-inputs-and-standard-lengths]]\n-\n-ë”ë¯¸ ì…ë ¥(dummy inputs)ì€ ëª¨ë¸ì˜ ìˆœì „íŒŒ(forward pass)ì— ì‚¬ìš©ë©ë‹ˆë‹¤. \n-ì…ë ¥ ê°’ì´ ë ˆì´ì–´ë¥¼ í†µí•´ ì „íŒŒë˜ëŠ” ë™ì•ˆ, PyTorchëŠ” ê° í…ì„œì—ì„œ ì‹¤í–‰ëœ ë‹¤ë¥¸ ì—°ì‚°ì„ ì¶”ì í•©ë‹ˆë‹¤. \n-ì´ëŸ¬í•œ ê¸°ë¡ëœ ì—°ì‚°ì€ ëª¨ë¸ì˜ *ì¶”ì (trace)*ì„ ìƒì„±í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n-\n-ì¶”ì ì€ ì…ë ¥ì˜ ì°¨ì›ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒì„±ë©ë‹ˆë‹¤. \n-ë”°ë¼ì„œ ë”ë¯¸ ì…ë ¥ì˜ ì°¨ì›ì— ì œí•œë˜ì–´, ë‹¤ë¥¸ ì‹œí€€ìŠ¤ ê¸¸ì´ë‚˜ ë°°ì¹˜ í¬ê¸°ì—ì„œëŠ” ì‘ë™í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. \n-ë‹¤ë¥¸ í¬ê¸°ë¡œ ì‹œë„í•  ê²½ìš° ë‹¤ìŒê³¼ ê°™ì€ ì˜¤ë¥˜ê°€ ë°œìƒí•©ë‹ˆë‹¤:\n-\n-```\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n-```\n-ì¶”ë¡  ì¤‘ ëª¨ë¸ì— ê³µê¸‰ë  ê°€ì¥ í° ì…ë ¥ë§Œí¼ í° ë”ë¯¸ ì…ë ¥ í¬ê¸°ë¡œ ëª¨ë¸ì„ ì¶”ì í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. \n-íŒ¨ë”©ì€ ëˆ„ë½ëœ ê°’ì„ ì±„ìš°ëŠ” ë° ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-ê·¸ëŸ¬ë‚˜ ëª¨ë¸ì´ ë” í° ì…ë ¥ í¬ê¸°ë¡œ ì¶”ì ë˜ê¸° ë•Œë¬¸ì—, í–‰ë ¬ì˜ ì°¨ì›ì´ ì»¤ì§€ê³  ê³„ì‚°ëŸ‰ì´ ë§ì•„ì§‘ë‹ˆë‹¤.\n-\n-ë‹¤ì–‘í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ ëª¨ë¸ì„ ë‚´ë³´ë‚¼ ë•ŒëŠ” ê° ì…ë ¥ì— ëŒ€í•´ ìˆ˜í–‰ë˜ëŠ” ì´ ì—°ì‚° íšŸìˆ˜ì— ì£¼ì˜í•˜ê³  ì„±ëŠ¥ì„ ì£¼ì˜ ê¹Šê²Œ í™•ì¸í•˜ì„¸ìš”.\n-\n-## Pythonì—ì„œ TorchScript ì‚¬ìš©í•˜ê¸°[[using-torchscript-in-python]]\n-\n-ì´ ì„¹ì…˜ì—ì„œëŠ” ëª¨ë¸ì„ ì €ì¥í•˜ê³  ê°€ì ¸ì˜¤ëŠ” ë°©ë²•, ì¶”ì ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n-\n-### ëª¨ë¸ ì €ì¥í•˜ê¸°[[saving-a-model]]\n-\n-`BertModel`ì„ TorchScriptë¡œ ë‚´ë³´ë‚´ë ¤ë©´ `BertConfig` í´ë˜ìŠ¤ì—ì„œ `BertModel`ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•œ ë‹¤ìŒ, `traced_bert.pt`ë¼ëŠ” íŒŒì¼ëª…ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥í•˜ë©´ ë©ë‹ˆë‹¤.\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# ì…ë ¥ í…ìŠ¤íŠ¸ í† í°í™”í•˜ê¸°\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# ì…ë ¥ í† í° ì¤‘ í•˜ë‚˜ë¥¼ ë§ˆìŠ¤í‚¹í•˜ê¸°\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# ë”ë¯¸ ì…ë ¥ ë§Œë“¤ê¸°\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# torchscript í”Œë˜ê·¸ë¡œ ëª¨ë¸ ì´ˆê¸°í™”í•˜ê¸°\n-# ì´ ëª¨ë¸ì€ LM í—¤ë“œê°€ ì—†ìœ¼ë¯€ë¡œ í•„ìš”í•˜ì§€ ì•Šì§€ë§Œ, í”Œë˜ê·¸ë¥¼ Trueë¡œ ì„¤ì •í•©ë‹ˆë‹¤.\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# ëª¨ë¸ì„ ì¸ìŠ¤í„´íŠ¸í™”í•˜ê¸°\n-model = BertModel(config)\n-\n-# ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ë‘ì–´ì•¼ í•©ë‹ˆë‹¤.\n-model.eval()\n-\n-# ë§Œì•½ *from_pretrained*ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ëŠ” ê²½ìš°, TorchScript í”Œë˜ê·¸ë¥¼ ì‰½ê²Œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# ì¶”ì  ìƒì„±í•˜ê¸°\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-### ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°[[loading-a-model]]\n-\n-ì´ì œ ì´ì „ì— ì €ì¥í•œ `BertModel`, ì¦‰ `traced_bert.pt`ë¥¼ ë””ìŠ¤í¬ì—ì„œ ê°€ì ¸ì˜¤ê³ , ì´ì „ì— ì´ˆê¸°í™”í•œ `dummy_input`ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-### ì¶”ì ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ê¸°[[using-a-traced-model-for-inference]]\n-\n-`__call__` ì´ì¤‘ ì–¸ë”ìŠ¤ì½”ì–´(dunder) ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ ì— ì¶”ì ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì„¸ìš”:\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-## Neuron SDKë¡œ Hugging Face TorchScript ëª¨ë¸ì„ AWSì— ë°°í¬í•˜ê¸°[[deploy-hugging-face-torchscript-models-to-aws-with-the-neuron-sdk]]\n-\n-AWSê°€ í´ë¼ìš°ë“œì—ì„œ ì €ë¹„ìš©, ê³ ì„±ëŠ¥ ë¨¸ì‹  ëŸ¬ë‹ ì¶”ë¡ ì„ ìœ„í•œ [Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) ì¸ìŠ¤í„´ìŠ¤ ì œí’ˆêµ°ì„ ì¶œì‹œí–ˆìŠµë‹ˆë‹¤. \n-Inf1 ì¸ìŠ¤í„´ìŠ¤ëŠ” ë”¥ëŸ¬ë‹ ì¶”ë¡  ì›Œí¬ë¡œë“œì— íŠ¹í™”ëœ ë§ì¶¤ í•˜ë“œì›¨ì–´ ê°€ì†ê¸°ì¸ AWS Inferentia ì¹©ìœ¼ë¡œ êµ¬ë™ë©ë‹ˆë‹¤. \n-[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#)ì€ Inferentiaë¥¼ ìœ„í•œ SDKë¡œ, Inf1ì— ë°°í¬í•˜ê¸° ìœ„í•œ transformers ëª¨ë¸ ì¶”ì  ë° ìµœì í™”ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. \n-Neuron SDKëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤:\n-\n-1. ì½”ë“œ í•œ ì¤„ë§Œ ë³€ê²½í•˜ë©´ í´ë¼ìš°ë“œ ì¶”ë¡ ë¥¼ ìœ„í•´ TorchScript ëª¨ë¸ì„ ì¶”ì í•˜ê³  ìµœì í™”í•  ìˆ˜ ìˆëŠ” ì‰¬ìš´ API\n-2. ì¦‰ì‹œ ì‚¬ìš© ê°€ëŠ¥í•œ ì„±ëŠ¥ ìµœì í™”ë¡œ [ë¹„ìš© íš¨ìœ¨ í–¥ìƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/>)\n-3. [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html) ë˜ëŠ” [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)ë¡œ êµ¬ì¶•ëœ Hugging Face transformers ëª¨ë¸ ì§€ì›\n-\n-### ì‹œì‚¬ì [[implications]]\n-\n-[BERT (Bidirectional Encoder Representations from Transformers)](https://huggingface.co/docs/transformers/main/model_doc/bert) ì•„í‚¤í…ì²˜ ë˜ëŠ” ê·¸ ë³€í˜•ì¸ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert) ë° [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta)ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ Transformers ëª¨ë¸ì€ ì¶”ì¶œ ê¸°ë°˜ ì§ˆì˜ì‘ë‹µ, ì‹œí€€ìŠ¤ ë¶„ë¥˜ ë° í† í° ë¶„ë¥˜ì™€ ê°™ì€ ë¹„ìƒì„± ì‘ì—… ì‹œ Inf1ì—ì„œ ìµœìƒì˜ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤. \n-ê·¸ëŸ¬ë‚˜ í…ìŠ¤íŠ¸ ìƒì„± ì‘ì—…ë„ [AWS Neuron MarianMT íŠœí† ë¦¬ì–¼](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)ì„ ë”°ë¼ Inf1ì—ì„œ ì‹¤í–‰ë˜ë„ë¡ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-Inferentiaì—ì„œ ë°”ë¡œ ë³€í™˜í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì— ëŒ€í•œ ìì„¸í•œ ì •ë³´ëŠ” Neuron ë¬¸ì„œì˜ [Model Architecture Fit](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia) ì„¹ì…˜ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-### ì¢…ì†ì„±[[dependencies]]\n-\n-AWS Neuronì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë³€í™˜í•˜ë ¤ë©´ [Neuron SDK í™˜ê²½](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ì´ í•„ìš”í•©ë‹ˆë‹¤.\n- ì´ëŠ” [AWS Deep Learning AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ì— ë¯¸ë¦¬ êµ¬ì„±ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n-\n-### AWS Neuronìœ¼ë¡œ ëª¨ë¸ ë³€í™˜í•˜ê¸°[[converting-a-model-for-aws-neuron]]\n-\n-`BertModel`ì„ ì¶”ì í•˜ë ¤ë©´, [Pythonì—ì„œ TorchScript ì‚¬ìš©í•˜ê¸°](torchscript#using-torchscript-in-python)ì—ì„œì™€ ë™ì¼í•œ ì½”ë“œë¥¼ ì‚¬ìš©í•´ì„œ AWS NEURONìš© ëª¨ë¸ì„ ë³€í™˜í•©ë‹ˆë‹¤. \n-`torch.neuron` í”„ë ˆì„ì›Œí¬ ìµìŠ¤í…ì…˜ì„ ê°€ì ¸ì™€ Python APIë¥¼ í†µí•´ Neuron SDKì˜ êµ¬ì„± ìš”ì†Œì— ì ‘ê·¼í•©ë‹ˆë‹¤:\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-\n-ë‹¤ìŒ ì¤„ë§Œ ìˆ˜ì •í•˜ë©´ ë©ë‹ˆë‹¤:\n-\n-```diff\n-- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-ì´ë¡œì¨ Neuron SDKê°€ ëª¨ë¸ì„ ì¶”ì í•˜ê³  Inf1 ì¸ìŠ¤í„´ìŠ¤ì— ìµœì í™”í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n-\n-AWS Neuron SDKì˜ ê¸°ëŠ¥, ë„êµ¬, ì˜ˆì œ íŠœí† ë¦¬ì–¼ ë° ìµœì‹  ì—…ë°ì´íŠ¸ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [AWS NeuronSDK ë¬¸ì„œ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”."
        },
        {
            "sha": "9e81eb8fd3374e57ee94e3cc5eddd9904cc24a53",
            "filename": "docs/source/zh/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fzh%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/docs%2Fsource%2Fzh%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2F_toctree.yml?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -44,8 +44,6 @@\n     title: èŠå¤©æ¨¡å‹çš„æ¨¡æ¿\n   - local: serialization\n     title: å¯¼å‡ºä¸º ONNX\n-  - local: torchscript\n-    title: å¯¼å‡ºä¸º TorchScript\n   - local: gguf\n     title: ä¸ GGUF æ ¼å¼çš„äº’æ“ä½œæ€§\n   - local: tiktoken"
        },
        {
            "sha": "d3106c5241808f5c090b38b8b8fe106fc644608f",
            "filename": "docs/source/zh/torchscript.md",
            "status": "removed",
            "additions": 0,
            "deletions": 197,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fzh%2Ftorchscript.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7370a1babde67227464d7d7fb15fb19f29c6ea86/docs%2Fsource%2Fzh%2Ftorchscript.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftorchscript.md?ref=7370a1babde67227464d7d7fb15fb19f29c6ea86",
            "patch": "@@ -1,197 +0,0 @@\n-<!--\n-Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n--->\n-\n-# å¯¼å‡ºä¸º TorchScript\n-\n-<Tip>\n-\n-è¿™æ˜¯å¼€å§‹ä½¿ç”¨ TorchScript è¿›è¡Œå®éªŒçš„èµ·ç‚¹ï¼Œæˆ‘ä»¬ä»åœ¨æ¢ç´¢å…¶åœ¨å˜é‡è¾“å…¥å¤§å°æ¨¡å‹ä¸­çš„èƒ½åŠ›ã€‚\n-è¿™æ˜¯æˆ‘ä»¬å…³æ³¨çš„ç„¦ç‚¹ï¼Œæˆ‘ä»¬å°†åœ¨å³å°†å‘å¸ƒçš„ç‰ˆæœ¬ä¸­æ·±å…¥åˆ†æï¼Œæä¾›æ›´å¤šçš„ä»£ç ç¤ºä¾‹ã€æ›´çµæ´»çš„å®ç°ä»¥åŠæ¯”è¾ƒ\n-Python ä»£ç ä¸ç¼–è¯‘ TorchScript çš„æ€§èƒ½åŸºå‡†ã€‚\n-\n-</Tip>\n-\n-æ ¹æ® [TorchScript æ–‡æ¡£](https://pytorch.org/docs/stable/jit.html)ï¼š\n-\n-> TorchScript æ˜¯ä» PyTorch ä»£ç åˆ›å»ºå¯åºåˆ—åŒ–å’Œå¯ä¼˜åŒ–çš„æ¨¡å‹çš„ä¸€ç§æ–¹å¼ã€‚\n-\n-æœ‰ä¸¤ä¸ª PyTorch æ¨¡å—ï¼š[JIT å’Œ TRACE](https://pytorch.org/docs/stable/jit.html)ã€‚\n-è¿™ä¸¤ä¸ªæ¨¡å—å…è®¸å¼€å‘äººå‘˜å°†å…¶æ¨¡å‹å¯¼å‡ºåˆ°å…¶ä»–ç¨‹åºä¸­é‡ç”¨ï¼Œæ¯”å¦‚é¢å‘æ•ˆç‡çš„ C++ ç¨‹åºã€‚\n-\n-æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªæ¥å£ï¼Œå…è®¸æ‚¨å°† ğŸ¤— Transformers æ¨¡å‹å¯¼å‡ºä¸º TorchScriptï¼Œ\n-ä»¥ä¾¿åœ¨ä¸åŸºäº PyTorch çš„ Python ç¨‹åºä¸åŒçš„ç¯å¢ƒä¸­é‡ç”¨ã€‚\n-æœ¬æ–‡è§£é‡Šå¦‚ä½•ä½¿ç”¨ TorchScript å¯¼å‡ºå¹¶ä½¿ç”¨æˆ‘ä»¬çš„æ¨¡å‹ã€‚\n-\n-å¯¼å‡ºæ¨¡å‹éœ€è¦ä¸¤ä¸ªæ­¥éª¤ï¼š\n-\n-- ä½¿ç”¨ `torchscript` å‚æ•°å®ä¾‹åŒ–æ¨¡å‹\n-- ä½¿ç”¨è™šæ‹Ÿè¾“å…¥è¿›è¡Œå‰å‘ä¼ é€’\n-\n-è¿™äº›å¿…è¦æ¡ä»¶æ„å‘³ç€å¼€å‘äººå‘˜åº”è¯¥æ³¨æ„ä»¥ä¸‹è¯¦ç»†ä¿¡æ¯ã€‚\n-\n-## TorchScript å‚æ•°å’Œç»‘å®šæƒé‡\n-\n-`torchscript` å‚æ•°æ˜¯å¿…éœ€çš„ï¼Œå› ä¸ºå¤§å¤šæ•° ğŸ¤— Transformers è¯­è¨€æ¨¡å‹çš„ `Embedding` å±‚å’Œ\n-`Decoding` å±‚ä¹‹é—´æœ‰ç»‘å®šæƒé‡ã€‚TorchScript ä¸å…è®¸å¯¼å‡ºå…·æœ‰ç»‘å®šæƒé‡çš„æ¨¡å‹ï¼Œå› æ­¤å¿…é¡»äº‹å…ˆè§£ç»‘å’Œå…‹éš†æƒé‡ã€‚\n-\n-ä½¿ç”¨ `torchscript` å‚æ•°å®ä¾‹åŒ–çš„æ¨¡å‹å°†å…¶ `Embedding` å±‚å’Œ `Decoding` å±‚åˆ†å¼€ï¼Œ\n-è¿™æ„å‘³ç€å®ƒä»¬ä¸åº”è¯¥åœ¨åç»­è¿›è¡Œè®­ç»ƒã€‚è®­ç»ƒå°†å¯¼è‡´è¿™ä¸¤å±‚ä¸åŒæ­¥ï¼Œäº§ç”Ÿæ„å¤–ç»“æœã€‚\n-\n-å¯¹äºæ²¡æœ‰è¯­è¨€æ¨¡å‹å¤´éƒ¨çš„æ¨¡å‹ï¼Œæƒ…å†µä¸åŒï¼Œå› ä¸ºè¿™äº›æ¨¡å‹æ²¡æœ‰ç»‘å®šæƒé‡ã€‚\n-è¿™äº›æ¨¡å‹å¯ä»¥å®‰å…¨åœ°å¯¼å‡ºè€Œæ— éœ€ `torchscript` å‚æ•°ã€‚\n-\n-## è™šæ‹Ÿè¾“å…¥å’Œæ ‡å‡†é•¿åº¦\n-\n-è™šæ‹Ÿè¾“å…¥ç”¨äºæ¨¡å‹çš„å‰å‘ä¼ é€’ã€‚å½“è¾“å…¥çš„å€¼ä¼ æ’­åˆ°å„å±‚æ—¶ï¼ŒPyTorch ä¼šè·Ÿè¸ªåœ¨æ¯ä¸ªå¼ é‡ä¸Šæ‰§è¡Œçš„ä¸åŒæ“ä½œã€‚\n-ç„¶åä½¿ç”¨è®°å½•çš„æ“ä½œæ¥åˆ›å»ºæ¨¡å‹çš„ *trace* ã€‚\n-\n-è·Ÿè¸ªæ˜¯ç›¸å¯¹äºè¾“å…¥çš„ç»´åº¦åˆ›å»ºçš„ã€‚å› æ­¤ï¼Œå®ƒå—åˆ°è™šæ‹Ÿè¾“å…¥çš„ç»´åº¦é™åˆ¶ï¼Œå¯¹äºä»»ä½•å…¶ä»–åºåˆ—é•¿åº¦æˆ–æ‰¹é‡å¤§å°éƒ½ä¸èµ·ä½œç”¨ã€‚\n-å½“å°è¯•ä½¿ç”¨ä¸åŒå¤§å°æ—¶ï¼Œä¼šå¼•å‘ä»¥ä¸‹é”™è¯¯ï¼š\n-\n-```text\n-`The expanded size of the tensor (3) must match the existing size (7) at non-singleton dimension 2`\n-```\n-\n-æˆ‘ä»¬å»ºè®®ä½¿ç”¨è‡³å°‘ä¸æ¨æ–­æœŸé—´å°†é¦ˆé€åˆ°æ¨¡å‹çš„æœ€å¤§è¾“å…¥ä¸€æ ·å¤§çš„è™šæ‹Ÿè¾“å…¥å¤§å°è¿›è¡Œè·Ÿè¸ªã€‚\n-å¡«å……å¯ä»¥å¸®åŠ©å¡«è¡¥ç¼ºå¤±çš„å€¼ã€‚ç„¶è€Œï¼Œç”±äºæ¨¡å‹æ˜¯ä½¿ç”¨æ›´å¤§çš„è¾“å…¥å¤§å°è¿›è¡Œè·Ÿè¸ªçš„ï¼ŒçŸ©é˜µçš„ç»´åº¦ä¹Ÿä¼šå¾ˆå¤§ï¼Œå¯¼è‡´æ›´å¤šçš„è®¡ç®—ã€‚\n-\n-åœ¨æ¯ä¸ªè¾“å…¥ä¸Šæ‰§è¡Œçš„æ“ä½œæ€»æ•°è¦ä»”ç»†è€ƒè™‘ï¼Œå¹¶åœ¨å¯¼å‡ºä¸åŒåºåˆ—é•¿åº¦æ¨¡å‹æ—¶å¯†åˆ‡å…³æ³¨æ€§èƒ½ã€‚\n-\n-## åœ¨ Python ä¸­ä½¿ç”¨ TorchScript\n-\n-æœ¬èŠ‚æ¼”ç¤ºäº†å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨ trace è¿›è¡Œæ¨æ–­ã€‚\n-\n-### ä¿å­˜æ¨¡å‹\n-\n-è¦ä½¿ç”¨ TorchScript å¯¼å‡º `BertModel`ï¼Œè¯·ä» `BertConfig` ç±»å®ä¾‹åŒ– `BertModel`ï¼Œ\n-ç„¶åå°†å…¶ä¿å­˜åˆ°åä¸º `traced_bert.pt` çš„ç£ç›˜æ–‡ä»¶ä¸­ï¼š\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-\n-enc = BertTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-# å¯¹è¾“å…¥æ–‡æœ¬åˆ†è¯\n-text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n-tokenized_text = enc.tokenize(text)\n-\n-# å±è”½ä¸€ä¸ªè¾“å…¥ token\n-masked_index = 8\n-tokenized_text[masked_index] = \"[MASK]\"\n-indexed_tokens = enc.convert_tokens_to_ids(tokenized_text)\n-segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n-\n-# åˆ›å»ºè™šæ‹Ÿè¾“å…¥\n-tokens_tensor = torch.tensor([indexed_tokens])\n-segments_tensors = torch.tensor([segments_ids])\n-dummy_input = [tokens_tensor, segments_tensors]\n-\n-# ä½¿ç”¨ torchscript å‚æ•°åˆå§‹åŒ–æ¨¡å‹\n-# å³ä½¿æ­¤æ¨¡å‹æ²¡æœ‰ LM Headï¼Œä¹Ÿå°†å‚æ•°è®¾ç½®ä¸º Trueã€‚\n-config = BertConfig(\n-    vocab_size_or_config_json_file=32000,\n-    hidden_size=768,\n-    num_hidden_layers=12,\n-    num_attention_heads=12,\n-    intermediate_size=3072,\n-    torchscript=True,\n-)\n-\n-# å®ä¾‹åŒ–æ¨¡å‹\n-model = BertModel(config)\n-\n-# æ¨¡å‹éœ€è¦å¤„äºè¯„ä¼°æ¨¡å¼\n-model.eval()\n-\n-# å¦‚æœæ‚¨ä½¿ç”¨ *from_pretrained* å®ä¾‹åŒ–æ¨¡å‹ï¼Œè¿˜å¯ä»¥è½»æ¾è®¾ç½® TorchScript å‚æ•°\n-model = BertModel.from_pretrained(\"google-bert/bert-base-uncased\", torchscript=True)\n-\n-# åˆ›å»º trace\n-traced_model = torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-torch.jit.save(traced_model, \"traced_bert.pt\")\n-```\n-\n-### åŠ è½½æ¨¡å‹\n-\n-ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä»ç£ç›˜åŠ è½½å…ˆå‰ä¿å­˜çš„ `BertModel`ã€`traced_bert.pt`ï¼Œå¹¶åœ¨å…ˆå‰åˆå§‹åŒ–çš„ `dummy_input` ä¸Šä½¿ç”¨ï¼š\n-\n-```python\n-loaded_model = torch.jit.load(\"traced_bert.pt\")\n-loaded_model.eval()\n-\n-all_encoder_layers, pooled_output = loaded_model(*dummy_input)\n-```\n-\n-### ä½¿ç”¨ trace æ¨¡å‹è¿›è¡Œæ¨æ–­\n-\n-é€šè¿‡ä½¿ç”¨å…¶ `__call__` dunder æ–¹æ³•ä½¿ç”¨ trace æ¨¡å‹è¿›è¡Œæ¨æ–­ï¼š\n-\n-```python\n-traced_model(tokens_tensor, segments_tensors)\n-```\n-\n-## ä½¿ç”¨ Neuron SDK å°† Hugging Face TorchScript æ¨¡å‹éƒ¨ç½²åˆ° AWS\n-\n-AWS å¼•å…¥äº†ç”¨äºäº‘ç«¯ä½æˆæœ¬ã€é«˜æ€§èƒ½æœºå™¨å­¦ä¹ æ¨ç†çš„\n-[Amazon EC2 Inf1](https://aws.amazon.com/ec2/instance-types/inf1/) å®ä¾‹ç³»åˆ—ã€‚\n-Inf1 å®ä¾‹ç”± AWS Inferentia èŠ¯ç‰‡æä¾›æ”¯æŒï¼Œè¿™æ˜¯ä¸€æ¬¾ä¸“ä¸ºæ·±åº¦å­¦ä¹ æ¨ç†å·¥ä½œè´Ÿè½½è€Œæ„å»ºçš„å®šåˆ¶ç¡¬ä»¶åŠ é€Ÿå™¨ã€‚\n-[AWS Neuron](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/#) æ˜¯\n-Inferentia çš„ SDKï¼Œæ”¯æŒå¯¹ transformers æ¨¡å‹è¿›è¡Œè·Ÿè¸ªå’Œä¼˜åŒ–ï¼Œä»¥ä¾¿åœ¨ Inf1 ä¸Šéƒ¨ç½²ã€‚Neuron SDK æä¾›ï¼š\n-\n-1. ç®€å•æ˜“ç”¨çš„ APIï¼Œåªéœ€æ›´æ”¹ä¸€è¡Œä»£ç å³å¯ä¸ºäº‘ç«¯æ¨ç†è·Ÿè¸ªå’Œä¼˜åŒ– TorchScript æ¨¡å‹ã€‚\n-2. é’ˆå¯¹[æ”¹è¿›çš„æ€§èƒ½æˆæœ¬](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/benchmark/)çš„å³æ’å³ç”¨æ€§èƒ½ä¼˜åŒ–ã€‚\n-3. æ”¯æŒä½¿ç”¨ [PyTorch](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/bert_tutorial/tutorial_pretrained_bert.html)\n-   æˆ– [TensorFlow](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/tensorflow/huggingface_bert/huggingface_bert.html)\n-   æ„å»ºçš„ Hugging Face transformers æ¨¡å‹ã€‚\n-\n-### å½±å“\n-\n-åŸºäº [BERTï¼ˆæ¥è‡ª Transformers çš„åŒå‘ç¼–ç å™¨è¡¨ç¤ºï¼‰](https://huggingface.co/docs/transformers/main/model_doc/bert)æ¶æ„çš„\n-transformers æ¨¡å‹ï¼Œæˆ–å…¶å˜ä½“ï¼Œå¦‚ [distilBERT](https://huggingface.co/docs/transformers/main/model_doc/distilbert)\n-å’Œ [roBERTa](https://huggingface.co/docs/transformers/main/model_doc/roberta) åœ¨ Inf1 ä¸Šè¿è¡Œæœ€ä½³ï¼Œ\n-å¯ç”¨äºç”ŸæˆæŠ½å–å¼é—®ç­”ã€åºåˆ—åˆ†ç±»å’Œæ ‡è®°åˆ†ç±»ç­‰ä»»åŠ¡ã€‚ç„¶è€Œï¼Œæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä»å¯ä»¥é€‚åº”åœ¨ Inf1 ä¸Šè¿è¡Œï¼Œ\n-å¦‚è¿™ç¯‡ [AWS Neuron MarianMT æ•™ç¨‹](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/src/examples/pytorch/transformers-marianmt.html)æ‰€è¿°ã€‚\n-æœ‰å…³å¯ä»¥ç›´æ¥åœ¨ Inferentia ä¸Šè½¬æ¢çš„æ¨¡å‹çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜… Neuron æ–‡æ¡£çš„[æ¨¡å‹æ¶æ„é€‚é…](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/models/models-inferentia.html#models-inferentia)ç« èŠ‚ã€‚\n-\n-### ä¾èµ–å…³ç³»\n-\n-ä½¿ç”¨ AWS Neuron å°†æ¨¡å‹è½¬æ¢ä¸ºæ¨¡å‹éœ€è¦ä¸€ä¸ª\n-[Neuron SDK ç¯å¢ƒ](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/neuron-guide/neuron-frameworks/pytorch-neuron/index.html#installation-guide)ï¼Œ\n-å®ƒå·²ç»é¢„å…ˆé…ç½®åœ¨ [AWS æ·±åº¦å­¦ä¹  AMI](https://docs.aws.amazon.com/dlami/latest/devguide/tutorial-inferentia-launching.html)ä¸Šã€‚\n-\n-### å°†æ¨¡å‹è½¬æ¢ä¸º AWS Neuron\n-\n-ä½¿ç”¨ä¸ [Python ä¸­ä½¿ç”¨ TorchScript](torchscript#using-torchscript-in-python) ç›¸åŒçš„ä»£ç æ¥è·Ÿè¸ª\n-`BertModel` ä»¥å°†æ¨¡å‹è½¬æ¢ä¸º AWS NEURONã€‚å¯¼å…¥ `torch.neuron` æ¡†æ¶æ‰©å±•ä»¥é€šè¿‡ Python API è®¿é—® Neuron SDK çš„ç»„ä»¶ï¼š\n-\n-```python\n-from transformers import BertModel, BertTokenizer, BertConfig\n-import torch\n-import torch.neuron\n-```\n-\n-æ‚¨åªéœ€è¦ä¿®æ”¹ä¸‹é¢è¿™ä¸€è¡Œï¼š\n-\n-```diff\n-- torch.jit.trace(model, [tokens_tensor, segments_tensors])\n-+ torch.neuron.trace(model, [token_tensor, segments_tensors])\n-```\n-\n-è¿™æ ·å°±èƒ½ä½¿ Neuron SDK è·Ÿè¸ªæ¨¡å‹å¹¶å¯¹å…¶è¿›è¡Œä¼˜åŒ–ï¼Œä»¥åœ¨ Inf1 å®ä¾‹ä¸Šè¿è¡Œã€‚\n-\n-è¦äº†è§£æœ‰å…³ AWS Neuron SDK åŠŸèƒ½ã€å·¥å…·ã€ç¤ºä¾‹æ•™ç¨‹å’Œæœ€æ–°æ›´æ–°çš„æ›´å¤šä¿¡æ¯ï¼Œ\n-è¯·å‚é˜… [AWS NeuronSDK æ–‡æ¡£](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)ã€‚"
        },
        {
            "sha": "dd80767421ba94dfb8a7c8f41de330c794d5fec0",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -167,8 +167,6 @@ class PreTrainedConfig(PushToHubMixin):\n \n         > PyTorch specific parameters\n \n-        torchscript (`bool`, *optional*, defaults to `False`):\n-            Whether or not the model should be used with Torchscript.\n         tie_word_embeddings (`bool`, *optional*, defaults to `True`):\n             Whether the model's input and output word embeddings should be tied. Note that this is only relevant if the\n             model has a output word embedding layer.\n@@ -206,7 +204,6 @@ def __init__(\n         output_hidden_states: bool = False,\n         output_attentions: bool = False,\n         return_dict: bool = True,\n-        torchscript: bool = False,\n         dtype: Optional[Union[str, \"torch.dtype\"]] = None,\n         # Common arguments\n         tie_word_embeddings: bool = True,\n@@ -268,7 +265,6 @@ def __init__(\n         # Attributes common for all models\n         self.return_dict = return_dict\n         self.output_hidden_states = output_hidden_states\n-        self.torchscript = torchscript\n         self.dtype = dtype\n         self._output_attentions = output_attentions  # has public property\n \n@@ -373,8 +369,7 @@ def use_return_dict(self) -> bool:\n         \"\"\"\n         `bool`: Whether or not return [`~utils.ModelOutput`] instead of tuples.\n         \"\"\"\n-        # If torchscript is set, force `return_dict=False` to avoid jit errors\n-        return self.return_dict and not self.torchscript\n+        return self.return_dict\n \n     @property\n     def num_labels(self) -> int:"
        },
        {
            "sha": "383d29ef404b76e3ae1cb34309b7086de60e7ef2",
            "filename": "src/transformers/loss/loss_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_d_fine.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -18,31 +18,21 @@\n import torch.nn.functional as F\n \n from ..utils import is_vision_available\n-from .loss_for_object_detection import (\n-    box_iou,\n-)\n+from .loss_for_object_detection import box_iou\n from .loss_rt_detr import RTDetrHungarianMatcher, RTDetrLoss\n \n \n if is_vision_available():\n     from transformers.image_transforms import center_to_corners_format\n \n \n-@torch.jit.unused\n def _set_aux_loss(outputs_class, outputs_coord):\n-    # this is a workaround to make torchscript happy, as torchscript\n-    # doesn't support dictionary with non-homogeneous values, such\n-    # as a dict having both a Tensor and a list.\n     return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n \n-@torch.jit.unused\n def _set_aux_loss2(\n     outputs_class, outputs_coord, outputs_corners, outputs_ref, teacher_corners=None, teacher_logits=None\n ):\n-    # this is a workaround to make torchscript happy, as torchscript\n-    # doesn't support dictionary with non-homogeneous values, such\n-    # as a dict having both a Tensor and a list.\n     return [\n         {\n             \"logits\": a,"
        },
        {
            "sha": "c265a57e8635f8ffea85326da2c36afcd116084b",
            "filename": "src/transformers/loss/loss_for_object_detection.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_for_object_detection.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_for_object_detection.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -478,11 +478,7 @@ def nested_tensor_from_tensor_list(tensor_list: list[Tensor]):\n \n \n # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-@torch.jit.unused\n def _set_aux_loss(outputs_class, outputs_coord):\n-    # this is a workaround to make torchscript happy, as torchscript\n-    # doesn't support dictionary with non-homogeneous values, such\n-    # as a dict having both a Tensor and a list.\n     return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n "
        },
        {
            "sha": "7d344e570b4b492470e2bf35eacb6a255002f7f8",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -35,11 +35,7 @@\n \n \n # different for RT-DETR: not slicing the last element like in DETR one\n-@torch.jit.unused\n def _set_aux_loss(outputs_class, outputs_coord):\n-    # this is a workaround to make torchscript happy, as torchscript\n-    # doesn't support dictionary with non-homogeneous values, such\n-    # as a dict having both a Tensor and a list.\n     return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n "
        },
        {
            "sha": "7ab90f1433fc6d49cd5887140d7d012453e79ea6",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -2706,14 +2706,11 @@ def tie_embeddings_and_encoder_decoder(self):\n         \"\"\"\n         If set in the config, tie the weights between the input embeddings and the output embeddings,\n         and the encoder and decoder.\n-\n-        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n-        weights instead.\n         \"\"\"\n         if getattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\", True):\n             output_embeddings = self.get_output_embeddings()\n             if output_embeddings is not None:\n-                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n+                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n \n         if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n             if hasattr(self, self.base_model_prefix):\n@@ -2829,12 +2826,9 @@ def tie_encoder_to_decoder_recursively(\n             )\n         return tied_weights\n \n-    def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n-        \"\"\"Tie or clone module weights depending of whether we are using TorchScript or not\"\"\"\n-        if self.config.torchscript:\n-            output_embeddings.weight = nn.Parameter(input_embeddings.weight.clone())\n-        else:\n-            output_embeddings.weight = input_embeddings.weight\n+    def _tie_embedding_weights(self, output_embeddings, input_embeddings):\n+        \"\"\"Tie weights, and add hooks and flags if using TP.\"\"\"\n+        output_embeddings.weight = input_embeddings.weight\n \n         # Passing hooks over to the embeddings if needed\n         # (currently limited to tensor parallel hooks and flags only)\n@@ -2850,10 +2844,7 @@ def _tie_or_clone_weights(self, output_embeddings, input_embeddings):\n         if getattr(output_embeddings, \"bias\", None) is not None:\n             output_embeddings.bias.data = nn.functional.pad(\n                 output_embeddings.bias.data,\n-                (\n-                    0,\n-                    output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0],\n-                ),\n+                (0, output_embeddings.weight.shape[0] - output_embeddings.bias.shape[0]),\n                 \"constant\",\n                 0,\n             )"
        },
        {
            "sha": "69f0a0dc91729c3f02964a8401ba462f8dd9634d",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1033,15 +1033,12 @@ def _tie_weights(self):\n \n             for i in range(self.config.n_codes_total - self.config.n_codes_given):\n                 # self.input_embeds_layers[i + 1].weight = self.lm_heads[i].weight\n-                self._tie_or_clone_weights(output_embeddings[i], input_embeddings[i + 1])\n+                self._tie_embedding_weights(output_embeddings[i], input_embeddings[i + 1])\n                 self._tied_weights_keys.append(f\"lm_heads.{i}.weight\")\n \n     def tie_weights(self):\n         \"\"\"\n         Tie the weights between the input embeddings list and the output embeddings list.\n-\n-        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n-        weights instead.\n         \"\"\"\n         for module in self.modules():\n             if hasattr(module, \"_tie_weights\"):\n@@ -1586,9 +1583,6 @@ def generate(\n     def tie_weights(self):\n         \"\"\"\n         Tie the weights between the input embeddings list and the output embeddings list.\n-\n-        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n-        weights instead.\n         \"\"\"\n         for module in self.modules():\n             if hasattr(module, \"_tie_weights\"):"
        },
        {
            "sha": "ee9d7b6e842c3524871e12239763dd24411ea79b",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -920,11 +920,11 @@ def _tie_weights(self):\n             if self.shared.weight.device == torch.device(\n                 \"meta\"\n             ) and self.decoder.embed_tokens.weight.device != torch.device(\"meta\"):\n-                self._tie_or_clone_weights(self.encoder.embed_tokens, self.decoder.embed_tokens)\n-                self._tie_or_clone_weights(self.shared, self.decoder.embed_tokens)\n+                self._tie_embedding_weights(self.encoder.embed_tokens, self.decoder.embed_tokens)\n+                self._tie_embedding_weights(self.shared, self.decoder.embed_tokens)\n             else:\n-                self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-                self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+                self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+                self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_input_embeddings(self):\n         return self.shared\n@@ -1089,7 +1089,7 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self.model._tie_weights()\n-            self._tie_or_clone_weights(self.lm_head, self.model.shared)\n+            self._tie_embedding_weights(self.lm_head, self.model.shared)\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ada977bfe7fa65853fa8b99943be2cbc47c9d457",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -2102,8 +2102,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -2250,7 +2250,7 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self.model._tie_weights()\n-            self._tie_or_clone_weights(self.lm_head, self.model.shared)\n+            self._tie_embedding_weights(self.lm_head, self.model.shared)\n \n     @auto_docstring\n     # Ignore copy"
        },
        {
            "sha": "af63b5ef66f22e8e7d62a42fcb5d0b177ff8c53a",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -160,21 +160,14 @@ def backward(ctx, grad_output: torch.Tensor) -> torch.Tensor:\n \n class BloomGelu(nn.Module):\n     \"\"\"\n-    BloomBiasGelu wrapper function that make use of the simple function on inference mode to make the model\n-    torchscriptable and use the autograd function in training mode to get the accurate results of the gradients Partly\n-    copied from Megatron-DeepSpeed code and adapted for our needs\n-\n-    See here why autograd functions are not torchscriptable: https://github.com/pytorch/pytorch/issues/22329\n+    Partly copied from Megatron-DeepSpeed code and adapted for our needs\n     \"\"\"\n \n     def __init__(self):\n         super().__init__()\n \n     def forward(self, x: torch.Tensor) -> torch.Tensor:\n-        if self.training:\n-            return GeLUFunction.apply(x)\n-        else:\n-            return bloom_gelu_forward(x)\n+        return GeLUFunction.apply(x)\n \n \n class BloomAttention(nn.Module):"
        },
        {
            "sha": "a9e04ec546b2675aa567e71a182ba6809af137be",
            "filename": "src/transformers/models/conditional_detr/modeling_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodeling_conditional_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1513,11 +1513,7 @@ def __init__(self, config: ConditionalDetrConfig):\n         self.post_init()\n \n     # taken from https://github.com/Atten4Vis/conditionalDETR/blob/master/models/conditional_detr.py\n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n     @auto_docstring"
        },
        {
            "sha": "f41df5cc84578279ea84a3c13e5b62c98a0ede3f",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -763,7 +763,7 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_codebooks_embeddings:\n-            self._tie_or_clone_weights(\n+            self._tie_embedding_weights(\n                 self.backbone_model.embed_tokens.embed_audio_tokens,\n                 self.depth_decoder.model.embed_tokens,\n             )"
        },
        {
            "sha": "dafb289c5113e7558d405b35754c5918f2ef58ce",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -443,7 +443,7 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_codebooks_embeddings:\n-            self._tie_or_clone_weights(\n+            self._tie_embedding_weights(\n                 self.backbone_model.embed_tokens.embed_audio_tokens,\n                 self.depth_decoder.model.embed_tokens,\n             )"
        },
        {
            "sha": "701e37b2619d9224df3280824137cd8742ce0a02",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1578,11 +1578,7 @@ def __init__(self, config: DFineConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n     @auto_docstring"
        },
        {
            "sha": "b5aafb5b8b28c77ea20e15c5e7fa37dc241d1400",
            "filename": "src/transformers/models/dab_detr/modeling_dab_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fmodeling_dab_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1457,11 +1457,7 @@ def __init__(self, config: DabDetrConfig):\n         self.post_init()\n \n     # taken from https://github.com/Atten4Vis/conditionalDETR/blob/master/models/dab_detr.py\n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n     @auto_docstring"
        },
        {
            "sha": "4c881c4365a067b6c2a66e85cfa586a766d61648",
            "filename": "src/transformers/models/deprecated/deta/modeling_deta.py",
            "status": "modified",
            "additions": 5,
            "deletions": 22,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fmodeling_deta.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1837,11 +1837,7 @@ def __init__(self, config: DetaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         aux_loss = [\n             {\"logits\": logits, \"pred_boxes\": pred_boxes}\n             for logits, pred_boxes in zip(outputs_class.transpose(0, 1)[:-1], outputs_coord.transpose(0, 1)[:-1])\n@@ -2444,20 +2440,6 @@ def generalized_box_iou(boxes1, boxes2):\n     return iou - (area - union) / area\n \n \n-# from https://github.com/facebookresearch/detectron2/blob/cbbc1ce26473cb2a5cc8f58e8ada9ae14cb41052/detectron2/layers/wrappers.py#L100\n-def nonzero_tuple(x):\n-    \"\"\"\n-    A 'as_tuple=True' version of torch.nonzero to support torchscript. because of\n-    https://github.com/pytorch/pytorch/issues/38718\n-    \"\"\"\n-    if torch.jit.is_scripting():\n-        if x.dim() == 0:\n-            return x.unsqueeze(0).nonzero().unbind(1)\n-        return x.nonzero().unbind(1)\n-    else:\n-        return x.nonzero(as_tuple=True)\n-\n-\n # from https://github.com/facebookresearch/detectron2/blob/9921a2caa585d4fa66c4b534b6fab6e74d89b582/detectron2/modeling/matcher.py#L9\n class DetaMatcher:\n     \"\"\"\n@@ -2496,7 +2478,6 @@ def __init__(self, thresholds: list[float], labels: list[int], allow_low_quality\n             raise ValueError(\"Thresholds should be positive\")\n         thresholds.insert(0, -float(\"inf\"))\n         thresholds.append(float(\"inf\"))\n-        # Currently torchscript does not support all + generator\n         if not all(low <= high for (low, high) in zip(thresholds[:-1], thresholds[1:])):\n             raise ValueError(\"Thresholds should be sorted.\")\n         if not all(l in [-1, 0, 1] for l in labels):\n@@ -2561,7 +2542,9 @@ def set_low_quality_matches_(self, match_labels, match_quality_matrix):\n         # Find the highest quality match available, even if it is low, including ties.\n         # Note that the matches qualities must be positive due to the use of\n         # `torch.nonzero`.\n-        _, pred_inds_with_highest_quality = nonzero_tuple(match_quality_matrix == highest_quality_foreach_gt[:, None])\n+        _, pred_inds_with_highest_quality = (match_quality_matrix == highest_quality_foreach_gt[:, None]).nonzero(\n+            as_tuple=True\n+        )\n         # If an anchor was labeled positive only due to a low-quality match\n         # with gt_A, but it has larger overlap with gt_B, it's matched index will still be gt_B.\n         # This follows the implementation in Detectron, and is found to have no significant impact.\n@@ -2593,8 +2576,8 @@ def subsample_labels(labels: torch.Tensor, num_samples: int, positive_fraction:\n         pos_idx, neg_idx (Tensor):\n             1D vector of indices. The total length of both is `num_samples` or fewer.\n     \"\"\"\n-    positive = nonzero_tuple((labels != -1) & (labels != bg_label))[0]\n-    negative = nonzero_tuple(labels == bg_label)[0]\n+    positive = ((labels != -1) & (labels != bg_label)).nonzero(as_tuple=True)[0]\n+    negative = (labels == bg_label).nonzero(as_tuple=True)[0]\n \n     num_pos = int(num_samples * positive_fraction)\n     # protect against not enough positive examples"
        },
        {
            "sha": "b7b8fdb4ce87b173b9592a4739d02460b98444fc",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -866,8 +866,7 @@ def __init__(self, config: GPTSanJapaneseConfig):\n         self.model = GPTSanJapaneseModel(config)\n         self.register_buffer(\"final_logits_bias\", torch.zeros([1, config.vocab_size]))\n         self.lm_head = nn.Linear(config.d_model, config.vocab_size, bias=False)\n-        if not self.config.torchscript:\n-            self.lm_head.weight = self.model.embed_tokens.weight\n+        self.lm_head.weight = self.model.embed_tokens.weight\n \n     def forward(\n         self,"
        },
        {
            "sha": "d422bf22b0fbe523dfb6f363d4aff765d30644d4",
            "filename": "src/transformers/models/deprecated/transfo_xl/modeling_transfo_xl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftransfo_xl%2Fmodeling_transfo_xl.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -877,19 +877,13 @@ def tie_weights(self):\n \n         if self.config.tie_word_embeddings:\n             for i in range(len(self.crit.out_layers)):\n-                self._tie_or_clone_weights(self.crit.out_layers[i], self.transformer.word_emb.emb_layers[i])\n+                self._tie_embedding_weights(self.crit.out_layers[i], self.transformer.word_emb.emb_layers[i])\n         if self.config.tie_projs:\n             for i, tie_proj in enumerate(self.config.tie_projs):\n                 if tie_proj and self.config.div_val == 1 and self.config.d_model != self.config.d_embed:\n-                    if self.config.torchscript:\n-                        self.crit.out_projs[i] = nn.Parameter(self.transformer.word_emb.emb_projs[0].clone())\n-                    else:\n-                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]\n+                    self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[0]\n                 elif tie_proj and self.config.div_val != 1:\n-                    if self.config.torchscript:\n-                        self.crit.out_projs[i] = nn.Parameter(self.transformer.word_emb.emb_projs[i].clone())\n-                    else:\n-                        self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]\n+                    self.crit.out_projs[i] = self.transformer.word_emb.emb_projs[i]\n \n     def reset_memory_length(self, mem_len):\n         self.transformer.reset_memory_length(mem_len)"
        },
        {
            "sha": "bf44f7c19f3485fa154b245256bea8e020799f37",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1640,8 +1640,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.word_embeddings, self.word_embeddings)\n-            self._tie_or_clone_weights(self.decoder.word_embeddings, self.word_embeddings)\n+            self._tie_embedding_weights(self.encoder.word_embeddings, self.word_embeddings)\n+            self._tie_embedding_weights(self.decoder.word_embeddings, self.word_embeddings)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1751,7 +1751,7 @@ def __init__(self, config: XLMProphetNetConfig):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.prophetnet.word_embeddings, self.lm_head)\n+            self._tie_embedding_weights(self.prophetnet.word_embeddings, self.lm_head)\n \n     def get_input_embeddings(self):\n         return self.prophetnet.word_embeddings\n@@ -1964,7 +1964,7 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)\n+            self._tie_embedding_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)\n \n     def set_decoder(self, decoder):\n         self.prophetnet.decoder = decoder\n@@ -2173,7 +2173,7 @@ def __init__(self, config: XLMProphetNetConfig):\n         self.post_init()\n \n     def _tie_weights(self):\n-        self._tie_or_clone_weights(self.word_embeddings, self.decoder.get_input_embeddings())\n+        self._tie_embedding_weights(self.word_embeddings, self.decoder.get_input_embeddings())\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "f2b45525dfea56fa1fd25f53c8aa255d2a06eea9",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -848,8 +848,8 @@ def get_encoder(self):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n-            self._tie_or_clone_weights(self.decoder.output_projection, self.get_input_embeddings())\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n+            self._tie_embedding_weights(self.decoder.output_projection, self.get_input_embeddings())\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f9c3ef714680b3cd4512a5084f5d7fbbd3e51d2e",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1186,7 +1186,7 @@ def _try_load_missing_tied_module(self, key):\n                 return\n             module = getattr(module, sub_key)\n \n-        self._tie_or_clone_weights(module, self.shared)\n+        self._tie_embedding_weights(module, self.shared)\n \n     @classmethod\n     def from_pretrained(self, *args, **kwargs):\n@@ -1638,8 +1638,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1806,8 +1806,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1984,7 +1984,7 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "b37b4a1e3e6d3ab81553cb7e279acc640d0c7415",
            "filename": "src/transformers/models/luke/modeling_luke.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fluke%2Fmodeling_luke.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1069,7 +1069,7 @@ def __init__(self, config):\n \n     def tie_weights(self):\n         super().tie_weights()\n-        self._tie_or_clone_weights(self.entity_predictions.decoder, self.luke.entity_embeddings.entity_embeddings)\n+        self._tie_embedding_weights(self.entity_predictions.decoder, self.luke.entity_embeddings.entity_embeddings)\n \n     def get_output_embeddings(self):\n         return self.lm_head.decoder"
        },
        {
            "sha": "772026b7b46526eb10dcc678f018375d6d8081fd",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -945,8 +945,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "fe0f264581bc00afa4f3033ef35c096eabb7ad72",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1143,15 +1143,12 @@ def set_output_embeddings(self, new_embeddings: nn.Embedding):\n     def tie_weights(self):\n         \"\"\"\n         Tie the weights between the input embeddings and the output embeddings.\n-\n-        If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n-        weights instead.\n         \"\"\"\n         output_embeddings = self.get_output_embeddings()\n         if output_embeddings is not None and getattr(self.config, \"tie_word_embeddings\", True):\n             # if embeddings are shared this will return shared embeddings otherwise decoder embed_tokens\n             word_embeddings = self.get_decoder().get_input_embeddings()\n-            self._tie_or_clone_weights(output_embeddings, word_embeddings)\n+            self._tie_embedding_weights(output_embeddings, word_embeddings)\n \n         if getattr(self.config, \"is_encoder_decoder\", False) and getattr(self.config, \"tie_encoder_decoder\", False):\n             if hasattr(self, self.base_model_prefix):"
        },
        {
            "sha": "3f10516ed046fa5aedd916a1fdf80fa9aae2df66",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -926,8 +926,8 @@ def get_encoder(self):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.get_input_embeddings())\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.get_input_embeddings())\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.get_input_embeddings())\n \n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c57af7cb5f514a10920b8fa68748c226610bc01a",
            "filename": "src/transformers/models/mgp_str/modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fmodeling_mgp_str.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -176,7 +176,7 @@ def forward(self, hidden_states):\n             .reshape(batch_size, num, 3, self.num_heads, channel // self.num_heads)\n             .permute(2, 0, 3, 1, 4)\n         )\n-        query, key, value = qkv[0], qkv[1], qkv[2]  # make torchscript happy (cannot use tensor as tuple)\n+        query, key, value = qkv[0], qkv[1], qkv[2]\n \n         attention_probs = (query @ key.transpose(-2, -1)) * self.scale\n         attention_probs = attention_probs.softmax(dim=-1)"
        },
        {
            "sha": "dc4fb4e22bd1118e244072caa444c9213ddf004c",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -913,8 +913,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "929d21fa341aa73ab820560865112b50fe48371e",
            "filename": "src/transformers/models/oneformer/modeling_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fmodeling_oneformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -2302,11 +2302,7 @@ def forward_prediction_heads(self, output, mask_features, attention_mask_target_\n \n         return outputs_class, outputs_mask, attention_mask\n \n-    @torch.jit.unused\n     def _get_aux_predictions(self, outputs_class, outputs_seg_masks):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         aux_list = [\n             {\"class_queries_logits\": a, \"masks_queries_logits\": b}\n             for a, b in zip(outputs_class[:-1], outputs_seg_masks[:-1])"
        },
        {
            "sha": "e3f66106f979dc0ca852cb8c60947c07bc674c43",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -856,8 +856,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "0d17549a2d00f8b0868e31f49141bc3f7c00bed2",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -91,8 +91,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "8cc5eae250bc8f8636fd75d0340f1c97f0cfe2e6",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1429,8 +1429,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.word_embeddings, self.word_embeddings)\n-            self._tie_or_clone_weights(self.decoder.word_embeddings, self.word_embeddings)\n+            self._tie_embedding_weights(self.encoder.word_embeddings, self.word_embeddings)\n+            self._tie_embedding_weights(self.decoder.word_embeddings, self.word_embeddings)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1555,7 +1555,7 @@ def __init__(self, config: ProphetNetConfig):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.prophetnet.word_embeddings, self.lm_head)\n+            self._tie_embedding_weights(self.prophetnet.word_embeddings, self.lm_head)\n \n     def get_input_embeddings(self):\n         return self.prophetnet.word_embeddings\n@@ -1748,7 +1748,7 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)\n+            self._tie_embedding_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)\n \n     def set_decoder(self, decoder):\n         self.prophetnet.decoder = decoder\n@@ -1938,7 +1938,7 @@ def __init__(self, config: ProphetNetConfig):\n         self.post_init()\n \n     def _tie_weights(self):\n-        self._tie_or_clone_weights(self.word_embeddings, self.decoder.get_input_embeddings())\n+        self._tie_embedding_weights(self.word_embeddings, self.decoder.get_input_embeddings())\n \n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)"
        },
        {
            "sha": "acead9a87055a670430d437145edc1b1806f7dcf",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1843,11 +1843,7 @@ def __init__(self, config: RTDetrConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n     @auto_docstring"
        },
        {
            "sha": "5b736796cfe66229ca4ef9031903cc982a4a0664",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1832,11 +1832,7 @@ def __init__(self, config: RTDetrV2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n \n     @auto_docstring"
        },
        {
            "sha": "2388556f06e3edab69fdb5fd7ee0a38764441ba9",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -2096,7 +2096,7 @@ def _tie_weights(self) -> None:\n         if getattr(self.config, \"tie_word_embeddings\", True):\n             output_embeddings = self.get_output_embeddings()\n             if output_embeddings is not None:\n-                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n+                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n \n \n ############ VOCODER related code ################\n@@ -2487,9 +2487,9 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_COMMON_CUSTOM_ARGS)\n     def forward(\n@@ -2741,8 +2741,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_COMMON_CUSTOM_ARGS)\n     def forward(\n@@ -3010,9 +3010,9 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_COMMON_CUSTOM_ARGS)\n     def forward(\n@@ -3331,8 +3331,8 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_COMMON_CUSTOM_ARGS)\n     def forward(\n@@ -3685,9 +3685,9 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_COMMON_CUSTOM_ARGS)\n     def forward("
        },
        {
            "sha": "2775f8297f65dfbf3ce656d7987821cd591765f0",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 14,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -2292,7 +2292,7 @@ def _tie_weights(self) -> None:\n         if getattr(self.config, \"tie_word_embeddings\", True):\n             output_embeddings = self.get_output_embeddings()\n             if output_embeddings is not None:\n-                self._tie_or_clone_weights(output_embeddings, self.get_input_embeddings())\n+                self._tie_embedding_weights(output_embeddings, self.get_input_embeddings())\n \n \n ############ VOCODER related code ################\n@@ -2694,9 +2694,9 @@ def set_input_embeddings(self, value):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)\n     def forward(\n@@ -2954,8 +2954,8 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.forward\n@@ -3231,9 +3231,9 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.forward with SeamlessM4T->SeamlessM4Tv2\n@@ -3590,8 +3590,8 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.forward with SeamlessM4T->SeamlessM4Tv2\n@@ -3981,9 +3981,9 @@ def set_input_embeddings(self, value):\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.text_encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.text_decoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.lm_head, self.shared)\n+            self._tie_embedding_weights(self.text_encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.text_decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.lm_head, self.shared)\n \n     @auto_docstring(custom_args=SEAMLESS_M4T_V2_COMMON_CUSTOM_ARGS)\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.forward with SeamlessM4T->SeamlessM4Tv2"
        },
        {
            "sha": "8df2f2297d91aa5f7ef48d2a02a3d9277cb7ca97",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -948,8 +948,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1107,8 +1107,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1258,7 +1258,7 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "5b26ea6b849cf8a919ff86aa1b808d68e275dd8d",
            "filename": "src/transformers/models/switch_transformers/modular_switch_transformers.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodular_switch_transformers.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -704,8 +704,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -798,8 +798,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -949,7 +949,7 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "366337dae99f72e30b20359c1a5a9868735c3623",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1010,8 +1010,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1178,8 +1178,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1359,7 +1359,7 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder\n@@ -1670,8 +1670,8 @@ def set_input_embeddings(self, new_embeddings):\n \n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "43bc2946f52ae8ab1f586cc9f191be926abd3200",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -956,7 +956,7 @@ def get_output_embeddings(self):\n     def _tie_weights(self):\n         # Decoder input and output embeddings are tied.\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n+            self._tie_embedding_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n \n     def get_encoder(self):\n         return self.model.encoder"
        },
        {
            "sha": "1fcee39ad99be747284ad28f6ff5dd35c572ded1",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1021,7 +1021,7 @@ def get_output_embeddings(self):\n     def _tie_weights(self):\n         # Decoder input and output embeddings are tied.\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n+            self._tie_embedding_weights(self.lm_head.out_proj, self.get_decoder().get_input_embeddings())\n \n     def get_encoder(self):\n         return self.model.encoder"
        },
        {
            "sha": "f749d0ce740c0129aa5e4d0d882e1162c5ab809a",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -1080,7 +1080,7 @@ def __init__(self, config, embed_tokens=None, embed_patches=None):\n     def _tie_weights(self):\n         for bias in self.relative_bias.biases:\n             if isinstance(bias, RelativePositionBias1D):\n-                self._tie_or_clone_weights(\n+                self._tie_embedding_weights(\n                     bias.relative_attention_bias, self.block[0].layer[0].SelfAttention.relative_attention_bias\n                 )\n "
        },
        {
            "sha": "a3be7ad0846851b3d3eed43a76945b8565c1b30e",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -955,8 +955,8 @@ def set_input_embeddings(self, new_embeddings):\n     # Copied from transformers.models.t5.modeling_t5.T5Model._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     # Copied from transformers.models.t5.modeling_t5.T5Model.get_encoder\n     def get_encoder(self):\n@@ -1141,8 +1141,8 @@ def set_input_embeddings(self, new_embeddings):\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder\n     def get_encoder(self):\n@@ -1341,7 +1341,7 @@ def set_input_embeddings(self, new_embeddings):\n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n \n     # Copied from transformers.models.t5.modeling_t5.T5EncoderModel.get_encoder\n     def get_encoder(self):\n@@ -1660,8 +1660,8 @@ def set_input_embeddings(self, new_embeddings):\n     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering._tie_weights\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n-            self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n-            self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.encoder.embed_tokens, self.shared)\n+            self._tie_embedding_weights(self.decoder.embed_tokens, self.shared)\n \n     # Copied from transformers.models.t5.modeling_t5.T5ForQuestionAnswering.get_encoder\n     def get_encoder(self):"
        },
        {
            "sha": "527b4d34c3b124a6b78ef1575d899db700aa9722",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -561,11 +561,7 @@ def __init__(self, config: YolosConfig):\n         self.post_init()\n \n     # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-    @torch.jit.unused\n     def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n         return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n \n     @can_return_tuple"
        },
        {
            "sha": "01d9b210d0b7bc3741f96cb596f5c22478356853",
            "filename": "tests/models/aimv2/test_modeling_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faimv2%2Ftest_modeling_aimv2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -182,7 +182,6 @@ class Aimv2VisionModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Aimv2VisionModelTester(self)\n@@ -312,7 +311,6 @@ class Aimv2TextModelTest(Aimv2ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Aimv2TextModelTester(self)\n@@ -392,7 +390,6 @@ class Aimv2ModelTest(Aimv2ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     )\n     fx_compatible = False\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n     _is_composite = True"
        },
        {
            "sha": "9a9399775cb80be8bd538c9e45f4e0f16c221271",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch ALIGN model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -32,7 +31,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -487,77 +485,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # ALIGN needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "31a4a08d7147f4c3325255974658a284c2f8951a",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,8 +14,6 @@\n \"\"\"Testing suite for the PyTorch AltCLIP model.\"\"\"\n \n import inspect\n-import os\n-import tempfile\n import unittest\n \n import numpy as np\n@@ -28,7 +26,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -463,77 +460,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # CLIP needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"BAAI/AltCLIP\""
        },
        {
            "sha": "0be791e5db4a62d078a851f9d0bae93e9512f500",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -191,7 +191,6 @@ class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMi\n \n     all_model_classes = (AriaModel, AriaForConditionalGeneration) if is_torch_available() else ()\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "9da8abce966581779e406522aeae02dac9f6ce8b",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -208,7 +208,6 @@ class AutoformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     pipeline_model_mapping = {\"feature-extraction\": AutoformerModel} if is_torch_available() else {}\n \n     test_missing_keys = False\n-    test_torchscript = False\n     test_inputs_embeds = False\n \n     def setUp(self):"
        },
        {
            "sha": "80f94e4e4c15f9a416dd0ed203b791ce66dde03c",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -172,7 +172,6 @@ class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n     )\n     fx_compatible = False\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "e438107d3dfb915735e15de8c0639169e0567107",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -694,9 +694,6 @@ class BarkFineModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_missing_keys = False\n \n-    # torchscript disabled for now because forward with an int\n-    test_torchscript = False\n-\n     test_resize_embeddings = True\n \n     def setUp(self):"
        },
        {
            "sha": "2af061f4ba3d42d3123bc13d1c622eea7d67e2dd",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -409,10 +409,6 @@ def create_and_check_for_change_to_full_attn(\n \n @require_torch\n class BigBirdModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    # torchscript should be possible, but takes prohibitively long to test.\n-    # Also torchscript is not an important feature to have in the beginning.\n-    test_torchscript = False\n-\n     all_model_classes = (\n         (\n             BigBirdModel,"
        },
        {
            "sha": "3076634362acca2d8309ab3976368756bdbc7351",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -266,10 +266,6 @@ class BigBirdPegasusModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineT\n     is_encoder_decoder = True\n     test_missing_keys = False\n \n-    # torchscript tests are not passing for now.\n-    # Also torchscript is not an important feature to have in the beginning.\n-    test_torchscript = False\n-\n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "6754cc21ad2e8ce6ea80d63213f119fa50fcd21d",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 218,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch Blip model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -35,7 +34,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -455,77 +453,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # Blip needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -768,7 +695,6 @@ class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = BlipVQAModelTester(self)\n@@ -846,7 +772,6 @@ class BlipTextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = BlipTextRetrievalModelTester(self)\n@@ -945,77 +870,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # Blip needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -1047,7 +901,6 @@ class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = BlipTextImageModelsModelTester(self)\n@@ -1134,77 +987,6 @@ def test_training_gradient_checkpointing(self):\n             loss = model(**inputs).loss\n             loss.backward()\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # Blip needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "3b30d8c857ed9d78203501eeaf9e7dc69229e254",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -465,7 +465,6 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):\n@@ -796,7 +795,6 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n     _is_composite = True\n \n     # TODO: Fix the failed tests\n@@ -1076,7 +1074,6 @@ class Blip2TextModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Blip2TextModelWithProjectionTester(self)\n@@ -1233,7 +1230,6 @@ class Blip2VisionModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Blip2VisionModelWithProjectionTester(self)\n@@ -1383,7 +1379,6 @@ class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Blip2TextRetrievalModelTester(self)"
        },
        {
            "sha": "4ce8e83328650e7f410365452f1b4fd154e9ccad",
            "filename": "tests/models/bloom/test_modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_bloom.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -170,7 +170,6 @@ class BloomModelTest(CausalLMModelTest, unittest.TestCase):\n     model_tester_class = BloomModelTester\n     fx_compatible = True\n     test_missing_keys = False\n-    test_torchscript = True  # torch.autograd functions seems not to be supported\n \n     def test_bloom_model_past(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()"
        },
        {
            "sha": "74268c81c03454191564f55f352c72c3e802a8bf",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -308,7 +308,6 @@ class BridgeTowerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n \n     is_training = False\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False\n "
        },
        {
            "sha": "5df0240b41ee28cb422987d7293279df88c809c5",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -270,7 +270,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class BrosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n     test_mismatched_shapes = False\n \n     all_model_classes = ("
        },
        {
            "sha": "ac7fe07b64fabd39d5fcd26b9c4b3dc624608afd",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -240,14 +240,6 @@ def setUp(self):\n         # we set has_text_modality to False as the config has no vocab_size attribute\n         self.config_tester = ConfigTester(self, config_class=CanineConfig, has_text_modality=False, hidden_size=37)\n \n-    @unittest.skip(\"failing. Will fix only when the community opens an issue for it.\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @unittest.skip(\"failing. Will fix only when the community opens an issue for it.\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "eb169209952242319dbc74ee7ad561ece339e75d",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,8 +14,6 @@\n \"\"\"Testing suite for the PyTorch Chinese-CLIP model.\"\"\"\n \n import inspect\n-import os\n-import tempfile\n import unittest\n \n import requests\n@@ -28,7 +26,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -571,77 +568,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # CHINESE_CLIP needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"OFA-Sys/chinese-clip-vit-base-patch16\""
        },
        {
            "sha": "553f7444e8a4daa244b6a99d55b22304f273f7d6",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch CLAP model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -28,7 +27,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -524,77 +522,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                input_features = inputs_dict[\"input_features\"]  # CLAP needs input_features\n-                traced_model = torch.jit.trace(model, (input_ids, input_features))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_audio_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "9a3d0eb2becae76831854c162d47b6731808fcf8",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch CLIP model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -39,7 +38,6 @@\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     is_flaky,\n@@ -562,77 +560,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # CLIP needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "1ecd3fcab4bee9b610d42ae28122d555977d0cfe",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch CLIPSeg model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -33,7 +32,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -490,77 +488,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # CLIPSeg needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "6b49a2b669aea37cddb3cfdd135bac704183b461",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -162,8 +162,6 @@ def create_and_check_model(self, speech_config, input_ids, input_mask):\n class ClvpEncoderTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (ClvpEncoder,) if is_torch_available() else ()\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = ClvpEncoderTester(self)\n         self.encoder_config_tester = ConfigTester(self, config_class=ClvpEncoderConfig, hidden_size=32)\n@@ -410,7 +408,6 @@ class ClvpModelForConditionalGenerationTest(ModelTesterMixin, unittest.TestCase)\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = ClvpModelForConditionalGenerationTester(self)"
        },
        {
            "sha": "ba219d0614ccd42daf7762e3da0bbacf97ab20c1",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -159,7 +159,6 @@ class Cohere2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     )\n     fx_compatible = False\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "b457dfbf0eb83814bef6cc3fcf3fb796974de1fa",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -186,7 +186,6 @@ class ColPaliForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (ColPaliForRetrieval,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = True\n     additional_model_inputs = [\"token_type_ids\"]"
        },
        {
            "sha": "39452ca641048b86060c9e8e359d0bec7d01d440",
            "filename": "tests/models/colqwen2/test_modeling_colqwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_modeling_colqwen2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -201,7 +201,6 @@ class ColQwen2ForRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (ColQwen2ForRetrieval,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = True\n "
        },
        {
            "sha": "692235be402cf99884b6f4a86f3708f286be79f5",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -187,7 +187,6 @@ class ConditionalDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.T\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     zero_init_hidden_state = True"
        },
        {
            "sha": "ccfc4e32c7b5aea1de720643a23cf6c05a739799",
            "filename": "tests/models/csm/test_modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcsm%2Ftest_modeling_csm.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -265,10 +265,6 @@ def test_prompt_lookup_decoding_stops_at_eos(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @pytest.mark.skip(reason=\"CSM has custom embedding approach (text and audio embeddings).\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @pytest.mark.generate\n     @unittest.skip(reason=\"CSM does not support beam search.\")\n     def test_generate_from_inputs_embeds_1_beam_search(self, _, num_beams):"
        },
        {
            "sha": "80a16ba8c115d8a72ba445986e8f08423e575f29",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -154,7 +154,6 @@ class CvtModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False\n     test_torch_exportable = True"
        },
        {
            "sha": "8eaaa41d8f6d77372195bf26dc0d63c2cd50a3e2",
            "filename": "tests/models/d_fine/test_modeling_d_fine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fd_fine%2Ftest_modeling_d_fine.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -291,7 +291,6 @@ class DFineModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     test_torch_exportable = True"
        },
        {
            "sha": "0802301890819904a05be454e9ee94c50ff66510",
            "filename": "tests/models/dab_detr/test_modeling_dab_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdab_detr%2Ftest_modeling_dab_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -182,7 +182,6 @@ class DabDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     zero_init_hidden_state = True"
        },
        {
            "sha": "c266817ed665e162cda158c6e7bbc05cf48c2a9a",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 1,
            "deletions": 101,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,8 +14,6 @@\n \"\"\"Testing suite for the PyTorch Dac model.\"\"\"\n \n import inspect\n-import os\n-import tempfile\n import unittest\n \n import numpy as np\n@@ -26,7 +24,7 @@\n from transformers.testing_utils import is_torch_available, require_torch, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -174,104 +172,6 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @unittest.skip(\"The DacModel is not transformers based, thus it does not have the usual `attention` logic\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(\"The DacModel is not transformers based, thus it does not have the usual `hidden_states` logic\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            return\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            main_input_name = model_class.main_input_name\n-\n-            try:\n-                main_input = inputs[main_input_name]\n-                model(main_input)\n-                traced_model = torch.jit.trace(model, main_input)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(\"The DacModel is not transformers based, thus it does not have the usual `attention` logic\")\n     def test_attention_outputs(self):\n         pass"
        },
        {
            "sha": "c8ecc3bbd52626a57e9afc7dca8389ac94638d71",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -238,7 +238,6 @@ class DebertaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     )\n \n     fx_compatible = True\n-    test_torchscript = False\n \n     is_encoder_decoder = False\n "
        },
        {
            "sha": "895e23139916c8a3b93eb4ce64e76235bbafb438",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -252,7 +252,6 @@ class DebertaV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n     )\n \n     fx_compatible = True\n-    test_torchscript = False\n \n     is_encoder_decoder = False\n "
        },
        {
            "sha": "7b1d8690c0e392e77abc11f3c2c671a2a2d3e404",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -137,7 +137,6 @@ class DecisionTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unitte\n     test_hidden_states_output = False\n     test_inputs_embeds = False\n     test_gradient_checkpointing = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = DecisionTransformerModelTester(self)"
        },
        {
            "sha": "c6813ccd8aa005d54382c0e5220408fda6c5d1e4",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -55,7 +55,6 @@ def __init__(\n @require_torch\n class DeepseekV2ModelTest(CausalLMModelTest, unittest.TestCase):\n     fx_compatible = False\n-    test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = DeepseekV2ModelTester\n     model_split_percents = [0.5, 0.7, 0.8]"
        },
        {
            "sha": "43aab28bfa4ffe6a6450dc63b97bc096f13f87c0",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -193,7 +193,6 @@ class DeformableDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     test_torch_exportable = True"
        },
        {
            "sha": "acd06280d4bb61cb515d0117981eef3a07003951",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -187,7 +187,6 @@ class DetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     zero_init_hidden_state = True"
        },
        {
            "sha": "557097014b0e520a5498877d7ba13687a94171a4",
            "filename": "tests/models/dia/test_modeling_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_modeling_dia.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -497,14 +497,6 @@ def test_hidden_states_output(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @unittest.skip(reason=\"Theoretically works but kernel library causes issues.\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Theoretically works but kernel library causes issues.\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n     @unittest.skip(reason=\"Encoder-Decoder cache can not be initialized.\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "2d36617f97f95df282ac22224562e17e2c786914",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -212,8 +212,6 @@ class DinatModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n     fx_compatible = False\n \n-    test_torchscript = False\n-\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "d8557085b3cd8816e347762cfbab74d3a8f22385",
            "filename": "tests/models/doge/test_modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdoge%2Ftest_modeling_doge.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -275,7 +275,6 @@ class DogeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     )\n     has_attentions = False\n \n-    test_torchscript = False\n     fx_compatible = False\n \n     # Need to use `0.8` instead of `0.9` for `test_cpu_offload`"
        },
        {
            "sha": "fe07de425059e4ce98b303464e6e256f9c056c8c",
            "filename": "tests/models/edgetam/test_modeling_edgetam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fedgetam%2Ftest_modeling_edgetam.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -237,7 +237,6 @@ class EdgeTamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "0cfc6b4221a0dfe0b75089cc8e3d8a33a356b273",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 104,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -16,7 +16,6 @@\n import copy\n import inspect\n import os\n-import tempfile\n import unittest\n \n import numpy as np\n@@ -32,7 +31,7 @@\n )\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -197,108 +196,6 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"The EncodecModel is not transformers based, thus it does not have the usual `attention` logic\"\n-    )\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"The EncodecModel is not transformers based, thus it does not have the usual `hidden_states` logic\"\n-    )\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            main_input_name = model_class.main_input_name\n-\n-            try:\n-                main_input = inputs[main_input_name]\n-                model(main_input)\n-                traced_model = torch.jit.trace(model, main_input)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(\n         reason=\"The EncodecModel is not transformers based, thus it does not have the usual `attention` logic\"\n     )"
        },
        {
            "sha": "6547a0562f2f302b046fad10a18871c10ce98d65",
            "filename": "tests/models/esm/test_modeling_esmfold.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -224,18 +224,6 @@ def test_model_outputs_equivalence(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(reason=\"ESMFold doesn't support torchscript compilation.\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"ESMFold doesn't support torchscript compilation.\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @unittest.skip(reason=\"ESMFold doesn't support torchscript compilation.\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n     @unittest.skip(reason=\"ESMFold doesn't support data parallel.\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "50a03a683b61c303b925fd592d5db615dfac4206",
            "filename": "tests/models/evolla/test_modeling_evolla.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fevolla%2Ftest_modeling_evolla.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -200,7 +200,6 @@ class EvollaModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (EvollaModel, EvollaForProteinText2Text) if is_torch_available() else ()\n     pipeline_model_mapping = {\"feature-extraction\": EvollaModel} if is_torch_available() else {}\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     maxDiff = None\n "
        },
        {
            "sha": "3102990833f8854e878c792439c565575fde4aaf",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -264,7 +264,6 @@ class FalconMambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTest\n     all_model_classes = (FalconMambaModel, FalconMambaForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # FalconMamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n-    test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "5e729eae8eb0f7dcf092acc2998f1a43f6ddfb12",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -126,7 +126,6 @@ def prepare_config_and_inputs_for_common(self):\n class FastSpeech2ConformerModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     is_encoder_decoder = True\n \n@@ -546,7 +545,6 @@ def prepare_config_and_inputs_for_common(self):\n class FastSpeech2ConformerWithHifiGanTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerWithHifiGan,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     is_encoder_decoder = True\n "
        },
        {
            "sha": "501208452474633ef3d065fd25677d18cb8b01e1",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 89,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch FLAVA model.\"\"\"\n \n import inspect\n-import os\n import random\n import tempfile\n import unittest\n@@ -35,7 +34,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -163,7 +161,6 @@ class FlavaImageModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (FlavaImageModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -432,8 +429,6 @@ def prepare_config_and_inputs_for_common(self):\n class FlavaTextModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaTextModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = FlavaTextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=FlavaTextConfig, hidden_size=37)\n@@ -570,7 +565,6 @@ class FlavaMultimodalModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaMultimodalModel,) if is_torch_available() else ()\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = FlavaMultimodalModelTester(self)\n@@ -684,7 +678,6 @@ class FlavaImageCodebookTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FlavaImageCodebook,) if is_torch_available() else ()\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n \n     def setUp(self):\n@@ -916,87 +909,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        configs_no_init.return_loss = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # FLAVA needs pixel_values\n-\n-                if \"input_ids_masked\" in inputs_dict:\n-                    # For pretraining\n-                    inputs = (input_ids, inputs_dict[\"input_ids_masked\"], pixel_values)\n-                else:\n-                    inputs = (input_ids, pixel_values)\n-\n-                traced_model = torch.jit.trace(model, inputs)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-            # Non persistent buffers won't be in original state dict\n-            loaded_model_state_dict.pop(\"text_model.embeddings.token_type_ids\", None)\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_image_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -1177,7 +1089,6 @@ def _test_model(self, config, inputs, test_image=False, test_text=False):\n class FlavaForPreTrainingTest(FlavaModelTest):\n     all_model_classes = (FlavaForPreTraining,) if is_torch_available() else ()\n     class_for_tester = FlavaForPreTrainingTester\n-    test_torchscript = False\n \n     @unittest.skip(\n         reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\""
        },
        {
            "sha": "0bee1ff464890669c1c4fc41f69935851c138ab4",
            "filename": "tests/models/flex_olmo/test_modeling_flex_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflex_olmo%2Ftest_modeling_flex_olmo.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -48,7 +48,6 @@ class FlexOlmoModelTester(CausalLMModelTester):\n @require_torch\n class FlexOlmoModelTest(CausalLMModelTest, unittest.TestCase):\n     fx_compatible = False\n-    test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = FlexOlmoModelTester\n "
        },
        {
            "sha": "5a4014e60961ddc805206419e3666ae3730764a0",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -297,10 +297,6 @@ def test_inputs_embeds(self):\n     def test_inputs_embeds_matches_input_ids(self):\n         pass\n \n-    @unittest.skip(reason=\"model weights aren't tied in FSMT.\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"TODO: Decoder embeddings cannot be resized at the moment\")\n     def test_resize_embeddings_untied(self):\n         pass"
        },
        {
            "sha": "58753226e05a3a3eb2302955264b1515c4476db2",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -382,7 +382,6 @@ class GitModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n         else {}\n     )\n     fx_compatible = False\n-    test_torchscript = False\n \n     # special case for GitForCausalLM model\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):"
        },
        {
            "sha": "6373e77093fecaff79be0454d2a0a8be703defc0",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -172,7 +172,6 @@ def prepare_config_and_inputs_for_common(self):\n class Glm4vModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Glm4vModel, Glm4vForConditionalGeneration) if is_torch_available() else ()\n \n-    test_torchscript = False\n     model_split_percents = [0.7, 0.9]  # model too big to split at 0.5\n     _is_composite = True\n "
        },
        {
            "sha": "3e2e0074089baca211b57017475687d836ab877f",
            "filename": "tests/models/glm4v_moe/test_modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v_moe%2Ftest_modeling_glm4v_moe.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -183,7 +183,6 @@ def prepare_config_and_inputs_for_common(self):\n class Glm4vMoeModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Glm4vMoeModel, Glm4vMoeForConditionalGeneration) if is_torch_available() else ()\n \n-    test_torchscript = False\n     model_split_percents = [0.7, 0.9]  # model too big to split at 0.5\n     _is_composite = True\n "
        },
        {
            "sha": "f4885cf4d8cd771e9afd41edeafd2d080f686d5e",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -396,7 +396,6 @@ class GPTBigCodeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n     fx_compatible = False\n     test_missing_keys = False\n \n-    test_torchscript = False\n     multi_query = True\n \n     # special case for DoubleHeads model"
        },
        {
            "sha": "026c1f2d6b908c4ab1e8b355bdeb2dd13fc9e516",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -247,7 +247,6 @@ def create_and_check_object_detection_head_model(self, config, pixel_values, pix\n class GroundingDinoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (GroundingDinoModel, GroundingDinoForObjectDetection) if is_torch_available() else ()\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     pipeline_model_mapping = ("
        },
        {
            "sha": "7a656e404d13a747b7c844390994735ac0497d86",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch GroupViT model.\"\"\"\n \n import inspect\n-import os\n import random\n import tempfile\n import unittest\n@@ -29,7 +28,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -143,7 +141,6 @@ class GroupViTVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (GroupViTVisionModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n \n     def setUp(self):\n@@ -564,77 +561,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # GROUPVIT needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "fe774bd88f040953c63fc1e4a021b5bf7d30fe83",
            "filename": "tests/models/ibert/test_modeling_ibert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fibert%2Ftest_modeling_ibert.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -224,7 +224,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class IBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n     test_resize_embeddings = False\n \n     all_model_classes = ("
        },
        {
            "sha": "eba24af6f51b2fe84249faa208bdf9da32a490b8",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -324,8 +324,6 @@ class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMi\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n         # XXX: IdeficsForVisionText2TextTest has no MODEL_FOR group yet, but it should be the same"
        },
        {
            "sha": "097ba36eefad44c925a788bdab9b97ec52a88d06",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -178,7 +178,6 @@ class Idefics2ModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (Idefics2Model,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = True\n     _is_composite = True\n@@ -372,7 +371,6 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     fx_compatible = False\n \n     test_resize_embeddings = True\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Idefics2VisionText2TextModelTester(self)"
        },
        {
            "sha": "3d3f6562f84b8ab976fc9d360642d92f2288693e",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -168,7 +168,6 @@ class Idefics3ModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (Idefics3Model,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = True\n \n@@ -337,7 +336,6 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     fx_compatible = False\n \n     test_resize_embeddings = True\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Idefics3VisionText2TextModelTester(self)"
        },
        {
            "sha": "43c8ff471e03f30ebac42dfc27d72d46dcdd23fb",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -196,7 +196,6 @@ class InformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     is_encoder_decoder = True\n \n     test_missing_keys = False\n-    test_torchscript = False\n     test_inputs_embeds = False\n \n     def setUp(self):"
        },
        {
            "sha": "95e2b763ef3829a501b6ee9f63af4122dd275b47",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -479,7 +479,6 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "211fcd3cff438a96ba2beb84dc20ca645224fc2b",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -494,7 +494,6 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "734a7ea97be629fc750031c76ad215e5087d02bc",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 131,
            "changes": 131,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -15,7 +15,6 @@\n \n import copy\n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -44,7 +43,6 @@\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -388,55 +386,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    # overwrite from common in order to use `config.text_config.vocab_size` instead of `config.vocab_size`\n-    def test_tie_model_weights(self):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_same_values(layer_1, layer_2):\n-            equal = True\n-            for p1, p2 in zip(layer_1.weight, layer_2.weight):\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    equal = False\n-            return equal\n-\n-        for model_class in self.all_model_classes:\n-            config.torchscript = True\n-            model_not_tied = model_class(config)\n-            if model_not_tied.get_output_embeddings() is None:\n-                continue\n-\n-            config_tied = copy.deepcopy(config)\n-            config_tied.torchscript = False\n-            model_tied = model_class(config_tied)\n-            params_tied = list(model_tied.parameters())\n-            # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # # Check that after modification, they remain the same.\n-            # embeddings.weight.data.div_(2)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(embeddings.weight.shape, decoding.weight.shape)\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # # Check that after modification, they remain the same.\n-            # decoding.weight.data.div_(4)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(embeddings.weight.shape, decoding.weight.shape)\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # Check that after resize they remain tied.\n-            model_tied.resize_token_embeddings(config.text_config.vocab_size + 10)\n-            params_tied_2 = list(model_tied.parameters())\n-            self.assertEqual(len(params_tied_2), len(params_tied))\n-\n-            # decoding.weight.data.mul_(20)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)\n-            # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n     def test_eager_matches_sdpa_inference(\n@@ -487,86 +436,6 @@ def test_model_from_pretrained(self):\n         model = Kosmos2Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            main_input_name = model_class.main_input_name\n-\n-            try:\n-                main_input = inputs[main_input_name]\n-                model(main_input, inputs[\"input_ids\"], inputs[\"image_embeds_position_mask\"])\n-                traced_model = torch.jit.trace(\n-                    model, (main_input, inputs[\"input_ids\"], inputs[\"image_embeds_position_mask\"])\n-                )\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     @pytest.mark.generate\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n     def test_generate_from_inputs_embeds(self, _, num_beams):"
        },
        {
            "sha": "994bd4c393533ee2d5590bb35480630049ab7a63",
            "filename": "tests/models/kosmos2_5/test_modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_modeling_kosmos2_5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -452,55 +452,6 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n-    # overwrite from common in order to use `config.text_config.vocab_size` instead of `config.vocab_size`\n-    def test_tie_model_weights(self):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_same_values(layer_1, layer_2):\n-            equal = True\n-            for p1, p2 in zip(layer_1.weight, layer_2.weight):\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    equal = False\n-            return equal\n-\n-        for model_class in self.all_model_classes:\n-            config.torchscript = True\n-            model_not_tied = model_class(config)\n-            if model_not_tied.get_output_embeddings() is None:\n-                continue\n-\n-            config_tied = copy.deepcopy(config)\n-            config_tied.torchscript = False\n-            model_tied = model_class(config_tied)\n-            params_tied = list(model_tied.parameters())\n-            # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # # Check that after modification, they remain the same.\n-            # embeddings.weight.data.div_(2)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(embeddings.weight.shape, decoding.weight.shape)\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # # Check that after modification, they remain the same.\n-            # decoding.weight.data.div_(4)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(embeddings.weight.shape, decoding.weight.shape)\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # Check that after resize they remain tied.\n-            model_tied.resize_token_embeddings(config.text_config.vocab_size + 10)\n-            params_tied_2 = list(model_tied.parameters())\n-            self.assertEqual(len(params_tied_2), len(params_tied))\n-\n-            # decoding.weight.data.mul_(20)\n-            # # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)\n-            # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"microsoft/kosmos-2.5\""
        },
        {
            "sha": "f009f12d0df433c159978115f53968d7d493379d",
            "filename": "tests/models/kyutai_speech_to_text/test_modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkyutai_speech_to_text%2Ftest_modeling_kyutai_speech_to_text.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -296,10 +296,6 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @pytest.mark.skip(reason=\"Moshi ASR has custom embedding approach (text and audio embeddings).\")\n     def test_resize_embeddings_untied(self):\n         pass"
        },
        {
            "sha": "fda76cf864efd311f91134f21862da003ccff69f",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -265,7 +265,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n @require_detectron2\n class LayoutLMv2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = True\n     test_mismatched_shapes = False\n \n     all_model_classes = (\n@@ -473,7 +472,7 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                     single_batch_shape = value.shape[0] // batch_size\n                     single_row_input[key] = value[:single_batch_shape]\n                 elif hasattr(value, \"tensor\"):\n-                    # layoutlmv2uses ImageList instead of pixel values (needs for torchscript)\n+                    # layoutlmv2uses ImageList instead of pixel values\n                     single_row_input[key] = value.tensor[:single_batch_shape]\n \n             with torch.no_grad():"
        },
        {
            "sha": "1c7698342929a5c99592a37f44d9238e1f15f813",
            "filename": "tests/models/layoutlmv3/test_modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -270,7 +270,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class LayoutLMv3ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n     test_mismatched_shapes = False\n \n     all_model_classes = ("
        },
        {
            "sha": "97ad98035af1536af454b2d7a433276af71355e1",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -284,7 +284,6 @@ class LEDModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     is_encoder_decoder = True\n \n     test_missing_keys = False\n-    test_torchscript = False\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "88560bdb3dac56f669fe7a2910aa14f5342aadfb",
            "filename": "tests/models/levit/test_modeling_levit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -182,7 +182,6 @@ class LevitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     has_attentions = False\n "
        },
        {
            "sha": "fe5d03afbc84ad95d1d1cf2acb2a2b7398a666f3",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -304,8 +304,6 @@ def prepare_config_and_inputs_for_question_answering(self):\n \n @require_torch\n class LongformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n-\n     all_model_classes = (\n         (\n             LongformerModel,"
        },
        {
            "sha": "3ec760ad342feb67ee09d71b10f996602414621f",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -508,7 +508,6 @@ class LongT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     )\n     fx_compatible = False\n \n-    test_torchscript = True\n     test_resize_embeddings = True\n     is_encoder_decoder = True\n \n@@ -1001,7 +1000,6 @@ def prepare_config_and_inputs_for_common(self):\n class LongT5EncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (LongT5EncoderModel,) if is_torch_available() else ()\n \n-    test_torchscript = True\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "f98ef0070f4a7f926279bbcd030c92e0525006a9",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -614,7 +614,6 @@ class LukeModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = True\n \n     # TODO: Fix the failed tests"
        },
        {
            "sha": "234fe380e1f9eae8a023a23196456d1126b6282b",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -531,8 +531,6 @@ class LxmertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     fx_compatible = True\n \n-    test_torchscript = False\n-\n     # overwrite function because qa models takes different input label shape\n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)"
        },
        {
            "sha": "6d8dc3b82670bc0dc2177c651ed5f69d2c22767b",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -236,7 +236,6 @@ class MambaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     all_model_classes = (MambaModel, MambaForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @ArthurZucker\n-    test_torchscript = False  # FIXME let's try to support this @ArthurZucker\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "ae96d256334c37cdb0e7a0162ab49c8b8863759f",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -239,7 +239,6 @@ class Mamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMix\n     all_model_classes = (Mamba2Model, Mamba2ForCausalLM) if is_torch_available() else ()\n     has_attentions = False  # Mamba does not support attentions\n     fx_compatible = False  # FIXME let's try to support this @molbap\n-    test_torchscript = False  # FIXME I think this should be doable @molbap @ArthurZucker\n     test_missing_keys = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "dcb58e9310537e163116a7a0728eb3fe13d6028e",
            "filename": "tests/models/maskformer/test_modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer_swin.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -175,7 +175,6 @@ class MaskFormerSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Te\n     )\n     pipeline_model_mapping = {\"feature-extraction\": MaskFormerSwinModel} if is_torch_available() else {}\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = False\n     test_torch_exportable = True"
        },
        {
            "sha": "b7a39687f7c39593744fa2140651d264c5e4c73b",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 73,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch MetaClip2 model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -38,7 +37,6 @@\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     is_flaky,\n@@ -572,77 +570,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # MetaClip2 needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "c25d84e650bb1b8a6a443c707f28da163c8d1f37",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 102,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch Mimi model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -35,7 +34,7 @@\n )\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n \n \n if is_torch_available():\n@@ -165,7 +164,6 @@ class MimiModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does support returning hidden states\n@@ -213,105 +211,6 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @unittest.skip(reason=\"The MimiModel does not have the usual `attention` logic\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"The MimiModel does not have the usual `hidden_states` logic\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    # Copied from transformers.tests.encodec.test_modeling_encodec.MimiModelTest._create_and_check_torchscript\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            main_input_name = model_class.main_input_name\n-\n-            try:\n-                main_input = inputs[main_input_name]\n-                model(main_input)\n-                traced_model = torch.jit.trace(model, main_input)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(reason=\"The MimiModel does not have the usual `attention` logic\")\n     def test_attention_outputs(self):\n         pass"
        },
        {
            "sha": "c88ab4f3aa8d1356cc3e7ceb474342fa3485f6a4",
            "filename": "tests/models/mlcd/test_modeling_mlcd.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmlcd%2Ftest_modeling_mlcd.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -124,7 +124,6 @@ class MLCDVisionModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (MLCDVisionModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "537b1c1066f6c1f752022bdf8a6eaf5ffd8043b7",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -279,7 +279,6 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n     )\n     pipeline_model_mapping = {\"image-text-to-text\": MllamaForConditionalGeneration} if is_torch_available() else ()\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "dc6a6d5b39b4a1db3b77cf3f9feaf9f9e874d5d0",
            "filename": "tests/models/mm_grounding_dino/test_modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmm_grounding_dino%2Ftest_modeling_mm_grounding_dino.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -249,7 +249,6 @@ def create_and_check_object_detection_head_model(self, config, pixel_values, pix\n class MMGroundingDinoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (MMGroundingDinoModel, MMGroundingDinoForObjectDetection) if is_torch_available() else ()\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     pipeline_model_mapping = ("
        },
        {
            "sha": "6fc707dc6324f9c9f4cd9edfcb5ecc9392b56e0d",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -236,8 +236,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class ModernBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n-\n     all_model_classes = (\n         (\n             ModernBertModel,"
        },
        {
            "sha": "e8f527cbc7a76ed43b3f9d61df3955702ea16f9c",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -148,14 +148,6 @@ def setUp(self):\n         self.model_tester = MoonshineModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=MoonshineConfig)\n \n-    @unittest.skip(\"failing. Will fix only when the community opens an issue for it.\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @unittest.skip(\"failing. Will fix only when the community opens an issue for it.\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "b274036b6efa6b2f035046dffb8852c6422059d8",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -533,7 +533,6 @@ class MoshiTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (MoshiForConditionalGeneration,) if is_torch_available() else ()\n     # training is not supported yet for Moshi\n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = MoshiTester(self)"
        },
        {
            "sha": "b30ac90276a43e66c8d3c3dc71ad2241482997da",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -353,7 +353,6 @@ class MptModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     fx_compatible = False\n     test_missing_keys = False\n \n-    test_torchscript = False\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": MptModel,"
        },
        {
            "sha": "a3cb330c0a3cb3b2c0c3b519d716cf8614670e82",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -261,7 +261,6 @@ class MraModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else ()\n     )\n \n-    test_torchscript = False\n     has_attentions = False\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "baba4181f7308bb0fc8f8e2098f97da5002de9ca",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -267,10 +267,6 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_model_outputs_equivalence(self):\n         pass\n \n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass\n@@ -574,9 +570,6 @@ class MusicgenTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin,\n     additional_model_inputs = [\"decoder_input_ids\"]\n     # training is not supported yet for MusicGen\n     test_resize_embeddings = False\n-    # not to test torchscript as the model tester doesn't prepare `input_values` and `padding_mask`\n-    # (and `torchscript` hates `None` values).\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):\n@@ -772,10 +765,6 @@ def test_gradient_checkpointing_backward_compatibility(self):\n             model = model_class(config)\n             self.assertTrue(model.is_gradient_checkpointing)\n \n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass"
        },
        {
            "sha": "564cb661e527202e2ffbe3eb2a6d137bb79b6857",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -276,10 +276,6 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_model_outputs_equivalence(self):\n         pass\n \n-    @unittest.skip(reason=\"this model has multiple inputs embeds and lm heads that should not be tied\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"this model has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass\n@@ -595,9 +591,6 @@ class MusicgenMelodyTest(ModelTesterMixin, GenerationTesterMixin, PipelineTester\n     additional_model_inputs = [\"decoder_input_ids\"]\n     # training is not supported yet for MusicGen\n     test_resize_embeddings = False\n-    # not to test torchscript as the model tester doesn't prepare `input_features` and `padding_mask`\n-    # (and `torchscript` hates `None` values).\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):\n@@ -776,10 +769,6 @@ def test_gradient_checkpointing_backward_compatibility(self):\n             model = model_class(config)\n             self.assertTrue(model.is_gradient_checkpointing)\n \n-    @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied.\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"MusicGen has multiple inputs embeds and lm heads that should not be tied\")\n     def test_tied_weights_keys(self):\n         pass"
        },
        {
            "sha": "e2085ca4ffa5730ebaa1cc98ee627fb94d86cef8",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -245,7 +245,6 @@ class NllbMoeModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     fx_compatible = False\n \n     test_missing_keys = True\n-    test_torchscript = False\n \n     # TODO: Fix the failed tests when this model gets more usage\n     def is_pipeline_test_to_skip("
        },
        {
            "sha": "80ad08b5a1720452f2de375d079e25abcbff957f",
            "filename": "tests/models/olmo3/test_modeling_olmo3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -53,7 +53,6 @@ class Olmo3ModelTester(CausalLMModelTester):\n @require_torch\n class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n     fx_compatible = False\n-    test_torchscript = False\n     test_all_params_have_gradient = False\n     model_tester_class = Olmo3ModelTester\n "
        },
        {
            "sha": "d99cc71d7c626584805e27a40cfcafc269286293",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -232,22 +232,6 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_inputs_embeds(self):\n         pass\n \n-    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n-    def test_torchscript_output_hidden_states(self):\n-        pass\n-\n-    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n-    @unittest.skip(reason=\"OmDet-Turbo does not have 'input_ids' and 'attention_mask'\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n     def test_resize_tokens_embeddings(self):\n         # rewrite as OmDet-Turbo does not have \"input_ids\" and \"decoder_input_ids\"\n         ("
        },
        {
            "sha": "abc7cdea33a86f22856bbf50fcafb38b76a26752",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -280,18 +280,6 @@ def test_model_main_input_name(self):\n             observed_main_input_name = list(model_signature.parameters.keys())[1:3]\n             self.assertEqual(model_class.main_input_name, observed_main_input_name)\n \n-    @unittest.skip(reason=\"OneFormer uses two main inputs\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n-    @unittest.skip(reason=\"OneFormer uses two main inputs\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"OneFormer uses two main inputs\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n     @unittest.skip(reason=\"OneFormer does not use inputs_embeds\")\n     def test_inputs_embeds(self):\n         pass"
        },
        {
            "sha": "36853299f76684804ee8cf0f7b78f244b1fc2570",
            "filename": "tests/models/ovis2/test_modeling_ovis2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fovis2%2Ftest_modeling_ovis2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -176,8 +176,6 @@ class Ovis2ModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase)\n     pipeline_model_mapping = {\"image-text-to-text\": Ovis2ForConditionalGeneration} if is_torch_available() else {}\n     _is_composite = True\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = Ovis2VisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Ovis2Config, has_text_modality=False)"
        },
        {
            "sha": "68c5a6d3f0fd0f0515c9bb29d0f4178b834d1ff2",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch Owlv2 model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -35,7 +34,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -458,73 +456,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init).to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # OWLV2 needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            loaded_model = loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -667,73 +598,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init).to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # OWLV2 needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            loaded_model = loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"google/owlv2-base-patch16-ensemble\""
        },
        {
            "sha": "b919bfb47807cd4aa34797d95842099f62a7d5ab",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch OwlViT model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -35,7 +34,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -453,73 +451,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init).to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # OWLVIT needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            loaded_model = loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -660,73 +591,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init).to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # OWLVIT needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            loaded_model = loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @slow\n     def test_model_from_pretrained(self):\n         model_name = \"google/owlvit-base-patch32\""
        },
        {
            "sha": "15570bbc9e272d98bdaaf0f571dd197e1a30de5b",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -192,7 +192,6 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     additional_model_inputs = [\"token_type_ids\"]\n     fx_compatible = False\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "46914326712a47195c12f24dce65fd388744fc80",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -171,7 +171,6 @@ class PaliGemma2ForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     fx_compatible = False\n \n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "a4e245dc85e110a1b2e2d95e1af80917056ebcca",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -222,7 +222,6 @@ class PatchTSMixerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n     is_encoder_decoder = False\n \n     test_missing_keys = False\n-    test_torchscript = False\n     test_inputs_embeds = False\n \n     test_resize_embeddings = True"
        },
        {
            "sha": "13feefcc207faed7441b456c268fb62200d9e08f",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -162,7 +162,6 @@ class PatchTSTModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     is_encoder_decoder = False\n \n     test_missing_keys = True\n-    test_torchscript = False\n     test_inputs_embeds = False\n \n     test_resize_embeddings = True"
        },
        {
            "sha": "fb44c919e1c121f0807f2da4c58e9f37d1433095",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -220,12 +220,6 @@ def setUp(self):\n         self.model_tester = PegasusXModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=PegasusXConfig)\n \n-    @unittest.skip(\n-        \"`PegasusXGlobalLocalAttention` returns attentions as dictionary - not compatible with torchscript \"\n-    )\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "41dd1fa516cc2aabfffa1c7decabe6464d0c817f",
            "filename": "tests/models/perceiver/test_modeling_perceiver.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -306,8 +306,6 @@ class PerceiverModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCas\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     maxDiff = None\n \n     def setUp(self):"
        },
        {
            "sha": "eddc28d7a672f5724c82c00c1b79df78fb334cc3",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -15,7 +15,6 @@\n \n import copy\n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -30,7 +29,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -414,7 +412,6 @@ class Pix2StructModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTeste\n \n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = Pix2StructModelTester(self)\n@@ -619,77 +616,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                flattened_patches = inputs_dict[\"flattened_patches\"]  # Pix2Struct needs flattened_patches\n-                traced_model = torch.jit.trace(model, (input_ids, flattened_patches))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "8634073360927644ef43b001e2d598f58ed12d57",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -111,7 +111,6 @@ class PixtralVisionModelModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (PixtralVisionModel,) if is_torch_available() else ()\n     additional_model_inputs = [\"image_sizes\"]\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "78e09fdf3c899a66ae9f6be42a6c10f8c0d6ed2b",
            "filename": "tests/models/poolformer/test_modeling_poolformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -127,7 +127,6 @@ class PoolFormerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n     )\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "d0f71dcad8007818e1cdab2b4cc3b1ecda728aa3",
            "filename": "tests/models/pvt/test_modeling_pvt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -144,7 +144,6 @@ class PvtModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "a9ef785f9228f6011e692012bd4968b2c736014f",
            "filename": "tests/models/pvt_v2/test_modeling_pvt_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -150,7 +150,6 @@ class PvtV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "ef89ab0c301b726389c1e3b25f4ce30c0667a41d",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -603,7 +603,6 @@ class ReformerLocalAttnModelTest(ReformerTesterMixin, GenerationTesterMixin, Mod\n         else ()\n     )\n \n-    test_torchscript = False\n     test_sequence_classification_problem_types = True\n \n     def setUp(self):\n@@ -745,8 +744,6 @@ class ReformerLSHAttnModelTest(\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     # TODO: Fix the failed tests\n     def is_pipeline_test_to_skip(\n         self,"
        },
        {
            "sha": "8bc971c06f63a398bb9218834bc5b829b72f89d8",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -258,7 +258,6 @@ class RTDetrModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     test_torch_exportable = True"
        },
        {
            "sha": "21769fd82049a8a328d90f403d3cebb9131bb1c7",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -262,7 +262,6 @@ class RTDetrV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     test_torch_exportable = True"
        },
        {
            "sha": "f472292a9d5c7505808ab43483e86f1c93b1445f",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -161,7 +161,6 @@ class SamVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     def setUp(self):\n@@ -516,7 +515,6 @@ class SamModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     _is_composite = True\n \n     # TODO: Fix me @Arthur: `run_batch_test` in `tests/test_pipeline_mixin.py` not working"
        },
        {
            "sha": "647b9f7a7dffb78fa23ad8e373c6bdcc681f5775",
            "filename": "tests/models/sam2/test_modeling_sam2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam2%2Ftest_modeling_sam2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -144,7 +144,6 @@ class Sam2VisionModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     def setUp(self):\n@@ -467,7 +466,6 @@ class Sam2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):"
        },
        {
            "sha": "300317c12f1880746243cc79736bfe9b2d564c69",
            "filename": "tests/models/sam_hq/test_modeling_sam_hq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam_hq%2Ftest_modeling_sam_hq.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -169,7 +169,6 @@ class SamHQVisionModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     def setUp(self):\n@@ -548,7 +547,6 @@ class SamHQModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_cpu_offload = False\n     test_disk_offload_bin = False\n     test_disk_offload_safetensors = False"
        },
        {
            "sha": "27363fb4a61a17e666aee6c95a38d9d2acd444b0",
            "filename": "tests/models/seamless_m4t/test_modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_modeling_seamless_m4t.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -340,7 +340,6 @@ class SeamlessM4TModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase):\n     test_missing_keys = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     all_model_classes = (\n         (\n@@ -534,7 +533,6 @@ class SeamlessM4TModelWithTextInputTest(ModelTesterMixin, PipelineTesterMixin, u\n     test_missing_keys = False\n \n     test_resize_embeddings = True\n-    test_torchscript = False\n \n     all_model_classes = (\n         ("
        },
        {
            "sha": "97fa54188d342517a8653fb154435894f5d92585",
            "filename": "tests/models/seamless_m4t_v2/test_modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t_v2%2Ftest_modeling_seamless_m4t_v2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -366,7 +366,6 @@ class SeamlessM4Tv2ModelWithSpeechInputTest(ModelTesterMixin, unittest.TestCase)\n     test_missing_keys = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     all_model_classes = (\n         (\n@@ -559,7 +558,6 @@ class SeamlessM4Tv2ModelWithTextInputTest(ModelTesterMixin, unittest.TestCase):\n     test_missing_keys = False\n \n     test_resize_embeddings = True\n-    test_torchscript = False\n \n     all_model_classes = (\n         ("
        },
        {
            "sha": "603cca98ab616d2db3d0ad4d43540b363b9ca6b5",
            "filename": "tests/models/seggpt/test_modeling_seggpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -170,7 +170,6 @@ class SegGptModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     pipeline_model_mapping = ("
        },
        {
            "sha": "fe8bff0e37e9afcc90cb4ef2187de1a4f813fea9",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -304,8 +304,6 @@ class SEWDModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = SEWDModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=SEWDConfig, hidden_size=37)"
        },
        {
            "sha": "9c7a4d0fe199e580ba36f28fffcdccae7df0a0a3",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch SigLIP model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -38,7 +37,6 @@\n from ...test_modeling_common import (\n     TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -487,78 +485,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    # Copied from tests.models.clip.test_modeling_clip.CLIPModelTest._create_and_check_torchscript with CLIP->Siglip\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # Siglip needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     # Copied from tests.models.clip.test_modeling_clip.CLIPModelTest.test_load_vision_text_config with CLIP->Siglip\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e9f4efc5f1ae973cad3d9235413450b3e77255ab",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -168,7 +168,6 @@ class SmolVLMModelTest(ModelTesterMixin, unittest.TestCase):\n \n     all_model_classes = (SmolVLMModel,) if is_torch_available() else ()\n     fx_compatible = False\n-    test_torchscript = False\n \n     test_resize_embeddings = True\n \n@@ -334,7 +333,6 @@ class SmolVLMForConditionalGenerationModelTest(GenerationTesterMixin, ModelTeste\n     fx_compatible = False\n \n     test_resize_embeddings = True\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = SmolVLMVisionText2TextModelTester(self)"
        },
        {
            "sha": "40d6927db58158c2769301b28fd3e83e95318dfd",
            "filename": "tests/models/speech_to_text/test_modeling_speech_to_text.py",
            "status": "modified",
            "additions": 1,
            "deletions": 78,
            "changes": 79,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_speech_to_text.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -15,7 +15,6 @@\n \n import copy\n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -34,7 +33,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -602,82 +601,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            try:\n-                model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                input_features = inputs[\"input_features\"]\n-                attention_mask = inputs[\"attention_mask\"]\n-                decoder_input_ids = inputs[\"decoder_input_ids\"]\n-                decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                traced_model = torch.jit.trace(\n-                    model, (input_features, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                )\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     @unittest.skip(reason=\"Test failing,  @RocketNight is looking into it\")\n     def test_tf_from_pt_safetensors(self):\n         pass"
        },
        {
            "sha": "8d608ce0ff82336c221cf02d81b0a5809ba8dd51",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -203,21 +203,6 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @slow\n-    @unittest.skip(reason=\"Model does not have decoder_input_ids\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model does not have decoder_input_ids\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model does not have decoder_input_ids\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n \n @require_torch\n class SpeechT5ForSpeechToTextTester:\n@@ -943,22 +928,6 @@ def test_save_load(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_simple(self):\n-        # disabled because this model doesn't have decoder_input_ids\n-        pass\n-\n     @unittest.skip(reason=\"training is not supported yet\")\n     def test_training(self):\n         pass\n@@ -1616,21 +1585,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_save_load(self):\n         pass\n \n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    @slow\n-    @unittest.skip(reason=\"Model doesn't have decoder_input_ids\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n     @unittest.skip(reason=\"Training is not supported yet\")\n     def test_training(self):\n         pass\n@@ -1739,7 +1693,6 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class SpeechT5HifiGanTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (SpeechT5HifiGan,) if is_torch_available() else ()\n-    test_torchscript = False\n \n     test_resize_embeddings = False\n     test_resize_position_embeddings = False"
        },
        {
            "sha": "89893aff10eb7b044c00b7e548d8ecc16d075065",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -169,7 +169,6 @@ class Swin2SRModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     def setUp(self):"
        },
        {
            "sha": "30c07bee291da5e4cf53fd8e470a8823c3a4d56a",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -570,7 +570,6 @@ class SwitchTransformersModelTest(ModelTesterMixin, GenerationTesterMixin, Pipel\n \n     test_resize_embeddings = True\n     is_encoder_decoder = True\n-    test_torchscript = False\n     # The small SWITCH_TRANSFORMERS model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n     # `SwitchTransformers` is a MOE in which not all experts will get gradients because they are not all used in a single forward pass\n@@ -821,7 +820,6 @@ class SwitchTransformersEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase\n     test_resize_embeddings = False\n     test_model_parallel = False\n     test_head_masking = False\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = SwitchTransformersEncoderOnlyModelTester(self)"
        },
        {
            "sha": "7b167673ce0a0e46cd8fe0643115871f50eb2265",
            "filename": "tests/models/t5gemma/test_modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5gemma%2Ftest_modeling_t5gemma.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -601,7 +601,6 @@ class T5GemmaModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMi\n     _torch_compile_train_attn_implementation = \"eager\"\n \n     # won't fix\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = T5GemmaModelTester(self)\n@@ -1456,7 +1455,6 @@ class T5GemmaEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n \n     # won't fix\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = T5GemmaEncoderOnlyModelTester(self)"
        },
        {
            "sha": "a11af57cd660a9b888557b7b0a63607627de7cc0",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -202,7 +202,6 @@ class TableTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.\n         else {}\n     )\n     is_encoder_decoder = True\n-    test_torchscript = False\n \n     test_missing_keys = False\n     zero_init_hidden_state = True"
        },
        {
            "sha": "2aba8c17303ad7dc46a46ed591d8b185ffef99de",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -183,7 +183,6 @@ class TimeSeriesTransformerModelTest(ModelTesterMixin, PipelineTesterMixin, unit\n     is_encoder_decoder = True\n \n     test_missing_keys = False\n-    test_torchscript = False\n     test_inputs_embeds = False\n \n     def setUp(self):"
        },
        {
            "sha": "91a0ec1a2f4f8096d5838c41739c82f5dd0c7b95",
            "filename": "tests/models/timesformer/test_modeling_timesformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -163,7 +163,6 @@ class TimesformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestC\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "665bc19a3f668e4a250b1d8d0041e99df9605232",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -167,10 +167,6 @@ def test_can_load_with_global_device_set(self):\n     def test_cannot_load_with_meta_device_context_manager(self):\n         pass\n \n-    @unittest.skip(reason=\"model weights aren't tied in TimmBackbone.\")\n-    def test_tie_model_weights(self):\n-        pass\n-\n     @unittest.skip(reason=\"Only checkpoints on timm can be loaded into TimmBackbone\")\n     def test_load_save_without_tied_weights(self):\n         pass\n@@ -183,10 +179,6 @@ def test_model_weights_reload_no_missing_tied_weights(self):\n     def test_channels(self):\n         pass\n \n-    @unittest.skip(reason=\"TimmBackbone doesn't support output_attentions.\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n     @unittest.skip(reason=\"Safetensors is not supported by timm.\")\n     def test_can_use_safetensors(self):\n         pass"
        },
        {
            "sha": "0aefe9e51deec31a97226bd48efceca72902851c",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -145,10 +145,6 @@ def test_inputs_embeds(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @unittest.skip(reason=\"TimmWrapper doesn't support output_attentions=True.\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n     @unittest.skip(reason=\"TimmWrapper doesn't support this.\")\n     def test_retain_grad_hidden_states_attentions(self):\n         pass"
        },
        {
            "sha": "7647ab9b55a288311e401598f01769c5cfcf84c6",
            "filename": "tests/models/tvp/test_modeling_tvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -176,7 +176,6 @@ class TVPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     # TODO: Enable this once this model gets more usage\n-    test_torchscript = False\n \n     def setUp(self):\n         self.model_tester = TVPModelTester(self)"
        },
        {
            "sha": "cf80324a4e7780b46c21fac828e46acdbf4b0ca5",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -275,7 +275,6 @@ class UdopModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     )\n     fx_compatible = False\n \n-    test_torchscript = False\n     test_resize_embeddings = True\n     is_encoder_decoder = True\n     test_cpu_offload = False\n@@ -546,7 +545,6 @@ def create_and_check_model_fp16_forward(\n class UdopEncoderOnlyModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (UdopEncoderModel,) if is_torch_available() else ()\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n \n     def setUp(self):"
        },
        {
            "sha": "987b742cfb19d16cfa5f00c2e44bb85b72cdfc7a",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -252,7 +252,6 @@ class UMT5ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin\n     fx_compatible = False\n \n     test_missing_keys = True\n-    test_torchscript = True\n     # The small UMT5 model needs higher percentages for CPU/MP tests\n     model_split_percents = [0.5, 0.8, 0.9]\n "
        },
        {
            "sha": "dc4b64e4d83cfff8ce29218b2cce2707ac38e547",
            "filename": "tests/models/unispeech_sat/test_modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -363,8 +363,6 @@ class UniSpeechSatModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = UniSpeechSatModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=UniSpeechSatConfig, hidden_size=37)\n@@ -537,8 +535,6 @@ class UniSpeechSatRobustModelTest(ModelTesterMixin, unittest.TestCase):\n         else ()\n     )\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = UniSpeechSatModelTester(\n             self, conv_stride=(3, 3, 3), feat_extract_norm=\"layer\", do_stable_layer_norm=True"
        },
        {
            "sha": "91ec0a78ec15a3bf81b50cd94e93975d8f85fce3",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -101,7 +101,6 @@ def prepare_config_and_inputs_for_common(self):\n class UnivNetModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (UnivNetModel,) if is_torch_available() else ()\n     # UnivNetModel currently cannot be traced with torch.jit.trace.\n-    test_torchscript = False\n     # The UnivNetModel is not a transformer and does not use any attention mechanisms, so skip transformer/attention\n     # related tests.\n "
        },
        {
            "sha": "341587f9a845208877178c8028cde4448c17cbf6",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -152,7 +152,6 @@ class UperNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     fx_compatible = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n     test_torch_exportable = True\n     test_torch_exportable_strictly = get_torch_major_and_minor_version() != \"2.7\""
        },
        {
            "sha": "2c07a74d7d551fcb688435b502bff41c5574ed1e",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -195,7 +195,6 @@ class VideoMAEModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase\n     # Addition keys that are required for forward, used in tests where we manipulate and create new input dict from scratch\n     additional_model_inputs = [\"bool_masked_pos\"]\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "207414f2bd72c0af105e850357d7bfc25a7ad876",
            "filename": "tests/models/vilt/test_modeling_vilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -230,7 +230,6 @@ class ViltModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     model_split_percents = [0.5, 0.8, 0.9]\n \n     # ViltForMaskedLM, ViltForQuestionAnswering and ViltForImagesAndTextClassification require special treatment"
        },
        {
            "sha": "67fe5af6e8c0a1b30c365207379f37ed92af7a7b",
            "filename": "tests/models/visual_bert/test_modeling_visual_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -312,7 +312,6 @@ class VisualBertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCa\n         else ()\n     )\n     pipeline_model_mapping = {\"feature-extraction\": VisualBertModel} if is_torch_available() else {}\n-    test_torchscript = False\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = copy.deepcopy(inputs_dict)"
        },
        {
            "sha": "caf2f4dcba59101f2fed8da72acc3b75014dab55",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -179,7 +179,6 @@ class ViTMAEModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (ViTMAEModel, ViTMAEForPreTraining) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-feature-extraction\": ViTMAEModel} if is_torch_available() else {}\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "d67afe1b6dcd449287f59a424e6588647e3fe82c",
            "filename": "tests/models/vit_msn/test_modeling_vit_msn.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -161,7 +161,6 @@ class ViTMSNModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "46b417f04b00d55d9811a90b7b6d2345f29a3459",
            "filename": "tests/models/vits/test_modeling_vits.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -162,7 +162,6 @@ class VitsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     is_encoder_decoder = False\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     has_attentions = False\n \n     def setUp(self):"
        },
        {
            "sha": "827b133e6c26ec93017532b8f521b9e5eff66245",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -171,7 +171,6 @@ class VivitModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else {}\n     )\n \n-    test_torchscript = False\n     test_resize_embeddings = False\n     test_torch_exportable = True\n "
        },
        {
            "sha": "966b2c50d7b8ce5166c48c4e5d1d316a67728d41",
            "filename": "tests/models/wav2vec2_bert/test_modeling_wav2vec2_bert.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_modeling_wav2vec2_bert.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -419,8 +419,6 @@ class Wav2Vec2BertModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.Test\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = Wav2Vec2BertModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Wav2Vec2BertConfig, hidden_size=37)"
        },
        {
            "sha": "ba0752927521d5049e99f4a90a4a6eeb0e532be7",
            "filename": "tests/models/wav2vec2_conformer/test_modeling_wav2vec2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_conformer%2Ftest_modeling_wav2vec2_conformer.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -410,8 +410,6 @@ class Wav2Vec2ConformerModelTest(ModelTesterMixin, PipelineTesterMixin, unittest\n         else {}\n     )\n \n-    test_torchscript = False\n-\n     def setUp(self):\n         self.model_tester = Wav2Vec2ConformerModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=Wav2Vec2ConformerConfig, hidden_size=37)"
        },
        {
            "sha": "8fcd66392f0947efc9861f95d99e62bfa076b1ae",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 100,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -45,7 +45,7 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -843,90 +843,6 @@ def test_resize_embeddings_untied(self):\n     def test_generate_without_input_ids(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init._attn_implementation = \"eager\"\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            try:\n-                model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                input_features = inputs[\"input_features\"]\n-                decoder_input_ids = inputs[\"decoder_input_ids\"]\n-                decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                # prepare `attention_mask` with shape (batch_size, sequence_length)\n-                attention_mask = torch.ones(\n-                    input_features.shape[0],\n-                    input_features.shape[-1],\n-                    device=input_features.device,\n-                    dtype=input_features.dtype,\n-                )\n-                traced_model = torch.jit.trace(\n-                    model, (input_features, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                )\n-\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_mask_feature_prob(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.mask_feature_prob = 0.2\n@@ -1311,21 +1227,6 @@ def test_generate_compile_model_forward_fullgraph(self):\n     def test_generate_compilation_all_outputs(self):\n         pass\n \n-    # TODO (cyril): fix me :)\n-    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    # TODO (cyril): fix me :)\n-    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    # TODO (cyril): fix me :)\n-    @unittest.skip(reason=\"Torchscript doesn't work with the new mask creation functions\")\n-    def test_torchscript_simple(self):\n-        pass\n-\n \n @require_torch\n @require_torchaudio"
        },
        {
            "sha": "fc47cc899d944900b1f000e95220b63c99bc0741",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 74,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -14,7 +14,6 @@\n \"\"\"Testing suite for the PyTorch XCLIP model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -35,7 +34,6 @@\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import (\n     ModelTesterMixin,\n-    _config_zero_init,\n     floats_tensor,\n     ids_tensor,\n     random_attention_mask,\n@@ -523,7 +521,6 @@ class XCLIPModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n \n     test_resize_embeddings = False\n     test_attention_outputs = False\n-    test_torchscript = False\n     maxdiff = None\n \n     def setUp(self):\n@@ -560,77 +557,6 @@ def test_model_get_set_embeddings(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            try:\n-                input_ids = inputs_dict[\"input_ids\"]\n-                pixel_values = inputs_dict[\"pixel_values\"]  # X-CLIP needs pixel_values\n-                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict:\n-                if key not in model_state_dict:\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                p2 = loaded_model_state_dict[layer_name]\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n     def test_load_vision_text_config(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "b876680f450c49c8532b4e564137a68b3db80816",
            "filename": "tests/models/xcodec/test_modeling_xcodec.py",
            "status": "modified",
            "additions": 1,
            "deletions": 103,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxcodec%2Ftest_modeling_xcodec.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -16,8 +16,6 @@\n import inspect\n import json\n import math\n-import os\n-import tempfile\n import unittest\n from pathlib import Path\n \n@@ -26,7 +24,7 @@\n from parameterized import parameterized\n \n from tests.test_configuration_common import ConfigTester\n-from tests.test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from transformers import AutoFeatureExtractor, XcodecConfig\n from transformers.testing_utils import (\n     is_torch_available,\n@@ -112,7 +110,6 @@ class XcodecModelTest(ModelTesterMixin, unittest.TestCase):\n     is_encoder_decoder = True\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         # model does not support returning hidden states\n@@ -173,105 +170,6 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n-    @unittest.skip(reason=\"The XcodecModel does not have the usual `attention` logic\")\n-    def test_torchscript_output_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"The XcodecModel does not have the usual `hidden_states` logic\")\n-    def test_torchscript_output_hidden_state(self):\n-        pass\n-\n-    # Copied from transformers.tests.encodec.test_modeling_encodec.XcodecModelTest._create_and_check_torchscript\n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to False\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        configs_no_init.return_dict = False\n-        for model_class in self.all_model_classes:\n-            model = model_class(config=configs_no_init)\n-            model.to(torch_device)\n-            model.eval()\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            main_input_name = model_class.main_input_name\n-\n-            try:\n-                main_input = inputs[main_input_name]\n-                model(main_input)\n-                traced_model = torch.jit.trace(model, main_input)\n-            except RuntimeError:\n-                self.fail(\"Couldn't trace module.\")\n-\n-            with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                try:\n-                    torch.jit.save(traced_model, pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't save module.\")\n-\n-                try:\n-                    loaded_model = torch.jit.load(pt_file_name)\n-                except Exception:\n-                    self.fail(\"Couldn't load module.\")\n-\n-            model.to(torch_device)\n-            model.eval()\n-\n-            loaded_model.to(torch_device)\n-            loaded_model.eval()\n-\n-            model_state_dict = model.state_dict()\n-            loaded_model_state_dict = loaded_model.state_dict()\n-\n-            non_persistent_buffers = {}\n-            for key in loaded_model_state_dict.keys():\n-                if key not in model_state_dict.keys():\n-                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-            loaded_model_state_dict = {\n-                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-            }\n-\n-            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            model_buffers = list(model.buffers())\n-            for non_persistent_buffer in non_persistent_buffers.values():\n-                found_buffer = False\n-                for i, model_buffer in enumerate(model_buffers):\n-                    if torch.equal(non_persistent_buffer, model_buffer):\n-                        found_buffer = True\n-                        break\n-\n-                self.assertTrue(found_buffer)\n-                model_buffers.pop(i)\n-\n-            models_equal = True\n-            for layer_name, p1 in model_state_dict.items():\n-                if layer_name in loaded_model_state_dict:\n-                    p2 = loaded_model_state_dict[layer_name]\n-                    if p1.data.ne(p2.data).sum() > 0:\n-                        models_equal = False\n-\n-            self.assertTrue(models_equal)\n-\n-            # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-            # (Even with this call, there are still memory leak by ~0.04MB)\n-            self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(reason=\"The XcodecModel does not have the usual `attention` logic\")\n     def test_attention_outputs(self):\n         pass"
        },
        {
            "sha": "18e7d2b14ba03436dc1936a744bba4b2771375f0",
            "filename": "tests/models/xlstm/test_modeling_xlstm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlstm%2Ftest_modeling_xlstm.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -151,7 +151,6 @@ class xLSTMModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixi\n     all_generative_model_classes = (xLSTMForCausalLM,) if is_torch_available() else ()\n     has_attentions = False  # xLSTM does not support attentions\n     fx_compatible = False\n-    test_torchscript = False\n \n     pipeline_model_mapping = (\n         {\"feature-extraction\": xLSTMModel, \"text-generation\": xLSTMForCausalLM} if is_torch_available() else {}"
        },
        {
            "sha": "fd558551a65f4c1cfe5bf57768ef20510b367324",
            "filename": "tests/models/yolos/test_modeling_yolos.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -175,7 +175,6 @@ class YolosModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     )\n \n     test_resize_embeddings = False\n-    test_torchscript = False\n     test_torch_exportable = True\n \n     # special case for head model"
        },
        {
            "sha": "8a3a3e67d8bba715e116b64d4f42d1a25666e572",
            "filename": "tests/models/yoso/test_modeling_yoso.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -260,8 +260,6 @@ class YosoModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n         else ()\n     )\n \n-    test_torchscript = False\n-\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": YosoModel,"
        },
        {
            "sha": "c82c1a475c437f688896a5aa222e77c8908fe2cf",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -292,7 +292,6 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class Zamba2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    test_torchscript = False\n     all_model_classes = (\n         (\n             Zamba2Model,"
        },
        {
            "sha": "41d5cce294f8743797bcd15f71304de8a591dc45",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 193,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -20,7 +20,6 @@\n import random\n import re\n import tempfile\n-import unittest\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -122,7 +121,7 @@\n     from torch import nn\n \n     from transformers import MODEL_MAPPING\n-    from transformers.cache_utils import Cache, DynamicCache\n+    from transformers.cache_utils import DynamicCache\n     from transformers.modeling_utils import load_state_dict\n     from transformers.pytorch_utils import id_tensor_storage\n \n@@ -567,7 +566,6 @@ class ModelTesterMixin:\n     model_tester = None\n     all_model_classes = ()\n     fx_compatible = False\n-    test_torchscript = True\n     test_resize_embeddings = True\n     test_resize_position_embeddings = False\n     test_mismatched_shapes = True\n@@ -1351,26 +1349,6 @@ def test_attention_outputs(self):\n                     [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n                 )\n \n-    @unittest.skip(\"many failing tests after #39120. Will fix when the community ask for it.\")\n-    @slow\n-    def test_torchscript_simple(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        self._create_and_check_torchscript(config, inputs_dict)\n-\n-    @unittest.skip(\"many failing tests after #39120. Will fix when the community ask for it.\")\n-    @slow\n-    def test_torchscript_output_attentions(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_attentions = True\n-        self._create_and_check_torchscript(config, inputs_dict)\n-\n-    @unittest.skip(\"many failing tests after #39120. Will fix when the community ask for it.\")\n-    @slow\n-    def test_torchscript_output_hidden_state(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        self._create_and_check_torchscript(config, inputs_dict)\n-\n     # This is copied from `torch/testing/_internal/jit_utils.py::clear_class_registry`\n     def clear_torch_jit_class_registry(self):\n         torch._C._jit_clear_class_registry()\n@@ -1379,144 +1357,6 @@ def clear_torch_jit_class_registry(self):\n         if hasattr(torch.jit._state, \"_clear_class_state\"):\n             torch.jit._state._clear_class_state()\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to `False`\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            for attn_implementation in [\"eager\", \"sdpa\"]:\n-                if attn_implementation == \"sdpa\" and not model_class._supports_sdpa or config.output_attentions:\n-                    continue\n-\n-                configs_no_init._attn_implementation = attn_implementation\n-                model = model_class(config=configs_no_init)\n-                model.to(torch_device)\n-                model.eval()\n-                inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                main_input_name = model_class.main_input_name\n-\n-                try:\n-                    if model.config.is_encoder_decoder:\n-                        model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                        main_input = inputs[main_input_name]\n-                        attention_mask = inputs[\"attention_mask\"]\n-                        decoder_input_ids = inputs[\"decoder_input_ids\"]\n-                        decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                        outputs = model(main_input, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        # `torchscript` doesn't work with outputs containing `Cache` object. However, #35235 makes\n-                        # several models to use `Cache` by default instead of the legacy cache (tuple), and\n-                        # their `torchscript` tests are failing. We won't support them anyway, but we still want to keep\n-                        # the tests for encoder models like `BERT`. So we skip the checks if the model's output contains\n-                        # a `Cache` object.\n-                        if any(isinstance(x, Cache) for x in outputs):\n-                            continue\n-                        traced_model = torch.jit.trace(\n-                            model, (main_input, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        )\n-                    elif \"bbox\" in inputs and \"image\" in inputs:  # LayoutLMv2 requires additional inputs\n-                        input_ids = inputs[\"input_ids\"]\n-                        bbox = inputs[\"bbox\"]\n-                        image = inputs[\"image\"].tensor\n-                        outputs = model(input_ids, bbox, image)\n-                        if any(isinstance(x, Cache) for x in outputs):\n-                            continue\n-                        traced_model = torch.jit.trace(\n-                            model, (input_ids, bbox, image), check_trace=False\n-                        )  # when traced model is checked, an error is produced due to name mangling\n-                    elif \"bbox\" in inputs:  # Bros requires additional inputs (bbox)\n-                        input_ids = inputs[\"input_ids\"]\n-                        bbox = inputs[\"bbox\"]\n-                        outputs = model(input_ids, bbox)\n-                        if any(isinstance(x, Cache) for x in outputs):\n-                            continue\n-                        traced_model = torch.jit.trace(\n-                            model, (input_ids, bbox), check_trace=False\n-                        )  # when traced model is checked, an error is produced due to name mangling\n-                    elif (\n-                        \"pixel_values\" in inputs and \"prompt_pixel_values\" in inputs and \"prompt_masks\" in inputs\n-                    ):  # SegGpt requires additional inputs\n-                        pixel_values = inputs[\"pixel_values\"]\n-                        prompt_pixel_values = inputs[\"prompt_pixel_values\"]\n-                        prompt_masks = inputs[\"prompt_masks\"]\n-                        outputs = model(pixel_values, prompt_pixel_values, prompt_masks)\n-                        if any(isinstance(x, Cache) for x in outputs):\n-                            continue\n-                        traced_model = torch.jit.trace(\n-                            model, (pixel_values, prompt_pixel_values, prompt_masks), check_trace=False\n-                        )  # when traced model is checked, an error is produced due to name mangling\n-                    elif \"Siglip2\" in model_class.__name__:\n-                        outputs = model(**inputs)\n-                        example_inputs = [t for t in inputs.values() if isinstance(t, torch.Tensor)]\n-                        traced_model = torch.jit.trace(model, example_inputs, check_trace=False)\n-                    else:\n-                        main_input = inputs[main_input_name]\n-                        outputs = model(main_input)\n-                        if any(isinstance(x, Cache) for x in outputs):\n-                            continue\n-                        traced_model = torch.jit.trace(model, (main_input,))\n-                except RuntimeError:\n-                    self.fail(\"Couldn't trace module.\")\n-\n-                with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                    try:\n-                        torch.jit.save(traced_model, pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't save module.\")\n-\n-                    try:\n-                        loaded_model = torch.jit.load(pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't load module.\")\n-\n-                model.to(torch_device)\n-                model.eval()\n-\n-                loaded_model.to(torch_device)\n-                loaded_model.eval()\n-\n-                model_state_dict = model.state_dict()\n-                loaded_model_state_dict = loaded_model.state_dict()\n-\n-                non_persistent_buffers = {}\n-                for key in loaded_model_state_dict:\n-                    if key not in model_state_dict:\n-                        non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-                loaded_model_state_dict = {\n-                    key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-                }\n-\n-                self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-                model_buffers = list(model.buffers())\n-                for non_persistent_buffer in non_persistent_buffers.values():\n-                    found_buffer = False\n-                    for i, model_buffer in enumerate(model_buffers):\n-                        if torch.equal(non_persistent_buffer, model_buffer):\n-                            found_buffer = True\n-                            break\n-\n-                    self.assertTrue(found_buffer)\n-                    model_buffers.pop(i)\n-\n-                models_equal = True\n-                for layer_name, p1 in model_state_dict.items():\n-                    if layer_name in loaded_model_state_dict:\n-                        p2 = loaded_model_state_dict[layer_name]\n-                        if p1.data.ne(p2.data).sum() > 0:\n-                            models_equal = False\n-\n-                self.assertTrue(models_equal)\n-\n-                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-                # (Even with this call, there are still memory leak by ~0.04MB)\n-                self.clear_torch_jit_class_registry()\n-\n     def test_torch_fx(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         self._create_and_check_torch_fx_tracing(config, inputs_dict)\n@@ -2249,38 +2089,6 @@ def test_correct_missing_keys(self):\n                     model, loading_info = model_class.from_pretrained(temp_dir_name, output_loading_info=True)\n                     self.assertGreater(len(loading_info[\"missing_keys\"]), 0, model.__class__.__name__)\n \n-    def test_tie_model_weights(self):\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to `False`\")\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_same_values(layer_1, layer_2):\n-            equal = True\n-            for p1, p2 in zip(layer_1.weight, layer_2.weight):\n-                if p1.data.ne(p2.data).sum() > 0:\n-                    equal = False\n-            return equal\n-\n-        for model_class in self.all_model_classes:\n-            config.torchscript = True\n-            model_not_tied = model_class(copy.deepcopy(config))\n-            if model_not_tied.get_output_embeddings() is None:\n-                continue\n-\n-            config_tied = copy.deepcopy(config)\n-            config_tied.torchscript = False\n-            model_tied = model_class(config_tied)\n-            params_tied = list(model_tied.parameters())\n-            # Check that the embedding layer and decoding layer are the same in size and in value\n-            # self.assertTrue(check_same_values(embeddings, decoding))\n-\n-            # Check that after resize they remain tied.\n-            vocab_size = config.get_text_config().vocab_size\n-            model_tied.resize_token_embeddings(vocab_size + 10)\n-            params_tied_2 = list(model_tied.parameters())\n-            self.assertEqual(len(params_tied_2), len(params_tied))\n-\n     def test_can_use_safetensors(self):\n         for model_class in self.all_model_classes:\n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "64bbe9808f0d673ffe9d86be05156d5e7165b772",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -37,7 +37,6 @@\n     \"return_dict\": False,\n     \"output_hidden_states\": True,\n     \"output_attentions\": True,\n-    \"torchscript\": True,\n     \"dtype\": \"float16\",\n     \"tie_word_embeddings\": False,\n     \"is_decoder\": True,"
        },
        {
            "sha": "e99621d2d7ec64ded7e4871ba99c4ecb42e605e9",
            "filename": "tests/utils/test_generic.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Futils%2Ftest_generic.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/tests%2Futils%2Ftest_generic.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_generic.py?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -299,15 +299,6 @@ def test_decorator_torch_export(self):\n         model = self._get_model(config)\n         torch.export.export(model, args=(torch.tensor(10),))\n \n-    def test_decorator_torchscript(self):\n-        \"\"\"Test that the can_return_tuple decorator works with torch.jit.trace.\"\"\"\n-        config = PreTrainedConfig(return_dict=False)\n-        model = self._get_model(config)\n-        inputs = torch.tensor(10)\n-        traced_module = torch.jit.trace(model, inputs)\n-        output = traced_module(inputs)\n-        self.assertIsInstance(output, tuple)\n-\n     def test_attribute_cleanup(self):\n         \"\"\"Test that the `_is_top_level_module` attribute is removed after the forward call.\"\"\"\n "
        },
        {
            "sha": "2f327a23634b59312cfc4107f654151cd7ee42a8",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/0b3aef1da94c853b1070f12f4caea5d9d05e2f56/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=0b3aef1da94c853b1070f12f4caea5d9d05e2f56",
            "patch": "@@ -305,7 +305,6 @@ docs/source/en/tasks/visual_question_answering.md\n docs/source/en/tasks/zero_shot_image_classification.md\n docs/source/en/tasks/zero_shot_object_detection.md\n docs/source/en/tokenizer_summary.md\n-docs/source/en/torchscript.md\n docs/source/en/training.md\n docs/source/en/troubleshooting.md\n src/transformers/activations.py"
        }
    ],
    "stats": {
        "total": 4057,
        "additions": 123,
        "deletions": 3934
    }
}