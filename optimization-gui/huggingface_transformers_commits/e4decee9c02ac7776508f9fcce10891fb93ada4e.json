{
    "author": "gante",
    "message": "[whisper] small changes for faster tests (#38236)",
    "sha": "e4decee9c02ac7776508f9fcce10891fb93ada4e",
    "files": [
        {
            "sha": "2085d9f28445aa61dad48316ecd5b99dbd4fc3ca",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 12,
            "deletions": 38,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4decee9c02ac7776508f9fcce10891fb93ada4e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4decee9c02ac7776508f9fcce10891fb93ada4e/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=e4decee9c02ac7776508f9fcce10891fb93ada4e",
            "patch": "@@ -27,7 +27,6 @@\n from huggingface_hub import hf_hub_download\n from parameterized import parameterized\n \n-import transformers\n from transformers import WhisperConfig\n from transformers.testing_utils import (\n     is_flaky,\n@@ -41,7 +40,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_available, is_torch_xpu_available, is_torchaudio_available\n+from transformers.utils import is_torch_available, is_torch_xpu_available, is_torchaudio_available\n from transformers.utils.import_utils import is_datasets_available\n \n from ...generation.test_utils import GenerationTesterMixin\n@@ -1432,33 +1431,22 @@ def test_generate_compilation_all_outputs(self):\n @require_torch\n @require_torchaudio\n class WhisperModelIntegrationTests(unittest.TestCase):\n-    def setUp(self):\n-        self._unpatched_generation_mixin_generate = transformers.GenerationMixin.generate\n-\n-    def tearDown(self):\n-        transformers.GenerationMixin.generate = self._unpatched_generation_mixin_generate\n-\n-    @cached_property\n-    def default_processor(self):\n-        return WhisperProcessor.from_pretrained(\"openai/whisper-base\")\n+    _dataset = None\n+\n+    @classmethod\n+    def _load_dataset(cls):\n+        # Lazy loading of the dataset. Because it is a class method, it will only be loaded once per pytest process.\n+        if cls._dataset is None:\n+            cls._dataset = datasets.load_dataset(\n+                \"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\"\n+            )\n \n     def _load_datasamples(self, num_samples):\n-        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n-        # automatic decoding with librispeech\n+        self._load_dataset()\n+        ds = self._dataset\n         speech_samples = ds.sort(\"id\").select(range(num_samples))[:num_samples][\"audio\"]\n-\n         return [x[\"array\"] for x in speech_samples]\n \n-    def _patch_generation_mixin_generate(self, check_args_fn=None):\n-        test = self\n-\n-        def generate(self, *args, **kwargs):\n-            if check_args_fn is not None:\n-                check_args_fn(*args, **kwargs)\n-            return test._unpatched_generation_mixin_generate(self, *args, **kwargs)\n-\n-        transformers.GenerationMixin.generate = generate\n-\n     @slow\n     def test_tiny_logits_librispeech(self):\n         torch_device = \"cpu\"\n@@ -1586,8 +1574,6 @@ def test_large_logits_librispeech(self):\n \n     @slow\n     def test_tiny_en_generation(self):\n-        torch_device = \"cpu\"\n-        set_seed(0)\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny.en\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny.en\")\n         model.to(torch_device)\n@@ -1605,8 +1591,6 @@ def test_tiny_en_generation(self):\n \n     @slow\n     def test_tiny_generation(self):\n-        torch_device = \"cpu\"\n-        set_seed(0)\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n         model.to(torch_device)\n@@ -1623,8 +1607,6 @@ def test_tiny_generation(self):\n \n     @slow\n     def test_large_generation(self):\n-        torch_device = \"cpu\"\n-        set_seed(0)\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n         model.to(torch_device)\n@@ -1643,7 +1625,6 @@ def test_large_generation(self):\n \n     @slow\n     def test_large_generation_multilingual(self):\n-        set_seed(0)\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-large-v3\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large-v3\")\n         model.to(torch_device)\n@@ -1710,8 +1691,6 @@ def test_large_batched_generation(self):\n \n     @slow\n     def test_large_batched_generation_multilingual(self):\n-        torch_device = \"cpu\"\n-        set_seed(0)\n         processor = WhisperProcessor.from_pretrained(\"openai/whisper-large\")\n         model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-large\")\n         model.to(torch_device)\n@@ -2727,11 +2706,6 @@ def test_whisper_longform_single_batch_beam(self):\n             \"renormalize_logits\": True,  # necessary to match OAI beam search implementation\n         }\n \n-        def check_gen_kwargs(inputs, generation_config, *args, **kwargs):\n-            self.assertEqual(generation_config.num_beams, gen_kwargs[\"num_beams\"])\n-\n-        self._patch_generation_mixin_generate(check_args_fn=check_gen_kwargs)\n-\n         torch.manual_seed(0)\n         result = model.generate(input_features, **gen_kwargs)\n         decoded = processor.batch_decode(result, skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 12,
        "deletions": 38
    }
}