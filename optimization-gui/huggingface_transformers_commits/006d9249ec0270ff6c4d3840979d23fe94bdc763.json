{
    "author": "jadechoghari",
    "message": "Adding RT-DETRv2 for object detection (#34773)\n\n* cookiecutter add rtdetrv2\r\n\r\n* make modular working\r\n\r\n* working modelgit add .\r\n\r\n* working modelgit add .\r\n\r\n* finalize moduar inheritence\r\n\r\n* finalize moduar inheritence\r\n\r\n* Update src/transformers/models/rtdetrv2/modular_rtdetrv2.py\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\n\r\n* update modular and add rename\r\n\r\n* remove output ckpt\r\n\r\n* define loss_kwargs\r\n\r\n* fix CamelCase naming\r\n\r\n* fix naming + files\r\n\r\n* fix modular and convert file\r\n\r\n* additional changes\r\n\r\n* fix modular\r\n\r\n* fix import error (switch to lazy)\r\n\r\n* fix autobackbone\r\n\r\n* make style\r\n\r\n* add\r\n\r\n* update testing\r\n\r\n* fix loss\r\n\r\n* remove old folder\r\n\r\n* fix testing for v2\r\n\r\n* update docstring\r\n\r\n* fix docstring\r\n\r\n* add resnetv2 (with modular bug to fix)\r\n\r\n* remove resnetv2 backbone\r\n\r\n* fix changes\r\n\r\n* small fixes\r\n\r\n* remove rtdetrv2resnetconfig\r\n\r\n* add rtdetrv2 name to convert\r\n\r\n* make style\r\n\r\n* Update docs/source/en/model_doc/rt_detr_v2.md\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* Update src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py\r\n\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\n\r\n* fix modular typo after review\r\n\r\n* add reviewed changes\r\n\r\n* add final review changes\r\n\r\n* Update docs/source/en/model_doc/rt_detr_v2.md\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\n\r\n* Update src/transformers/models/rt_detr_v2/__init__.py\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\n\r\n* Update src/transformers/models/rt_detr_v2/convert_rt_detr_v2_weights_to_hf.py\r\n\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\n\r\n* add review changes\r\n\r\n* remove rtdetrv2 resnet\r\n\r\n* removing this weird project change\r\n\r\n* change ckpt name from jadechoghari to author\r\n\r\n* implement review and update testing\r\n\r\n* update naming and remove wrong ckpt\r\n\r\n* name\r\n\r\n* make fix-copies\r\n\r\n* Fix RT-DETR loss\r\n\r\n* Add resources, fix name\r\n\r\n* Fix repo in docs\r\n\r\n* Fix table name\r\n\r\n---------\r\n\r\nCo-authored-by: jadechoghari <jadechoghari@users.noreply.huggingface.co>\r\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\r\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\r\nCo-authored-by: qubvel <qubvel@gmail.com>",
    "sha": "006d9249ec0270ff6c4d3840979d23fe94bdc763",
    "files": [
        {
            "sha": "dcf3a02bcc0367748848ee8a6d59f5ec45a4dc0e",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -709,6 +709,8 @@\n         title: ResNet\n       - local: model_doc/rt_detr\n         title: RT-DETR\n+      - local: model_doc/rt_detr_v2\n+        title: RT-DETRv2\n       - local: model_doc/segformer\n         title: SegFormer\n       - local: model_doc/seggpt"
        },
        {
            "sha": "e87d0fa09ec9c9fafd91b75d2072824fe9864a35",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -305,6 +305,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                      [RoFormer](model_doc/roformer)                      |       ‚úÖ        |         ‚úÖ         |      ‚úÖ      |\n |                       [RT-DETR](model_doc/rt_detr)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                [RT-DETR-ResNet](model_doc/rt_detr_resnet)                |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                    [RT-DETRv2](model_doc/rt_detr_v2)                     |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                          [RWKV](model_doc/rwkv)                          |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                           [SAM](model_doc/sam)                           |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |\n |                  [SeamlessM4T](model_doc/seamless_m4t)                   |       ‚úÖ        |         ‚ùå         |      ‚ùå      |"
        },
        {
            "sha": "0c125af3d2a119de3587ef31e8343761f84e4396",
            "filename": "docs/source/en/model_doc/rt_detr_v2.md",
            "status": "added",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frt_detr_v2.md?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,97 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# RT-DETRv2\n+\n+## Overview\n+\n+The RT-DETRv2 model was proposed in [RT-DETRv2: Improved Baseline with Bag-of-Freebies for Real-Time Detection Transformer](https://arxiv.org/abs/2407.17140) by Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, Yi Liu.\n+\n+RT-DETRv2 refines RT-DETR by introducing selective multi-scale feature extraction, a discrete sampling operator for broader deployment compatibility, and improved training strategies like dynamic data augmentation and scale-adaptive hyperparameters. These changes enhance flexibility and practicality while maintaining real-time performance.\n+\n+The abstract from the paper is the following:\n+\n+*In this report, we present RT-DETRv2, an improved Real-Time DEtection TRansformer (RT-DETR). RT-DETRv2 builds upon the previous state-of-the-art real-time detector, RT-DETR, and opens up a set of bag-of-freebies for flexibility and practicality, as well as optimizing the training strategy to achieve enhanced performance. To improve the flexibility, we suggest setting a distinct number of sampling points for features at different scales in the deformable attention to achieve selective multi-scale feature extraction by the decoder. To enhance practicality, we propose an optional discrete sampling operator to replace the grid_sample operator that is specific to RT-DETR compared to YOLOs. This removes the deployment constraints typically associated with DETRs. For the training strategy, we propose dynamic data augmentation and scale-adaptive hyperparameters customization to improve performance without loss of speed.*\n+\n+This model was contributed by [jadechoghari](https://huggingface.co/jadechoghari).\n+The original code can be found [here](https://github.com/lyuwenyu/RT-DETR).\n+\n+## Usage tips \n+\n+This second version of RT-DETR improves how the decoder finds objects in an image. \n+\n+- **better sampling** ‚Äì adjusts offsets so the model looks at the right areas\n+- **flexible attention** ‚Äì can use smooth (bilinear) or fixed (discrete) sampling\n+- **optimized processing** ‚Äì improves how attention weights mix information\n+\n+```py\n+>>> import torch\n+>>> import requests\n+\n+>>> from PIL import Image\n+>>> from transformers import RTDetrV2ForObjectDetection, RTDetrImageProcessor\n+\n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> image_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n+>>> model = RTDetrV2ForObjectDetection.from_pretrained(\"PekingU/rtdetr_v2_r18vd\")\n+\n+>>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> results = image_processor.post_process_object_detection(outputs, target_sizes=torch.tensor([(image.height, image.width)]), threshold=0.5)\n+\n+>>> for result in results:\n+...     for score, label_id, box in zip(result[\"scores\"], result[\"labels\"], result[\"boxes\"]):\n+...         score, label = score.item(), label_id.item()\n+...         box = [round(i, 2) for i in box.tolist()]\n+...         print(f\"{model.config.id2label[label]}: {score:.2f} {box}\")\n+cat: 0.97 [341.14, 25.11, 639.98, 372.89]\n+cat: 0.96 [12.78, 56.35, 317.67, 471.34]\n+remote: 0.95 [39.96, 73.12, 175.65, 117.44]\n+sofa: 0.86 [-0.11, 2.97, 639.89, 473.62]\n+sofa: 0.82 [-0.12, 1.78, 639.87, 473.52]\n+remote: 0.79 [333.65, 76.38, 370.69, 187.48]\n+```\n+\n+## Resources\n+\n+A list of official Hugging Face and community (indicated by üåé) resources to help you get started with RT-DETRv2.\n+\n+<PipelineTag pipeline=\"object-detection\"/>\n+\n+- Scripts for finetuning [`RTDetrV2ForObjectDetection`] with [`Trainer`] or [Accelerate](https://huggingface.co/docs/accelerate/index) can be found [here](https://github.com/huggingface/transformers/tree/main/examples/pytorch/object-detection).\n+- See also: [Object detection task guide](../tasks/object_detection).\n+- Notebooks for [inference](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_inference.ipynb) and [fine-tuning](https://github.com/qubvel/transformers-notebooks/blob/main/notebooks/RT_DETR_v2_finetune_on_a_custom_dataset.ipynb) RT-DETRv2 on a custom dataset (üåé).\n+\n+\n+## RTDetrV2Config\n+\n+[[autodoc]] RTDetrV2Config\n+\n+\n+## RTDetrV2Model\n+\n+[[autodoc]] RTDetrV2Model\n+    - forward\n+ \n+## RTDetrV2ForObjectDetection\n+\n+[[autodoc]] RTDetrV2ForObjectDetection\n+    - forward"
        },
        {
            "sha": "80438981047d7857b700b4eb75ebd6580a5b7655",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -748,6 +748,7 @@\n         \"RoFormerTokenizer\",\n     ],\n     \"models.rt_detr\": [\"RTDetrConfig\", \"RTDetrResNetConfig\"],\n+    \"models.rt_detr_v2\": [\"RTDetrV2Config\"],\n     \"models.rwkv\": [\"RwkvConfig\"],\n     \"models.sam\": [\n         \"SamConfig\",\n@@ -3454,6 +3455,9 @@\n             \"RTDetrResNetPreTrainedModel\",\n         ]\n     )\n+    _import_structure[\"models.rt_detr_v2\"].extend(\n+        [\"RTDetrV2ForObjectDetection\", \"RTDetrV2Model\", \"RTDetrV2PreTrainedModel\"]\n+    )\n     _import_structure[\"models.rwkv\"].extend(\n         [\n             \"RwkvForCausalLM\",\n@@ -5875,6 +5879,7 @@\n         RTDetrConfig,\n         RTDetrResNetConfig,\n     )\n+    from .models.rt_detr_v2 import RTDetrV2Config\n     from .models.rwkv import RwkvConfig\n     from .models.sam import (\n         SamConfig,\n@@ -8171,6 +8176,7 @@\n             RTDetrResNetBackbone,\n             RTDetrResNetPreTrainedModel,\n         )\n+        from .models.rt_detr_v2 import RTDetrV2ForObjectDetection, RTDetrV2Model, RTDetrV2PreTrainedModel\n         from .models.rwkv import (\n             RwkvForCausalLM,\n             RwkvModel,"
        },
        {
            "sha": "fdfdd041cfc477121b42f745fa390654a27c2c86",
            "filename": "src/transformers/loss/loss_rt_detr.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Floss%2Floss_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_rt_detr.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -18,7 +18,6 @@\n \n from ..utils import is_scipy_available, is_vision_available, requires_backends\n from .loss_for_object_detection import (\n-    _set_aux_loss,\n     box_iou,\n     dice_loss,\n     generalized_box_iou,\n@@ -35,6 +34,15 @@\n     from transformers.image_transforms import center_to_corners_format\n \n \n+# different for RT-DETR: not slicing the last element like in DETR one\n+@torch.jit.unused\n+def _set_aux_loss(outputs_class, outputs_coord):\n+    # this is a workaround to make torchscript happy, as torchscript\n+    # doesn't support dictionary with non-homogeneous values, such\n+    # as a dict having both a Tensor and a list.\n+    return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class, outputs_coord)]\n+\n+\n class RTDetrHungarianMatcher(nn.Module):\n     \"\"\"This class computes an assignment between the targets and the predictions of the network\n "
        },
        {
            "sha": "bf1ae53f5a737b76e0b9f235550c0aaf0e9be66d",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -132,4 +132,5 @@ def ForTokenClassification(logits, labels, config, **kwargs):\n     \"GroundingDinoForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n+    \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,\n }"
        },
        {
            "sha": "ec38a94aa5d5622c043179fe6e925575b7a5d664",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -233,6 +233,7 @@\n     roc_bert,\n     roformer,\n     rt_detr,\n+    rt_detr_v2,\n     rwkv,\n     sam,\n     seamless_m4t,"
        },
        {
            "sha": "f52fc2b12ff70db99805aeb6534a0b758063d303",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -259,6 +259,7 @@\n         (\"roformer\", \"RoFormerConfig\"),\n         (\"rt_detr\", \"RTDetrConfig\"),\n         (\"rt_detr_resnet\", \"RTDetrResNetConfig\"),\n+        (\"rt_detr_v2\", \"RTDetrV2Config\"),\n         (\"rwkv\", \"RwkvConfig\"),\n         (\"sam\", \"SamConfig\"),\n         (\"seamless_m4t\", \"SeamlessM4TConfig\"),\n@@ -600,6 +601,7 @@\n         (\"roformer\", \"RoFormer\"),\n         (\"rt_detr\", \"RT-DETR\"),\n         (\"rt_detr_resnet\", \"RT-DETR-ResNet\"),\n+        (\"rt_detr_v2\", \"RT-DETRv2\"),\n         (\"rwkv\", \"RWKV\"),\n         (\"sam\", \"SAM\"),\n         (\"seamless_m4t\", \"SeamlessM4T\"),"
        },
        {
            "sha": "686c9c930f60d56a64642b887b4ad38a60804529",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -238,6 +238,7 @@\n         (\"roc_bert\", \"RoCBertModel\"),\n         (\"roformer\", \"RoFormerModel\"),\n         (\"rt_detr\", \"RTDetrModel\"),\n+        (\"rt_detr_v2\", \"RTDetrV2Model\"),\n         (\"rwkv\", \"RwkvModel\"),\n         (\"sam\", \"SamModel\"),\n         (\"seamless_m4t\", \"SeamlessM4TModel\"),\n@@ -897,6 +898,7 @@\n         (\"deta\", \"DetaForObjectDetection\"),\n         (\"detr\", \"DetrForObjectDetection\"),\n         (\"rt_detr\", \"RTDetrForObjectDetection\"),\n+        (\"rt_detr_v2\", \"RTDetrV2ForObjectDetection\"),\n         (\"table-transformer\", \"TableTransformerForObjectDetection\"),\n         (\"yolos\", \"YolosForObjectDetection\"),\n     ]"
        },
        {
            "sha": "5973c080398283be07588910e3800b280dd65649",
            "filename": "src/transformers/models/rt_detr/modeling_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodeling_rt_detr.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -2115,9 +2115,8 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs, enc_topk_logits, enc_topk_bboxes = None, None, None, None, None\n         if labels is not None:\n-            if self.training and denoising_meta_values is not None:\n-                enc_topk_logits = outputs.enc_topk_logits if return_dict else outputs[-5]\n-                enc_topk_bboxes = outputs.enc_topk_bboxes if return_dict else outputs[-4]\n+            enc_topk_logits = outputs.enc_topk_logits if return_dict else outputs[-5]\n+            enc_topk_bboxes = outputs.enc_topk_bboxes if return_dict else outputs[-4]\n             loss, loss_dict, auxiliary_outputs = self.loss_function(\n                 logits,\n                 labels,"
        },
        {
            "sha": "efc3772ca6d015271102638c3948958777498fd9",
            "filename": "src/transformers/models/rt_detr_v2/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2F__init__.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_rt_detr_v2 import *\n+    from .modeling_rt_detr_v2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "0feb6d8a3e725d81ceba4aeabfa8f7fdc1ef558c",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "added",
            "additions": 383,
            "deletions": 0,
            "changes": 383,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,383 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_rt_detr_v2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import verify_backbone_config_arguments\n+from ..auto import CONFIG_MAPPING\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class RTDetrV2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`RTDetrV2Model`]. It is used to instantiate a\n+    RT-DETR model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the RT-DETR architecture.\n+\n+    e.g. [PekingU/rtdetr_r18vd](https://huggingface.co/PekingU/rtdetr_r18vd)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.01):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_bias_prior_prob (`float`, *optional*):\n+            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.\n+            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        backbone_config (`Dict`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        freeze_backbone_batch_norms (`bool`, *optional*, defaults to `True`):\n+            Whether to freeze the batch normalization layers in the backbone.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        encoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            Dimension of the layers in hybrid encoder.\n+        encoder_in_channels (`list`, *optional*, defaults to `[512, 1024, 2048]`):\n+            Multi level features input for encoder.\n+        feat_strides (`List[int]`, *optional*, defaults to `[8, 16, 32]`):\n+            Strides used in each feature map.\n+        encoder_layers (`int`, *optional*, defaults to 1):\n+            Total of layers to be used by the encoder.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        encode_proj_layers (`List[int]`, *optional*, defaults to `[2]`):\n+            Indexes of the projected layers to be used in the encoder.\n+        positional_encoding_temperature (`int`, *optional*, defaults to 10000):\n+            The temperature parameter used to create the positional encodings.\n+        encoder_activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        activation_function (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the general layer. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        eval_size (`Tuple[int, int]`, *optional*):\n+            Height and width used to compute the effective height and width of the position embeddings after taking\n+            into account the stride.\n+        normalize_before (`bool`, *optional*, defaults to `False`):\n+            Determine whether to apply layer normalization in the transformer encoder layer before self-attention and\n+            feed-forward modules.\n+        hidden_expansion (`float`, *optional*, defaults to 1.0):\n+            Expansion ratio to enlarge the dimension size of RepVGGBlock and CSPRepLayer.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers exclude hybrid encoder.\n+        num_queries (`int`, *optional*, defaults to 300):\n+            Number of object queries.\n+        decoder_in_channels (`list`, *optional*, defaults to `[256, 256, 256]`):\n+            Multi level features dimension for decoder\n+        decoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of input feature levels.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        decoder_activation_function (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the decoder. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_denoising (`int`, *optional*, defaults to 100):\n+            The total number of denoising tasks or queries to be used for contrastive denoising.\n+        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n+            The fraction of denoising labels to which random noise should be added.\n+        box_noise_scale (`float`, *optional*, defaults to 1.0):\n+            Scale or magnitude of noise to be added to the bounding boxes.\n+        learn_initial_query (`bool`, *optional*, defaults to `False`):\n+            Indicates whether the initial query embeddings for the decoder should be learned during training\n+        anchor_image_size (`Tuple[int, int]`, *optional*):\n+            Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n+        disable_custom_kernels (`bool`, *optional*, defaults to `True`):\n+            Whether to disable custom kernels.\n+        with_box_refine (`bool`, *optional*, defaults to `True`):\n+            Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n+            based on the predictions from the previous layer.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the architecture has an encoder decoder structure.\n+        matcher_alpha (`float`, *optional*, defaults to 0.25):\n+            Parameter alpha used by the Hungarian Matcher.\n+        matcher_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used by the Hungarian Matcher.\n+        matcher_class_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the class loss used by the Hungarian Matcher.\n+        matcher_bbox_cost (`float`, *optional*, defaults to 5.0):\n+            The relative weight of the bounding box loss used by the Hungarian Matcher.\n+        matcher_giou_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the giou loss of used by the Hungarian Matcher.\n+        use_focal_loss (`bool`, *optional*, defaults to `True`):\n+            Parameter informing if focal loss should be used.\n+        auxiliary_loss (`bool`, *optional*, defaults to `True`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        focal_loss_alpha (`float`, *optional*, defaults to 0.75):\n+            Parameter alpha used to compute the focal loss.\n+        focal_loss_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used to compute the focal loss.\n+        weight_loss_vfl (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the varifocal loss in the object detection loss.\n+        weight_loss_bbox (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        weight_loss_giou (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        eos_coefficient (`float`, *optional*, defaults to 0.0001):\n+            Relative classification weight of the 'no-object' class in the object detection loss.\n+        decoder_n_levels (`int`, *optional*, defaults to 3):\n+            The number of feature levels used by the decoder.\n+        decoder_offset_scale (`float`, *optional*, defaults to 0.5):\n+            Scaling factor applied to the attention offsets in the decoder.\n+        decoder_method (`str`, *optional*, defaults to `\"default\"`):\n+            The method to use for the decoder: `\"default\"` or `\"discrete\"`.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import RTDetrV2Config, RTDetrV2Model\n+\n+    >>> # Initializing a RT-DETR configuration\n+    >>> configuration = RTDetrV2Config()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = RTDetrV2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"rt_detr_v2\"\n+    layer_types = [\"basic\", \"bottleneck\"]\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        initializer_range=0.01,\n+        initializer_bias_prior_prob=None,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        freeze_backbone_batch_norms=True,\n+        backbone_kwargs=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=256,\n+        encoder_in_channels=[512, 1024, 2048],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=1024,\n+        encoder_attention_heads=8,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        hidden_expansion=1.0,\n+        # decoder RTDetrV2Transformer\n+        d_model=256,\n+        num_queries=300,\n+        decoder_in_channels=[256, 256, 256],\n+        decoder_ffn_dim=1024,\n+        num_feature_levels=3,\n+        decoder_n_points=4,\n+        decoder_layers=6,\n+        decoder_attention_heads=8,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=100,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        disable_custom_kernels=True,\n+        with_box_refine=True,\n+        is_encoder_decoder=True,\n+        # Loss\n+        matcher_alpha=0.25,\n+        matcher_gamma=2.0,\n+        matcher_class_cost=2.0,\n+        matcher_bbox_cost=5.0,\n+        matcher_giou_cost=2.0,\n+        use_focal_loss=True,\n+        auxiliary_loss=True,\n+        focal_loss_alpha=0.75,\n+        focal_loss_gamma=2.0,\n+        weight_loss_vfl=1.0,\n+        weight_loss_bbox=5.0,\n+        weight_loss_giou=2.0,\n+        eos_coefficient=1e-4,\n+        decoder_n_levels=3,  # default value\n+        decoder_offset_scale=0.5,  # default value\n+        decoder_method=\"default\",\n+        **kwargs,\n+    ):\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.initializer_range = initializer_range\n+        self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        # backbone\n+        if backbone_config is None and backbone is None:\n+            logger.info(\n+                \"`backbone_config` and `backbone` are `None`. Initializing the config with the default `RTDetrV2-ResNet` backbone.\"\n+            )\n+            backbone_model_type = \"rt_detr_resnet\"\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            # this will map it to RTDetrResNetConfig\n+            # note: we can instead create RTDetrV2ResNetConfig but it will be exactly the same as V1\n+            # and we would need to create RTDetrV2ResNetModel\n+            backbone_config = config_class(\n+                num_channels=3,\n+                embedding_size=64,\n+                hidden_sizes=[256, 512, 1024, 2048],\n+                depths=[3, 4, 6, 3],\n+                layer_type=\"bottleneck\",\n+                hidden_act=\"relu\",\n+                downsample_in_first_stage=False,\n+                downsample_in_bottleneck=False,\n+                out_features=None,\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.freeze_backbone_batch_norms = freeze_backbone_batch_norms\n+        self.backbone_kwargs = backbone_kwargs\n+        # encoder\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.encoder_layers = encoder_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.hidden_expansion = hidden_expansion\n+        self.num_queries = num_queries\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_in_channels = decoder_in_channels\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.auxiliary_loss = auxiliary_loss\n+        self.disable_custom_kernels = disable_custom_kernels\n+        self.with_box_refine = with_box_refine\n+        # Loss\n+        self.matcher_alpha = matcher_alpha\n+        self.matcher_gamma = matcher_gamma\n+        self.matcher_class_cost = matcher_class_cost\n+        self.matcher_bbox_cost = matcher_bbox_cost\n+        self.matcher_giou_cost = matcher_giou_cost\n+        self.use_focal_loss = use_focal_loss\n+        self.focal_loss_alpha = focal_loss_alpha\n+        self.focal_loss_gamma = focal_loss_gamma\n+        self.weight_loss_vfl = weight_loss_vfl\n+        self.weight_loss_bbox = weight_loss_bbox\n+        self.weight_loss_giou = weight_loss_giou\n+        self.eos_coefficient = eos_coefficient\n+\n+        if not hasattr(self, \"d_model\"):\n+            self.d_model = d_model\n+\n+        if not hasattr(self, \"encoder_attention_heads\"):\n+            self.encoder_attention_heads = encoder_attention_heads\n+        # add the new attributes with the given values or defaults\n+        self.decoder_n_levels = decoder_n_levels\n+        self.decoder_offset_scale = decoder_offset_scale\n+        self.decoder_method = decoder_method\n+\n+    @classmethod\n+    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+        \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n+        configuration.\n+\n+            Args:\n+                backbone_config ([`PretrainedConfig`]):\n+                    The backbone configuration.\n+\n+            Returns:\n+                [`RTDetrV2Config`]: An instance of a configuration object\n+        \"\"\"\n+        return cls(\n+            backbone_config=backbone_config,\n+            **kwargs,\n+        )\n+\n+\n+__all__ = [\"RTDetrV2Config\"]"
        },
        {
            "sha": "51372b74e4261c646be19198018837b9eaf8c742",
            "filename": "src/transformers/models/rt_detr_v2/convert_rt_detr_v2_weights_to_hf.py",
            "status": "added",
            "additions": 363,
            "deletions": 0,
            "changes": 363,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconvert_rt_detr_v2_weights_to_hf.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,363 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert RT Detr V2 checkpoints with Timm backbone\"\"\"\n+\n+import argparse\n+import json\n+import re\n+from pathlib import Path\n+\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+from torchvision import transforms\n+\n+from transformers import RTDetrImageProcessor, RTDetrV2Config, RTDetrV2ForObjectDetection\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+def get_rt_detr_v2_config(model_name: str) -> RTDetrV2Config:\n+    config = RTDetrV2Config()\n+\n+    config.num_labels = 80\n+    repo_id = \"huggingface/label-files\"\n+    filename = \"coco-detection-mmdet-id2label.json\"\n+    id2label = json.load(open(hf_hub_download(repo_id, filename, repo_type=\"dataset\"), \"r\"))\n+    id2label = {int(k): v for k, v in id2label.items()}\n+    config.id2label = id2label\n+    config.label2id = {v: k for k, v in id2label.items()}\n+\n+    if model_name == \"rtdetr_v2_r18vd\":\n+        config.backbone_config.hidden_sizes = [64, 128, 256, 512]\n+        config.backbone_config.depths = [2, 2, 2, 2]\n+        config.backbone_config.layer_type = \"basic\"\n+        config.encoder_in_channels = [128, 256, 512]\n+        config.hidden_expansion = 0.5\n+        config.decoder_layers = 3\n+    elif model_name == \"rtdetr_v2_r34vd\":\n+        config.backbone_config.hidden_sizes = [64, 128, 256, 512]\n+        config.backbone_config.depths = [3, 4, 6, 3]\n+        config.backbone_config.layer_type = \"basic\"\n+        config.encoder_in_channels = [128, 256, 512]\n+        config.hidden_expansion = 0.5\n+        config.decoder_layers = 4\n+    # TODO: check this not working\n+    elif model_name == \"rtdetr_v2_r50vd_m\":\n+        config.hidden_expansion = 0.5\n+    elif model_name == \"rtdetr_v2_r50vd\":\n+        pass\n+    elif model_name == \"rtdetr_v2_r101vd\":\n+        config.backbone_config.depths = [3, 4, 23, 3]\n+        config.encoder_ffn_dim = 2048\n+        config.encoder_hidden_dim = 384\n+        config.decoder_in_channels = [384, 384, 384]\n+\n+    return config\n+\n+\n+# Define a mapping from original keys to converted keys using regex\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    r\"backbone.conv1.conv1_1.conv.weight\": r\"model.backbone.model.embedder.embedder.0.convolution.weight\",\n+    r\"backbone.conv1.conv1_1.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.embedder.0.normalization.\\1\",\n+    r\"backbone.conv1.conv1_2.conv.weight\": r\"model.backbone.model.embedder.embedder.1.convolution.weight\",\n+    r\"backbone.conv1.conv1_2.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.embedder.1.normalization.\\1\",\n+    r\"backbone.conv1.conv1_3.conv.weight\": r\"model.backbone.model.embedder.embedder.2.convolution.weight\",\n+    r\"backbone.conv1.conv1_3.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.embedder.embedder.2.normalization.\\1\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2a.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.0.convolution.weight\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2a.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.0.normalization.\\3\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2b.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.1.convolution.weight\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2b.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.1.normalization.\\3\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2c.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.2.convolution.weight\",\n+    r\"backbone.res_layers.(\\d+).blocks.(\\d+).branch2c.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.layers.\\2.layer.2.normalization.\\3\",\n+    r\"encoder.encoder.(\\d+).layers.0.self_attn.out_proj.weight\": r\"model.encoder.encoder.\\1.layers.0.self_attn.out_proj.weight\",\n+    r\"encoder.encoder.(\\d+).layers.0.self_attn.out_proj.bias\": r\"model.encoder.encoder.\\1.layers.0.self_attn.out_proj.bias\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear1.weight\": r\"model.encoder.encoder.\\1.layers.0.fc1.weight\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear1.bias\": r\"model.encoder.encoder.\\1.layers.0.fc1.bias\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear2.weight\": r\"model.encoder.encoder.\\1.layers.0.fc2.weight\",\n+    r\"encoder.encoder.(\\d+).layers.0.linear2.bias\": r\"model.encoder.encoder.\\1.layers.0.fc2.bias\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm1.weight\": r\"model.encoder.encoder.\\1.layers.0.self_attn_layer_norm.weight\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm1.bias\": r\"model.encoder.encoder.\\1.layers.0.self_attn_layer_norm.bias\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm2.weight\": r\"model.encoder.encoder.\\1.layers.0.final_layer_norm.weight\",\n+    r\"encoder.encoder.(\\d+).layers.0.norm2.bias\": r\"model.encoder.encoder.\\1.layers.0.final_layer_norm.bias\",\n+    r\"encoder.input_proj.(\\d+).conv.weight\": r\"model.encoder_input_proj.\\1.0.weight\",\n+    r\"encoder.input_proj.(\\d+).norm.(.*)\": r\"model.encoder_input_proj.\\1.1.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).conv(\\d+).conv.weight\": r\"model.encoder.fpn_blocks.\\1.conv\\2.conv.weight\",\n+    # r\"encoder.fpn_blocks.(\\d+).conv(\\d+).norm.(.*)\": r\"model.encoder.fpn_blocks.\\1.conv\\2.norm.\\3\",\n+    r\"encoder.fpn_blocks.(\\d+).conv(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.fpn_blocks.\\1.conv\\2.norm.\\3\",\n+    r\"encoder.lateral_convs.(\\d+).conv.weight\": r\"model.encoder.lateral_convs.\\1.conv.weight\",\n+    r\"encoder.lateral_convs.(\\d+).norm.(.*)\": r\"model.encoder.lateral_convs.\\1.norm.\\2\",\n+    r\"encoder.fpn_blocks.(\\d+).bottlenecks.(\\d+).conv(\\d+).conv.weight\": r\"model.encoder.fpn_blocks.\\1.bottlenecks.\\2.conv\\3.conv.weight\",\n+    r\"encoder.fpn_blocks.(\\d+).bottlenecks.(\\d+).conv(\\d+).norm.(\\w+)\": r\"model.encoder.fpn_blocks.\\1.bottlenecks.\\2.conv\\3.norm.\\4\",\n+    r\"encoder.pan_blocks.(\\d+).conv(\\d+).conv.weight\": r\"model.encoder.pan_blocks.\\1.conv\\2.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).conv(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.conv\\2.norm.\\3\",\n+    r\"encoder.pan_blocks.(\\d+).bottlenecks.(\\d+).conv(\\d+).conv.weight\": r\"model.encoder.pan_blocks.\\1.bottlenecks.\\2.conv\\3.conv.weight\",\n+    r\"encoder.pan_blocks.(\\d+).bottlenecks.(\\d+).conv(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.pan_blocks.\\1.bottlenecks.\\2.conv\\3.norm.\\4\",\n+    r\"encoder.downsample_convs.(\\d+).conv.weight\": r\"model.encoder.downsample_convs.\\1.conv.weight\",\n+    r\"encoder.downsample_convs.(\\d+).norm.(weight|bias|running_mean|running_var)\": r\"model.encoder.downsample_convs.\\1.norm.\\2\",\n+    r\"decoder.decoder.layers.(\\d+).self_attn.out_proj.weight\": r\"model.decoder.layers.\\1.self_attn.out_proj.weight\",\n+    r\"decoder.decoder.layers.(\\d+).self_attn.out_proj.bias\": r\"model.decoder.layers.\\1.self_attn.out_proj.bias\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.sampling_offsets.weight\": r\"model.decoder.layers.\\1.encoder_attn.sampling_offsets.weight\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.sampling_offsets.bias\": r\"model.decoder.layers.\\1.encoder_attn.sampling_offsets.bias\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.attention_weights.weight\": r\"model.decoder.layers.\\1.encoder_attn.attention_weights.weight\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.attention_weights.bias\": r\"model.decoder.layers.\\1.encoder_attn.attention_weights.bias\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.value_proj.weight\": r\"model.decoder.layers.\\1.encoder_attn.value_proj.weight\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.value_proj.bias\": r\"model.decoder.layers.\\1.encoder_attn.value_proj.bias\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.output_proj.weight\": r\"model.decoder.layers.\\1.encoder_attn.output_proj.weight\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.output_proj.bias\": r\"model.decoder.layers.\\1.encoder_attn.output_proj.bias\",\n+    r\"decoder.decoder.layers.(\\d+).norm1.weight\": r\"model.decoder.layers.\\1.self_attn_layer_norm.weight\",\n+    r\"decoder.decoder.layers.(\\d+).norm1.bias\": r\"model.decoder.layers.\\1.self_attn_layer_norm.bias\",\n+    r\"decoder.decoder.layers.(\\d+).norm2.weight\": r\"model.decoder.layers.\\1.encoder_attn_layer_norm.weight\",\n+    r\"decoder.decoder.layers.(\\d+).norm2.bias\": r\"model.decoder.layers.\\1.encoder_attn_layer_norm.bias\",\n+    r\"decoder.decoder.layers.(\\d+).linear1.weight\": r\"model.decoder.layers.\\1.fc1.weight\",\n+    r\"decoder.decoder.layers.(\\d+).linear1.bias\": r\"model.decoder.layers.\\1.fc1.bias\",\n+    r\"decoder.decoder.layers.(\\d+).linear2.weight\": r\"model.decoder.layers.\\1.fc2.weight\",\n+    r\"decoder.decoder.layers.(\\d+).linear2.bias\": r\"model.decoder.layers.\\1.fc2.bias\",\n+    r\"decoder.decoder.layers.(\\d+).norm3.weight\": r\"model.decoder.layers.\\1.final_layer_norm.weight\",\n+    r\"decoder.decoder.layers.(\\d+).norm3.bias\": r\"model.decoder.layers.\\1.final_layer_norm.bias\",\n+    r\"decoder.decoder.layers.(\\d+).cross_attn.num_points_scale\": r\"model.decoder.layers.\\1.encoder_attn.n_points_scale\",\n+    r\"decoder.dec_score_head.(\\d+).weight\": r\"model.decoder.class_embed.\\1.weight\",\n+    r\"decoder.dec_score_head.(\\d+).bias\": r\"model.decoder.class_embed.\\1.bias\",\n+    r\"decoder.dec_bbox_head.(\\d+).layers.(\\d+).(weight|bias)\": r\"model.decoder.bbox_embed.\\1.layers.\\2.\\3\",\n+    r\"decoder.denoising_class_embed.weight\": r\"model.denoising_class_embed.weight\",\n+    r\"decoder.query_pos_head.layers.0.weight\": r\"model.decoder.query_pos_head.layers.0.weight\",\n+    r\"decoder.query_pos_head.layers.0.bias\": r\"model.decoder.query_pos_head.layers.0.bias\",\n+    r\"decoder.query_pos_head.layers.1.weight\": r\"model.decoder.query_pos_head.layers.1.weight\",\n+    r\"decoder.query_pos_head.layers.1.bias\": r\"model.decoder.query_pos_head.layers.1.bias\",\n+    r\"decoder.enc_output.proj.weight\": r\"model.enc_output.0.weight\",\n+    r\"decoder.enc_output.proj.bias\": r\"model.enc_output.0.bias\",\n+    r\"decoder.enc_output.norm.weight\": r\"model.enc_output.1.weight\",\n+    r\"decoder.enc_output.norm.bias\": r\"model.enc_output.1.bias\",\n+    r\"decoder.enc_score_head.weight\": r\"model.enc_score_head.weight\",\n+    r\"decoder.enc_score_head.bias\": r\"model.enc_score_head.bias\",\n+    r\"decoder.enc_bbox_head.layers.(\\d+).(weight|bias)\": r\"model.enc_bbox_head.layers.\\1.\\2\",\n+    r\"backbone.res_layers.0.blocks.0.short.conv.weight\": r\"model.backbone.model.encoder.stages.0.layers.0.shortcut.convolution.weight\",\n+    r\"backbone.res_layers.0.blocks.0.short.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.0.layers.0.shortcut.normalization.\\1\",\n+    r\"backbone.res_layers.(\\d+).blocks.0.short.conv.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.layers.0.shortcut.1.convolution.weight\",\n+    r\"backbone.res_layers.(\\d+).blocks.0.short.conv.norm.(\\w+)\": r\"model.backbone.model.encoder.stages.\\1.layers.0.shortcut.1.normalization.\\2\",\n+    # Mapping for subsequent blocks in other stages\n+    r\"backbone.res_layers.(\\d+).blocks.0.short.conv.weight\": r\"model.backbone.model.encoder.stages.\\1.layers.0.shortcut.1.convolution.weight\",\n+    r\"backbone.res_layers.(\\d+).blocks.0.short.norm.(weight|bias|running_mean|running_var)\": r\"model.backbone.model.encoder.stages.\\1.layers.0.shortcut.1.normalization.\\2\",\n+    r\"decoder.input_proj.(\\d+).conv.weight\": r\"model.decoder_input_proj.\\1.0.weight\",\n+    r\"decoder.input_proj.(\\d+).norm.(.*)\": r\"model.decoder_input_proj.\\1.1.\\2\",\n+}\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: dict = None):\n+    # Use the mapping to rename keys\n+    for original_key, converted_key in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+        for key in list(state_dict_keys.keys()):\n+            new_key = re.sub(original_key, converted_key, key)\n+            if new_key != key:\n+                state_dict_keys[new_key] = state_dict_keys.pop(key)\n+\n+    return state_dict_keys\n+\n+\n+def read_in_q_k_v(state_dict, config):\n+    prefix = \"\"\n+    encoder_hidden_dim = config.encoder_hidden_dim\n+\n+    # first: transformer encoder\n+    for i in range(config.encoder_layers):\n+        # read in weights + bias of input projection layer (in PyTorch's MultiHeadAttention, this is a single matrix + bias)\n+        in_proj_weight = state_dict.pop(f\"{prefix}encoder.encoder.{i}.layers.0.self_attn.in_proj_weight\")\n+        in_proj_bias = state_dict.pop(f\"{prefix}encoder.encoder.{i}.layers.0.self_attn.in_proj_bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.q_proj.weight\"] = in_proj_weight[\n+            :encoder_hidden_dim, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.q_proj.bias\"] = in_proj_bias[:encoder_hidden_dim]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.k_proj.weight\"] = in_proj_weight[\n+            encoder_hidden_dim : 2 * encoder_hidden_dim, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.k_proj.bias\"] = in_proj_bias[\n+            encoder_hidden_dim : 2 * encoder_hidden_dim\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.v_proj.weight\"] = in_proj_weight[\n+            -encoder_hidden_dim:, :\n+        ]\n+        state_dict[f\"model.encoder.encoder.{i}.layers.0.self_attn.v_proj.bias\"] = in_proj_bias[-encoder_hidden_dim:]\n+    # next: transformer decoder (which is a bit more complex because it also includes cross-attention)\n+    for i in range(config.decoder_layers):\n+        # read in weights + bias of input projection layer of self-attention\n+        in_proj_weight = state_dict.pop(f\"{prefix}decoder.decoder.layers.{i}.self_attn.in_proj_weight\")\n+        in_proj_bias = state_dict.pop(f\"{prefix}decoder.decoder.layers.{i}.self_attn.in_proj_bias\")\n+        # next, add query, keys and values (in that order) to the state dict\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.weight\"] = in_proj_weight[:256, :]\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.q_proj.bias\"] = in_proj_bias[:256]\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.weight\"] = in_proj_weight[256:512, :]\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.k_proj.bias\"] = in_proj_bias[256:512]\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.weight\"] = in_proj_weight[-256:, :]\n+        state_dict[f\"model.decoder.layers.{i}.self_attn.v_proj.bias\"] = in_proj_bias[-256:]\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    im = Image.open(requests.get(url, stream=True).raw)\n+\n+    return im\n+\n+\n+@torch.no_grad()\n+def write_model_and_image_processor(model_name, output_dir, push_to_hub, repo_id):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our RTDETR structure.\n+    \"\"\"\n+\n+    # load default config\n+    config = get_rt_detr_v2_config(model_name)\n+\n+    # load original model from torch hub\n+    model_name_to_checkpoint_url = {\n+        \"rtdetr_v2_r18vd\": \"https://github.com/lyuwenyu/storage/releases/download/v0.2/rtdetrv2_r18vd_120e_coco_rerun_48.1.pth\",\n+        \"rtdetr_v2_r34vd\": \"https://github.com/lyuwenyu/storage/releases/download/v0.1/rtdetrv2_r34vd_120e_coco_ema.pth\",\n+        \"rtdetr_v2_r50vd\": \"https://github.com/lyuwenyu/storage/releases/download/v0.1/rtdetrv2_r50vd_6x_coco_ema.pth\",\n+        \"rtdetr_v2_r101vd\": \"https://github.com/lyuwenyu/storage/releases/download/v0.1/rtdetrv2_r101vd_6x_coco_from_paddle.pth\",\n+    }\n+    logger.info(f\"Converting model {model_name}...\")\n+    state_dict = torch.hub.load_state_dict_from_url(model_name_to_checkpoint_url[model_name], map_location=\"cpu\")[\n+        \"ema\"\n+    ][\"module\"]\n+    # rename keys\n+    state_dict = convert_old_keys_to_new_keys(state_dict)\n+    for key in state_dict.copy().keys():\n+        if key.endswith(\"num_batches_tracked\"):\n+            del state_dict[key]\n+    # query, key and value matrices need special treatment\n+    read_in_q_k_v(state_dict, config)\n+    # important: we need to prepend a prefix to each of the base model keys as the head models use different attributes for them\n+    for key in state_dict.copy().keys():\n+        if key.endswith(\"num_batches_tracked\"):\n+            del state_dict[key]\n+        # for two_stage\n+        if \"bbox_embed\" in key or (\"class_embed\" in key and \"denoising_\" not in key):\n+            state_dict[key.split(\"model.decoder.\")[-1]] = state_dict[key]\n+\n+    # no need in ckpt\n+    del state_dict[\"decoder.anchors\"]\n+    del state_dict[\"decoder.valid_mask\"]\n+    # finally, create HuggingFace model and load state dict\n+    model = RTDetrV2ForObjectDetection(config)\n+    model.load_state_dict(state_dict)\n+    model.eval()\n+\n+    # load image processor\n+    image_processor = RTDetrImageProcessor()\n+\n+    # prepare image\n+    img = prepare_img()\n+\n+    # preprocess image\n+    transformations = transforms.Compose(\n+        [\n+            transforms.Resize([640, 640], interpolation=transforms.InterpolationMode.BILINEAR),\n+            transforms.ToTensor(),\n+        ]\n+    )\n+    original_pixel_values = transformations(img).unsqueeze(0)  # insert batch dimension\n+\n+    encoding = image_processor(images=img, return_tensors=\"pt\")\n+    pixel_values = encoding[\"pixel_values\"]\n+\n+    assert torch.allclose(original_pixel_values, pixel_values)\n+\n+    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n+    model.to(device)\n+    pixel_values = pixel_values.to(device)\n+\n+    # Pass image by the model\n+    with torch.no_grad():\n+        outputs = model(pixel_values)\n+\n+    if model_name == \"rtdetr_v2_r18vd\":\n+        expected_slice_logits = torch.tensor(\n+            [[-3.7045, -5.1913, -6.1787], [-4.0106, -9.3450, -5.2043], [-4.1287, -4.7463, -5.8634]]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [[0.2582, 0.5497, 0.4764], [0.1684, 0.1985, 0.2120], [0.7665, 0.4146, 0.4669]]\n+        )\n+    elif model_name == \"rtdetr_v2_r34vd\":\n+        expected_slice_logits = torch.tensor(\n+            [[-4.6108, -5.9453, -3.8505], [-3.8702, -6.1136, -5.5677], [-3.7790, -6.4538, -5.9449]]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [[0.1691, 0.1984, 0.2118], [0.2594, 0.5506, 0.4736], [0.7669, 0.4136, 0.4654]]\n+        )\n+    elif model_name == \"rtdetr_v2_r50vd\":\n+        expected_slice_logits = torch.tensor(\n+            [[-4.7881, -4.6754, -6.1624], [-5.4441, -6.6486, -4.3840], [-3.5455, -4.9318, -6.3544]]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [[0.2588, 0.5487, 0.4747], [0.5497, 0.2760, 0.0573], [0.7688, 0.4133, 0.4634]]\n+        )\n+    elif model_name == \"rtdetr_v2_r101vd\":\n+        expected_slice_logits = torch.tensor(\n+            [[-4.6162, -4.9189, -4.6656], [-4.4701, -4.4997, -4.9659], [-5.6641, -7.9000, -5.0725]]\n+        )\n+        expected_slice_boxes = torch.tensor(\n+            [[0.7707, 0.4124, 0.4585], [0.2589, 0.5492, 0.4735], [0.1688, 0.1993, 0.2108]]\n+        )\n+    else:\n+        raise ValueError(f\"Unknown rt_detr_v2_name: {model_name}\")\n+    assert torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits.to(outputs.logits.device), atol=1e-4)\n+    assert torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes.to(outputs.pred_boxes.device), atol=1e-3)\n+\n+    if output_dir is not None:\n+        Path(output_dir).mkdir(exist_ok=True)\n+        print(f\"Saving model {model_name} to {output_dir}\")\n+        model.save_pretrained(output_dir)\n+        print(f\"Saving image processor to {output_dir}\")\n+        image_processor.save_pretrained(output_dir)\n+\n+    if push_to_hub:\n+        # Upload model, image processor and config to the hub\n+        logger.info(\"Uploading PyTorch model and image processor to the hub...\")\n+        config.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add config from convert_rt_detr_v2_original_pytorch_checkpoint_to_pytorch.py\",\n+        )\n+        model.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add model from convert_rt_detr_v2_original_pytorch_checkpoint_to_pytorch.py\",\n+        )\n+        image_processor.push_to_hub(\n+            repo_id=repo_id,\n+            commit_message=\"Add image processor from convert_rt_detr_v2_original_pytorch_checkpoint_to_pytorch.py\",\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"rtdetr_v2_r18vd\",\n+        type=str,\n+        help=\"model_name of the checkpoint you'd like to convert.\",\n+    )\n+    parser.add_argument(\"--output_dir\", default=None, type=str, help=\"Location to write HF model and image processor\")\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether to push the model to the hub or not.\")\n+    parser.add_argument(\n+        \"--repo_id\",\n+        type=str,\n+        help=\"repo_id where the model will be pushed to.\",\n+    )\n+    args = parser.parse_args()\n+    write_model_and_image_processor(args.model_name, args.output_dir, args.push_to_hub, args.repo_id)"
        },
        {
            "sha": "11530a0800f693251fe86eb781a1a5a74fcfd6b3",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "added",
            "additions": 2122,
            "deletions": 0,
            "changes": 2122,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763"
        },
        {
            "sha": "0faff32514c0928259bd58386fea82f1013b3b75",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "added",
            "additions": 607,
            "deletions": 0,
            "changes": 607,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,607 @@\n+# coding=utf-8\n+# Copyright 2025 Baidu Inc and The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from functools import partial\n+from typing import List, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import Tensor, nn\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+from ...utils.backbone_utils import (\n+    verify_backbone_config_arguments,\n+)\n+from ..auto import CONFIG_MAPPING\n+from ..rt_detr.modeling_rt_detr import (\n+    RTDetrDecoder,\n+    RTDetrDecoderLayer,\n+    RTDetrForObjectDetection,\n+    RTDetrMLPPredictionHead,\n+    RTDetrModel,\n+    RTDetrMultiscaleDeformableAttention,\n+    RTDetrPreTrainedModel,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class RTDetrV2Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`RTDetrV2Model`]. It is used to instantiate a\n+    RT-DETR model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the RT-DETR architecture.\n+\n+    e.g. [PekingU/rtdetr_r18vd](https://huggingface.co/PekingU/rtdetr_r18vd)\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        initializer_range (`float`, *optional*, defaults to 0.01):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_bias_prior_prob (`float`, *optional*):\n+            The prior probability used by the bias initializer to initialize biases for `enc_score_head` and `class_embed`.\n+            If `None`, `prior_prob` computed as `prior_prob = 1 / (num_labels + 1)` while initializing model weights.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        backbone_config (`Dict`, *optional*, defaults to `RTDetrV2ResNetConfig()`):\n+            The configuration of the backbone model.\n+        backbone (`str`, *optional*):\n+            Name of backbone to use when `backbone_config` is `None`. If `use_pretrained_backbone` is `True`, this\n+            will load the corresponding pretrained weights from the timm or transformers library. If `use_pretrained_backbone`\n+            is `False`, this loads the backbone's config and uses that to initialize the backbone with random weights.\n+        use_pretrained_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to use pretrained weights for the backbone.\n+        use_timm_backbone (`bool`, *optional*, defaults to `False`):\n+            Whether to load `backbone` from the timm library. If `False`, the backbone is loaded from the transformers\n+            library.\n+        freeze_backbone_batch_norms (`bool`, *optional*, defaults to `True`):\n+            Whether to freeze the batch normalization layers in the backbone.\n+        backbone_kwargs (`dict`, *optional*):\n+            Keyword arguments to be passed to AutoBackbone when loading from a checkpoint\n+            e.g. `{'out_indices': (0, 1, 2, 3)}`. Cannot be specified if `backbone_config` is set.\n+        encoder_hidden_dim (`int`, *optional*, defaults to 256):\n+            Dimension of the layers in hybrid encoder.\n+        encoder_in_channels (`list`, *optional*, defaults to `[512, 1024, 2048]`):\n+            Multi level features input for encoder.\n+        feat_strides (`List[int]`, *optional*, defaults to `[8, 16, 32]`):\n+            Strides used in each feature map.\n+        encoder_layers (`int`, *optional*, defaults to 1):\n+            Total of layers to be used by the encoder.\n+        encoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        encoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        dropout (`float`, *optional*, defaults to 0.0):\n+            The ratio for all dropout layers.\n+        activation_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for activations inside the fully connected layer.\n+        encode_proj_layers (`List[int]`, *optional*, defaults to `[2]`):\n+            Indexes of the projected layers to be used in the encoder.\n+        positional_encoding_temperature (`int`, *optional*, defaults to 10000):\n+            The temperature parameter used to create the positional encodings.\n+        encoder_activation_function (`str`, *optional*, defaults to `\"gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        activation_function (`str`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the general layer. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        eval_size (`Tuple[int, int]`, *optional*):\n+            Height and width used to compute the effective height and width of the position embeddings after taking\n+            into account the stride.\n+        normalize_before (`bool`, *optional*, defaults to `False`):\n+            Determine whether to apply layer normalization in the transformer encoder layer before self-attention and\n+            feed-forward modules.\n+        hidden_expansion (`float`, *optional*, defaults to 1.0):\n+            Expansion ratio to enlarge the dimension size of RepVGGBlock and CSPRepLayer.\n+        d_model (`int`, *optional*, defaults to 256):\n+            Dimension of the layers exclude hybrid encoder.\n+        num_queries (`int`, *optional*, defaults to 300):\n+            Number of object queries.\n+        decoder_in_channels (`list`, *optional*, defaults to `[256, 256, 256]`):\n+            Multi level features dimension for decoder\n+        decoder_ffn_dim (`int`, *optional*, defaults to 1024):\n+            Dimension of the \"intermediate\" (often named feed-forward) layer in decoder.\n+        num_feature_levels (`int`, *optional*, defaults to 3):\n+            The number of input feature levels.\n+        decoder_n_points (`int`, *optional*, defaults to 4):\n+            The number of sampled keys in each feature level for each attention head in the decoder.\n+        decoder_layers (`int`, *optional*, defaults to 6):\n+            Number of decoder layers.\n+        decoder_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        decoder_activation_function (`str`, *optional*, defaults to `\"relu\"`):\n+            The non-linear activation function (function or string) in the decoder. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"silu\"` and `\"gelu_new\"` are supported.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_denoising (`int`, *optional*, defaults to 100):\n+            The total number of denoising tasks or queries to be used for contrastive denoising.\n+        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n+            The fraction of denoising labels to which random noise should be added.\n+        box_noise_scale (`float`, *optional*, defaults to 1.0):\n+            Scale or magnitude of noise to be added to the bounding boxes.\n+        learn_initial_query (`bool`, *optional*, defaults to `False`):\n+            Indicates whether the initial query embeddings for the decoder should be learned during training\n+        anchor_image_size (`Tuple[int, int]`, *optional*):\n+            Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n+        disable_custom_kernels (`bool`, *optional*, defaults to `True`):\n+            Whether to disable custom kernels.\n+        with_box_refine (`bool`, *optional*, defaults to `True`):\n+            Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n+            based on the predictions from the previous layer.\n+        is_encoder_decoder (`bool`, *optional*, defaults to `True`):\n+            Whether the architecture has an encoder decoder structure.\n+        matcher_alpha (`float`, *optional*, defaults to 0.25):\n+            Parameter alpha used by the Hungarian Matcher.\n+        matcher_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used by the Hungarian Matcher.\n+        matcher_class_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the class loss used by the Hungarian Matcher.\n+        matcher_bbox_cost (`float`, *optional*, defaults to 5.0):\n+            The relative weight of the bounding box loss used by the Hungarian Matcher.\n+        matcher_giou_cost (`float`, *optional*, defaults to 2.0):\n+            The relative weight of the giou loss of used by the Hungarian Matcher.\n+        use_focal_loss (`bool`, *optional*, defaults to `True`):\n+            Parameter informing if focal loss should be used.\n+        auxiliary_loss (`bool`, *optional*, defaults to `True`):\n+            Whether auxiliary decoding losses (loss at each decoder layer) are to be used.\n+        focal_loss_alpha (`float`, *optional*, defaults to 0.75):\n+            Parameter alpha used to compute the focal loss.\n+        focal_loss_gamma (`float`, *optional*, defaults to 2.0):\n+            Parameter gamma used to compute the focal loss.\n+        weight_loss_vfl (`float`, *optional*, defaults to 1.0):\n+            Relative weight of the varifocal loss in the object detection loss.\n+        weight_loss_bbox (`float`, *optional*, defaults to 5.0):\n+            Relative weight of the L1 bounding box loss in the object detection loss.\n+        weight_loss_giou (`float`, *optional*, defaults to 2.0):\n+            Relative weight of the generalized IoU loss in the object detection loss.\n+        eos_coefficient (`float`, *optional*, defaults to 0.0001):\n+            Relative classification weight of the 'no-object' class in the object detection loss.\n+        decoder_n_levels (`int`, *optional*, defaults to 3):\n+            The number of feature levels used by the decoder.\n+        decoder_offset_scale (`float`, *optional*, defaults to 0.5):\n+            Scaling factor applied to the attention offsets in the decoder.\n+        decoder_method (`str`, *optional*, defaults to `\"default\"`):\n+            The method to use for the decoder: `\"default\"` or `\"discrete\"`.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import RTDetrV2Config, RTDetrV2Model\n+\n+    >>> # Initializing a RT-DETR configuration\n+    >>> configuration = RTDetrV2Config()\n+\n+    >>> # Initializing a model (with random weights) from the configuration\n+    >>> model = RTDetrV2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"rt_detr_v2\"\n+    layer_types = [\"basic\", \"bottleneck\"]\n+    attribute_map = {\n+        \"hidden_size\": \"d_model\",\n+        \"num_attention_heads\": \"encoder_attention_heads\",\n+    }\n+\n+    def __init__(\n+        self,\n+        initializer_range=0.01,\n+        initializer_bias_prior_prob=None,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        backbone=None,\n+        use_pretrained_backbone=False,\n+        use_timm_backbone=False,\n+        freeze_backbone_batch_norms=True,\n+        backbone_kwargs=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=256,\n+        encoder_in_channels=[512, 1024, 2048],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=1024,\n+        encoder_attention_heads=8,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        hidden_expansion=1.0,\n+        # decoder RTDetrV2Transformer\n+        d_model=256,\n+        num_queries=300,\n+        decoder_in_channels=[256, 256, 256],\n+        decoder_ffn_dim=1024,\n+        num_feature_levels=3,\n+        decoder_n_points=4,\n+        decoder_layers=6,\n+        decoder_attention_heads=8,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=100,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        disable_custom_kernels=True,\n+        with_box_refine=True,\n+        is_encoder_decoder=True,\n+        # Loss\n+        matcher_alpha=0.25,\n+        matcher_gamma=2.0,\n+        matcher_class_cost=2.0,\n+        matcher_bbox_cost=5.0,\n+        matcher_giou_cost=2.0,\n+        use_focal_loss=True,\n+        auxiliary_loss=True,\n+        focal_loss_alpha=0.75,\n+        focal_loss_gamma=2.0,\n+        weight_loss_vfl=1.0,\n+        weight_loss_bbox=5.0,\n+        weight_loss_giou=2.0,\n+        eos_coefficient=1e-4,\n+        decoder_n_levels=3,  # default value\n+        decoder_offset_scale=0.5,  # default value\n+        decoder_method=\"default\",\n+        **kwargs,\n+    ):\n+        super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n+        self.initializer_range = initializer_range\n+        self.initializer_bias_prior_prob = initializer_bias_prior_prob\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        # backbone\n+        if backbone_config is None and backbone is None:\n+            logger.info(\n+                \"`backbone_config` and `backbone` are `None`. Initializing the config with the default `RTDetrV2-ResNet` backbone.\"\n+            )\n+            backbone_model_type = \"rt_detr_resnet\"\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            # this will map it to RTDetrResNetConfig\n+            # note: we can instead create RTDetrV2ResNetConfig but it will be exactly the same as V1\n+            # and we would need to create RTDetrV2ResNetModel\n+            backbone_config = config_class(\n+                num_channels=3,\n+                embedding_size=64,\n+                hidden_sizes=[256, 512, 1024, 2048],\n+                depths=[3, 4, 6, 3],\n+                layer_type=\"bottleneck\",\n+                hidden_act=\"relu\",\n+                downsample_in_first_stage=False,\n+                downsample_in_bottleneck=False,\n+                out_features=None,\n+                out_indices=[2, 3, 4],\n+            )\n+        elif isinstance(backbone_config, dict):\n+            backbone_model_type = backbone_config.pop(\"model_type\")\n+            config_class = CONFIG_MAPPING[backbone_model_type]\n+            backbone_config = config_class.from_dict(backbone_config)\n+\n+        verify_backbone_config_arguments(\n+            use_timm_backbone=use_timm_backbone,\n+            use_pretrained_backbone=use_pretrained_backbone,\n+            backbone=backbone,\n+            backbone_config=backbone_config,\n+            backbone_kwargs=backbone_kwargs,\n+        )\n+\n+        self.backbone_config = backbone_config\n+        self.backbone = backbone\n+        self.use_pretrained_backbone = use_pretrained_backbone\n+        self.use_timm_backbone = use_timm_backbone\n+        self.freeze_backbone_batch_norms = freeze_backbone_batch_norms\n+        self.backbone_kwargs = backbone_kwargs\n+        # encoder\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.encoder_layers = encoder_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.hidden_expansion = hidden_expansion\n+        self.num_queries = num_queries\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.decoder_in_channels = decoder_in_channels\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.auxiliary_loss = auxiliary_loss\n+        self.disable_custom_kernels = disable_custom_kernels\n+        self.with_box_refine = with_box_refine\n+        # Loss\n+        self.matcher_alpha = matcher_alpha\n+        self.matcher_gamma = matcher_gamma\n+        self.matcher_class_cost = matcher_class_cost\n+        self.matcher_bbox_cost = matcher_bbox_cost\n+        self.matcher_giou_cost = matcher_giou_cost\n+        self.use_focal_loss = use_focal_loss\n+        self.focal_loss_alpha = focal_loss_alpha\n+        self.focal_loss_gamma = focal_loss_gamma\n+        self.weight_loss_vfl = weight_loss_vfl\n+        self.weight_loss_bbox = weight_loss_bbox\n+        self.weight_loss_giou = weight_loss_giou\n+        self.eos_coefficient = eos_coefficient\n+\n+        if not hasattr(self, \"d_model\"):\n+            self.d_model = d_model\n+\n+        if not hasattr(self, \"encoder_attention_heads\"):\n+            self.encoder_attention_heads = encoder_attention_heads\n+        # add the new attributes with the given values or defaults\n+        self.decoder_n_levels = decoder_n_levels\n+        self.decoder_offset_scale = decoder_offset_scale\n+        self.decoder_method = decoder_method\n+\n+    @classmethod\n+    def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n+        \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model\n+        configuration.\n+\n+            Args:\n+                backbone_config ([`PretrainedConfig`]):\n+                    The backbone configuration.\n+\n+            Returns:\n+                [`RTDetrV2Config`]: An instance of a configuration object\n+        \"\"\"\n+        return cls(\n+            backbone_config=backbone_config,\n+            **kwargs,\n+        )\n+\n+\n+def multi_scale_deformable_attention_v2(\n+    value: Tensor,\n+    value_spatial_shapes: Tensor,\n+    sampling_locations: Tensor,\n+    attention_weights: Tensor,\n+    num_points_list: List[int],\n+    method=\"default\",\n+) -> Tensor:\n+    batch_size, _, num_heads, hidden_dim = value.shape\n+    _, num_queries, num_heads, num_levels, num_points = sampling_locations.shape\n+    value_list = (\n+        value.permute(0, 2, 3, 1)\n+        .flatten(0, 1)\n+        .split([height.item() * width.item() for height, width in value_spatial_shapes], dim=-1)\n+    )\n+    # sampling_offsets [8, 480, 8, 12, 2]\n+    if method == \"default\":\n+        sampling_grids = 2 * sampling_locations - 1\n+    elif method == \"discrete\":\n+        sampling_grids = sampling_locations\n+    sampling_grids = sampling_grids.permute(0, 2, 1, 3, 4).flatten(0, 1)\n+    sampling_grids = sampling_grids.split(num_points_list, dim=-2)\n+    sampling_value_list = []\n+    for level_id, (height, width) in enumerate(value_spatial_shapes):\n+        # batch_size, height*width, num_heads, hidden_dim\n+        # -> batch_size, height*width, num_heads*hidden_dim\n+        # -> batch_size, num_heads*hidden_dim, height*width\n+        # -> batch_size*num_heads, hidden_dim, height, width\n+        value_l_ = value_list[level_id].reshape(batch_size * num_heads, hidden_dim, height, width)\n+        # batch_size, num_queries, num_heads, num_points, 2\n+        # -> batch_size, num_heads, num_queries, num_points, 2\n+        # -> batch_size*num_heads, num_queries, num_points, 2\n+        sampling_grid_l_ = sampling_grids[level_id]\n+        # batch_size*num_heads, hidden_dim, num_queries, num_points\n+        if method == \"default\":\n+            sampling_value_l_ = nn.functional.grid_sample(\n+                value_l_, sampling_grid_l_, mode=\"bilinear\", padding_mode=\"zeros\", align_corners=False\n+            )\n+        elif method == \"discrete\":\n+            sampling_coord = (sampling_grid_l_ * torch.tensor([[width, height]], device=value.device) + 0.5).to(\n+                torch.int64\n+            )\n+\n+            # Separate clamping for x and y coordinates\n+            sampling_coord_x = sampling_coord[..., 0].clamp(0, width - 1)\n+            sampling_coord_y = sampling_coord[..., 1].clamp(0, height - 1)\n+\n+            # Combine the clamped coordinates\n+            sampling_coord = torch.stack([sampling_coord_x, sampling_coord_y], dim=-1)\n+            sampling_coord = sampling_coord.reshape(batch_size * num_heads, num_queries * num_points_list[level_id], 2)\n+            sampling_idx = (\n+                torch.arange(sampling_coord.shape[0], device=value.device)\n+                .unsqueeze(-1)\n+                .repeat(1, sampling_coord.shape[1])\n+            )\n+            sampling_value_l_ = value_l_[sampling_idx, :, sampling_coord[..., 1], sampling_coord[..., 0]]\n+            sampling_value_l_ = sampling_value_l_.permute(0, 2, 1).reshape(\n+                batch_size * num_heads, hidden_dim, num_queries, num_points_list[level_id]\n+            )\n+        sampling_value_list.append(sampling_value_l_)\n+    # (batch_size, num_queries, num_heads, num_levels, num_points)\n+    # -> (batch_size, num_heads, num_queries, num_levels, num_points)\n+    # -> (batch_size, num_heads, 1, num_queries, num_levels*num_points)\n+    attention_weights = attention_weights.permute(0, 2, 1, 3).reshape(\n+        batch_size * num_heads, 1, num_queries, sum(num_points_list)\n+    )\n+    output = (\n+        (torch.concat(sampling_value_list, dim=-1) * attention_weights)\n+        .sum(-1)\n+        .view(batch_size, num_heads * hidden_dim, num_queries)\n+    )\n+    return output.transpose(1, 2).contiguous()\n+\n+\n+# the main change\n+class RTDetrV2MultiscaleDeformableAttention(RTDetrMultiscaleDeformableAttention):\n+    \"\"\"\n+    RTDetrV2 version of multiscale deformable attention, extending the base implementation\n+    with improved offset handling and initialization.\n+    \"\"\"\n+\n+    def __init__(self, config: RTDetrV2Config):\n+        num_heads = config.decoder_attention_heads\n+        n_points = config.decoder_n_points\n+        # Initialize parent class with config parameters\n+        super().__init__(config=config, num_heads=num_heads, n_points=n_points)\n+\n+        # V2-specific attributes\n+        self.n_levels = config.decoder_n_levels\n+        self.offset_scale = config.decoder_offset_scale\n+        self.method = config.decoder_method\n+        # Initialize n_points list and scale\n+        n_points_list = [self.n_points for _ in range(self.n_levels)]\n+        self.n_points_list = n_points_list\n+        n_points_scale = [1 / n for n in n_points_list for _ in range(n)]\n+        self.register_buffer(\"n_points_scale\", torch.tensor(n_points_scale, dtype=torch.float32))\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        level_start_index=None,\n+        output_attentions: bool = False,\n+        **kwargs,\n+    ):\n+        # Process inputs up to sampling locations calculation using parent class logic\n+        if position_embeddings is not None:\n+            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+\n+        batch_size, num_queries, _ = hidden_states.shape\n+        batch_size, sequence_length, _ = encoder_hidden_states.shape\n+        if (spatial_shapes[:, 0] * spatial_shapes[:, 1]).sum() != sequence_length:\n+            raise ValueError(\n+                \"Make sure to align the spatial shapes with the sequence length of the encoder hidden states\"\n+            )\n+\n+        value = self.value_proj(encoder_hidden_states)\n+        if attention_mask is not None:\n+            value = value.masked_fill(~attention_mask[..., None], float(0))\n+        value = value.view(batch_size, sequence_length, self.n_heads, self.d_model // self.n_heads)\n+\n+        # V2-specific sampling offsets shape\n+        sampling_offsets = self.sampling_offsets(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points, 2\n+        )\n+\n+        attention_weights = self.attention_weights(hidden_states).view(\n+            batch_size, num_queries, self.n_heads, self.n_levels * self.n_points\n+        )\n+        attention_weights = F.softmax(attention_weights, -1)\n+\n+        # V2-specific sampling locations calculation\n+        if reference_points.shape[-1] == 2:\n+            offset_normalizer = torch.stack([spatial_shapes[..., 1], spatial_shapes[..., 0]], -1)\n+            sampling_locations = (\n+                reference_points[:, :, None, :, None, :]\n+                + sampling_offsets / offset_normalizer[None, None, None, :, None, :]\n+            )\n+        elif reference_points.shape[-1] == 4:\n+            n_points_scale = self.n_points_scale.to(dtype=hidden_states.dtype).unsqueeze(-1)\n+            offset = sampling_offsets * n_points_scale * reference_points[:, :, None, :, 2:] * self.offset_scale\n+            sampling_locations = reference_points[:, :, None, :, :2] + offset\n+        else:\n+            raise ValueError(f\"Last dim of reference_points must be 2 or 4, but got {reference_points.shape[-1]}\")\n+\n+        # V2-specific attention implementation choice\n+        output = multi_scale_deformable_attention_v2(\n+            value, spatial_shapes, sampling_locations, attention_weights, self.n_points_list, self.method\n+        )\n+\n+        output = self.output_proj(output)\n+        return output, attention_weights\n+\n+\n+class RTDetrV2DecoderLayer(RTDetrDecoderLayer):\n+    def __init__(self, config: RTDetrV2Config):\n+        # initialize parent class\n+        super().__init__(config)\n+        # override only the encoder attention module with v2 version\n+        self.encoder_attn = RTDetrV2MultiscaleDeformableAttention(config)\n+\n+\n+class RTDetrV2PreTrainedModel(RTDetrPreTrainedModel):\n+    pass\n+\n+\n+class RTDetrV2Decoder(RTDetrDecoder):\n+    def __init__(self, config: RTDetrV2Config):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList([RTDetrV2DecoderLayer(config) for _ in range(config.decoder_layers)])\n+\n+\n+class RTDetrV2Model(RTDetrModel):\n+    def __init__(self, config: RTDetrV2Config):\n+        super().__init__(config)\n+        # decoder\n+        self.decoder = RTDetrV2Decoder(config)\n+\n+\n+class RTDetrV2MLPPredictionHead(RTDetrMLPPredictionHead):\n+    pass\n+\n+\n+class RTDetrV2ForObjectDetection(RTDetrForObjectDetection, RTDetrV2PreTrainedModel):\n+    def __init__(self, config: RTDetrV2Config):\n+        RTDetrV2PreTrainedModel.__init__(config)\n+        # RTDETR encoder-decoder model\n+        self.model = RTDetrV2Model(config)\n+\n+        # Detection heads on top\n+        class_embed = partial(nn.Linear, config.d_model, config.num_labels)\n+        bbox_embed = partial(RTDetrV2MLPPredictionHead, config, config.d_model, config.d_model, 4, num_layers=3)\n+\n+        self.class_embed = nn.ModuleList([class_embed() for _ in range(config.decoder_layers)])\n+        self.bbox_embed = nn.ModuleList([bbox_embed() for _ in range(config.decoder_layers)])\n+\n+        self.model.decoder.class_embed = self.class_embed\n+        self.model.decoder.bbox_embed = self.bbox_embed\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+__all__ = [\n+    \"RTDetrV2Config\",\n+    \"RTDetrV2Model\",\n+    \"RTDetrV2PreTrainedModel\",\n+    \"RTDetrV2ForObjectDetection\",\n+]"
        },
        {
            "sha": "aaf6a9196a13fd12f3e2522c1062bac377640412",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -8492,6 +8492,27 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class RTDetrV2ForObjectDetection(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class RTDetrV2Model(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class RTDetrV2PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class RwkvForCausalLM(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/rt_detr_v2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/tests%2Fmodels%2Frt_detr_v2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/tests%2Fmodels%2Frt_detr_v2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2F__init__.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763"
        },
        {
            "sha": "64664d7b940f025dac5a986f7cae7d278f53565c",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "added",
            "additions": 769,
            "deletions": 0,
            "changes": 769,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -0,0 +1,769 @@\n+# coding = utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch RT_DETR_V2 model.\"\"\"\n+\n+import inspect\n+import math\n+import tempfile\n+import unittest\n+\n+from parameterized import parameterized\n+\n+from transformers import (\n+    RTDetrResNetConfig,\n+    RTDetrV2Config,\n+    is_torch_available,\n+    is_vision_available,\n+)\n+from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.utils import cached_property\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import RTDetrV2ForObjectDetection, RTDetrV2Model\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import RTDetrImageProcessor\n+\n+\n+CHECKPOINT = \"PekingU/rtdetr_v2_r18vd\"\n+\n+\n+class RTDetrV2ModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=3,\n+        is_training=True,\n+        use_labels=True,\n+        n_targets=3,\n+        num_labels=10,\n+        initializer_range=0.02,\n+        layer_norm_eps=1e-5,\n+        batch_norm_eps=1e-5,\n+        # backbone\n+        backbone_config=None,\n+        # encoder HybridEncoder\n+        encoder_hidden_dim=32,\n+        encoder_in_channels=[128, 256, 512],\n+        feat_strides=[8, 16, 32],\n+        encoder_layers=1,\n+        encoder_ffn_dim=64,\n+        encoder_attention_heads=2,\n+        dropout=0.0,\n+        activation_dropout=0.0,\n+        encode_proj_layers=[2],\n+        positional_encoding_temperature=10000,\n+        encoder_activation_function=\"gelu\",\n+        activation_function=\"silu\",\n+        eval_size=None,\n+        normalize_before=False,\n+        # decoder RTDetrV2Transformer\n+        d_model=32,\n+        num_queries=30,\n+        decoder_in_channels=[32, 32, 32],\n+        decoder_ffn_dim=64,\n+        num_feature_levels=3,\n+        decoder_n_points=4,\n+        decoder_n_levels=3,\n+        decoder_layers=2,\n+        decoder_attention_heads=2,\n+        decoder_activation_function=\"relu\",\n+        attention_dropout=0.0,\n+        num_denoising=0,\n+        label_noise_ratio=0.5,\n+        box_noise_scale=1.0,\n+        learn_initial_query=False,\n+        anchor_image_size=None,\n+        image_size=64,\n+        disable_custom_kernels=True,\n+        with_box_refine=True,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = 3\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.n_targets = n_targets\n+        self.num_labels = num_labels\n+        self.initializer_range = initializer_range\n+        self.layer_norm_eps = layer_norm_eps\n+        self.batch_norm_eps = batch_norm_eps\n+        self.backbone_config = backbone_config\n+        self.encoder_hidden_dim = encoder_hidden_dim\n+        self.encoder_in_channels = encoder_in_channels\n+        self.feat_strides = feat_strides\n+        self.encoder_layers = encoder_layers\n+        self.encoder_ffn_dim = encoder_ffn_dim\n+        self.encoder_attention_heads = encoder_attention_heads\n+        self.dropout = dropout\n+        self.activation_dropout = activation_dropout\n+        self.encode_proj_layers = encode_proj_layers\n+        self.positional_encoding_temperature = positional_encoding_temperature\n+        self.encoder_activation_function = encoder_activation_function\n+        self.activation_function = activation_function\n+        self.eval_size = eval_size\n+        self.normalize_before = normalize_before\n+        self.d_model = d_model\n+        self.num_queries = num_queries\n+        self.decoder_in_channels = decoder_in_channels\n+        self.decoder_ffn_dim = decoder_ffn_dim\n+        self.num_feature_levels = num_feature_levels\n+        self.decoder_n_points = decoder_n_points\n+        self.decoder_n_levels = decoder_n_levels\n+        self.decoder_layers = decoder_layers\n+        self.decoder_attention_heads = decoder_attention_heads\n+        self.decoder_activation_function = decoder_activation_function\n+        self.attention_dropout = attention_dropout\n+        self.num_denoising = num_denoising\n+        self.label_noise_ratio = label_noise_ratio\n+        self.box_noise_scale = box_noise_scale\n+        self.learn_initial_query = learn_initial_query\n+        self.anchor_image_size = anchor_image_size\n+        self.image_size = image_size\n+        self.disable_custom_kernels = disable_custom_kernels\n+        self.with_box_refine = with_box_refine\n+\n+        self.encoder_seq_length = math.ceil(self.image_size / 32) * math.ceil(self.image_size / 32)\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+\n+        pixel_mask = torch.ones([self.batch_size, self.image_size, self.image_size], device=torch_device)\n+\n+        labels = None\n+        if self.use_labels:\n+            # labels is a list of Dict (each Dict being the labels for a given example in the batch)\n+            labels = []\n+            for i in range(self.batch_size):\n+                target = {}\n+                target[\"class_labels\"] = torch.randint(\n+                    high=self.num_labels, size=(self.n_targets,), device=torch_device\n+                )\n+                target[\"boxes\"] = torch.rand(self.n_targets, 4, device=torch_device)\n+                labels.append(target)\n+\n+        config = self.get_config()\n+        config.num_labels = self.num_labels\n+        return config, pixel_values, pixel_mask, labels\n+\n+    def get_config(self):\n+        hidden_sizes = [10, 20, 30, 40]\n+        backbone_config = RTDetrResNetConfig(\n+            embeddings_size=10,\n+            hidden_sizes=hidden_sizes,\n+            depths=[1, 1, 2, 1],\n+            out_features=[\"stage2\", \"stage3\", \"stage4\"],\n+            out_indices=[2, 3, 4],\n+        )\n+        return RTDetrV2Config.from_backbone_configs(\n+            backbone_config=backbone_config,\n+            encoder_hidden_dim=self.encoder_hidden_dim,\n+            encoder_in_channels=hidden_sizes[1:],\n+            feat_strides=self.feat_strides,\n+            encoder_layers=self.encoder_layers,\n+            encoder_ffn_dim=self.encoder_ffn_dim,\n+            encoder_attention_heads=self.encoder_attention_heads,\n+            dropout=self.dropout,\n+            activation_dropout=self.activation_dropout,\n+            encode_proj_layers=self.encode_proj_layers,\n+            positional_encoding_temperature=self.positional_encoding_temperature,\n+            encoder_activation_function=self.encoder_activation_function,\n+            activation_function=self.activation_function,\n+            eval_size=self.eval_size,\n+            normalize_before=self.normalize_before,\n+            d_model=self.d_model,\n+            num_queries=self.num_queries,\n+            decoder_in_channels=self.decoder_in_channels,\n+            decoder_ffn_dim=self.decoder_ffn_dim,\n+            num_feature_levels=self.num_feature_levels,\n+            decoder_n_points=self.decoder_n_points,\n+            decoder_n_levels=self.decoder_n_levels,\n+            decoder_layers=self.decoder_layers,\n+            decoder_attention_heads=self.decoder_attention_heads,\n+            decoder_activation_function=self.decoder_activation_function,\n+            attention_dropout=self.attention_dropout,\n+            num_denoising=self.num_denoising,\n+            label_noise_ratio=self.label_noise_ratio,\n+            box_noise_scale=self.box_noise_scale,\n+            learn_initial_query=self.learn_initial_query,\n+            anchor_image_size=self.anchor_image_size,\n+            image_size=self.image_size,\n+            disable_custom_kernels=self.disable_custom_kernels,\n+            with_box_refine=self.with_box_refine,\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_mask, labels = self.prepare_config_and_inputs()\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+    def create_and_check_rt_detr_model(self, config, pixel_values, pixel_mask, labels):\n+        model = RTDetrV2Model(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.num_queries, self.d_model))\n+\n+    def create_and_check_rt_detr_object_detection_head_model(self, config, pixel_values, pixel_mask, labels):\n+        model = RTDetrV2ForObjectDetection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n+        result = model(pixel_values)\n+\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+        result = model(pixel_values=pixel_values, pixel_mask=pixel_mask, labels=labels)\n+\n+        self.parent.assertEqual(result.loss.shape, ())\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_queries, self.num_labels))\n+        self.parent.assertEqual(result.pred_boxes.shape, (self.batch_size, self.num_queries, 4))\n+\n+\n+@require_torch\n+class RTDetrV2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (RTDetrV2Model, RTDetrV2ForObjectDetection) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"image-feature-extraction\": RTDetrV2Model, \"object-detection\": RTDetrV2ForObjectDetection}\n+        if is_torch_available()\n+        else {}\n+    )\n+    is_encoder_decoder = True\n+    test_torchscript = False\n+    test_pruning = False\n+    test_head_masking = False\n+    test_missing_keys = False\n+\n+    # special case for head models\n+    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n+        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n+\n+        if return_labels:\n+            if model_class.__name__ == \"RTDetrV2ForObjectDetection\":\n+                labels = []\n+                for i in range(self.model_tester.batch_size):\n+                    target = {}\n+                    target[\"class_labels\"] = torch.ones(\n+                        size=(self.model_tester.n_targets,), device=torch_device, dtype=torch.long\n+                    )\n+                    target[\"boxes\"] = torch.ones(\n+                        self.model_tester.n_targets, 4, device=torch_device, dtype=torch.float\n+                    )\n+                    labels.append(target)\n+                inputs_dict[\"labels\"] = labels\n+\n+        return inputs_dict\n+\n+    def setUp(self):\n+        self.model_tester = RTDetrV2ModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self,\n+            config_class=RTDetrV2Config,\n+            has_text_modality=False,\n+            common_properties=[\"hidden_size\", \"num_attention_heads\"],\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_rt_detr_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_rt_detr_model(*config_and_inputs)\n+\n+    def test_rt_detr_object_detection_head_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_rt_detr_object_detection_head_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"RTDetrV2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RTDetrV2 does not use test_inputs_embeds_matches_input_ids\")\n+    def test_inputs_embeds_matches_input_ids(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RTDetrV2 does not support input and output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RTDetrV2 does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    @unittest.skip(reason=\"RTDetrV2 does not use token embeddings\")\n+    def test_resize_tokens_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Feed forward chunking is not implemented\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_attention_outputs(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.return_dict = True\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = False\n+            config.return_dict = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions\n+            self.assertEqual(len(attentions), self.model_tester.encoder_layers)\n+\n+            # check that output_attentions also work using config\n+            del inputs_dict[\"output_attentions\"]\n+            config.output_attentions = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+            attentions = outputs.encoder_attentions\n+            self.assertEqual(len(attentions), self.model_tester.encoder_layers)\n+\n+            self.assertListEqual(\n+                list(attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.encoder_attention_heads,\n+                    self.model_tester.encoder_seq_length,\n+                    self.model_tester.encoder_seq_length,\n+                ],\n+            )\n+            out_len = len(outputs)\n+\n+            correct_outlen = 13\n+\n+            # loss is at first position\n+            if \"labels\" in inputs_dict:\n+                correct_outlen += 1  # loss is added to beginning\n+            # Object Detection model returns pred_logits and pred_boxes\n+            if model_class.__name__ == \"RTDetrV2ForObjectDetection\":\n+                correct_outlen += 2\n+\n+            self.assertEqual(out_len, correct_outlen)\n+\n+            # decoder attentions\n+            decoder_attentions = outputs.decoder_attentions\n+            self.assertIsInstance(decoder_attentions, (list, tuple))\n+            self.assertEqual(len(decoder_attentions), self.model_tester.decoder_layers)\n+            self.assertListEqual(\n+                list(decoder_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.decoder_attention_heads,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_queries,\n+                ],\n+            )\n+\n+            # cross attentions\n+            cross_attentions = outputs.cross_attentions\n+            self.assertIsInstance(cross_attentions, (list, tuple))\n+            self.assertEqual(len(cross_attentions), self.model_tester.decoder_layers)\n+            self.assertListEqual(\n+                list(cross_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.num_queries,\n+                    self.model_tester.decoder_attention_heads,\n+                    self.model_tester.decoder_n_levels * self.model_tester.decoder_n_points,\n+                ],\n+            )\n+\n+            # Check attention is always last and order is fine\n+            inputs_dict[\"output_attentions\"] = True\n+            inputs_dict[\"output_hidden_states\"] = True\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if hasattr(self.model_tester, \"num_hidden_states_types\"):\n+                added_hidden_states = self.model_tester.num_hidden_states_types\n+            else:\n+                # RTDetrV2 should maintin encoder_hidden_states output\n+                added_hidden_states = 2\n+            self.assertEqual(out_len + added_hidden_states, len(outputs))\n+\n+            self_attentions = outputs.encoder_attentions\n+\n+            self.assertEqual(len(self_attentions), self.model_tester.encoder_layers)\n+            self.assertListEqual(\n+                list(self_attentions[0].shape[-3:]),\n+                [\n+                    self.model_tester.encoder_attention_heads,\n+                    self.model_tester.encoder_seq_length,\n+                    self.model_tester.encoder_seq_length,\n+                ],\n+            )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            expected_num_layers = getattr(\n+                self.model_tester, \"expected_num_hidden_layers\", len(self.model_tester.encoder_in_channels) - 1\n+            )\n+            self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+            self.assertListEqual(\n+                list(hidden_states[1].shape[-2:]),\n+                [\n+                    self.model_tester.image_size // self.model_tester.feat_strides[-1],\n+                    self.model_tester.image_size // self.model_tester.feat_strides[-1],\n+                ],\n+            )\n+\n+            if config.is_encoder_decoder:\n+                hidden_states = outputs.decoder_hidden_states\n+\n+                expected_num_layers = getattr(\n+                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.decoder_layers + 1\n+                )\n+\n+                self.assertIsInstance(hidden_states, (list, tuple))\n+                self.assertEqual(len(hidden_states), expected_num_layers)\n+\n+                self.assertListEqual(\n+                    list(hidden_states[0].shape[-2:]),\n+                    [self.model_tester.num_queries, self.model_tester.d_model],\n+                )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            inputs_dict[\"output_hidden_states\"] = True\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+            # check that output_hidden_states also work using config\n+            del inputs_dict[\"output_hidden_states\"]\n+            config.output_hidden_states = True\n+\n+            check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    def test_retain_grad_hidden_states_attentions(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        config.output_hidden_states = True\n+        config.output_attentions = True\n+\n+        model_class = self.all_model_classes[0]\n+        model = model_class(config)\n+        model.to(torch_device)\n+\n+        inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+        outputs = model(**inputs)\n+\n+        # we take the first output since last_hidden_state is the first item\n+        output = outputs[0]\n+\n+        encoder_hidden_states = outputs.encoder_hidden_states[0]\n+        encoder_attentions = outputs.encoder_attentions[0]\n+        encoder_hidden_states.retain_grad()\n+        encoder_attentions.retain_grad()\n+\n+        decoder_attentions = outputs.decoder_attentions[0]\n+        decoder_attentions.retain_grad()\n+\n+        cross_attentions = outputs.cross_attentions[0]\n+        cross_attentions.retain_grad()\n+\n+        output.flatten()[0].backward(retain_graph=True)\n+\n+        self.assertIsNotNone(encoder_hidden_states.grad)\n+        self.assertIsNotNone(encoder_attentions.grad)\n+        self.assertIsNotNone(decoder_attentions.grad)\n+        self.assertIsNotNone(cross_attentions.grad)\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            arg_names = [*signature.parameters.keys()]\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_different_timm_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # let's pick a random timm backbone\n+        config.backbone = \"tf_mobilenetv3_small_075\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"RTDetrV2ForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_labels,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n+            else:\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n+\n+            self.assertTrue(outputs)\n+\n+    def test_hf_backbone(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Load a pretrained HF checkpoint as backbone\n+        config.backbone = \"microsoft/resnet-18\"\n+        config.backbone_config = None\n+        config.use_timm_backbone = False\n+        config.use_pretrained_backbone = True\n+        config.backbone_kwargs = {\"out_indices\": [2, 3, 4]}\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            if model_class.__name__ == \"RTDetrV2ForObjectDetection\":\n+                expected_shape = (\n+                    self.model_tester.batch_size,\n+                    self.model_tester.num_queries,\n+                    self.model_tester.num_labels,\n+                )\n+                self.assertEqual(outputs.logits.shape, expected_shape)\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.model.backbone.intermediate_channel_sizes), 3)\n+            else:\n+                # Confirm out_indices was propogated to backbone\n+                self.assertEqual(len(model.backbone.intermediate_channel_sizes), 3)\n+\n+            self.assertTrue(outputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        configs_no_init.initializer_bias_prior_prob = 0.2\n+        bias_value = -1.3863  # log_e ((1 - 0.2) / 0.2)\n+\n+        failed_cases = []\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            # Skip the check for the backbone\n+            for name, module in model.named_modules():\n+                if module.__class__.__name__ == \"RTDetrV2ConvEncoder\":\n+                    backbone_params = [f\"{name}.{key}\" for key in module.state_dict().keys()]\n+                    break\n+\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    if (\"class_embed\" in name and \"bias\" in name) or \"enc_score_head.bias\" in name:\n+                        bias_tensor = torch.full_like(param.data, bias_value)\n+                        if not torch.allclose(param.data, bias_tensor, atol=1e-4):\n+                            failed_cases.append(\n+                                f\"Parameter {name} of model {model_class} seems not properly initialized. \"\n+                                f\"Biases should be initialized to {bias_value}, got {param.data}\"\n+                            )\n+                    elif (\n+                        \"level_embed\" in name\n+                        or \"sampling_offsets.bias\" in name\n+                        or \"value_proj\" in name\n+                        or \"output_proj\" in name\n+                        or \"reference_points\" in name\n+                        or \"enc_score_head.weight\" in name\n+                        or (\"class_embed\" in name and \"weight\" in name)\n+                        or name in backbone_params\n+                    ):\n+                        continue\n+                    else:\n+                        mean = param.data.mean()\n+                        round_mean = (mean * 1e9).round() / 1e9\n+                        round_mean = round_mean.item()\n+                        if round_mean not in [0.0, 1.0]:\n+                            failed_cases.append(\n+                                f\"Parameter {name} of model {model_class} seems not properly initialized. \"\n+                                f\"Mean is {round_mean}, but should be in [0, 1]\"\n+                            )\n+\n+        message = \"\\n\" + \"\\n\".join(failed_cases)\n+        self.assertTrue(not failed_cases, message)\n+\n+    @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n+    @require_torch_gpu\n+    @slow\n+    def test_inference_with_different_dtypes(self, torch_dtype_str):\n+        torch_dtype = {\n+            \"float32\": torch.float32,\n+            \"float16\": torch.float16,\n+            \"bfloat16\": torch.bfloat16,\n+        }[torch_dtype_str]\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device).to(torch_dtype)\n+            model.eval()\n+            for key, tensor in inputs_dict.items():\n+                if tensor.dtype == torch.float32:\n+                    inputs_dict[key] = tensor.to(torch_dtype)\n+            with torch.no_grad():\n+                _ = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+    @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n+    @require_torch_gpu\n+    @slow\n+    def test_inference_equivalence_for_static_and_dynamic_anchors(self, torch_dtype_str):\n+        torch_dtype = {\n+            \"float32\": torch.float32,\n+            \"float16\": torch.float16,\n+            \"bfloat16\": torch.bfloat16,\n+        }[torch_dtype_str]\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        h, w = inputs_dict[\"pixel_values\"].shape[-2:]\n+\n+        # convert inputs to the desired dtype\n+        for key, tensor in inputs_dict.items():\n+            if tensor.dtype == torch.float32:\n+                inputs_dict[key] = tensor.to(torch_dtype)\n+\n+        for model_class in self.all_model_classes:\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model_class(config).save_pretrained(tmpdirname)\n+                model_static = model_class.from_pretrained(\n+                    tmpdirname, anchor_image_size=[h, w], device_map=torch_device, torch_dtype=torch_dtype\n+                ).eval()\n+                model_dynamic = model_class.from_pretrained(\n+                    tmpdirname, anchor_image_size=None, device_map=torch_device, torch_dtype=torch_dtype\n+                ).eval()\n+\n+            self.assertIsNotNone(model_static.config.anchor_image_size)\n+            self.assertIsNone(model_dynamic.config.anchor_image_size)\n+\n+            with torch.no_grad():\n+                outputs_static = model_static(**self._prepare_for_class(inputs_dict, model_class))\n+                outputs_dynamic = model_dynamic(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            self.assertTrue(\n+                torch.allclose(\n+                    outputs_static.last_hidden_state, outputs_dynamic.last_hidden_state, rtol=1e-4, atol=1e-4\n+                ),\n+                f\"Max diff: {(outputs_static.last_hidden_state - outputs_dynamic.last_hidden_state).abs().max()}\",\n+            )\n+\n+\n+TOLERANCE = 1e-4\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n+    return image\n+\n+\n+@require_torch\n+@require_vision\n+class RTDetrV2ModelIntegrationTest(unittest.TestCase):\n+    @cached_property\n+    def default_image_processor(self):\n+        return RTDetrImageProcessor.from_pretrained(CHECKPOINT) if is_vision_available() else None\n+\n+    def test_inference_object_detection_head(self):\n+        model = RTDetrV2ForObjectDetection.from_pretrained(CHECKPOINT).to(torch_device)\n+\n+        image_processor = self.default_image_processor\n+        image = prepare_img()\n+        inputs = image_processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        expected_shape_logits = torch.Size((1, 300, model.config.num_labels))\n+        self.assertEqual(outputs.logits.shape, expected_shape_logits)\n+\n+        expected_logits = torch.tensor(\n+            [\n+                [-3.7047, -5.1914, -6.1787],\n+                [-4.0108, -9.3449, -5.2047],\n+                [-4.1287, -4.7461, -5.8633],\n+            ]\n+        ).to(torch_device)\n+        expected_boxes = torch.tensor(\n+            [\n+                [0.2582, 0.5497, 0.4764],\n+                [0.1684, 0.1985, 0.2120],\n+                [0.7665, 0.4146, 0.4669],\n+            ]\n+        ).to(torch_device)\n+\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, atol=1e-4, rtol=1e-4)\n+\n+        expected_shape_boxes = torch.Size((1, 300, 4))\n+        self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4, rtol=1e-4)\n+\n+        # verify postprocessing\n+        results = image_processor.post_process_object_detection(\n+            outputs, threshold=0.0, target_sizes=[image.size[::-1]]\n+        )[0]\n+        expected_scores = torch.tensor([0.9652, 0.9599, 0.9462, 0.8613], device=torch_device)\n+        expected_labels = [15, 15, 65, 57]\n+        expected_slice_boxes = torch.tensor(\n+            [\n+                [3.4114e02, 2.5111e01, 6.3998e02, 3.7289e02],\n+                [1.2780e01, 5.6346e01, 3.1767e02, 4.7134e02],\n+                [3.9959e01, 7.3117e01, 1.7565e02, 1.1744e02],\n+                [-1.0521e-01, 2.9717e00, 6.3989e02, 4.7362e02],\n+            ],\n+            device=torch_device,\n+        )\n+        self.assertTrue(torch.allclose(results[\"scores\"][:4], expected_scores, atol=1e-3, rtol=1e-4))\n+        self.assertSequenceEqual(results[\"labels\"][:4].tolist(), expected_labels)\n+        torch.testing.assert_close(results[\"boxes\"][:4], expected_slice_boxes, atol=1e-3, rtol=1e-4)"
        },
        {
            "sha": "87b7e8be0adfec306f93caec29dc82c017fcfeb1",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/006d9249ec0270ff6c4d3840979d23fe94bdc763/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/006d9249ec0270ff6c4d3840979d23fe94bdc763/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=006d9249ec0270ff6c4d3840979d23fe94bdc763",
            "patch": "@@ -203,6 +203,20 @@\n         \"weight_loss_giou\",\n         \"weight_loss_vfl\",\n     ],\n+    \"RTDetrV2Config\": [\n+        \"eos_coefficient\",\n+        \"focal_loss_alpha\",\n+        \"focal_loss_gamma\",\n+        \"matcher_alpha\",\n+        \"matcher_bbox_cost\",\n+        \"matcher_class_cost\",\n+        \"matcher_gamma\",\n+        \"matcher_giou_cost\",\n+        \"use_focal_loss\",\n+        \"weight_loss_bbox\",\n+        \"weight_loss_giou\",\n+        \"weight_loss_vfl\",\n+    ],\n     \"YolosConfig\": [\n         \"bbox_cost\",\n         \"bbox_loss_coefficient\","
        }
    ],
    "stats": {
        "total": 4435,
        "additions": 4431,
        "deletions": 4
    }
}