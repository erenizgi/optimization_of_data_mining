{
    "author": "McPatate",
    "message": "feat: add benchmark v2 ci with results pushed to dataset (#41672)",
    "sha": "71db0d49e99884566026c140f8b12b61056fa8dc",
    "files": [
        {
            "sha": "46f24c3f901d12f5de91756ddf6e58740debd570",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 10,
            "deletions": 21,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -1,14 +1,19 @@\n name: Self-hosted runner (benchmark)\r\n \r\n on:\r\n-  workflow_dispatch:\r\n+  push:\r\n+    branches: [main]\r\n+  pull_request:\r\n+    types: [ opened, labeled, reopened, synchronize ]\r\n \r\n concurrency:\r\n   group: ${{ github.workflow }}-${{ github.head_ref || github.run_id }}\r\n   cancel-in-progress: true\r\n \r\n env:\r\n   HF_HOME: /mnt/cache\r\n+  DATASET_ID: hf-benchmarks/transformers\r\n+  MODEL_ID: meta-llama/Llama-3.1-8B-Instruct\r\n \r\n jobs:\r\n   benchmark:\r\n@@ -31,26 +36,12 @@ jobs:\n         with:\r\n           ref: ${{ github.event.pull_request.head.sha || github.sha }}\r\n \r\n-      - name: Install libpq-dev & psql\r\n-        run: |\r\n-          apt update\r\n-          apt install -y libpq-dev postgresql-client\r\n-\r\n       - name: Install benchmark script dependencies\r\n-        run: python3 -m pip install -r benchmark/requirements.txt\r\n+        run: python3 -m pip install -r benchmark_v2/requirements.txt kernels\r\n \r\n       - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\r\n         working-directory: /transformers\r\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\"\r\n-\r\n-      - name: Run database init script\r\n-        run: |\r\n-          psql -f benchmark/utils/init_db.sql\r\n-        env:\r\n-          PGDATABASE: metrics\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n+        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e \".[torch]\" && python3 -m pip uninstall -y torchvision # temp fix\r\n \r\n       - name: Run benchmark\r\n         run: |\r\n@@ -61,13 +52,11 @@ jobs:\n             commit_id=$GITHUB_SHA\r\n           fi\r\n           commit_msg=$(git show -s --format=%s | cut -c1-70)\r\n-          python3 benchmark/benchmarks_entrypoint.py \"huggingface/transformers\" \"$BRANCH_NAME\" \"$commit_id\" \"$commit_msg\"\r\n+          python3 benchmark_v2/run_benchmarks.py -b 32 -s 128 -n 256 --branch-name \"$BRANCH_NAME\" --commit-id \"$commit_id\" --commit-message \"$commit_msg\" --model-id \"$MODEL_ID\" --log-level INFO --push-result-to-dataset \"$DATASET_ID\"\r\n         env:\r\n           HF_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\r\n+          PUSH_TO_HUB_TOKEN: ${{ secrets.PUSH_TO_HUB_TOKEN }}\r\n           # Enable this to see debug logs\r\n           # HF_HUB_VERBOSITY: debug\r\n           # TRANSFORMERS_VERBOSITY: debug\r\n-          PGHOST: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGHOST }}\r\n-          PGUSER: transformers_benchmarks\r\n-          PGPASSWORD: ${{ secrets.TRANSFORMERS_BENCHMARKS_PGPASSWORD }}\r\n           BRANCH_NAME: ${{ github.head_ref || github.ref_name }}\r"
        },
        {
            "sha": "314302a7669970bfa90ec9cd35fdb68016f3f90e",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -22,7 +22,7 @@ def __init__(\n         self,\n         warmup_iterations: int = 5,\n         measurement_iterations: int = 20,\n-        gpu_monitoring: bool = False,  # False by default because it slows down the benchmark by a lot\n+        gpu_monitoring: bool = True,  # NOTE: you may want to disable this at times as we have obsvered it could heavily slow down benchmarks on AMD\n         batch_size: int = 1,\n         sequence_length: int = 128,\n         num_tokens_to_generate: int = 128,\n@@ -136,7 +136,7 @@ def cross_generate_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,  # this slows down the benchmark by a lot so we disable it by default\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -169,7 +169,7 @@ def generate_all_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n+    gpu_monitoring: bool = True,\n ) -> list[BenchmarkConfig]:\n     all_attn_implementations = [\n         (\"flash_attention_2\", None),\n@@ -197,7 +197,6 @@ def generate_main_configs(\n     batch_size: int = 1,\n     sequence_length: int = 128,\n     num_tokens_to_generate: int = 128,\n-    gpu_monitoring: bool = False,\n ) -> list[BenchmarkConfig]:\n     # Create kwargs common to all configs\n     kwargs = {\n@@ -206,10 +205,10 @@ def generate_main_configs(\n         \"batch_size\": batch_size,\n         \"sequence_length\": sequence_length,\n         \"num_tokens_to_generate\": num_tokens_to_generate,\n-        \"gpu_monitoring\": gpu_monitoring,\n     }\n     return [  # TODO: test max-autotune instead of default\n-        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", **kwargs),\n-        BenchmarkConfig(attn_implementation=\"flash_attention_2\", **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=False, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"eager\", compile_mode=\"default\", gpu_monitoring=True, **kwargs),\n+        BenchmarkConfig(attn_implementation=\"flash_attention_2\", gpu_monitoring=True, **kwargs),\n     ]"
        },
        {
            "sha": "cbaf0bab42786fe4e7d92a3b40255c7fbc13cf9b",
            "filename": "benchmark_v2/framework/benchmark_runner.py",
            "status": "modified",
            "additions": 76,
            "deletions": 9,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fbenchmark_runner.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_runner.py?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -4,13 +4,16 @@\n import os\n import pathlib\n import re\n+import tempfile\n import time\n from contextlib import nullcontext\n from datetime import datetime\n from queue import Queue\n from typing import Any\n \n import torch\n+from datasets import Dataset\n+from huggingface_hub import HfApi\n from tqdm import trange\n \n from transformers import (\n@@ -50,6 +53,8 @@\n     \"Its instability ended in the coup of 18 Brumaire and the establishment of the Consulate, with Napoleon Bonaparte as First Consul.\",\n ])  # fmt: skip\n \n+PUSH_TO_HUB_TOKEN = os.getenv(\"PUSH_TO_HUB_TOKEN\", None)\n+\n \n def compact_json_numeric_arrays(data: dict):\n     # Match arrays that contain only numbers (ints/floats), whitespace, commas, and newlines\n@@ -120,15 +125,19 @@ def flush_memory():\n \n class BenchmarkStreamer(BaseStreamer):\n     def __init__(self, **kwargs) -> None:\n+        self.timeout = kwargs.pop(\"timeout\", 10)\n         self.timestamps = []\n         self.text_queue = Queue()\n+        self.stop_signal = None\n \n     def put(self, value):\n         \"\"\"Receives tokens and logs the timestamp of the generation.\"\"\"\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(value)\n \n     def end(self):\n         self.timestamps.append(time.perf_counter())\n+        self.text_queue.put(self.stop_signal)\n \n     def __iter__(self):\n         return self\n@@ -144,13 +153,22 @@ def __next__(self):\n class BenchmarkRunner:\n     \"\"\"Main benchmark runner that coordinates benchmark execution.\"\"\"\n \n-    def __init__(self, logger: logging.Logger, output_dir: str | None = None, commit_id: str | None = None) -> None:\n+    def __init__(\n+        self,\n+        logger: logging.Logger,\n+        output_dir: str | None = None,\n+        branch_name: str | None = None,\n+        commit_id: str | None = None,\n+        commit_message: str | None = None,\n+    ) -> None:\n         # Those stay constant for the whole run\n         self.logger = logger\n         if output_dir is None:\n             output_dir = os.path.join(os.path.dirname(os.path.dirname(__file__)), \"benchmark_results\")\n         self.output_dir = output_dir\n+        self.branch_name = branch_name\n         self.commit_id = get_git_revision() if commit_id is None else commit_id\n+        self.commit_message = commit_message\n         os.makedirs(self.output_dir, exist_ok=True)\n         self.profile_dir = None\n         # Attributes that are reset for each model\n@@ -163,7 +181,7 @@ def cleanup(self) -> None:\n         self.model = None\n         flush_memory()\n \n-    def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n+    def setup_benchmark(self, model_id: str, config: BenchmarkConfig) -> None:\n         # Some attributes only need to be set once per model\n         if self._setup_for != model_id:\n             self.tokenizer = AutoTokenizer.from_pretrained(model_id)\n@@ -200,10 +218,13 @@ def setup_one_run(self, model_id: str, config: BenchmarkConfig) -> None:\n         self.model = self.model.eval().to(config.device)\n \n         # Kernelize the model if needed\n-        if config.kernelize:\n+        if config.kernelize and kernelize is not None and Mode is not None:\n             self.model = kernelize(self.model, mode=Mode.INFERENCE)\n \n-    def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0) -> None:\n+    def run_benchmark(\n+        self, model_id: str, config: BenchmarkConfig, num_tokens_to_profile: int = 0\n+    ) -> dict[str, Any] | None:\n+        \"\"\"Run a single benchmark with the given model ID and config.\"\"\"\n         sdpa_ctx = nullcontext()\n         if config.attn_implementation == \"sdpa\":\n             sdpa_backend = get_sdpa_backend(config.sdpa_backend)\n@@ -243,7 +264,12 @@ def run_one_benchmark(self, model_id: str, config: BenchmarkConfig, num_tokens_t\n                 self.profile_generate(num_tokens_to_profile, config.name)\n \n             return {\n-                \"metadata\": BenchmarkMetadata(model_id=model_id, commit_id=self.commit_id),\n+                \"metadata\": BenchmarkMetadata(\n+                    model_id=model_id,\n+                    branch_name=self.branch_name,\n+                    commit_id=self.commit_id,\n+                    commit_message=self.commit_message,\n+                ),\n                 \"measurements\": result,\n                 \"config\": config,\n             }\n@@ -305,7 +331,8 @@ def run_benchmarks(\n         benchmark_configs: list[BenchmarkConfig],\n         num_tokens_to_profile: int = 0,\n         pretty_print_summary: bool = True,\n-    ) -> dict[str, Any]:\n+    ) -> tuple[str, dict[str, Any]]:\n+        \"\"\"Run multiple benchmarks for the given model ID and list of benchmark configs.\"\"\"\n         all_results = {}\n         timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n         start_time = time.perf_counter()\n@@ -324,14 +351,14 @@ def run_benchmarks(\n                 continue\n \n             # Otherwise, run the benchmark\n-            self.setup_one_run(model_id, config)\n+            self.setup_benchmark(model_id, config)\n             self.logger.info(\n                 f\"Running benchmark of model {model_id} with scenario: {config.name} ({i + 1}/{n_configs})\"\n             )\n \n             # Launch benchmark in a try/except block to avoid stopping the whole run if one benchmark fails\n             try:\n-                results = self.run_one_benchmark(model_id, config, num_tokens_to_profile)\n+                results = self.run_benchmark(model_id, config, num_tokens_to_profile)\n                 if results is not None:\n                     all_results[config.hash] = results\n \n@@ -358,7 +385,7 @@ def run_benchmarks(\n                 result[\"measurements\"].pprint(batch_size=result[\"config\"].batch_size, tabs=1)\n             print(\"=\" * 100)\n \n-        return all_results\n+        return (timestamp, all_results)\n \n     def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> str:\n         \"\"\"Save benchmark results to JSON file.\"\"\"\n@@ -387,3 +414,43 @@ def save_results(self, model_name: str, results: dict, timestamp: str = \"\") -> s\n \n         self.logger.info(f\"Results saved to {filepath}\")\n         return filepath\n+\n+    def push_results_to_hub(self, dataset_id: str, results: dict[Any, Any], timestamp: str) -> None:\n+        if PUSH_TO_HUB_TOKEN is None:\n+            raise ValueError(\n+                \"PUSH_TO_HUB_TOKEN is not set, cannot push results to the Hub. When setting dataset_id, please also set the PUSH_TO_HUB_TOKEN environment variable.\"\n+            )\n+\n+        n_results = len(results)\n+        self.logger.info(f\"Pushing {n_results} results to: {dataset_id}\")\n+        rows = []\n+        for cfg_hash, entry in results.items():\n+            row = {\n+                \"benchmark_config_hash\": cfg_hash,\n+                \"config\": entry[\"config\"].to_dict(),\n+                \"measurements\": entry[\"measurements\"].to_dict(),\n+                \"metadata\": entry[\"metadata\"].to_dict(),\n+            }\n+            rows.append(row)\n+\n+        ds = Dataset.from_list(rows)\n+        with tempfile.TemporaryDirectory() as tmp:\n+            jsonl_path = os.path.join(tmp, \"data.jsonl\")\n+            with open(jsonl_path, \"w\") as f:\n+                json_lines = []\n+                for ex in ds:\n+                    json_lines.append(json.dumps(ex, ensure_ascii=False))\n+                f.write(\"\\n\".join(json_lines))\n+\n+            api = HfApi()\n+            # NOTE: we expect the repository to already exist\n+            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") if not timestamp else timestamp\n+            file_name = f\"benchmark_run_{timestamp}.jsonl\"\n+            api.upload_file(\n+                path_or_fileobj=jsonl_path,\n+                path_in_repo=file_name,\n+                repo_id=dataset_id,\n+                repo_type=\"dataset\",\n+                token=PUSH_TO_HUB_TOKEN,\n+            )\n+        self.logger.info(f\"Succesfully uploaded results to: {dataset_id}\")"
        },
        {
            "sha": "ef5088256a1bb665eb7b5320921c50698b9ea0c3",
            "filename": "benchmark_v2/framework/data_classes.py",
            "status": "modified",
            "additions": 10,
            "deletions": 3,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fdata_classes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Fframework%2Fdata_classes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fdata_classes.py?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -1,5 +1,5 @@\n from dataclasses import dataclass\n-from datetime import datetime\n+from datetime import datetime, timezone\n from typing import Any\n \n import numpy as np\n@@ -59,19 +59,26 @@ class BenchmarkMetadata:\n \n     model_id: str\n     timestamp: str\n+    branch_name: str\n     commit_id: str\n+    commit_message: str\n     hardware_info: HardwareInfo\n \n-    def __init__(self, model_id: str, commit_id: str):\n+    def __init__(self, model_id: str, commit_id: str, branch_name: str = \"main\", commit_message: str = \"\") -> None:\n         self.model_id = model_id\n-        self.timestamp = datetime.utcnow().isoformat()\n+        self.timestamp = datetime.now(timezone.utc).isoformat()\n+        self.branch_name = branch_name\n         self.commit_id = commit_id\n+        self.commit_message = commit_message\n         self.hardware_info = HardwareInfo()\n \n     def to_dict(self) -> dict[str, Any]:\n         return {\n+            \"model_id\": self.model_id,\n             \"timestamp\": self.timestamp,\n+            \"branch_name\": self.branch_name,\n             \"commit_id\": self.commit_id,\n+            \"commit_message\": self.commit_message,\n             \"hardware_info\": self.hardware_info.to_dict(),\n         }\n "
        },
        {
            "sha": "ce5c1bbb64ea9f45c54e03a04e0e2d870b5b349c",
            "filename": "benchmark_v2/requirements.txt",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frequirements.txt?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -4,4 +4,4 @@ gpustat>=1.0.0\n torch>=2.0.0\n transformers>=4.30.0\n datasets>=2.10.0\n-huggingface_hub>=0.16.0 \n\\ No newline at end of file\n+huggingface_hub>=0.16.0"
        },
        {
            "sha": "94a66ebc5a116158166f3b01296da7a17c705e01",
            "filename": "benchmark_v2/run_benchmarks.py",
            "status": "modified",
            "additions": 32,
            "deletions": 6,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Frun_benchmarks.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/71db0d49e99884566026c140f8b12b61056fa8dc/benchmark_v2%2Frun_benchmarks.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Frun_benchmarks.py?ref=71db0d49e99884566026c140f8b12b61056fa8dc",
            "patch": "@@ -33,9 +33,8 @@\n     parser.add_argument(\"--output-dir\", type=str, default=None, help=\"Output dir for benchmark results\")\n     parser.add_argument(\"--log-level\", type=str, choices=[\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"], default=\"INFO\")\n     parser.add_argument(\"--model-id\", type=str, help=\"Specific model ID to benchmark (if supported by benchmarks)\")\n-\n-    parser.add_argument(\"--warmup\", type=int, default=3, help=\"Number of warmup iterations\")\n-    parser.add_argument(\"--iterations\", type=int, default=10, help=\"Number of measurement iterations\")\n+    parser.add_argument(\"--warmup\", \"-w\", type=int, default=3, help=\"Number of warmup iterations\")\n+    parser.add_argument(\"--iterations\", \"-i\", type=int, default=10, help=\"Number of measurement iterations\")\n \n     parser.add_argument(\"--batch-size\", \"-b\", type=int, nargs=\"+\", help=\"Batch size\")\n     parser.add_argument(\"--sequence-length\", \"-s\", type=int, nargs=\"+\", help=\"Sequence length\")\n@@ -44,7 +43,20 @@\n     parser.add_argument(\"--cross-generate\", action=\"store_true\", help=\"Cross-generate all combinations of configs\")\n     parser.add_argument(\"--num-tokens-to-profile\", \"-p\", type=int, default=0, help=\"Number of tokens to profile\")\n \n+    parser.add_argument(\"--branch-name\", type=str, help=\"Git branch name\")\n     parser.add_argument(\"--commit-id\", type=str, help=\"Git commit ID (if not provided, will auto-detect from git)\")\n+    parser.add_argument(\"--commit-message\", type=str, help=\"Git commit message\")\n+\n+    parser.add_argument(\n+        \"--no-gpu-monitoring\", action=\"store_true\", help=\"Disables GPU monitoring during benchmark runs\"\n+    )\n+\n+    parser.add_argument(\n+        \"--push-result-to-dataset\",\n+        type=str,\n+        default=None,\n+        help=\"Name of the dataset to push results to. If not provided, results are not pushed to the Hub.\",\n+    )\n     args = parser.parse_args()\n \n     # Setup logging\n@@ -76,6 +88,7 @@\n                 batch_size=args.batch_size[0],\n                 sequence_length=args.sequence_length[0],\n                 num_tokens_to_generate=args.num_tokens_to_generate[0],\n+                gpu_monitoring=not args.no_gpu_monitoring,\n             )\n         else:\n             benchmark_configs = generate_main_configs(\n@@ -106,11 +119,24 @@\n                     cfg_dict.pop(\"name\")\n                     benchmark_configs.append(BenchmarkConfig.from_dict(cfg_dict))\n \n-    runner = BenchmarkRunner(logger, args.output_dir, args.commit_id)\n-    results = runner.run_benchmarks(\n+    runner = BenchmarkRunner(\n+        logger,\n+        args.output_dir,\n+        args.branch_name,\n+        args.commit_id,\n+        args.commit_message,\n+    )\n+    timestamp, results = runner.run_benchmarks(\n         args.model_id,\n         benchmark_configs,\n         args.num_tokens_to_profile,\n         pretty_print_summary=True,\n     )\n-    # runner.save_results(args.model_id, results)\n+\n+    dataset_id = args.push_result_to_dataset\n+    if dataset_id is not None and len(results) > 0:\n+        runner.push_results_to_hub(\n+            dataset_id,\n+            results,\n+            timestamp,\n+        )"
        }
    ],
    "stats": {
        "total": 184,
        "additions": 136,
        "deletions": 48
    }
}