{
    "author": "MekkCyber",
    "message": "Adds Causal Conv 1D kernel for mamba models (#40765)\n\n* add kernel\n\n* make style\n\n* keep causal-conv1d\n\n* small fix\n\n* small fix\n\n* fix modular converter\n\n* modular fix + lazy loading\n\n* revert changes modular\n\n* nit\n\n* hub kernels update\n\n* update\n\n* small nit",
    "sha": "6e69b60806ae19f0b45edb96adb40b708172ba7a",
    "files": [
        {
            "sha": "dc593c979dc7c637f299d7607a6c9d53702af616",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 30,
            "deletions": 9,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=6e69b60806ae19f0b45edb96adb40b708172ba7a",
            "patch": "@@ -35,6 +35,7 @@\n from ...utils import ModelOutput, auto_docstring, logging\n from ...utils.import_utils import (\n     is_causal_conv1d_available,\n+    is_kernels_available,\n     is_mamba_ssm_available,\n     is_mambapy_available,\n )\n@@ -54,11 +55,6 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -166,6 +162,28 @@ def reset(self):\n             self.ssm_states[layer_idx].zero_()\n \n \n+def _lazy_load_causal_conv1d():\n+    global _causal_conv1d_cache\n+    if _causal_conv1d_cache is not None:\n+        return _causal_conv1d_cache\n+\n+    if is_kernels_available():\n+        from kernels import get_kernel\n+\n+        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+    elif is_causal_conv1d_available():\n+        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+\n+        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n+    else:\n+        _causal_conv1d_cache = (None, None)\n+    return _causal_conv1d_cache\n+\n+\n+_causal_conv1d_cache = None\n+\n+\n def rms_forward(hidden_states, variance_epsilon=1e-6):\n     \"\"\"\n     Calculates simple RMSNorm with no learnable weights. `MambaRMSNorm` will\n@@ -245,6 +263,7 @@ def __init__(self, config: FalconMambaConfig, layer_idx: int):\n         self.rms_eps = config.mixer_rms_eps\n \n     def warn_slow_implementation(self):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -253,8 +272,8 @@ def warn_slow_implementation(self):\n                 if is_mambapy_available():\n                     logger.warning_once(\n                         \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                        \" https://github.com/Dao-AILab/causal-conv1d\"\n+                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                        \" https://github.com/Dao-AILab/causal-conv1d or `pip install kernels` for causal-conv1d\"\n                     )\n                 else:\n                     raise ImportError(\n@@ -263,8 +282,8 @@ def warn_slow_implementation(self):\n             else:\n                 logger.warning_once(\n                     \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                    \" https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n+                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                    \" https://github.com/Dao-AILab/causal-conv1d or `pip install kernels` for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n                 )\n \n     def cuda_kernels_forward(\n@@ -299,6 +318,7 @@ def cuda_kernels_forward(\n             )\n \n         else:\n+            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -493,6 +513,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        },
        {
            "sha": "cfe2ec49a992b7c663e24da8187daee43bef97ed",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=6e69b60806ae19f0b45edb96adb40b708172ba7a",
            "patch": "@@ -21,7 +21,10 @@\n from torch import nn\n \n from ...utils import auto_docstring, logging\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available, is_mambapy_available\n+from ...utils.import_utils import (\n+    is_mamba_ssm_available,\n+    is_mambapy_available,\n+)\n from ..mamba.configuration_mamba import MambaConfig\n from ..mamba.modeling_mamba import (\n     MambaBlock,\n@@ -33,6 +36,7 @@\n     MambaOutput,\n     MambaPreTrainedModel,\n     MambaRMSNorm,\n+    _lazy_load_causal_conv1d,\n )\n \n \n@@ -51,10 +55,7 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n+_causal_conv1d_cache = None\n \n \n class FalconMambaConfig(MambaConfig):\n@@ -258,6 +259,7 @@ def rms_forward(hidden_states, variance_epsilon=1e-6):\n \n class FalconMambaMixer(MambaMixer):\n     def warn_slow_implementation(self):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -266,8 +268,8 @@ def warn_slow_implementation(self):\n                 if is_mambapy_available():\n                     logger.warning_once(\n                         \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                        \" https://github.com/Dao-AILab/causal-conv1d\"\n+                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                        \" https://github.com/Dao-AILab/causal-conv1d or `pip install kernels` for causal-conv1d\"\n                     )\n                 else:\n                     raise ImportError(\n@@ -276,8 +278,8 @@ def warn_slow_implementation(self):\n             else:\n                 logger.warning_once(\n                     \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                    \" https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n+                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                    \" https://github.com/Dao-AILab/causal-conv1d or `pip install kernels` for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n                 )\n \n     def __init__(self, config: FalconMambaConfig, layer_idx: int):\n@@ -323,6 +325,7 @@ def cuda_kernels_forward(\n             )\n \n         else:\n+            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -516,6 +519,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        },
        {
            "sha": "10616323e13f4a8162da4d186fb2e10f35203a48",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 33,
            "deletions": 9,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6e69b60806ae19f0b45edb96adb40b708172ba7a/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=6e69b60806ae19f0b45edb96adb40b708172ba7a",
            "patch": "@@ -33,7 +33,12 @@\n     auto_docstring,\n     logging,\n )\n-from ...utils.import_utils import is_causal_conv1d_available, is_mamba_ssm_available, is_mambapy_available\n+from ...utils.import_utils import (\n+    is_causal_conv1d_available,\n+    is_kernels_available,\n+    is_mamba_ssm_available,\n+    is_mambapy_available,\n+)\n from .configuration_mamba import MambaConfig\n \n \n@@ -50,10 +55,26 @@\n else:\n     selective_state_update, selective_scan_fn, mamba_inner_fn = None, None, None\n \n-if is_causal_conv1d_available():\n-    from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n-else:\n-    causal_conv1d_update, causal_conv1d_fn = None, None\n+_causal_conv1d_cache = None\n+\n+\n+def _lazy_load_causal_conv1d():\n+    global _causal_conv1d_cache\n+    if _causal_conv1d_cache is not None:\n+        return _causal_conv1d_cache\n+\n+    if is_kernels_available():\n+        from kernels import get_kernel\n+\n+        _causal_conv1d_kernel = get_kernel(\"kernels-community/causal-conv1d\")\n+        _causal_conv1d_cache = (_causal_conv1d_kernel.causal_conv1d_update, _causal_conv1d_kernel.causal_conv1d_fn)\n+    elif is_causal_conv1d_available():\n+        from causal_conv1d import causal_conv1d_fn, causal_conv1d_update\n+\n+        _causal_conv1d_cache = (causal_conv1d_update, causal_conv1d_fn)\n+    else:\n+        _causal_conv1d_cache = (None, None)\n+    return _causal_conv1d_cache\n \n \n class MambaCache:\n@@ -211,6 +232,7 @@ def __init__(self, config: MambaConfig, layer_idx: int):\n         self.warn_slow_implementation()\n \n     def warn_slow_implementation(self):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )\n@@ -219,8 +241,8 @@ def warn_slow_implementation(self):\n                 if is_mambapy_available():\n                     logger.warning_once(\n                         \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                        \" https://github.com/Dao-AILab/causal-conv1d\"\n+                        \" is None. Falling back to the mamba.py backend. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                        \" install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d\"\n                     )\n                 else:\n                     raise ImportError(\n@@ -229,8 +251,8 @@ def warn_slow_implementation(self):\n             else:\n                 logger.warning_once(\n                     \"The fast path is not available because one of `(selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)`\"\n-                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation and\"\n-                    \" https://github.com/Dao-AILab/causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n+                    \" is None. Falling back to the sequential implementation of Mamba, as use_mambapy is set to False. To install follow https://github.com/state-spaces/mamba/#installation for mamba-ssm and\"\n+                    \" install the kernels library using `pip install kernels` or https://github.com/Dao-AILab/causal-conv1d for causal-conv1d. For the mamba.py backend, follow https://github.com/alxndrTL/mamba.py.\"\n                 )\n \n     def cuda_kernels_forward(\n@@ -261,6 +283,7 @@ def cuda_kernels_forward(\n             )\n \n         else:\n+            causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n             hidden_states, gate = projected_states.chunk(2, dim=1)\n \n             if attention_mask is not None:\n@@ -424,6 +447,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n     ):\n+        causal_conv1d_update, causal_conv1d_fn = _lazy_load_causal_conv1d()\n         is_fast_path_available = all(\n             (selective_state_update, selective_scan_fn, causal_conv1d_fn, causal_conv1d_update, mamba_inner_fn)\n         )"
        }
    ],
    "stats": {
        "total": 103,
        "additions": 76,
        "deletions": 27
    }
}