{
    "author": "ydshieh",
    "message": "Fix blip2 tests (#38510)\n\n* fix 1: not sure\n\n* fix 2: _supports_flex_attn = False\n\n* fix 3: embedding_output = self.layernorm(query_embeds.to(self.layernorm.weight.dtype))\n\n* fix 4: query_embeds = query_embeds.to(self.layernorm.weight.dtype)\n\n* fix 5: text_embeds = text_embeds.to(dtype=torch.float16)\n\n* fix 5: question_embeds.to(dtype=torch.float16)\n\n* fix 6: text_embeds = text_embeds.to(dtype=self.itm_head.weight.dtype)\n\n* fix 7: image_embeds and question_embeds\n\n* fix 8: fix other 2 fp16 tests\n\n* fix 9: fix T5 OOM\n\n* fix 10: fix T5 OOM\n\n* fix 11: fix T5\n\n* fix 11: fix T5 beam\n\n* fix 12: _supports_sdpa=False\n\n* fix 12: style and expect\n\n* revert\n\n* revert\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "de4cf5a38e9678b9e465867a8a6b88ea727bea52",
    "files": [
        {
            "sha": "5945f4f48ce626e12b7cada369a7c522bd058e45",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/de4cf5a38e9678b9e465867a8a6b88ea727bea52/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de4cf5a38e9678b9e465867a8a6b88ea727bea52/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=de4cf5a38e9678b9e465867a8a6b88ea727bea52",
            "patch": "@@ -1196,6 +1196,8 @@ def forward(\n             query_length if query_length is not None else query_embeds.shape[1] if query_embeds is not None else 0\n         )\n \n+        # `Blip2QFormerModel` is kept as fp32\n+        query_embeds = query_embeds.to(self.layernorm.weight.dtype)\n         embedding_output = self.layernorm(query_embeds)\n         embedding_output = self.dropout(embedding_output)\n \n@@ -1737,6 +1739,7 @@ def forward(\n         )\n \n         pooled_output = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+        pooled_output = pooled_output.to(dtype=self.text_projection.weight.dtype)\n \n         text_embeds = self.text_projection(pooled_output)\n         text_embeds = nn.functional.normalize(text_embeds, dim=-1)\n@@ -1837,6 +1840,7 @@ def forward(\n         )\n \n         embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n+        embeds = embeds.to(dtype=self.vision_projection.weight.dtype)\n         image_embeds = self.vision_projection(embeds)\n         image_embeds = nn.functional.normalize(image_embeds, dim=-1)\n \n@@ -2395,6 +2399,7 @@ def forward(\n                 return_dict=return_dict,\n             )\n             text_embeds = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+            text_embeds = text_embeds.to(dtype=self.itm_head.weight.dtype)\n \n             output = self.itm_head(text_embeds[:, : query_tokens.size(1), :])\n             logits_per_image = output.mean(dim=1)\n@@ -2408,6 +2413,7 @@ def forward(\n                 return_dict=return_dict,\n             )\n             image_embeds = query_outputs[0] if not return_dict else query_outputs.last_hidden_state\n+            image_embeds = image_embeds.to(dtype=self.vision_projection.weight.dtype)\n \n             query_embeds = self.embeddings(\n                 input_ids=input_ids,\n@@ -2419,6 +2425,7 @@ def forward(\n                 return_dict=return_dict,\n             )\n             question_embeds = text_outputs[0] if not return_dict else text_outputs.last_hidden_state\n+            question_embeds = question_embeds.to(dtype=self.text_projection.weight.dtype)\n \n             # normalized features\n             image_embeds = nn.functional.normalize(self.vision_projection(image_embeds), dim=-1)"
        },
        {
            "sha": "38b3714d7c82c1d3b56dc60380d6926389e526a6",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 44,
            "deletions": 6,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/de4cf5a38e9678b9e465867a8a6b88ea727bea52/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de4cf5a38e9678b9e465867a8a6b88ea727bea52/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=de4cf5a38e9678b9e465867a8a6b88ea727bea52",
            "patch": "@@ -24,6 +24,8 @@\n \n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n     require_torch,\n     require_torch_accelerator,\n     require_torch_fp16,\n@@ -1620,6 +1622,12 @@ def prepare_img():\n @require_torch\n @slow\n class Blip2ModelIntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     def test_inference_opt(self):\n         processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n         model = Blip2ForConditionalGeneration.from_pretrained(\n@@ -1698,9 +1706,19 @@ def test_inference_t5(self):\n         predictions = model.generate(**inputs)\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    \"a woman is playing with her dog on the beach\",\n+                ]\n+            }\n+        )\n+        expected_outputs = expectations.get_expectation()\n+\n         # Test output\n-        self.assertEqual(predictions[0].tolist(), [0, 2335, 1556, 28, 1782, 30, 8, 2608, 1])\n-        self.assertEqual(\"woman playing with dog on the beach\", generated_text)\n+        self.assertEqual(predictions[0].tolist(), expected_outputs[0])\n+        self.assertEqual(expected_outputs[1], generated_text)\n \n         # image and context\n         prompt = \"Question: which city is this? Answer:\"\n@@ -1709,9 +1727,19 @@ def test_inference_t5(self):\n         predictions = model.generate(**inputs)\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    [0, 3, 7, 152, 2515, 11389, 3523, 1],\n+                    \"san francisco\",\n+                ]\n+            }\n+        )\n+        expected_outputs = expectations.get_expectation()\n+\n         # Test output\n-        self.assertEqual(predictions[0].tolist(), [0, 3, 7, 152, 67, 839, 1])\n-        self.assertEqual(generated_text, \"san diego\")\n+        self.assertEqual(predictions[0].tolist(), expected_outputs[0])\n+        self.assertEqual(generated_text, expected_outputs[1])\n \n     def test_inference_t5_batched_beam_search(self):\n         processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-flan-t5-xl\")\n@@ -1725,9 +1753,19 @@ def test_inference_t5_batched_beam_search(self):\n \n         predictions = model.generate(**inputs, num_beams=2)\n \n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 7): [\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                    [0, 3, 9, 2335, 19, 1556, 28, 160, 1782, 30, 8, 2608, 1],\n+                ]\n+            }\n+        )\n+        expected_predictions = expectations.get_expectation()\n+\n         # Test output (in this case, slightly different from greedy search)\n-        self.assertEqual(predictions[0].tolist(), [0, 2335, 1556, 28, 1782, 30, 8, 2608, 1])\n-        self.assertEqual(predictions[1].tolist(), [0, 2335, 1556, 28, 1782, 30, 8, 2608, 1])\n+        self.assertEqual(predictions[0].tolist(), expected_predictions[0])\n+        self.assertEqual(predictions[1].tolist(), expected_predictions[1])\n \n     @require_torch_multi_accelerator\n     def test_inference_opt_multi_accelerator(self):"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 51,
        "deletions": 6
    }
}