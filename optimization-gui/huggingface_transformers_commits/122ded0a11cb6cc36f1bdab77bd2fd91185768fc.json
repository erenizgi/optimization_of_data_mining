{
    "author": "alexsherstinsky",
    "message": "Bugfix/alexsherstinsky/fix none check for attention factor in rope scaling 2024 08 28 0 (#33188)\n\n* Fixing a bug in the way \"attention_factor\" is validated in ROPE utilities.\r\n\r\n* Fixing a bug in the way \"attention_factor\" is validated in ROPE utilities.\r\n\r\n* Fixing a bug in the way \"attention_factor\" is validated in ROPE utilities.",
    "sha": "122ded0a11cb6cc36f1bdab77bd2fd91185768fc",
    "files": [
        {
            "sha": "5788238b58b3db61de033f7a6863394c6fa81420",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/122ded0a11cb6cc36f1bdab77bd2fd91185768fc/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/122ded0a11cb6cc36f1bdab77bd2fd91185768fc/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=122ded0a11cb6cc36f1bdab77bd2fd91185768fc",
            "patch": "@@ -487,10 +487,11 @@ def _validate_longrope_parameters(config: PretrainedConfig):\n             logger.warning(f\"`rope_scaling`'s factor field must be a float >= 1, got {factor}\")\n \n         attention_factor = rope_scaling.get(\"attention_factor\")\n-        if attention_factor is not None and not isinstance(attention_factor, float) or attention_factor < 0:\n-            logger.warning(\n-                f\"`rope_scaling`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n-            )\n+        if attention_factor is not None:\n+            if not isinstance(attention_factor, float) or attention_factor < 0.0:\n+                logger.warning(\n+                    f\"`rope_scaling`'s attention_factor field must be a float greater than 0, got {attention_factor}\"\n+                )\n \n \n def _validate_llama3_parameters(config: PretrainedConfig):"
        },
        {
            "sha": "a1d1fd6b922ab36f60fb08677e8398180655817d",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/122ded0a11cb6cc36f1bdab77bd2fd91185768fc/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/122ded0a11cb6cc36f1bdab77bd2fd91185768fc/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=122ded0a11cb6cc36f1bdab77bd2fd91185768fc",
            "patch": "@@ -330,6 +330,16 @@ def test_longrope_rope_numerically(self):\n             _, attention_scale = rope_fn(config=config, device=torch_device, seq_len=1)\n             self.assertEqual(attention_scale, 0.5)\n \n+            config.rope_scaling = {\n+                \"rope_type\": \"longrope\",\n+                \"factor\": factor,\n+                \"short_factor\": short_factor,\n+                \"long_factor\": long_factor,\n+            }\n+            self.assertEqual(config.rope_scaling.get(\"attention_factor\"), None)\n+            # Verify that \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\" is not raised.\n+            rope_config_validation(config)\n+\n         # Check 2: Factor == 1.0 -> short factor is applied to the default frequencies\n         factor = 1.0\n         config.rope_scaling = {"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 15,
        "deletions": 4
    }
}