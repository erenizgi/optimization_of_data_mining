{
    "author": "fmo-mt",
    "message": "Support TF32 flag for MUSA backend (#33187)\n\n* Support MUSA (Moore Threads GPU) backend in transformers\nAdd accelerate version check, needs accelerate>=0.33.0\n\n* Support TF32 flag for MUSA backend\n\n* fix typo",
    "sha": "d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a",
    "files": [
        {
            "sha": "222620958f8be86b988b384b681df70e9170c425",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 17,
            "deletions": 7,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a",
            "patch": "@@ -1840,27 +1840,37 @@ def __post_init__(self):\n         if self.framework == \"pt\" and is_torch_available() and self.torch_compile:\n             if is_torch_tf32_available():\n                 if self.tf32 is None and not self.fp16 or self.bf16:\n+                    device_str = \"MUSA\" if is_torch_musa_available() else \"CUDA\"\n                     logger.info(\n-                        \"Setting TF32 in CUDA backends to speedup torch compile, you won't see any improvement\"\n+                        f\"Setting TF32 in {device_str} backends to speedup torch compile, you won't see any improvement\"\n                         \" otherwise.\"\n                     )\n-                    torch.backends.cuda.matmul.allow_tf32 = True\n-                    torch.backends.cudnn.allow_tf32 = True\n+                    if is_torch_musa_available():\n+                        torch.backends.mudnn.allow_tf32 = True\n+                    else:\n+                        torch.backends.cuda.matmul.allow_tf32 = True\n+                        torch.backends.cudnn.allow_tf32 = True\n             else:\n                 logger.warning(\n                     \"The speedups for torchdynamo mostly come with GPU Ampere or higher and which is not detected here.\"\n                 )\n         if self.framework == \"pt\" and is_torch_available() and self.tf32 is not None:\n             if self.tf32:\n                 if is_torch_tf32_available():\n-                    torch.backends.cuda.matmul.allow_tf32 = True\n-                    torch.backends.cudnn.allow_tf32 = True\n+                    if is_torch_musa_available():\n+                        torch.backends.mudnn.allow_tf32 = True\n+                    else:\n+                        torch.backends.cuda.matmul.allow_tf32 = True\n+                        torch.backends.cudnn.allow_tf32 = True\n                 else:\n                     raise ValueError(\"--tf32 requires Ampere or a newer GPU arch, cuda>=11 and torch>=1.7\")\n             else:\n                 if is_torch_tf32_available():\n-                    torch.backends.cuda.matmul.allow_tf32 = False\n-                    torch.backends.cudnn.allow_tf32 = False\n+                    if is_torch_musa_available():\n+                        torch.backends.mudnn.allow_tf32 = False\n+                    else:\n+                        torch.backends.cuda.matmul.allow_tf32 = False\n+                        torch.backends.cudnn.allow_tf32 = False\n                 # no need to assert on else\n \n         # if training args is specified, it will override the one specified in the accelerate config"
        },
        {
            "sha": "a0957ea30e238d928faac1dd66be3eed443a4501",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=d47ad91c3c2a60420a2cd4b564f4bd0e69cece6a",
            "patch": "@@ -718,6 +718,11 @@ def is_torch_tf32_available() -> bool:\n \n     import torch\n \n+    if is_torch_musa_available():\n+        device_info = torch.musa.get_device_properties(torch.musa.current_device())\n+        if f\"{device_info.major}{device_info.minor}\" >= \"22\":\n+            return True\n+        return False\n     if not torch.cuda.is_available() or torch.version.cuda is None:\n         return False\n     if torch.cuda.get_device_properties(torch.cuda.current_device()).major < 8:"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 22,
        "deletions": 7
    }
}