{
    "author": "gante",
    "message": "Whisper: remove redundant assisted generation tests (#34814)\n\n* remove redundant test\r\n\r\n* delete another test\r\n\r\n* revert default max_length\r\n\r\n* (wrong place, moving)",
    "sha": "1cc7ca32955f618f9dfd081d787769fb898497c1",
    "files": [
        {
            "sha": "2c4ab9c2a97489a1dc473e039856be7161ba99ec",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=1cc7ca32955f618f9dfd081d787769fb898497c1",
            "patch": "@@ -124,7 +124,7 @@ def __init__(\n         # Prepare the kwargs for the assistant model\n         assistant_kwargs = {}\n         for key, value in model_kwargs.items():  # deepcopy crashes if we attempt to copy encoder outputs with grads\n-            if key not in (\"encoder_outputs\", \"assistant_encoder_outputs\", \"past_key_values\"):\n+            if key not in (\"encoder_outputs\", \"past_key_values\"):\n                 assistant_kwargs[key] = (\n                     value.detach().to(device) if isinstance(value, torch.Tensor) else copy.deepcopy(value)\n                 )\n@@ -133,9 +133,8 @@ def __init__(\n         if \"logits_to_keep\" in assistant_kwargs.keys() and not assistant_model._supports_logits_to_keep():\n             del assistant_kwargs[\"logits_to_keep\"]\n \n-        if \"assistant_encoder_outputs\" in model_kwargs:\n-            assistant_kwargs[\"encoder_outputs\"] = model_kwargs[\"assistant_encoder_outputs\"]\n-        elif assistant_model.config.is_encoder_decoder:\n+        # If the assistant is an encoder-decoder model, assume the encoder is different on the assistant.\n+        if assistant_model.config.is_encoder_decoder:\n             inputs_tensor, model_input_name, assistant_kwargs = assistant_model._prepare_model_inputs(\n                 inputs_tensor, assistant_model.generation_config.bos_token_id, assistant_kwargs\n             )"
        },
        {
            "sha": "0f6f2a0041c919a003052a04bc0c18323d937c6e",
            "filename": "src/transformers/generation/flax_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Fflax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fflax_utils.py?ref=1cc7ca32955f618f9dfd081d787769fb898497c1",
            "patch": "@@ -396,7 +396,7 @@ def generate(\n                     \"(https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\"\n                 )\n             generation_config.max_length = generation_config.max_new_tokens + input_ids_seq_length\n-        else:  # by default let's always generate 10 new tokens\n+        else:  # by default let's always generate 20 new tokens\n             if generation_config.max_length == GenerationConfig().max_length:\n                 generation_config.max_length = generation_config.max_length + input_ids_seq_length\n                 max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)"
        },
        {
            "sha": "22081b38454e9fd046df632f810de69e1e8808e2",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc7ca32955f618f9dfd081d787769fb898497c1/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=1cc7ca32955f618f9dfd081d787769fb898497c1",
            "patch": "@@ -1385,10 +1385,6 @@ def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n                 decoder_model_args = set(inspect.signature(decoder.forward).parameters)\n                 model_args |= {f\"decoder_{x}\" for x in decoder_model_args}\n \n-            # allow assistant_encoder_outputs to be passed if we're doing assisted generating\n-            if \"assistant_encoder_outputs\" in model_kwargs:\n-                model_args |= {\"assistant_encoder_outputs\"}\n-\n         for key, value in model_kwargs.items():\n             if value is not None and key not in model_args:\n                 unused_model_args.append(key)"
        },
        {
            "sha": "dc88091f4cec89d75c3c28baa38d9272120cf570",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 145,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cc7ca32955f618f9dfd081d787769fb898497c1/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cc7ca32955f618f9dfd081d787769fb898497c1/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=1cc7ca32955f618f9dfd081d787769fb898497c1",
            "patch": "@@ -63,7 +63,6 @@\n         AutoModelForVision2Seq,\n         AutoProcessor,\n         AutoTokenizer,\n-        BartForCausalLM,\n         BartForConditionalGeneration,\n         BartTokenizer,\n         GPT2LMHeadModel,\n@@ -3629,150 +3628,6 @@ def test_model_kwarg_assisted_decoding_decoder_only(self):\n         )\n         self.assertListEqual(outputs_assisted.tolist(), outputs_tti.tolist())\n \n-    def test_model_kwarg_assisted_decoding_encoder_decoder(self):\n-        \"\"\"\n-        Tests that the following scenario is compatible with assisted generation:\n-        1. encoder-decoder main model\n-        2. encoder-decoder assistant model\n-        3. both have a custom input\n-        (e.g. Whisper)\n-        \"\"\"\n-\n-        # PT-only test: TF doesn't support assisted decoding yet.\n-        # Bart subclass with a kwarg that distorts the output\n-        class FakeBart(BartForConditionalGeneration):\n-            def forward(self, input_ids, past_key_values, foo=False, **kwargs):\n-                outs = super().forward(input_ids, past_key_values=past_key_values, **kwargs)\n-                if foo:\n-                    outs[\"logits\"][:, :, :] = 0.0\n-                return outs\n-\n-            def prepare_inputs_for_generation(self, *args, foo=False, encoder_outputs=None, **kwargs):\n-                kwargs[\"encoder_outputs\"] = encoder_outputs\n-                inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n-                inputs[\"foo\"] = foo\n-                return inputs\n-\n-        model = FakeBart.from_pretrained(\"hf-internal-testing/tiny-random-BartForConditionalGeneration\").to(\n-            torch_device\n-        )\n-        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-BartForConditionalGeneration\")\n-\n-        text = \"Hello world\"\n-        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n-        input_ids = tokenized_inputs.input_ids.to(torch_device)\n-\n-        # Traditional way of generating text\n-        outputs_normal = model.generate(input_ids)\n-        self.assertEqual(outputs_normal.shape, (1, 20))\n-\n-        # Should be different with foo\n-        outputs_foo = model.generate(input_ids, foo=True)\n-        with self.assertRaises(AssertionError):\n-            self.assertListEqual(outputs_foo.tolist(), outputs_normal.tolist())\n-\n-        # Assistant model\n-        assistant = FakeBart.from_pretrained(\"hf-internal-testing/tiny-random-BartForConditionalGeneration\").to(\n-            torch_device\n-        )\n-\n-        # If assisted generation passes model_kwargs correctly, should be same as previous\n-        outputs_assisted = model.generate(\n-            input_ids,\n-            foo=True,\n-            assistant_model=assistant,\n-        )\n-        self.assertListEqual(outputs_assisted.tolist(), outputs_foo.tolist())\n-\n-        # Check that passing encoder_outputs directly also works as expected\n-        encoder_outputs = assistant.get_encoder()(input_ids)\n-\n-        outputs_assisted = model.generate(\n-            foo=True,\n-            assistant_model=assistant,\n-            encoder_outputs=encoder_outputs,\n-            assistant_encoder_outputs=encoder_outputs,\n-        )\n-        self.assertListEqual(outputs_assisted.tolist(), outputs_foo.tolist())\n-\n-    def test_assisted_decoding_encoder_decoder_shared_encoder(self):\n-        \"\"\"\n-        Tests that the following scenario is compatible with assisted generation:\n-        1. encoder-decoder main model\n-        2. decoder-only assistant model\n-        3. both have a custom input\n-        (e.g. DistilWhisper)\n-        \"\"\"\n-\n-        # PT-only test: TF doesn't support assisted decoding yet.\n-        # Bart subclass with a kwarg called foo that distorts the output\n-        class FakeBartSeq2Seq(BartForConditionalGeneration):\n-            def forward(self, input_ids, foo=False, **kwargs):\n-                outs = super().forward(input_ids, **kwargs)\n-                if foo:\n-                    outs[\"logits\"][:, :, :] = 0.0\n-                return outs\n-\n-            def prepare_inputs_for_generation(self, *args, foo=False, encoder_outputs=None, **kwargs):\n-                kwargs[\"encoder_outputs\"] = encoder_outputs\n-                inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n-                inputs[\"foo\"] = foo\n-                return inputs\n-\n-        class FakeBartCausalLM(BartForCausalLM):\n-            def forward(self, input_ids, attention_mask, past_key_values, foo=False, **kwargs):\n-                outs = super().forward(input_ids, attention_mask, past_key_values=past_key_values, **kwargs)\n-                if foo:\n-                    outs[\"logits\"][:, :, :] = 0.0\n-                return outs\n-\n-            def prepare_inputs_for_generation(self, *args, foo=False, encoder_outputs=None, **kwargs):\n-                kwargs[\"encoder_outputs\"] = encoder_outputs\n-                inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n-                inputs[\"foo\"] = foo\n-                return inputs\n-\n-        model = FakeBartSeq2Seq.from_pretrained(\"hf-internal-testing/tiny-random-BartForConditionalGeneration\").to(\n-            torch_device\n-        )\n-        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-BartForConditionalGeneration\")\n-\n-        text = \"Hello world\"\n-        tokenized_inputs = tokenizer([text], return_tensors=\"pt\")\n-        input_ids = tokenized_inputs.input_ids.to(torch_device)\n-\n-        # Traditional way of generating text\n-        outputs_normal = model.generate(input_ids)\n-        self.assertEqual(outputs_normal.shape, (1, 20))\n-\n-        # Should be different with foo\n-        outputs_foo = model.generate(input_ids, foo=True)\n-        with self.assertRaises(AssertionError):\n-            self.assertListEqual(outputs_foo.tolist(), outputs_normal.tolist())\n-\n-        # Assistant model\n-        assistant = FakeBartCausalLM.from_pretrained(\n-            \"hf-internal-testing/tiny-random-BartForConditionalGeneration\"\n-        ).to(torch_device)\n-\n-        # If assisted generation passes model_kwargs correctly, should be same as previous\n-        outputs_assisted = model.generate(\n-            input_ids,\n-            foo=True,\n-            assistant_model=assistant,\n-        )\n-        self.assertListEqual(outputs_assisted.tolist(), outputs_foo.tolist())\n-\n-        # Check that passing encoder_outputs directly also works as expected\n-        encoder_outputs = model.get_encoder()(input_ids)\n-\n-        outputs_assisted = model.generate(\n-            foo=True,\n-            assistant_model=assistant,\n-            encoder_outputs=encoder_outputs,\n-        )\n-        self.assertListEqual(outputs_assisted.tolist(), outputs_foo.tolist())\n-\n     def test_assisted_decoding_num_assistant_tokens_heuristic_schedule(self):\n         # This test ensures that the assisted generation num_assistant_tokens 'heuristic' schedule works properly.\n "
        }
    ],
    "stats": {
        "total": 158,
        "additions": 4,
        "deletions": 154
    }
}