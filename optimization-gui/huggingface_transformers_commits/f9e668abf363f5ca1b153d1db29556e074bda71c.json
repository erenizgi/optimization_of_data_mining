{
    "author": "jiqing-feng",
    "message": "update torchao doc (#42139)\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "f9e668abf363f5ca1b153d1db29556e074bda71c",
    "files": [
        {
            "sha": "7c7beb44dbc178bb0e65d6a04e2911f55fe401e6",
            "filename": "docs/source/en/quantization/torchao.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f9e668abf363f5ca1b153d1db29556e074bda71c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f9e668abf363f5ca1b153d1db29556e074bda71c/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ftorchao.md?ref=f9e668abf363f5ca1b153d1db29556e074bda71c",
            "patch": "@@ -329,7 +329,7 @@ from torchao.dtypes import Int4XPULayout\n from torchao.quantization.quant_primitives import ZeroPointDomain\n \n \n-quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4XPULayout(), zero_point_domain=ZeroPointDomain.INT)\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4XPULayout(), zero_point_domain=ZeroPointDomain.INT, int4_packing_format=\"plain_int32\")\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model\n@@ -342,7 +342,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(\n \n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B-Instruct\")\n input_text = \"What are we having for dinner?\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(quantized_model.device)\n \n # auto-compile the quantized model with `cache_implementation=\"static\"` to get speed up\n output = quantized_model.generate(**input_ids, max_new_tokens=10, cache_implementation=\"static\")\n@@ -395,7 +395,7 @@ from transformers import TorchAoConfig, AutoModelForCausalLM, AutoTokenizer\n from torchao.quantization import Int4WeightOnlyConfig\n from torchao.dtypes import Int4CPULayout\n \n-quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout())\n+quant_config = Int4WeightOnlyConfig(group_size=128, layout=Int4CPULayout(), int4_packing_format=\"opaque\")\n quantization_config = TorchAoConfig(quant_type=quant_config)\n \n # Load and quantize the model"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}