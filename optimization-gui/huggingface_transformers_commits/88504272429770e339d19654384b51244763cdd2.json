{
    "author": "cwngan",
    "message": "Fix typo in tokenization_utils_base.py docstring (#38418)\n\nFix typo in tokenization_utils_base.py",
    "sha": "88504272429770e339d19654384b51244763cdd2",
    "files": [
        {
            "sha": "ec0bd53bd57af408e464b1a0702661060b7e7bcf",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/88504272429770e339d19654384b51244763cdd2/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/88504272429770e339d19654384b51244763cdd2/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=88504272429770e339d19654384b51244763cdd2",
            "patch": "@@ -1206,7 +1206,7 @@ def _set_model_specific_special_tokens(self, special_tokens: List[str]):\n                 Activates and controls padding. Accepts the following values:\n \n                 - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n+                  sequence is provided).\n                 - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n                   acceptable input length for the model if that argument is not provided.\n                 - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}