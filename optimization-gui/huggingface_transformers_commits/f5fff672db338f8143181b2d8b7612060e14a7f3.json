{
    "author": "hmellor",
    "message": "Add pipeline parallel plan to `PretrainedConfig` and `PreTrainedModel` (#36091)\n\n* Add `base_model_pp_plan` to `PretrainedConfig`\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add `_pp_plan` to `PreTrainedModel`\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add both to Llama for testing\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Fix type error\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Update to suggested schema\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* `_pp_plan` keys are not patterns\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Simplify schema\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Fix typing error\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Update input name for Llama\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Aria\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Bamba\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Cohere 1 & 2\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to diffllama and emu3\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Gemma 1 & 2\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to GLM and GPT NeoX\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Granite and Helium\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Mistral and Mixtral\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to OLMo 1 & 2\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan to Phi and Phi 3\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan for Qwen 2, 2 MoE, 2 VL and 2.5 VL\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add pp plan for Starcoder 2\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Add enum for accessing inputs and outputs\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Update type hints to use tuples\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Change outer list to tuple\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
    "sha": "f5fff672db338f8143181b2d8b7612060e14a7f3",
    "files": [
        {
            "sha": "581032ef7d243cb0d0d59d5ff4a687d9fc5ef7ac",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -74,6 +74,8 @@ class PretrainedConfig(PushToHubMixin):\n       naming of attributes.\n     - **base_model_tp_plan** (`Dict[str, Any]`) -- A dict that maps sub-modules FQNs of a base model to a tensor\n       parallel plan applied to the sub-module when `model.tensor_parallel` is called.\n+    - **base_model_pp_plan** (`Dict[str, Tuple[List[str]]]`) -- A dict that maps child-modules of a base model to a\n+      pipeline parallel plan that enables users to place the child-module on the appropriate device.\n \n     Common attributes (present in all subclasses):\n \n@@ -198,6 +200,7 @@ class PretrainedConfig(PushToHubMixin):\n     is_composition: bool = False\n     attribute_map: Dict[str, str] = {}\n     base_model_tp_plan: Optional[Dict[str, Any]] = None\n+    base_model_pp_plan: Optional[Dict[str, Tuple[List[str]]]] = None\n     _auto_class: Optional[str] = None\n \n     def __setattr__(self, key, value):\n@@ -860,6 +863,9 @@ def to_diff_dict(self) -> Dict[str, Any]:\n         # Do not serialize `base_model_tp_plan` for now\n         if \"base_model_tp_plan\" in serializable_config_dict:\n             del serializable_config_dict[\"base_model_tp_plan\"]\n+        # Do not serialize `base_model_pp_plan` for now\n+        if \"base_model_pp_plan\" in serializable_config_dict:\n+            del serializable_config_dict[\"base_model_pp_plan\"]\n \n         return serializable_config_dict\n \n@@ -882,6 +888,9 @@ def to_dict(self) -> Dict[str, Any]:\n         # Do not serialize `base_model_tp_plan` for now\n         if \"base_model_tp_plan\" in output:\n             del output[\"base_model_tp_plan\"]\n+        # Do not serialize `base_model_pp_plan` for now\n+        if \"base_model_pp_plan\" in output:\n+            del output[\"base_model_pp_plan\"]\n \n         # Transformers version when serializing the model\n         output[\"transformers_version\"] = __version__"
        },
        {
            "sha": "13c8719b36038b27196ab98758c4028b6849c92e",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -28,6 +28,7 @@\n import warnings\n from contextlib import contextmanager\n from dataclasses import dataclass\n+from enum import Enum\n from functools import partial, wraps\n from threading import Thread\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, TypeVar, Union\n@@ -923,6 +924,11 @@ def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n     return weights_name\n \n \n+class PipelineParallel(Enum):\n+    inputs: 0\n+    outputs: 1\n+\n+\n class ModuleUtilsMixin:\n     \"\"\"\n     A few utilities for `torch.nn.Modules`, to be used as a mixin.\n@@ -1312,6 +1318,17 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, GenerationMixin, PushToHubMix\n     # `config.base_model_tp_plan` during `post_init`.\n     _tp_plan = None\n \n+    # A pipeline parallel plan specifying the layers which may not be present\n+    # on all ranks when PP is enabled. For top-level models, this attribute is\n+    # currently defined in respective model code. For base models, this\n+    # attribute comes from `config.base_model_pp_plan` during `post_init`.\n+    #\n+    # The variable names for the inputs and outputs of the specified layers can\n+    # be indexed using the `PipelineParallel` enum as follows:\n+    # - `_pp_plan[\"layers\"][PipelineParallel.inputs]`\n+    # - `_pp_plan[\"layers\"][PipelineParallel.outputs]`\n+    _pp_plan = None\n+\n     # This flag signal that the model can be used as an efficient backend in TGI and vLLM\n     # In practice, it means that they support attention interface functions, fully pass the kwargs\n     # through all modules up to the Attention layer, can slice logits with Tensor, and have a default TP plan\n@@ -1374,6 +1391,9 @@ def post_init(self):\n         # If current model is a base model, attach `base_model_tp_plan` from config\n         if self.base_model is self:\n             self._tp_plan = self.config.base_model_tp_plan\n+        # If current model is a base model, attach `base_model_pp_plan` from config\n+        if self.base_model is self:\n+            self._pp_plan = self.config.base_model_pp_plan\n \n     def dequantize(self):\n         \"\"\"\n@@ -5196,6 +5216,15 @@ def tplize(mod: torch.nn.Module) -> None:\n         # function to every submodule.\n         self.apply(tplize)\n \n+    @property\n+    def supports_pp_plan(self):\n+        if self._pp_plan is not None:\n+            return True\n+        # Check if base model has PP plan\n+        if getattr(self.base_model, \"_pp_plan\", None) is not None:\n+            return True\n+        return False\n+\n     @property\n     def loss_function(self):\n         if hasattr(self, \"_loss_function\"):"
        },
        {
            "sha": "fed90c86b4a7b741c6d04118a76e0729ed66f64a",
            "filename": "src/transformers/models/aria/configuration_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fconfiguration_aria.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -144,6 +144,11 @@ class AriaTextConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n     base_config_key = \"text_config\"\n \n     def __init__("
        },
        {
            "sha": "dacc92b7951f9b61adba37c607364fa884b894da",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -1141,6 +1141,7 @@ class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n \n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config_class = AriaTextConfig\n \n     def __init__(self, config: AriaTextConfig):"
        },
        {
            "sha": "6fdce41e5a6865d0709c8e86a22d38beab63b04b",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -1446,6 +1446,7 @@ def _update_mamba_mask(self, attention_mask, cache_position):\n class BambaForCausalLM(BambaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "eeeb236428026ff04dc69d95efc14c821eaef5be",
            "filename": "src/transformers/models/cohere/configuration_cohere.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fconfiguration_cohere.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -148,6 +148,11 @@ class CohereConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "5101a0f9e0830e24a7ee06f7b6c0ba690a0f2f7b",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -780,6 +780,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "c792ab3f827864ae9ca19c09e364fc6034c30ff9",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -148,6 +148,11 @@ class Cohere2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "df0cb24d79c46ed48f239b5d0c0bd242c959d1e8",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -781,6 +781,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: Cohere2Config):\n         super().__init__(config)"
        },
        {
            "sha": "979b5abc2600860973072e7e898fdd903b3ec0f5",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -173,6 +173,11 @@ class Cohere2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "301668d21a822279ab7f300b691ff6ccc25b4d37",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -1019,6 +1019,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class DiffLlamaForCausalLM(DiffLlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ef086ab12ea886826c40b8f202782112b217546c",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -1598,6 +1598,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Emu3ForCausalLM(Emu3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n     config_class = Emu3TextConfig\n \n     def __init__(self, config):"
        },
        {
            "sha": "2aeb200580588e2c9b784d59ca902c5303d311c9",
            "filename": "src/transformers/models/gemma/configuration_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fconfiguration_gemma.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -102,6 +102,11 @@ class GemmaConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "59b7dc3dc3474aea4252bf4efe40806685f8c2ac",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -752,6 +752,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GemmaForCausalLM(GemmaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "dc8ced15f96240ba0a21373030658728a03848d2",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -126,6 +126,11 @@ class GemmaConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "c9e66f8beacef60a3e5d9c9ddaa7eb1950cac509",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -106,6 +106,11 @@ class Gemma2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "d55fafe05677713ce0f7ab23766fe896eeffba48",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -790,6 +790,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "76123af3ec5208679c6d69f11de0ab835b4f22bc",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -132,6 +132,11 @@ class Gemma2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f9a3ab53a931bf65eb8e8f44f05181c75a128eec",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -93,6 +93,11 @@ class GlmConfig(PretrainedConfig):\n         \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n         \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "54c138212e86a37ca3fd2ece4830e93bb3131b29",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -761,6 +761,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GlmForCausalLM(GlmPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "cea854eabb94c7224e52183f1be82431419c2348",
            "filename": "src/transformers/models/gpt_neox/configuration_gpt_neox.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fconfiguration_gpt_neox.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -137,6 +137,12 @@ class GPTNeoXConfig(PretrainedConfig):\n         \"layers.*.mlp.dense_h_to_4h\": \"colwise\",\n         \"layers.*.mlp.dense_4h_to_h\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_in\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"emb_dropout\": ([\"inputs_embeds\"], [\"hidden_states\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"final_layer_norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "efb298243177ae059fcb3a918886f31f12051fd8",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -758,6 +758,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"embed_out.weight\"]\n     _tp_plan = {\"embed_out\": \"colwise_rep\"}\n+    _pp_plan = {\"embed_out\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "3a7cc49542ef74a6f912156522a409f671458ed6",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -456,6 +456,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GPTNeoXForCausalLM(GPTNeoXPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"embed_out.weight\"]\n     _tp_plan = {\"embed_out\": \"colwise_rep\"}\n+    _pp_plan = {\"embed_out\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "fc651a94e1bdd8901cb89365984538591669c469",
            "filename": "src/transformers/models/granite/configuration_granite.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fconfiguration_granite.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -122,6 +122,11 @@ class GraniteConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "85c8e97c77ad3a9e4aca13115b658acb2ca07c26",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -764,6 +764,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class GraniteForCausalLM(GranitePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "7b27c6e54b69492ea805540ae2d50aa61ceb2617",
            "filename": "src/transformers/models/helium/configuration_helium.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fconfiguration_helium.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -95,6 +95,11 @@ class HeliumConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "86635f2d7200a9fff26dcd4f7bdc4650fba544bb",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -748,6 +748,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class HeliumForCausalLM(HeliumPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config: HeliumConfig):\n         super().__init__(config)"
        },
        {
            "sha": "066534f109fa4c2b7e3c1f2df4a36a6a44392973",
            "filename": "src/transformers/models/llama/configuration_llama.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconfiguration_llama.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -151,6 +151,11 @@ class LlamaConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "a06084e825674b815d8c68d4b1f3c5b13ed46945",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -750,6 +750,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class LlamaForCausalLM(LlamaPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "3a237bc7343bb7f3b6442bcce095f8a6c3b79302",
            "filename": "src/transformers/models/mistral/configuration_mistral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fconfiguration_mistral.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -107,6 +107,11 @@ class MistralConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "92e555b3d768232cf1f51e6cd1daf7b6a680e76f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -751,6 +751,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class MistralForCausalLM(MistralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "d9b02e10fc4c83e85ce200f50be51464dc6a0fcf",
            "filename": "src/transformers/models/mixtral/configuration_mixtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fconfiguration_mixtral.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -119,6 +119,11 @@ class MixtralConfig(PretrainedConfig):\n         \"layers.*.block_sparse_moe.experts.*.w2\": \"rowwise\",\n         \"layers.*.block_sparse_moe.experts.*.w3\": \"colwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "0835e33722e9191402bbba009a9a69215f0ed7eb",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -967,6 +967,7 @@ def load_balancing_loss_func(\n class MixtralForCausalLM(MixtralPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "ded0bf4f017c19cd8d62335490e5f1fdaf8615bc",
            "filename": "src/transformers/models/olmo/configuration_olmo.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fconfiguration_olmo.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -115,6 +115,11 @@ class OlmoConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "37d15475be89cfed69b3b79729702a8e412633c5",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -726,6 +726,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class OlmoForCausalLM(OlmoPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "222c8e1791542e82847b5ef83ebea8333739b7fe",
            "filename": "src/transformers/models/olmo2/configuration_olmo2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fconfiguration_olmo2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -98,6 +98,11 @@ class Olmo2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "40c912ef1a6391c4dcc21c43e7b29c124959a287",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -727,6 +727,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Olmo2ForCausalLM(Olmo2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "bc5a9b89d50100259c6865242e0f7cd696b62c9e",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -109,6 +109,11 @@ class Olmo2Config(OlmoConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "06e5cbec2eadc419fbe4f058e303455adf6eedf1",
            "filename": "src/transformers/models/phi/configuration_phi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fconfiguration_phi.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -146,6 +146,12 @@ class PhiConfig(PretrainedConfig):\n         \"layers.*.mlp.fc1\": \"colwise\",\n         \"layers.*.mlp.fc2\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"embed_dropout\": ([\"inputs_embeds\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"final_layernorm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8ab41d2a0cbec90c4ac9ac6952a3472f2fb598f5",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -724,6 +724,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class PhiForCausalLM(PhiPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "a6b7ec9baf563e014a66481b90f88ee42b9d3973",
            "filename": "src/transformers/models/phi3/configuration_phi3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fconfiguration_phi3.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -113,6 +113,11 @@ class Phi3Config(PretrainedConfig):\n         \"layers.*.mlp.gate_up_proj\": \"colwise_rep\",  # we need to replicate here due to the `chunk` operation\n         \"layers.*.mlp.down_proj\": \"rowwise_rep\",  # we need to replicate here due to the `chunk` operation\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "2595278048c8a89d6b067cf7e51fa6e2dc8d8df3",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -821,6 +821,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Phi3ForCausalLM(Phi3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "16979865e4fea8bfa20786497865537ab03440c5",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -139,6 +139,11 @@ class Qwen2Config(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "91eac84ffcb2d76742208a1f71c3cc9b58b3acff",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -735,6 +735,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Qwen2ForCausalLM(Qwen2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "b2bf37ba0c149ab09c5be8450c5a4e8295cc0381",
            "filename": "src/transformers/models/qwen2_5_vl/configuration_qwen2_5_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fconfiguration_qwen2_5_vl.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -184,6 +184,11 @@ class Qwen2_5_VLConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "a52b4204a6622d68a858a05327b24af62881149a",
            "filename": "src/transformers/models/qwen2_moe/configuration_qwen2_moe.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fconfiguration_qwen2_moe.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -160,6 +160,11 @@ class Qwen2MoeConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "8157408f42e2d5fdfe3c6d11b16a4eb29abda6b9",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -1217,6 +1217,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class Qwen2MoeForCausalLM(Qwen2MoePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "710738e3965443293405fe0ed5ac9a55b45cbf7a",
            "filename": "src/transformers/models/qwen2_vl/configuration_qwen2_vl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fconfiguration_qwen2_vl.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -173,6 +173,11 @@ class Qwen2VLConfig(PretrainedConfig):\n         \"layers.*.mlp.up_proj\": \"colwise\",\n         \"layers.*.mlp.down_proj\": \"rowwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "b617a1cad8429c65b8de7bbdfb317bff418c9891",
            "filename": "src/transformers/models/starcoder2/configuration_starcoder2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fconfiguration_starcoder2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -143,6 +143,11 @@ class Starcoder2Config(PretrainedConfig):\n         \"layers.*.mlp.c_fc\": \"colwise\",\n         \"layers.*.mlp.c_proj\": \"colwise\",\n     }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,"
        },
        {
            "sha": "f176d5311dd50d468313f120649f17de00399ee8",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5fff672db338f8143181b2d8b7612060e14a7f3/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=f5fff672db338f8143181b2d8b7612060e14a7f3",
            "patch": "@@ -747,6 +747,7 @@ class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n class Starcoder2ForCausalLM(Starcoder2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n     def __init__(self, config):\n         super().__init__(config)"
        }
    ],
    "stats": {
        "total": 188,
        "additions": 188,
        "deletions": 0
    }
}