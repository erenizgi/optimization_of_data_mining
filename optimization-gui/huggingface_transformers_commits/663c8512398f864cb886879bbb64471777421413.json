{
    "author": "guangy10",
    "message": "DistilBERT is ExecuTorch compatible (#34475)\n\n* DistillBERT is ExecuTorch compatible\r\n\r\n* [run_slow] distilbert\r\n\r\n* [run_slow] distilbert\r\n\r\n---------\r\n\r\nCo-authored-by: Guang Yang <guangyang@fb.com>",
    "sha": "663c8512398f864cb886879bbb64471777421413",
    "files": [
        {
            "sha": "d4c51cea125720f9d1c30caa461bf1d449f83c2f",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/663c8512398f864cb886879bbb64471777421413/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/663c8512398f864cb886879bbb64471777421413/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=663c8512398f864cb886879bbb64471777421413",
            "patch": "@@ -30,6 +30,7 @@\n     import torch\n \n     from transformers import (\n+        AutoTokenizer,\n         DistilBertForMaskedLM,\n         DistilBertForMultipleChoice,\n         DistilBertForQuestionAnswering,\n@@ -38,6 +39,7 @@\n         DistilBertModel,\n     )\n     from transformers.models.distilbert.modeling_distilbert import _create_sinusoidal_embeddings\n+    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_4\n \n \n class DistilBertModelTester:\n@@ -420,3 +422,45 @@ def test_inference_no_head_absolute_embedding(self):\n         )\n \n         self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+\n+    @slow\n+    def test_export(self):\n+        if not is_torch_greater_or_equal_than_2_4:\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        distilbert_model = \"distilbert-base-uncased\"\n+        device = \"cpu\"\n+        attn_implementation = \"sdpa\"\n+        max_length = 64\n+\n+        tokenizer = AutoTokenizer.from_pretrained(distilbert_model)\n+        inputs = tokenizer(\n+            f\"Paris is the {tokenizer.mask_token} of France.\",\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=max_length,\n+        )\n+\n+        model = DistilBertForMaskedLM.from_pretrained(\n+            distilbert_model,\n+            device_map=device,\n+            attn_implementation=attn_implementation,\n+        )\n+\n+        logits = model(**inputs).logits\n+        eager_predicted_mask = tokenizer.decode(logits[0, 4].topk(5).indices)\n+        self.assertEqual(\n+            eager_predicted_mask.split(),\n+            [\"capital\", \"birthplace\", \"northernmost\", \"centre\", \"southernmost\"],\n+        )\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+            strict=True,\n+        )\n+\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        exported_predicted_mask = tokenizer.decode(result.logits[0, 4].topk(5).indices)\n+        self.assertEqual(eager_predicted_mask, exported_predicted_mask)"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 44,
        "deletions": 0
    }
}