{
    "author": "yonigozlan",
    "message": "Remove test inheritance for EfficientLoftr, rename KeypointMatchingOutput to model specific name (#42365)\n\n* Rename KeypointMatchingOutput to make it model specific, remove test inheritance, remove irrelevant test\n\n* remove EfficientLoFTRKeypointMatchingOutput from public init",
    "sha": "9bd85b01438997c88d26f82dd3088003bb8ea815",
    "files": [
        {
            "sha": "1cdc3c81725760acf68c0df80bfdcabefb1e83f0",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -45,7 +45,7 @@\n     import PIL\n     from PIL import Image, ImageDraw\n \n-    from .modeling_efficientloftr import KeypointMatchingOutput\n+    from .modeling_efficientloftr import EfficientLoFTRKeypointMatchingOutput\n \n logger = logging.get_logger(__name__)\n \n@@ -344,15 +344,15 @@ def preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: \"KeypointMatchingOutput\",\n+        outputs: \"EfficientLoFTRKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`EfficientLoFTRKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`EfficientLoFTRKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the"
        },
        {
            "sha": "74a5d4577c9165d05a1fc1c9802bf64ec4427c7f",
            "filename": "src/transformers/models/efficientloftr/image_processing_efficientloftr_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fimage_processing_efficientloftr_fast.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -24,7 +24,7 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n from .image_processing_efficientloftr import EfficientLoFTRImageProcessorKwargs\n-from .modeling_efficientloftr import KeypointMatchingOutput\n+from .modeling_efficientloftr import EfficientLoFTRKeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -159,15 +159,15 @@ def _preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: \"KeypointMatchingOutput\",\n+        outputs: \"EfficientLoFTRKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`EfficientLoFTRKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`EfficientLoFTRKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the"
        },
        {
            "sha": "ab467db4fd928c4bcb712074a2d7a7005cffc4e8",
            "filename": "src/transformers/models/efficientloftr/modeling_efficientloftr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodeling_efficientloftr.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -40,15 +40,15 @@\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n-    Base class for outputs of keypoint matching models. Due to the nature of keypoint detection and matching, the number\n+    Base class for outputs of EfficientLoFTR keypoint matching models. Due to the nature of keypoint detection and matching, the number\n     of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the batch of\n-    images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask tensor is\n-    used to indicate which values in the keypoints, matches and matching_scores tensors are keypoint matching\n-    information.\n+    images, the maximum number of matches is set as the dimension of the matches and matching scores.\n     \"\"\"\n )\n-class KeypointMatchingOutput(ModelOutput):\n+class EfficientLoFTRKeypointMatchingOutput(ModelOutput):\n     r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n+        Loss computed during training.\n     matches (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n         Index of keypoint matched in the other image.\n     matching_scores (`torch.FloatTensor` of shape `(batch_size, 2, num_matches)`):\n@@ -64,6 +64,7 @@ class KeypointMatchingOutput(ModelOutput):\n         num_keypoints)`, returned when `output_attentions=True` is passed or when `config.output_attentions=True`)\n     \"\"\"\n \n+    loss: Optional[torch.FloatTensor] = None\n     matches: Optional[torch.FloatTensor] = None\n     matching_scores: Optional[torch.FloatTensor] = None\n     keypoints: Optional[torch.FloatTensor] = None\n@@ -1295,7 +1296,7 @@ def forward(\n         pixel_values: torch.FloatTensor,\n         labels: Optional[torch.LongTensor] = None,\n         **kwargs: Unpack[TransformersKwargs],\n-    ) -> KeypointMatchingOutput:\n+    ) -> EfficientLoFTRKeypointMatchingOutput:\n         r\"\"\"\n         Examples:\n \n@@ -1353,7 +1354,9 @@ def forward(\n         matching_keypoints[:, :, :, 0] = matching_keypoints[:, :, :, 0] / width\n         matching_keypoints[:, :, :, 1] = matching_keypoints[:, :, :, 1] / height\n \n-        return KeypointMatchingOutput(\n+        loss = None\n+        return EfficientLoFTRKeypointMatchingOutput(\n+            loss=loss,\n             matches=coarse_matched_indices,\n             matching_scores=coarse_matching_scores,\n             keypoints=matching_keypoints,"
        },
        {
            "sha": "6461690583e3b9e6bdf68b5aa556716338e2f651",
            "filename": "src/transformers/models/efficientloftr/modular_efficientloftr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fefficientloftr%2Fmodular_efficientloftr.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -4,21 +4,21 @@\n \n from ...utils import TensorType\n from ..superglue.image_processing_superglue_fast import SuperGlueImageProcessorFast\n-from .modeling_efficientloftr import KeypointMatchingOutput\n+from .modeling_efficientloftr import EfficientLoFTRKeypointMatchingOutput\n \n \n class EfficientLoFTRImageProcessorFast(SuperGlueImageProcessorFast):\n     def post_process_keypoint_matching(\n         self,\n-        outputs: \"KeypointMatchingOutput\",\n+        outputs: \"EfficientLoFTRKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`EfficientLoFTRKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`EfficientLoFTRKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `List[Tuple[Tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`Tuple[int, int]`) containing the"
        },
        {
            "sha": "30efc620c114d082f75c03705c58ee6f0a6dd122",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -17,7 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n import numpy as np\n import torch\n@@ -42,9 +42,11 @@\n from ...processing_utils import ImagesKwargs\n from ...utils import TensorType, logging, requires_backends\n from ...utils.import_utils import requires\n-from .modeling_lightglue import LightGlueKeypointMatchingOutput\n \n \n+if TYPE_CHECKING:\n+    from .modeling_lightglue import LightGlueKeypointMatchingOutput\n+\n if is_vision_available():\n     import PIL\n     from PIL import Image, ImageDraw\n@@ -341,15 +343,15 @@ def preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: LightGlueKeypointMatchingOutput,\n+        outputs: \"LightGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`LightGlueKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`LightGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the"
        },
        {
            "sha": "e99237cc104d90685cacbdc7a735e59ad441b859",
            "filename": "src/transformers/models/lightglue/image_processing_lightglue_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fimage_processing_lightglue_fast.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -17,7 +17,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Optional, Union\n+from typing import TYPE_CHECKING, Optional, Union\n \n import torch\n from torchvision.transforms.v2 import functional as F\n@@ -39,9 +39,11 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n from .image_processing_lightglue import LightGlueImageProcessorKwargs\n-from .modeling_lightglue import LightGlueKeypointMatchingOutput\n \n \n+if TYPE_CHECKING:\n+    from .modeling_lightglue import LightGlueKeypointMatchingOutput\n+\n if is_vision_available():\n     from PIL import Image, ImageDraw\n \n@@ -178,15 +180,15 @@ def _preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: LightGlueKeypointMatchingOutput,\n+        outputs: \"LightGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`LightGlueKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`LightGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the"
        },
        {
            "sha": "0d5044c5a40acf09da02826637462367d5ebb270",
            "filename": "src/transformers/models/lightglue/modeling_lightglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodeling_lightglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -869,7 +869,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[tuple, LightGlueKeypointMatchingOutput]:\n+    ) -> Union[tuple, \"LightGlueKeypointMatchingOutput\"]:\n         loss = None\n         if labels is not None:\n             raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")"
        },
        {
            "sha": "f61e86a67e0d54f43c811652ae258b6282677f79",
            "filename": "src/transformers/models/lightglue/modular_lightglue.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flightglue%2Fmodular_lightglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -222,7 +222,7 @@ class LightGlueImageProcessorKwargs(SuperGlueImageProcessorKwargs):\n class LightGlueImageProcessor(SuperGlueImageProcessor):\n     def post_process_keypoint_matching(\n         self,\n-        outputs: LightGlueKeypointMatchingOutput,\n+        outputs: \"LightGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n@@ -232,7 +232,7 @@ def post_process_keypoint_matching(\n class LightGlueImageProcessorFast(SuperGlueImageProcessorFast):\n     def post_process_keypoint_matching(\n         self,\n-        outputs: LightGlueKeypointMatchingOutput,\n+        outputs: \"LightGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n@@ -927,7 +927,7 @@ def forward(\n         labels: Optional[torch.LongTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n-    ) -> Union[tuple, LightGlueKeypointMatchingOutput]:\n+    ) -> Union[tuple, \"LightGlueKeypointMatchingOutput\"]:\n         loss = None\n         if labels is not None:\n             raise ValueError(\"LightGlue is not trainable, no labels should be provided.\")"
        },
        {
            "sha": "81c07448b4e1e6011a0435094e41d4bd40b8c817",
            "filename": "src/transformers/models/superglue/image_processing_superglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -44,7 +44,7 @@\n     import torch\n \n if TYPE_CHECKING:\n-    from .modeling_superglue import KeypointMatchingOutput\n+    from .modeling_superglue import SuperGlueKeypointMatchingOutput\n \n if is_vision_available():\n     import PIL\n@@ -345,15 +345,15 @@ def preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: \"KeypointMatchingOutput\",\n+        outputs: \"SuperGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`SuperGlueKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`SuperGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the"
        },
        {
            "sha": "ffbcc7ce95089a724ec7a98577ebdde840ee955e",
            "filename": "src/transformers/models/superglue/image_processing_superglue_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fimage_processing_superglue_fast.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -32,7 +32,7 @@\n from ...processing_utils import Unpack\n from ...utils import TensorType, auto_docstring\n from .image_processing_superglue import SuperGlueImageProcessorKwargs\n-from .modeling_superglue import KeypointMatchingOutput\n+from .modeling_superglue import SuperGlueKeypointMatchingOutput\n \n \n def _is_valid_image(image):\n@@ -167,15 +167,15 @@ def _preprocess(\n \n     def post_process_keypoint_matching(\n         self,\n-        outputs: \"KeypointMatchingOutput\",\n+        outputs: \"SuperGlueKeypointMatchingOutput\",\n         target_sizes: Union[TensorType, list[tuple]],\n         threshold: float = 0.0,\n     ) -> list[dict[str, torch.Tensor]]:\n         \"\"\"\n-        Converts the raw output of [`KeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n+        Converts the raw output of [`SuperGlueKeypointMatchingOutput`] into lists of keypoints, scores and descriptors\n         with coordinates absolute to the original image sizes.\n         Args:\n-            outputs ([`KeypointMatchingOutput`]):\n+            outputs ([`SuperGlueKeypointMatchingOutput`]):\n                 Raw outputs of the model.\n             target_sizes (`torch.Tensor` or `list[tuple[tuple[int, int]]]`, *optional*):\n                 Tensor of shape `(batch_size, 2, 2)` or list of tuples of tuples (`tuple[int, int]`) containing the"
        },
        {
            "sha": "d0af50a5709c69eaf118b6b4cbff46bc95e4166d",
            "filename": "src/transformers/models/superglue/modeling_superglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fmodeling_superglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -149,14 +149,14 @@ def arange_like(x, dim: int) -> torch.Tensor:\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n-    Base class for outputs of keypoint matching models. Due to the nature of keypoint detection and matching, the number\n+    Base class for outputs of SuperGlue keypoint matching models. Due to the nature of keypoint detection and matching, the number\n     of keypoints is not fixed and can vary from image to image, which makes batching non-trivial. In the batch of\n     images, the maximum number of matches is set as the dimension of the matches and matching scores. The mask tensor is\n     used to indicate which values in the keypoints, matches and matching_scores tensors are keypoint matching\n     information.\n     \"\"\"\n )\n-class KeypointMatchingOutput(ModelOutput):\n+class SuperGlueKeypointMatchingOutput(ModelOutput):\n     r\"\"\"\n     loss (`torch.FloatTensor` of shape `(1,)`, *optional*):\n         Loss computed during training.\n@@ -670,7 +670,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-    ) -> Union[tuple, KeypointMatchingOutput]:\n+    ) -> Union[tuple, SuperGlueKeypointMatchingOutput]:\n         r\"\"\"\n         Examples:\n \n@@ -738,7 +738,7 @@ def forward(\n                 if v is not None\n             )\n \n-        return KeypointMatchingOutput(\n+        return SuperGlueKeypointMatchingOutput(\n             loss=loss,\n             matches=matches,\n             matching_scores=matching_scores,"
        },
        {
            "sha": "100ee492a95081c5805a4747711728ae468c9f94",
            "filename": "tests/models/efficientloftr/test_image_processing_efficientloftr.py",
            "status": "modified",
            "additions": 361,
            "deletions": 14,
            "changes": 375,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_image_processing_efficientloftr.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -15,22 +15,26 @@\n import unittest\n \n import numpy as np\n+import pytest\n+from packaging import version\n+from parameterized import parameterized\n \n-from tests.models.superglue.test_image_processing_superglue import (\n-    SuperGlueImageProcessingTest,\n-    SuperGlueImageProcessingTester,\n-)\n from transformers.testing_utils import (\n     require_torch,\n+    require_torch_accelerator,\n     require_vision,\n+    slow,\n+    torch_device,\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n \n if is_torch_available():\n     import torch\n \n-    from transformers.models.efficientloftr.modeling_efficientloftr import KeypointMatchingOutput\n+    from transformers.models.efficientloftr.modeling_efficientloftr import EfficientLoFTRKeypointMatchingOutput\n \n if is_vision_available():\n     from transformers import EfficientLoFTRImageProcessor\n@@ -47,7 +51,7 @@ def random_tensor(size):\n     return torch.rand(size)\n \n \n-class EfficientLoFTRImageProcessingTester(SuperGlueImageProcessingTester):\n+class EfficientLoFTRImageProcessingTester:\n     \"\"\"Tester for EfficientLoFTRImageProcessor\"\"\"\n \n     def __init__(\n@@ -62,9 +66,41 @@ def __init__(\n         size=None,\n         do_grayscale=True,\n     ):\n-        super().__init__(\n-            parent, batch_size, num_channels, image_size, min_resolution, max_resolution, do_resize, size, do_grayscale\n+        size = size if size is not None else {\"height\": 480, \"width\": 640}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_grayscale = do_grayscale\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"do_grayscale\": self.do_grayscale,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return 2, self.num_channels, self.size[\"height\"], self.size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False, pairs=True, batch_size=None):\n+        batch_size = batch_size if batch_size is not None else self.batch_size\n+        image_inputs = prepare_image_inputs(\n+            batch_size=batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n         )\n+        if pairs:\n+            image_inputs = [image_inputs[i : i + 2] for i in range(0, len(image_inputs), 2)]\n+        return image_inputs\n \n     def prepare_keypoint_matching_output(self, pixel_values):\n         \"\"\"Prepare a fake output for the keypoint matching model with random matches between 50 keypoints per image.\"\"\"\n@@ -85,19 +121,293 @@ def prepare_keypoint_matching_output(self, pixel_values):\n             matches[i, 1, random_matches_indices0] = random_matches_indices1\n             scores[i, 0, random_matches_indices1] = torch.rand((random_number_matches,))\n             scores[i, 1, random_matches_indices0] = torch.rand((random_number_matches,))\n-        return KeypointMatchingOutput(keypoints=keypoints, matches=matches, matching_scores=scores)\n+        return EfficientLoFTRKeypointMatchingOutput(keypoints=keypoints, matches=matches, matching_scores=scores)\n \n \n @require_torch\n @require_vision\n-class EfficientLoFTRImageProcessingTest(SuperGlueImageProcessingTest, unittest.TestCase):\n+class EfficientLoFTRImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = EfficientLoFTRImageProcessor if is_vision_available() else None\n     fast_image_processing_class = EfficientLoFTRImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self) -> None:\n         super().setUp()\n         self.image_processor_tester = EfficientLoFTRImageProcessingTester(self)\n \n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processing(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_grayscale\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 480, \"width\": 640})\n+\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size={\"height\": 42, \"width\": 42}\n+            )\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+\n+    @unittest.skip(reason=\"SuperPointImageProcessor is always supposed to return a grayscaled image\")\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    def test_number_and_format_of_images_in_input(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+\n+            # Cases where the number of images and the format of lists in the input is correct\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=2)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=2)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((1, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=4)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((2, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=6)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual((3, 2, 3, 480, 640), tuple(image_processed[\"pixel_values\"].shape))\n+\n+            # Cases where the number of images or the format of lists in the input is incorrect\n+            ## List of 4 images\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=4)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+            ## List of 3 images\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=False, batch_size=3)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+            ## List of 2 pairs and 1 image\n+            image_input = self.image_processor_tester.prepare_image_inputs(pairs=True, batch_size=3)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    @parameterized.expand(\n+        [\n+            ([random_array((3, 100, 200)), random_array((3, 100, 200))], (1, 2, 3, 480, 640)),\n+            ([[random_array((3, 100, 200)), random_array((3, 100, 200))]], (1, 2, 3, 480, 640)),\n+            ([random_tensor((3, 100, 200)), random_tensor((3, 100, 200))], (1, 2, 3, 480, 640)),\n+            ([random_tensor((3, 100, 200)), random_tensor((3, 100, 200))], (1, 2, 3, 480, 640)),\n+        ],\n+    )\n+    def test_valid_image_shape_in_input(self, image_input, output):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_processed = image_processor.preprocess(image_input, return_tensors=\"pt\")\n+            self.assertEqual(output, tuple(image_processed[\"pixel_values\"].shape))\n+\n+    @parameterized.expand(\n+        [\n+            (random_array((3, 100, 200)),),\n+            ([random_array((3, 100, 200))],),\n+            (random_array((1, 3, 100, 200)),),\n+            ([[random_array((3, 100, 200))]],),\n+            ([[random_array((3, 100, 200))], [random_array((3, 100, 200))]],),\n+            ([random_array((1, 3, 100, 200)), random_array((1, 3, 100, 200))],),\n+            (random_array((1, 1, 3, 100, 200)),),\n+        ],\n+    )\n+    def test_invalid_image_shape_in_input(self, image_input):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            with self.assertRaises(ValueError) as cm:\n+                image_processor(image_input, return_tensors=\"pt\")\n+            self.assertEqual(ValueError, cm.exception.__class__)\n+\n+    def test_input_images_properly_paired(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor(image_inputs, return_tensors=\"pt\")\n+            self.assertEqual(len(pre_processed_images[\"pixel_values\"].shape), 5)\n+            self.assertEqual(pre_processed_images[\"pixel_values\"].shape[1], 2)\n+\n+    def test_input_not_paired_images_raises_error(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(pairs=False)\n+            with self.assertRaises(ValueError):\n+                image_processor(image_inputs[0])\n+\n+    def test_input_image_properly_converted_to_grayscale(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor(image_inputs, return_tensors=\"pt\")\n+            for image_pair in pre_processed_images[\"pixel_values\"]:\n+                for image in image_pair:\n+                    self.assertTrue(\n+                        torch.all(image[0, ...] == image[1, ...]) and torch.all(image[1, ...] == image[2, ...])\n+                    )\n+\n+    def test_call_numpy(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, numpify=True)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, numpify=True, pairs=False\n+            )\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_call_pil(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, pairs=False)\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_call_pytorch(self):\n+        # Test overwritten because SuperGlueImageProcessor combines images by pair to feed it into SuperGlue\n+\n+        # Initialize image_processing\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+            for image_pair in image_pairs:\n+                self.assertEqual(len(image_pair), 2)\n+\n+            expected_batch_size = int(self.image_processor_tester.batch_size / 2)\n+\n+            # Test with 2 images\n+            encoded_images = image_processing(image_pairs[0], return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+            # Test with list of pairs\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs)\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+            # Test without paired images\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, torchify=True, pairs=False\n+            )\n+            with self.assertRaises(ValueError):\n+                image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+\n+    def test_image_processor_with_list_of_two_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            image_pairs = self.image_processor_tester.prepare_image_inputs(\n+                equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+            )\n+            self.assertEqual(len(image_pairs), 2)\n+            self.assertTrue(isinstance(image_pairs[0], np.ndarray))\n+            self.assertTrue(isinstance(image_pairs[1], np.ndarray))\n+\n+            expected_batch_size = 1\n+            encoded_images = image_processing(image_pairs, return_tensors=\"pt\").pixel_values\n+            expected_output_image_shape = self.image_processor_tester.expected_output_image_shape(image_pairs[0])\n+            self.assertEqual(tuple(encoded_images.shape), (expected_batch_size, *expected_output_image_shape))\n+\n+    @require_torch\n+    def test_post_processing_keypoint_matching(self):\n+        def check_post_processed_output(post_processed_output, image_pair_size):\n+            for post_processed_output, (image_size0, image_size1) in zip(post_processed_output, image_pair_size):\n+                self.assertTrue(\"keypoints0\" in post_processed_output)\n+                self.assertTrue(\"keypoints1\" in post_processed_output)\n+                self.assertTrue(\"matching_scores\" in post_processed_output)\n+                keypoints0 = post_processed_output[\"keypoints0\"]\n+                keypoints1 = post_processed_output[\"keypoints1\"]\n+                all_below_image_size0 = torch.all(keypoints0[:, 0] <= image_size0[1]) and torch.all(\n+                    keypoints0[:, 1] <= image_size0[0]\n+                )\n+                all_below_image_size1 = torch.all(keypoints1[:, 0] <= image_size1[1]) and torch.all(\n+                    keypoints1[:, 1] <= image_size1[0]\n+                )\n+                all_above_zero0 = torch.all(keypoints0[:, 0] >= 0) and torch.all(keypoints0[:, 1] >= 0)\n+                all_above_zero1 = torch.all(keypoints0[:, 0] >= 0) and torch.all(keypoints0[:, 1] >= 0)\n+                self.assertTrue(all_below_image_size0)\n+                self.assertTrue(all_below_image_size1)\n+                self.assertTrue(all_above_zero0)\n+                self.assertTrue(all_above_zero1)\n+                all_scores_different_from_minus_one = torch.all(post_processed_output[\"matching_scores\"] != -1)\n+                self.assertTrue(all_scores_different_from_minus_one)\n+\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs()\n+            pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n+            outputs = self.image_processor_tester.prepare_keypoint_matching_output(**pre_processed_images)\n+\n+            tuple_image_sizes = [\n+                ((image_pair[0].size[0], image_pair[0].size[1]), (image_pair[1].size[0], image_pair[1].size[1]))\n+                for image_pair in image_inputs\n+            ]\n+            tuple_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tuple_image_sizes)\n+\n+            check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n+\n+            tensor_image_sizes = torch.tensor(\n+                [(image_pair[0].size, image_pair[1].size) for image_pair in image_inputs]\n+            ).flip(2)\n+            tensor_post_processed_outputs = image_processor.post_process_keypoint_matching(outputs, tensor_image_sizes)\n+\n+            check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)\n+\n     @unittest.skip(reason=\"Many failing cases. This test needs a more deep investigation.\")\n     def test_fast_is_faster_than_slow(self):\n         \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n@@ -129,7 +439,44 @@ def test_fast_is_faster_than_slow(self):\n             fast_time, slow_time * 1.2, \"Fast processor should not be significantly slower than slow processor\"\n         )\n \n-    # TODO: FIXME @yonigozlan\n-    @unittest.skip(reason=\"failing after #42018\")\n-    def test_post_processing_keypoint_matching_with_padded_match_indices(self):\n-        pass\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = self.image_processor_tester.prepare_image_inputs(\n+            equal_resolution=False, numpify=True, batch_size=2, pairs=False\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        self._assert_slow_fast_tensors_equivalence(encoding_slow.pixel_values, encoding_fast.pixel_values)\n+\n+    @slow\n+    @require_torch_accelerator\n+    @require_vision\n+    @pytest.mark.torch_compile_test\n+    def test_can_compile_fast_image_processor(self):\n+        \"\"\"Override the generic test since EfficientLoFTR requires image pairs.\"\"\"\n+        if self.fast_image_processing_class is None:\n+            self.skipTest(\"Skipping compilation test as fast image processor is not defined\")\n+        if version.parse(torch.__version__) < version.parse(\"2.3\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.3 to run.\")\n+\n+        torch.compiler.reset()\n+        input_image = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=False)\n+        image_processor = self.fast_image_processing_class(**self.image_processor_dict)\n+        output_eager = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+\n+        image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n+        output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n+        self._assert_slow_fast_tensors_equivalence(\n+            output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4, rtol=1e-4, mean_atol=1e-5\n+        )"
        },
        {
            "sha": "3991eb1d38e05564bbc8db635650fa6b27301bae",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "modified",
            "additions": 6,
            "deletions": 7,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bd85b01438997c88d26f82dd3088003bb8ea815/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bd85b01438997c88d26f82dd3088003bb8ea815/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=9bd85b01438997c88d26f82dd3088003bb8ea815",
            "patch": "@@ -28,17 +28,14 @@\n )\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n-from ...test_image_processing_common import (\n-    ImageProcessingTestMixin,\n-    prepare_image_inputs,\n-)\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n if is_torch_available():\n     import numpy as np\n     import torch\n \n-    from transformers.models.superglue.modeling_superglue import KeypointMatchingOutput\n+    from transformers.models.superglue.modeling_superglue import SuperGlueKeypointMatchingOutput\n \n if is_vision_available():\n     from transformers import SuperGlueImageProcessor\n@@ -125,7 +122,7 @@ def prepare_keypoint_matching_output(self, pixel_values):\n             matches[i, 1, random_matches_indices0] = random_matches_indices1\n             scores[i, 0, random_matches_indices1] = torch.rand((random_number_matches,))\n             scores[i, 1, random_matches_indices0] = torch.rand((random_number_matches,))\n-        return KeypointMatchingOutput(mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores)\n+        return SuperGlueKeypointMatchingOutput(mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores)\n \n \n @require_torch\n@@ -450,7 +447,9 @@ def test_post_processing_keypoint_matching_with_padded_match_indices(self):\n             matches[0, 0, 1] = 2  # Points to index 2, which is valid\n             scores[0, 0, 1] = 0.8\n \n-            outputs = KeypointMatchingOutput(mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores)\n+            outputs = SuperGlueKeypointMatchingOutput(\n+                mask=mask, keypoints=keypoints, matches=matches, matching_scores=scores\n+            )\n \n             image_sizes = [((480, 640), (480, 640))]\n "
        }
    ],
    "stats": {
        "total": 485,
        "additions": 419,
        "deletions": 66
    }
}