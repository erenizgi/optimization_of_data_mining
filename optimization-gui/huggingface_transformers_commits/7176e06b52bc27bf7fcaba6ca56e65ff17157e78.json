{
    "author": "jadechoghari",
    "message": "Add TextNet (#34979)\n\n* WIP\n\n* Add config and modeling for Fast model\n\n* Refactor modeling and add tests\n\n* More changes\n\n* WIP\n\n* Add tests\n\n* Add conversion script\n\n* Add conversion scripts, integration tests, image processor\n\n* Fix style and copies\n\n* Add fast model to init\n\n* Add fast model in docs and other places\n\n* Fix import of cv2\n\n* Rename image processing method\n\n* Fix build\n\n* Fix Build\n\n* fix style and fix copies\n\n* Fix build\n\n* Fix build\n\n* Fix Build\n\n* Clean up docstrings\n\n* Fix Build\n\n* Fix Build\n\n* Fix Build\n\n* Fix build\n\n* Add test for image_processing_fast and add documentation tests\n\n* some refactorings\n\n* Fix failing tests\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Introduce TextNet\n\n* Fix failures\n\n* Refactor textnet model\n\n* Fix failures\n\n* Add cv2 to setup\n\n* Fix failures\n\n* Fix failures\n\n* Add CV2 dependency\n\n* Fix bugs\n\n* Fix build issue\n\n* Fix failures\n\n* Remove textnet from modeling fast\n\n* Fix build and other things\n\n* Fix build\n\n* some cleanups\n\n* some cleanups\n\n* Some more cleanups\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* More cleanup\n\n* More cleanup\n\n* More cleanup\n\n* Fix build\n\n* Remove all the references of fast model\n\n* More cleanup\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Fix Build\n\n* Fix build\n\n* Fix build\n\n* Fix build\n\n* Fix build\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* Fix style\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* Fix image processing mean and std\n\n* Incorporate PR feedbacks\n\n* fix build failure\n\n* Add assertion to image processor\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* fix style failures\n\n* fix build\n\n* Fix Imageclassification's linear layer, also introduce TextNetImageProcessor\n\n* Fix build\n\n* Fix build\n\n* Fix build\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Fix build\n\n* Incorporate PR feedbacks\n\n* Remove some script\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Incorporate PR feedbacks\n\n* Fix image processing in textnet\n\n* Incorporate PR Feedbacks\n\n* Fix CI failures\n\n* Fix failing test\n\n* Fix failing test\n\n* Fix failing test\n\n* Fix failing test\n\n* Fix failing test\n\n* Fix failing test\n\n* Add textnet to readme\n\n* Improve readability\n\n* Incorporate PR feedbacks\n\n* fix code style\n\n* fix key error and convert working\n\n* tvlt shouldn't be here\n\n* fix test modeling test\n\n* Fix tests, make fixup\n\n* Make fixup\n\n* Make fixup\n\n* Remove TEXTNET_PRETRAINED_MODEL_ARCHIVE_LIST\n\n* improve type annotation\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update tests/models/textnet/test_image_processing_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* improve type annotation\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* space typo\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* improve type annotation\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/textnet/configuration_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* make conv layer kernel sizes and strides default to None\n\n* Update src/transformers/models/textnet/modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/textnet/modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* fix keyword bug\n\n* add batch init and make fixup\n\n* Make fixup\n\n* Update integration test\n\n* Add figure\n\n* Update textnet.md\n\n* add testing and fix errors (classification, imgprocess)\n\n* fix error check\n\n* make fixup\n\n* make fixup\n\n* revert to original docstring\n\n* add make style\n\n* remove conflict for now\n\n* Update modeling_auto.py\n\ngot a confusion in `timm_wrapper` - was giving some conflicts\n\n* Update tests/models/textnet/test_modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/textnet/modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update tests/models/textnet/test_modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Update src/transformers/models/textnet/modeling_textnet.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* add changes\n\n* Update textnet.md\n\n* add doc\n\n* add authors hf ckpt + rename\n\n* add feedback: classifier/docs\n\n---------\n\nCo-authored-by: raghavanone <opensourcemaniacfreak@gmail.com>\nCo-authored-by: jadechoghari <jadechoghari@users.noreply.huggingface.co>\nCo-authored-by: Niels <niels.rogge1@gmail.com>\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
    "files": [
        {
            "sha": "1c993deac0d06265e0532e0e31b89ba6c9c891b9",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -721,6 +721,8 @@\n         title: Swin2SR\n       - local: model_doc/table-transformer\n         title: Table Transformer\n+      - local: model_doc/textnet\n+        title: TextNet\n       - local: model_doc/timm_wrapper\n         title: Timm Wrapper\n       - local: model_doc/upernet"
        },
        {
            "sha": "127c80e9cf987d7cd2322823dc186d099d312b38",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -326,6 +326,7 @@ Flax), PyTorch, and/or TensorFlow.\n |             [Table Transformer](model_doc/table-transformer)             |       ✅        |         ❌         |      ❌      |\n |                         [TAPAS](model_doc/tapas)                         |       ✅        |         ✅         |      ❌      |\n |                         [TAPEX](model_doc/tapex)                         |       ✅        |         ✅         |      ✅      |\n+|                       [TextNet](model_doc/textnet)                       |       ✅        |         ❌         |      ❌      |\n |       [Time Series Transformer](model_doc/time_series_transformer)       |       ✅        |         ❌         |      ❌      |\n |                   [TimeSformer](model_doc/timesformer)                   |       ✅        |         ❌         |      ❌      |\n |                [TimmWrapperModel](model_doc/timm_wrapper)                |       ✅        |         ❌         |      ❌      |"
        },
        {
            "sha": "d6b431e648f21bf985964a4bbc0d85c9db95d824",
            "filename": "docs/source/en/model_doc/textnet.md",
            "status": "added",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ftextnet.md?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,55 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# TextNet\n+\n+## Overview\n+\n+The TextNet model was proposed in [FAST: Faster Arbitrarily-Shaped Text Detector with Minimalist Kernel Representation](https://arxiv.org/abs/2111.02394) by Zhe Chen, Jiahao Wang, Wenhai Wang, Guo Chen, Enze Xie, Ping Luo, Tong Lu. TextNet is a vision backbone useful for text detection tasks. It is the result of neural architecture search (NAS) on backbones with reward function as text detection task (to provide powerful features for text detection).\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/fast_architecture.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> TextNet backbone as part of FAST. Taken from the <a href=\"https://arxiv.org/abs/2111.02394\">original paper.</a> </small>\n+\n+This model was contributed by [Raghavan](https://huggingface.co/Raghavan), [jadechoghari](https://huggingface.co/jadechoghari) and [nielsr](https://huggingface.co/nielsr).\n+\n+## Usage tips\n+\n+TextNet is mainly used as a backbone network for the architecture search of text detection. Each stage of the backbone network is comprised of a stride-2 convolution and searchable blocks. \n+Specifically, we present a layer-level candidate set, defined as {conv3×3, conv1×3, conv3×1, identity}. As the 1×3 and 3×1 convolutions have asymmetric kernels and oriented structure priors, they may help to capture the features of extreme aspect-ratio and rotated text lines.\n+\n+TextNet is the backbone for Fast, but can also be used as an efficient text/image classification, we add a `TextNetForImageClassification` as is it would allow people to train an image classifier on top of the pre-trained textnet weights\n+\n+## TextNetConfig\n+\n+[[autodoc]] TextNetConfig\n+\n+## TextNetImageProcessor\n+\n+[[autodoc]] TextNetImageProcessor\n+    - preprocess\n+\n+## TextNetModel\n+\n+[[autodoc]] TextNetModel\n+    - forward\n+\n+## TextNetForImageClassification\n+\n+[[autodoc]] TextNetForImageClassification\n+    - forward\n+"
        },
        {
            "sha": "0c8765e30332621d2c3662057da3463b989a50ba",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -789,6 +789,7 @@\n         \"TapasConfig\",\n         \"TapasTokenizer\",\n     ],\n+    \"models.textnet\": [\"TextNetConfig\"],\n     \"models.time_series_transformer\": [\"TimeSeriesTransformerConfig\"],\n     \"models.timesformer\": [\"TimesformerConfig\"],\n     \"models.timm_backbone\": [\"TimmBackboneConfig\"],\n@@ -1258,6 +1259,7 @@\n     _import_structure[\"models.siglip\"].append(\"SiglipImageProcessor\")\n     _import_structure[\"models.superpoint\"].extend([\"SuperPointImageProcessor\"])\n     _import_structure[\"models.swin2sr\"].append(\"Swin2SRImageProcessor\")\n+    _import_structure[\"models.textnet\"].extend([\"TextNetImageProcessor\"])\n     _import_structure[\"models.tvp\"].append(\"TvpImageProcessor\")\n     _import_structure[\"models.video_llava\"].append(\"VideoLlavaImageProcessor\")\n     _import_structure[\"models.videomae\"].extend([\"VideoMAEFeatureExtractor\", \"VideoMAEImageProcessor\"])\n@@ -3584,6 +3586,14 @@\n             \"load_tf_weights_in_tapas\",\n         ]\n     )\n+    _import_structure[\"models.textnet\"].extend(\n+        [\n+            \"TextNetBackbone\",\n+            \"TextNetForImageClassification\",\n+            \"TextNetModel\",\n+            \"TextNetPreTrainedModel\",\n+        ]\n+    )\n     _import_structure[\"models.time_series_transformer\"].extend(\n         [\n             \"TimeSeriesTransformerForPrediction\",\n@@ -5813,6 +5823,7 @@\n         TapasConfig,\n         TapasTokenizer,\n     )\n+    from .models.textnet import TextNetConfig\n     from .models.time_series_transformer import (\n         TimeSeriesTransformerConfig,\n     )\n@@ -6293,6 +6304,7 @@\n         from .models.siglip import SiglipImageProcessor\n         from .models.superpoint import SuperPointImageProcessor\n         from .models.swin2sr import Swin2SRImageProcessor\n+        from .models.textnet import TextNetImageProcessor\n         from .models.tvp import TvpImageProcessor\n         from .models.video_llava import VideoLlavaImageProcessor\n         from .models.videomae import VideoMAEFeatureExtractor, VideoMAEImageProcessor\n@@ -8155,6 +8167,12 @@\n             TapasPreTrainedModel,\n             load_tf_weights_in_tapas,\n         )\n+        from .models.textnet import (\n+            TextNetBackbone,\n+            TextNetForImageClassification,\n+            TextNetModel,\n+            TextNetPreTrainedModel,\n+        )\n         from .models.time_series_transformer import (\n             TimeSeriesTransformerForPrediction,\n             TimeSeriesTransformerModel,"
        },
        {
            "sha": "7b4456240c77de59424f13f05d906e1aed17933e",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -252,6 +252,7 @@\n     t5,\n     table_transformer,\n     tapas,\n+    textnet,\n     time_series_transformer,\n     timesformer,\n     timm_backbone,"
        },
        {
            "sha": "1a5bb640395b1f434a1a06bd69f7613e64b4639e",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -279,6 +279,7 @@\n         (\"t5\", \"T5Config\"),\n         (\"table-transformer\", \"TableTransformerConfig\"),\n         (\"tapas\", \"TapasConfig\"),\n+        (\"textnet\", \"TextNetConfig\"),\n         (\"time_series_transformer\", \"TimeSeriesTransformerConfig\"),\n         (\"timesformer\", \"TimesformerConfig\"),\n         (\"timm_backbone\", \"TimmBackboneConfig\"),\n@@ -610,6 +611,7 @@\n         (\"table-transformer\", \"Table Transformer\"),\n         (\"tapas\", \"TAPAS\"),\n         (\"tapex\", \"TAPEX\"),\n+        (\"textnet\", \"TextNet\"),\n         (\"time_series_transformer\", \"Time Series Transformer\"),\n         (\"timesformer\", \"TimeSformer\"),\n         (\"timm_backbone\", \"TimmBackbone\"),"
        },
        {
            "sha": "0729caa6260665b7d3f58772593d2f7f2edf0621",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -257,6 +257,7 @@\n         (\"t5\", \"T5Model\"),\n         (\"table-transformer\", \"TableTransformerModel\"),\n         (\"tapas\", \"TapasModel\"),\n+        (\"textnet\", \"TextNetModel\"),\n         (\"time_series_transformer\", \"TimeSeriesTransformerModel\"),\n         (\"timesformer\", \"TimesformerModel\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n@@ -703,6 +704,7 @@\n         (\"swiftformer\", \"SwiftFormerForImageClassification\"),\n         (\"swin\", \"SwinForImageClassification\"),\n         (\"swinv2\", \"Swinv2ForImageClassification\"),\n+        (\"textnet\", \"TextNetForImageClassification\"),\n         (\"timm_wrapper\", \"TimmWrapperForImageClassification\"),\n         (\"van\", \"VanForImageClassification\"),\n         (\"vit\", \"ViTForImageClassification\"),\n@@ -1391,6 +1393,7 @@\n         (\"rt_detr_resnet\", \"RTDetrResNetBackbone\"),\n         (\"swin\", \"SwinBackbone\"),\n         (\"swinv2\", \"Swinv2Backbone\"),\n+        (\"textnet\", \"TextNetBackbone\"),\n         (\"timm_backbone\", \"TimmBackbone\"),\n         (\"vitdet\", \"VitDetBackbone\"),\n     ]"
        },
        {
            "sha": "8f04a680b21f4895675b2001ef5972b11d7ab1d0",
            "filename": "src/transformers/models/textnet/__init__.py",
            "status": "added",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2F__init__.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,28 @@\n+# Copyright 2024 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_textnet import *\n+    from .image_processing_textnet import *\n+    from .modeling_textnet import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "61ecaaeba8e554c526510e8169d0628e59c5422b",
            "filename": "src/transformers/models/textnet/configuration_textnet.py",
            "status": "added",
            "additions": 135,
            "deletions": 0,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconfiguration_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconfiguration_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconfiguration_textnet.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,135 @@\n+# coding=utf-8\n+# Copyright 2024 the Fast authors and HuggingFace Inc. team.  All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"TextNet model configuration\"\"\"\n+\n+from transformers import PretrainedConfig\n+from transformers.utils import logging\n+from transformers.utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class TextNetConfig(BackboneConfigMixin, PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`TextNextModel`]. It is used to instantiate a\n+    TextNext model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the\n+    [czczup/textnet-base](https://huggingface.co/czczup/textnet-base). Configuration objects inherit from\n+    [`PretrainedConfig`] and can be used to control the model outputs.Read the documentation from [`PretrainedConfig`]\n+    for more information.\n+\n+    Args:\n+        stem_kernel_size (`int`, *optional*, defaults to 3):\n+            The kernel size for the initial convolution layer.\n+        stem_stride (`int`, *optional*, defaults to 2):\n+            The stride for the initial convolution layer.\n+        stem_num_channels (`int`, *optional*, defaults to 3):\n+            The num of channels in input for the initial convolution layer.\n+        stem_out_channels (`int`, *optional*, defaults to 64):\n+            The num of channels in out for the initial convolution layer.\n+        stem_act_func (`str`, *optional*, defaults to `\"relu\"`):\n+            The activation function for the initial convolution layer.\n+        image_size (`Tuple[int, int]`, *optional*, defaults to `[640, 640]`):\n+            The size (resolution) of each image.\n+        conv_layer_kernel_sizes (`List[List[List[int]]]`, *optional*):\n+            A list of stage-wise kernel sizes. If `None`, defaults to:\n+            `[[[3, 3], [3, 3], [3, 3]], [[3, 3], [1, 3], [3, 3], [3, 1]], [[3, 3], [3, 3], [3, 1], [1, 3]], [[3, 3], [3, 1], [1, 3], [3, 3]]]`.\n+        conv_layer_strides (`List[List[int]]`, *optional*):\n+            A list of stage-wise strides. If `None`, defaults to:\n+            `[[1, 2, 1], [2, 1, 1, 1], [2, 1, 1, 1], [2, 1, 1, 1]]`.\n+        hidden_sizes (`List[int]`, *optional*, defaults to `[64, 64, 128, 256, 512]`):\n+            Dimensionality (hidden size) at each stage.\n+        batch_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the batch normalization layers.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        out_features (`List[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage.\n+        out_indices (`List[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage.\n+\n+    Examples:\n+\n+    ```python\n+    >>> from transformers import TextNetConfig, TextNetBackbone\n+\n+    >>> # Initializing a TextNetConfig\n+    >>> configuration = TextNetConfig()\n+\n+    >>> # Initializing a model (with random weights)\n+    >>> model = TextNetBackbone(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"textnet\"\n+\n+    def __init__(\n+        self,\n+        stem_kernel_size=3,\n+        stem_stride=2,\n+        stem_num_channels=3,\n+        stem_out_channels=64,\n+        stem_act_func=\"relu\",\n+        image_size=[640, 640],\n+        conv_layer_kernel_sizes=None,\n+        conv_layer_strides=None,\n+        hidden_sizes=[64, 64, 128, 256, 512],\n+        batch_norm_eps=1e-5,\n+        initializer_range=0.02,\n+        out_features=None,\n+        out_indices=None,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        if conv_layer_kernel_sizes is None:\n+            conv_layer_kernel_sizes = [\n+                [[3, 3], [3, 3], [3, 3]],\n+                [[3, 3], [1, 3], [3, 3], [3, 1]],\n+                [[3, 3], [3, 3], [3, 1], [1, 3]],\n+                [[3, 3], [3, 1], [1, 3], [3, 3]],\n+            ]\n+        if conv_layer_strides is None:\n+            conv_layer_strides = [[1, 2, 1], [2, 1, 1, 1], [2, 1, 1, 1], [2, 1, 1, 1]]\n+\n+        self.stem_kernel_size = stem_kernel_size\n+        self.stem_stride = stem_stride\n+        self.stem_num_channels = stem_num_channels\n+        self.stem_out_channels = stem_out_channels\n+        self.stem_act_func = stem_act_func\n+\n+        self.image_size = image_size\n+        self.conv_layer_kernel_sizes = conv_layer_kernel_sizes\n+        self.conv_layer_strides = conv_layer_strides\n+\n+        self.initializer_range = initializer_range\n+        self.hidden_sizes = hidden_sizes\n+        self.batch_norm_eps = batch_norm_eps\n+\n+        self.depths = [len(layer) for layer in self.conv_layer_kernel_sizes]\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, 5)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n+\n+\n+__all__ = [\"TextNetConfig\"]"
        },
        {
            "sha": "a8a004d18a35e10bb83cc6a62e68b07fd0309100",
            "filename": "src/transformers/models/textnet/convert_textnet_to_hf.py",
            "status": "added",
            "additions": 208,
            "deletions": 0,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconvert_textnet_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconvert_textnet_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fconvert_textnet_to_hf.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,208 @@\n+# coding=utf-8\n+# Copyright 2024 the Fast authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import argparse\n+import json\n+import logging\n+import re\n+from collections import OrderedDict\n+\n+import requests\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image\n+\n+from transformers import TextNetBackbone, TextNetConfig, TextNetImageProcessor\n+\n+\n+tiny_config_url = \"https://raw.githubusercontent.com/czczup/FAST/main/config/fast/nas-configs/fast_tiny.config\"\n+small_config_url = \"https://raw.githubusercontent.com/czczup/FAST/main/config/fast/nas-configs/fast_small.config\"\n+base_config_url = \"https://raw.githubusercontent.com/czczup/FAST/main/config/fast/nas-configs/fast_base.config\"\n+\n+rename_key_mappings = {\n+    \"module.backbone\": \"textnet\",\n+    \"first_conv\": \"stem\",\n+    \"bn\": \"batch_norm\",\n+    \"ver\": \"vertical\",\n+    \"hor\": \"horizontal\",\n+}\n+\n+\n+def prepare_config(size_config_url, size):\n+    config_dict = json.loads(requests.get(size_config_url).text)\n+\n+    backbone_config = {}\n+    for stage_ix in range(1, 5):\n+        stage_config = config_dict[f\"stage{stage_ix}\"]\n+\n+        merged_dict = {}\n+\n+        # Iterate through the list of dictionaries\n+        for layer in stage_config:\n+            for key, value in layer.items():\n+                if key != \"name\":\n+                    # Check if the key is already in the merged_dict\n+                    if key in merged_dict:\n+                        merged_dict[key].append(value)\n+                    else:\n+                        # If the key is not in merged_dict, create a new list with the value\n+                        merged_dict[key] = [value]\n+        backbone_config[f\"stage{stage_ix}\"] = merged_dict\n+\n+    neck_in_channels = []\n+    neck_out_channels = []\n+    neck_kernel_size = []\n+    neck_stride = []\n+    neck_dilation = []\n+    neck_groups = []\n+\n+    for i in range(1, 5):\n+        layer_key = f\"reduce_layer{i}\"\n+        layer_dict = config_dict[\"neck\"].get(layer_key)\n+\n+        if layer_dict:\n+            # Append values to the corresponding lists\n+            neck_in_channels.append(layer_dict[\"in_channels\"])\n+            neck_out_channels.append(layer_dict[\"out_channels\"])\n+            neck_kernel_size.append(layer_dict[\"kernel_size\"])\n+            neck_stride.append(layer_dict[\"stride\"])\n+            neck_dilation.append(layer_dict[\"dilation\"])\n+            neck_groups.append(layer_dict[\"groups\"])\n+\n+    textnet_config = TextNetConfig(\n+        stem_kernel_size=config_dict[\"first_conv\"][\"kernel_size\"],\n+        stem_stride=config_dict[\"first_conv\"][\"stride\"],\n+        stem_num_channels=config_dict[\"first_conv\"][\"in_channels\"],\n+        stem_out_channels=config_dict[\"first_conv\"][\"out_channels\"],\n+        stem_act_func=config_dict[\"first_conv\"][\"act_func\"],\n+        conv_layer_kernel_sizes=[\n+            backbone_config[\"stage1\"][\"kernel_size\"],\n+            backbone_config[\"stage2\"][\"kernel_size\"],\n+            backbone_config[\"stage3\"][\"kernel_size\"],\n+            backbone_config[\"stage4\"][\"kernel_size\"],\n+        ],\n+        conv_layer_strides=[\n+            backbone_config[\"stage1\"][\"stride\"],\n+            backbone_config[\"stage2\"][\"stride\"],\n+            backbone_config[\"stage3\"][\"stride\"],\n+            backbone_config[\"stage4\"][\"stride\"],\n+        ],\n+        hidden_sizes=[\n+            config_dict[\"first_conv\"][\"out_channels\"],\n+            backbone_config[\"stage1\"][\"out_channels\"][-1],\n+            backbone_config[\"stage2\"][\"out_channels\"][-1],\n+            backbone_config[\"stage3\"][\"out_channels\"][-1],\n+            backbone_config[\"stage4\"][\"out_channels\"][-1],\n+        ],\n+        out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"],\n+        out_indices=[1, 2, 3, 4],\n+    )\n+\n+    return textnet_config\n+\n+\n+def convert_textnet_checkpoint(checkpoint_url, checkpoint_config_filename, pytorch_dump_folder_path):\n+    config_filepath = hf_hub_download(repo_id=\"Raghavan/fast_model_config_files\", filename=\"fast_model_configs.json\")\n+\n+    with open(config_filepath) as f:\n+        content = json.loads(f.read())\n+\n+    size = content[checkpoint_config_filename][\"short_size\"]\n+\n+    if \"tiny\" in content[checkpoint_config_filename][\"config\"]:\n+        config = prepare_config(tiny_config_url, size)\n+        expected_slice_backbone = torch.tensor(\n+            [0.0000, 0.0000, 0.0000, 0.0000, 0.5300, 0.0000, 0.0000, 0.0000, 0.0000, 1.1221]\n+        )\n+    elif \"small\" in content[checkpoint_config_filename][\"config\"]:\n+        config = prepare_config(small_config_url, size)\n+        expected_slice_backbone = torch.tensor(\n+            [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1394]\n+        )\n+    else:\n+        config = prepare_config(base_config_url, size)\n+        expected_slice_backbone = torch.tensor(\n+            [0.9210, 0.6099, 0.0000, 0.0000, 0.0000, 0.0000, 3.2207, 2.6602, 1.8925, 0.0000]\n+        )\n+\n+    model = TextNetBackbone(config)\n+    textnet_image_processor = TextNetImageProcessor(size={\"shortest_edge\": size})\n+    state_dict = torch.hub.load_state_dict_from_url(checkpoint_url, map_location=\"cpu\", check_hash=True)[\"ema\"]\n+    state_dict_changed = OrderedDict()\n+    for key in state_dict:\n+        if \"backbone\" in key:\n+            val = state_dict[key]\n+            new_key = key\n+            for search, replacement in rename_key_mappings.items():\n+                if search in new_key:\n+                    new_key = new_key.replace(search, replacement)\n+\n+            pattern = r\"textnet\\.stage(\\d)\"\n+\n+            def adjust_stage(match):\n+                stage_number = int(match.group(1)) - 1\n+                return f\"textnet.encoder.stages.{stage_number}.stage\"\n+\n+            # Using regex to find and replace the pattern in the string\n+            new_key = re.sub(pattern, adjust_stage, new_key)\n+            state_dict_changed[new_key] = val\n+    model.load_state_dict(state_dict_changed)\n+    model.eval()\n+\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    image = Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n+\n+    original_pixel_values = torch.tensor(\n+        [0.1939, 0.3481, 0.4166, 0.3309, 0.4508, 0.4679, 0.4851, 0.4851, 0.3309, 0.4337]\n+    )\n+    pixel_values = textnet_image_processor(image, return_tensors=\"pt\").pixel_values\n+\n+    assert torch.allclose(original_pixel_values, pixel_values[0][0][3][:10], atol=1e-4)\n+\n+    with torch.no_grad():\n+        output = model(pixel_values)\n+\n+    assert torch.allclose(output[\"feature_maps\"][-1][0][10][12][:10].detach(), expected_slice_backbone, atol=1e-3)\n+\n+    model.save_pretrained(pytorch_dump_folder_path)\n+    textnet_image_processor.save_pretrained(pytorch_dump_folder_path)\n+    logging.info(\"The converted weights are saved here : \" + pytorch_dump_folder_path)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+\n+    parser.add_argument(\n+        \"--checkpoint_url\",\n+        default=\"https://github.com/czczup/FAST/releases/download/release/fast_base_ic17mlt_640.pth\",\n+        type=str,\n+        help=\"URL to the original PyTorch checkpoint (.pth file).\",\n+    )\n+    parser.add_argument(\n+        \"--checkpoint_config_filename\",\n+        default=\"fast_base_ic17mlt_640.py\",\n+        type=str,\n+        help=\"URL to the original PyTorch checkpoint (.pth file).\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\", default=None, type=str, help=\"Path to the folder to output PyTorch model.\"\n+    )\n+    args = parser.parse_args()\n+\n+    convert_textnet_checkpoint(\n+        args.checkpoint_url,\n+        args.checkpoint_config_filename,\n+        args.pytorch_dump_folder_path,\n+    )"
        },
        {
            "sha": "b3d4250b414985b5bd3746e73d6b24cdfdf68875",
            "filename": "src/transformers/models/textnet/image_processing_textnet.py",
            "status": "added",
            "additions": 355,
            "deletions": 0,
            "changes": 355,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fimage_processing_textnet.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,355 @@\n+# coding=utf-8\n+# Copyright 2024 the Fast authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for TextNet.\"\"\"\n+\n+from typing import Dict, List, Optional, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n+from ...image_transforms import (\n+    convert_to_rgb,\n+    get_resize_output_image_size,\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_kwargs,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, is_vision_available, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_vision_available():\n+    import PIL\n+\n+\n+class TextNetImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a TextNet image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions to the specified `size`. Can be overridden by\n+            `do_resize` in the `preprocess` method.\n+        size (`Dict[str, int]` *optional*, defaults to `{\"shortest_edge\": 224}`):\n+            Size of the image after resizing. The shortest edge of the image is resized to size[\"shortest_edge\"], with\n+            the longest edge resized to keep the input aspect ratio. Can be overridden by `size` in the `preprocess`\n+            method.\n+        size_divisor (`int`, *optional*, defaults to 32):\n+            Ensures height and width are rounded to a multiple of this value after resizing.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_center_crop (`bool`, *optional*, defaults to `False`):\n+            Whether to center crop the image to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n+            `preprocess` method.\n+        crop_size (`Dict[str, int]` *optional*, defaults to 224):\n+            Size of the output image after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n+            method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image. Can be overridden by `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.485, 0.456, 0.406]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.229, 0.224, 0.225]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        size: Dict[str, int] = None,\n+        size_divisor: int = 32,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_center_crop: bool = False,\n+        crop_size: Dict[str, int] = None,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = IMAGENET_DEFAULT_MEAN,\n+        image_std: Optional[Union[float, List[float]]] = IMAGENET_DEFAULT_STD,\n+        do_convert_rgb: bool = True,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        size = size if size is not None else {\"shortest_edge\": 224}\n+        size = get_size_dict(size, default_to_square=False)\n+        crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n+        crop_size = get_size_dict(crop_size, param_name=\"crop_size\")\n+\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.size_divisor = size_divisor\n+        self.resample = resample\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else IMAGENET_DEFAULT_MEAN\n+        self.image_std = image_std if image_std is not None else IMAGENET_DEFAULT_STD\n+        self.do_convert_rgb = do_convert_rgb\n+\n+        self._valid_processor_keys = [\n+            \"images\",\n+            \"do_resize\",\n+            \"size\",\n+            \"size_divisor\",\n+            \"resample\",\n+            \"do_center_crop\",\n+            \"crop_size\",\n+            \"do_rescale\",\n+            \"rescale_factor\",\n+            \"do_normalize\",\n+            \"image_mean\",\n+            \"image_std\",\n+            \"do_convert_rgb\",\n+            \"return_tensors\",\n+            \"data_format\",\n+            \"input_data_format\",\n+        ]\n+\n+    def resize(\n+        self,\n+        image: np.ndarray,\n+        size: Dict[str, int],\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        data_format: Optional[Union[str, ChannelDimension]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> np.ndarray:\n+        \"\"\"\n+        Resize an image. The shortest edge of the image is resized to size[\"shortest_edge\"] , with the longest edge\n+        resized to keep the input aspect ratio. Both the height and width are resized to be divisible by 32.\n+\n+        Args:\n+            image (`np.ndarray`):\n+                Image to resize.\n+            size (`Dict[str, int]`):\n+                Size of the output image.\n+            size_divisor (`int`, *optional*, defaults to `32`):\n+                Ensures height and width are rounded to a multiple of this value after resizing.\n+            resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BILINEAR`):\n+                Resampling filter to use when resiizing the image.\n+            data_format (`str` or `ChannelDimension`, *optional*):\n+                The channel dimension format of the image. If not provided, it will be the same as the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format of the input image. If not provided, it will be inferred.\n+            default_to_square (`bool`, *optional*, defaults to `False`):\n+                The value to be passed to `get_size_dict` as `default_to_square` when computing the image size. If the\n+                `size` argument in `get_size_dict` is an `int`, it determines whether to default to a square image or\n+                not.Note that this attribute is not used in computing `crop_size` via calling `get_size_dict`.\n+        \"\"\"\n+        if \"shortest_edge\" in size:\n+            size = size[\"shortest_edge\"]\n+        elif \"height\" in size and \"width\" in size:\n+            size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\"Size must contain either 'shortest_edge' or 'height' and 'width'.\")\n+\n+        height, width = get_resize_output_image_size(\n+            image, size=size, input_data_format=input_data_format, default_to_square=False\n+        )\n+        if height % self.size_divisor != 0:\n+            height += self.size_divisor - (height % self.size_divisor)\n+        if width % self.size_divisor != 0:\n+            width += self.size_divisor - (width % self.size_divisor)\n+\n+        return resize(\n+            image,\n+            size=(height, width),\n+            resample=resample,\n+            data_format=data_format,\n+            input_data_format=input_data_format,\n+            **kwargs,\n+        )\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        size_divisor: int = None,\n+        resample: PILImageResampling = None,\n+        do_center_crop: bool = None,\n+        crop_size: int = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            size_divisor (`int`, *optional*, defaults to `32`):\n+                Ensures height and width are rounded to a multiple of this value after resizing.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n+                Whether to center crop the image.\n+            crop_size (`Dict[str, int]`, *optional*, defaults to `self.crop_size`):\n+                Size of the center crop. Only has an effect if `do_center_crop` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, param_name=\"size\", default_to_square=False)\n+        size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n+        resample = resample if resample is not None else self.resample\n+        do_center_crop = do_center_crop if do_center_crop is not None else self.do_center_crop\n+        crop_size = crop_size if crop_size is not None else self.crop_size\n+        crop_size = get_size_dict(crop_size, param_name=\"crop_size\", default_to_square=True)\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+\n+        validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n+\n+        images = make_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_center_crop=do_center_crop,\n+            crop_size=crop_size,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if is_scaled_image(images[0]) and do_rescale:\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        all_images = []\n+        for image in images:\n+            if do_resize:\n+                image = self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+\n+            if do_center_crop:\n+                image = self.center_crop(image=image, size=crop_size, input_data_format=input_data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(\n+                    image=image, mean=image_mean, std=image_std, input_data_format=input_data_format\n+                )\n+\n+            all_images.append(image)\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+            for image in all_images\n+        ]\n+\n+        data = {\"pixel_values\": images}\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"TextNetImageProcessor\"]"
        },
        {
            "sha": "c895e66dc16f5219f9d65fcff1f6aca0426dd12d",
            "filename": "src/transformers/models/textnet/modeling_textnet.py",
            "status": "added",
            "additions": 487,
            "deletions": 0,
            "changes": 487,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftextnet%2Fmodeling_textnet.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,487 @@\n+# coding=utf-8\n+# Copyright 2024 the Fast authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch TextNet model.\"\"\"\n+\n+from typing import Any, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+from torch import Tensor\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from transformers import PreTrainedModel, add_start_docstrings\n+from transformers.activations import ACT2CLS\n+from transformers.modeling_outputs import (\n+    BackboneOutput,\n+    BaseModelOutputWithNoAttention,\n+    BaseModelOutputWithPoolingAndNoAttention,\n+    ImageClassifierOutputWithNoAttention,\n+)\n+from transformers.models.textnet.configuration_textnet import TextNetConfig\n+from transformers.utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from transformers.utils.backbone_utils import BackboneMixin\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"TextNetConfig\"\n+_CHECKPOINT_FOR_DOC = \"czczup/textnet-base\"\n+_EXPECTED_OUTPUT_SHAPE = [1, 512, 20, 27]\n+\n+\n+class TextNetConvLayer(nn.Module):\n+    def __init__(self, config: TextNetConfig):\n+        super().__init__()\n+\n+        self.kernel_size = config.stem_kernel_size\n+        self.stride = config.stem_stride\n+        self.activation_function = config.stem_act_func\n+\n+        padding = (\n+            (config.kernel_size[0] // 2, config.kernel_size[1] // 2)\n+            if isinstance(config.stem_kernel_size, tuple)\n+            else config.stem_kernel_size // 2\n+        )\n+\n+        self.conv = nn.Conv2d(\n+            config.stem_num_channels,\n+            config.stem_out_channels,\n+            kernel_size=config.stem_kernel_size,\n+            stride=config.stem_stride,\n+            padding=padding,\n+            bias=False,\n+        )\n+        self.batch_norm = nn.BatchNorm2d(config.stem_out_channels, config.batch_norm_eps)\n+\n+        self.activation = nn.Identity()\n+        if self.activation_function is not None:\n+            self.activation = ACT2CLS[self.activation_function]()\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.conv(hidden_states)\n+        hidden_states = self.batch_norm(hidden_states)\n+        return self.activation(hidden_states)\n+\n+\n+class TextNetRepConvLayer(nn.Module):\n+    r\"\"\"\n+    This layer supports re-parameterization by combining multiple convolutional branches\n+    (e.g., main convolution, vertical, horizontal, and identity branches) during training.\n+    At inference time, these branches can be collapsed into a single convolution for\n+    efficiency, as per the re-parameterization paradigm.\n+\n+    The \"Rep\" in the name stands for \"re-parameterization\" (introduced by RepVGG).\n+    \"\"\"\n+\n+    def __init__(self, config: TextNetConfig, in_channels: int, out_channels: int, kernel_size: int, stride: int):\n+        super().__init__()\n+\n+        self.num_channels = in_channels\n+        self.out_channels = out_channels\n+        self.kernel_size = kernel_size\n+        self.stride = stride\n+\n+        padding = ((kernel_size[0] - 1) // 2, (kernel_size[1] - 1) // 2)\n+\n+        self.activation_function = nn.ReLU()\n+\n+        self.main_conv = nn.Conv2d(\n+            in_channels=in_channels,\n+            out_channels=out_channels,\n+            kernel_size=kernel_size,\n+            stride=stride,\n+            padding=padding,\n+            bias=False,\n+        )\n+        self.main_batch_norm = nn.BatchNorm2d(num_features=out_channels, eps=config.batch_norm_eps)\n+\n+        vertical_padding = ((kernel_size[0] - 1) // 2, 0)\n+        horizontal_padding = (0, (kernel_size[1] - 1) // 2)\n+\n+        if kernel_size[1] != 1:\n+            self.vertical_conv = nn.Conv2d(\n+                in_channels=in_channels,\n+                out_channels=out_channels,\n+                kernel_size=(kernel_size[0], 1),\n+                stride=stride,\n+                padding=vertical_padding,\n+                bias=False,\n+            )\n+            self.vertical_batch_norm = nn.BatchNorm2d(num_features=out_channels, eps=config.batch_norm_eps)\n+        else:\n+            self.vertical_conv, self.vertical_batch_norm = None, None\n+\n+        if kernel_size[0] != 1:\n+            self.horizontal_conv = nn.Conv2d(\n+                in_channels=in_channels,\n+                out_channels=out_channels,\n+                kernel_size=(1, kernel_size[1]),\n+                stride=stride,\n+                padding=horizontal_padding,\n+                bias=False,\n+            )\n+            self.horizontal_batch_norm = nn.BatchNorm2d(num_features=out_channels, eps=config.batch_norm_eps)\n+        else:\n+            self.horizontal_conv, self.horizontal_batch_norm = None, None\n+\n+        self.rbr_identity = (\n+            nn.BatchNorm2d(num_features=in_channels, eps=config.batch_norm_eps)\n+            if out_channels == in_channels and stride == 1\n+            else None\n+        )\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        main_outputs = self.main_conv(hidden_states)\n+        main_outputs = self.main_batch_norm(main_outputs)\n+\n+        # applies a convolution with a vertical kernel\n+        if self.vertical_conv is not None:\n+            vertical_outputs = self.vertical_conv(hidden_states)\n+            vertical_outputs = self.vertical_batch_norm(vertical_outputs)\n+            main_outputs = main_outputs + vertical_outputs\n+\n+        # applies a convolution with a horizontal kernel\n+        if self.horizontal_conv is not None:\n+            horizontal_outputs = self.horizontal_conv(hidden_states)\n+            horizontal_outputs = self.horizontal_batch_norm(horizontal_outputs)\n+            main_outputs = main_outputs + horizontal_outputs\n+\n+        if self.rbr_identity is not None:\n+            id_out = self.rbr_identity(hidden_states)\n+            main_outputs = main_outputs + id_out\n+\n+        return self.activation_function(main_outputs)\n+\n+\n+class TextNetStage(nn.Module):\n+    def __init__(self, config: TextNetConfig, depth: int):\n+        super().__init__()\n+        kernel_size = config.conv_layer_kernel_sizes[depth]\n+        stride = config.conv_layer_strides[depth]\n+\n+        num_layers = len(kernel_size)\n+        stage_in_channel_size = config.hidden_sizes[depth]\n+        stage_out_channel_size = config.hidden_sizes[depth + 1]\n+\n+        in_channels = [stage_in_channel_size] + [stage_out_channel_size] * (num_layers - 1)\n+        out_channels = [stage_out_channel_size] * num_layers\n+\n+        stage = []\n+        for stage_config in zip(in_channels, out_channels, kernel_size, stride):\n+            stage.append(TextNetRepConvLayer(config, *stage_config))\n+        self.stage = nn.ModuleList(stage)\n+\n+    def forward(self, hidden_state):\n+        for block in self.stage:\n+            hidden_state = block(hidden_state)\n+        return hidden_state\n+\n+\n+class TextNetEncoder(nn.Module):\n+    def __init__(self, config: TextNetConfig):\n+        super().__init__()\n+\n+        stages = []\n+        num_stages = len(config.conv_layer_kernel_sizes)\n+        for stage_ix in range(num_stages):\n+            stages.append(TextNetStage(config, stage_ix))\n+\n+        self.stages = nn.ModuleList(stages)\n+\n+    def forward(\n+        self,\n+        hidden_state: torch.Tensor,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> BaseModelOutputWithNoAttention:\n+        hidden_states = [hidden_state]\n+        for stage in self.stages:\n+            hidden_state = stage(hidden_state)\n+            hidden_states.append(hidden_state)\n+\n+        if not return_dict:\n+            output = (hidden_state,)\n+            return output + (hidden_states,) if output_hidden_states else output\n+\n+        return BaseModelOutputWithNoAttention(last_hidden_state=hidden_state, hidden_states=hidden_states)\n+\n+\n+TEXTNET_START_DOCSTRING = r\"\"\"\n+    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n+    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n+    behavior.\n+\n+    Parameters:\n+        config ([`TextNetConfig`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+TEXTNET_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n+            [`TextNetImageProcessor.__call__`] for details.\n+\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class TextNetPreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = TextNetConfig\n+    base_model_prefix = \"textnet\"\n+    main_input_name = \"pixel_values\"\n+\n+    def _init_weights(self, module):\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.BatchNorm2d):\n+            module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+\n+@add_start_docstrings(\n+    \"The bare Textnet model outputting raw features without any specific head on top.\",\n+    TEXTNET_START_DOCSTRING,\n+)\n+class TextNetModel(TextNetPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.stem = TextNetConvLayer(config)\n+        self.encoder = TextNetEncoder(config)\n+        self.pooler = nn.AdaptiveAvgPool2d((2, 2))\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(TEXTNET_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_CHECKPOINT_FOR_DOC,\n+        output_type=BaseModelOutputWithPoolingAndNoAttention,\n+        config_class=_CONFIG_FOR_DOC,\n+        modality=\"vision\",\n+        expected_output=_EXPECTED_OUTPUT_SHAPE,\n+    )\n+    def forward(\n+        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+    ) -> Union[Tuple[Any, List[Any]], Tuple[Any], BaseModelOutputWithPoolingAndNoAttention]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        hidden_state = self.stem(pixel_values)\n+\n+        encoder_outputs = self.encoder(\n+            hidden_state, output_hidden_states=output_hidden_states, return_dict=return_dict\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        pooled_output = self.pooler(last_hidden_state)\n+\n+        if not return_dict:\n+            output = (last_hidden_state, pooled_output)\n+            return output + (encoder_outputs[1],) if output_hidden_states else output\n+\n+        return BaseModelOutputWithPoolingAndNoAttention(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs[1] if output_hidden_states else None,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    TextNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for\n+    ImageNet.\n+    \"\"\",\n+    TEXTNET_START_DOCSTRING,\n+)\n+class TextNetForImageClassification(TextNetPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.num_labels = config.num_labels\n+        self.textnet = TextNetModel(config)\n+        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))\n+        self.flatten = nn.Flatten()\n+        self.fc = nn.Linear(config.hidden_sizes[-1], config.num_labels) if config.num_labels > 0 else nn.Identity()\n+\n+        # classification head\n+        self.classifier = nn.ModuleList([self.avg_pool, self.flatten])\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(TEXTNET_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=ImageClassifierOutputWithNoAttention, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> ImageClassifierOutputWithNoAttention:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Returns:\n+\n+        Examples:\n+        ```python\n+        >>> import torch\n+        >>> import requests\n+        >>> from transformers import TextNetForImageClassification, TextNetImageProcessor\n+        >>> from PIL import Image\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = TextNetImageProcessor.from_pretrained(\"czczup/textnet-base\")\n+        >>> model = TextNetForImageClassification.from_pretrained(\"czczup/textnet-base\")\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\", size={\"height\": 640, \"width\": 640})\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+        >>> outputs.logits.shape\n+        torch.Size([1, 2])\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.textnet(pixel_values, output_hidden_states=output_hidden_states, return_dict=return_dict)\n+        last_hidden_state = outputs[0]\n+        for layer in self.classifier:\n+            last_hidden_state = layer(last_hidden_state)\n+        logits = self.fc(last_hidden_state)\n+        loss = None\n+\n+        if labels is not None:\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return ImageClassifierOutputWithNoAttention(loss=loss, logits=logits, hidden_states=outputs.hidden_states)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    TextNet backbone, to be used with frameworks like DETR and MaskFormer.\n+    \"\"\",\n+    TEXTNET_START_DOCSTRING,\n+)\n+class TextNetBackbone(TextNetPreTrainedModel, BackboneMixin):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.textnet = TextNetModel(config)\n+        self.num_features = config.hidden_sizes\n+\n+        # initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(TEXTNET_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BackboneOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self, pixel_values: Tensor, output_hidden_states: Optional[bool] = None, return_dict: Optional[bool] = None\n+    ) -> Union[Tuple[Tuple], BackboneOutput]:\n+        \"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> import torch\n+        >>> import requests\n+        >>> from PIL import Image\n+        >>> from transformers import AutoImageProcessor, AutoBackbone\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> processor = AutoImageProcessor.from_pretrained(\"czczup/textnet-base\")\n+        >>> model = AutoBackbone.from_pretrained(\"czczup/textnet-base\")\n+\n+        >>> inputs = processor(image, return_tensors=\"pt\")\n+        >>> with torch.no_grad():\n+        >>>     outputs = model(**inputs)\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs = self.textnet(pixel_values, output_hidden_states=True, return_dict=return_dict)\n+\n+        hidden_states = outputs.hidden_states if return_dict else outputs[2]\n+\n+        feature_maps = ()\n+        for idx, stage in enumerate(self.stage_names):\n+            if stage in self.out_features:\n+                feature_maps += (hidden_states[idx],)\n+\n+        if not return_dict:\n+            output = (feature_maps,)\n+            if output_hidden_states:\n+                hidden_states = outputs.hidden_states if return_dict else outputs[2]\n+                output += (hidden_states,)\n+            return output\n+\n+        return BackboneOutput(\n+            feature_maps=feature_maps,\n+            hidden_states=outputs.hidden_states if output_hidden_states else None,\n+            attentions=None,\n+        )\n+\n+\n+__all__ = [\"TextNetBackbone\", \"TextNetModel\", \"TextNetPreTrainedModel\", \"TextNetForImageClassification\"]"
        },
        {
            "sha": "20d449755f72e0929d0d149d3983d4250c602f67",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -9158,6 +9158,34 @@ def load_tf_weights_in_tapas(*args, **kwargs):\n     requires_backends(load_tf_weights_in_tapas, [\"torch\"])\n \n \n+class TextNetBackbone(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class TextNetForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class TextNetModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class TextNetPreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class TimeSeriesTransformerForPrediction(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "c51feffc599cbcd1e3bd193885ba08cc8295783a",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -618,6 +618,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class TextNetImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class TvpImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/textnet/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2F__init__.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78"
        },
        {
            "sha": "4fcd93e872fcdf1339ab4510808d77ecfe276310",
            "filename": "tests/models/textnet/test_image_processing_textnet.py",
            "status": "added",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,126 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_vision_available():\n+    from transformers import TextNetImageProcessor\n+\n+\n+class TextNetImageProcessingTester(unittest.TestCase):\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        size_divisor=32,\n+        do_center_crop=True,\n+        crop_size=None,\n+        do_normalize=True,\n+        image_mean=[0.48145466, 0.4578275, 0.40821073],\n+        image_std=[0.26862954, 0.26130258, 0.27577711],\n+        do_convert_rgb=True,\n+    ):\n+        size = size if size is not None else {\"shortest_edge\": 20}\n+        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.size_divisor = size_divisor\n+        self.do_center_crop = do_center_crop\n+        self.crop_size = crop_size\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"size\": self.size,\n+            \"size_divisor\": self.size_divisor,\n+            \"do_center_crop\": self.do_center_crop,\n+            \"crop_size\": self.crop_size,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"do_convert_rgb\": self.do_convert_rgb,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.num_channels, self.crop_size[\"height\"], self.crop_size[\"width\"]\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+class TextNetImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = TextNetImageProcessor if is_vision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = TextNetImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    def test_image_processor_properties(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+        self.assertTrue(hasattr(image_processing, \"size\"))\n+        self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+        self.assertTrue(hasattr(image_processing, \"do_center_crop\"))\n+        self.assertTrue(hasattr(image_processing, \"center_crop\"))\n+        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+        self.assertTrue(hasattr(image_processing, \"image_std\"))\n+        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 20})\n+        self.assertEqual(image_processor.crop_size, {\"height\": 18, \"width\": 18})\n+\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42, crop_size=84)\n+        self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n+        self.assertEqual(image_processor.crop_size, {\"height\": 84, \"width\": 84})"
        },
        {
            "sha": "cf5e48506e52a4141e697045e0c50eac09fabcfa",
            "filename": "tests/models/textnet/test_modeling_textnet.py",
            "status": "added",
            "additions": 348,
            "deletions": 0,
            "changes": 348,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -0,0 +1,348 @@\n+# coding=utf-8\n+# Copyright 2024 the Fast authors and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch TextNet model.\"\"\"\n+\n+import unittest\n+\n+import requests\n+from PIL import Image\n+\n+from transformers import TextNetConfig\n+from transformers.models.textnet.image_processing_textnet import TextNetImageProcessor\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import is_torch_available\n+\n+from ...test_backbone_common import BackboneTesterMixin\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import TextNetBackbone, TextNetForImageClassification, TextNetModel\n+\n+\n+class TextNetConfigTester(ConfigTester):\n+    def create_and_test_config_common_properties(self):\n+        config = self.config_class(**self.inputs_dict)\n+        self.parent.assertTrue(hasattr(config, \"hidden_sizes\"))\n+        self.parent.assertTrue(hasattr(config, \"num_attention_heads\"))\n+        self.parent.assertTrue(hasattr(config, \"num_encoder_blocks\"))\n+\n+\n+class TextNetModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        stem_kernel_size=3,\n+        stem_stride=2,\n+        stem_in_channels=3,\n+        stem_out_channels=32,\n+        stem_act_func=\"relu\",\n+        dropout_rate=0,\n+        ops_order=\"weight_bn_act\",\n+        conv_layer_kernel_sizes=[\n+            [[3, 3]],\n+            [[3, 3]],\n+            [[3, 3]],\n+            [[3, 3]],\n+        ],\n+        conv_layer_strides=[\n+            [2],\n+            [2],\n+            [2],\n+            [2],\n+        ],\n+        out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"],\n+        out_indices=[1, 2, 3, 4],\n+        batch_size=3,\n+        num_channels=3,\n+        image_size=[32, 32],\n+        is_training=True,\n+        use_labels=True,\n+        num_labels=3,\n+        hidden_sizes=[32, 32, 32, 32, 32],\n+    ):\n+        self.parent = parent\n+        self.stem_kernel_size = stem_kernel_size\n+        self.stem_stride = stem_stride\n+        self.stem_in_channels = stem_in_channels\n+        self.stem_out_channels = stem_out_channels\n+        self.act_func = stem_act_func\n+        self.dropout_rate = dropout_rate\n+        self.ops_order = ops_order\n+        self.conv_layer_kernel_sizes = conv_layer_kernel_sizes\n+        self.conv_layer_strides = conv_layer_strides\n+\n+        self.out_features = out_features\n+        self.out_indices = out_indices\n+\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.is_training = is_training\n+        self.use_labels = use_labels\n+        self.num_labels = num_labels\n+        self.hidden_sizes = hidden_sizes\n+\n+        self.num_stages = 5\n+\n+    def get_config(self):\n+        return TextNetConfig(\n+            stem_kernel_size=self.stem_kernel_size,\n+            stem_stride=self.stem_stride,\n+            stem_num_channels=self.stem_in_channels,\n+            stem_out_channels=self.stem_out_channels,\n+            act_func=self.act_func,\n+            dropout_rate=self.dropout_rate,\n+            ops_order=self.ops_order,\n+            conv_layer_kernel_sizes=self.conv_layer_kernel_sizes,\n+            conv_layer_strides=self.conv_layer_strides,\n+            out_features=self.out_features,\n+            out_indices=self.out_indices,\n+            hidden_sizes=self.hidden_sizes,\n+            image_size=self.image_size,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, labels):\n+        model = TextNetModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+        scale_h = self.image_size[0] // 32\n+        scale_w = self.image_size[1] // 32\n+        self.parent.assertEqual(\n+            result.last_hidden_state.shape,\n+            (self.batch_size, self.hidden_sizes[-1], scale_h, scale_w),\n+        )\n+\n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        config.num_labels = self.num_labels\n+        model = TextNetForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values, labels=labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size[0], self.image_size[1]])\n+\n+        labels = None\n+        if self.use_labels:\n+            labels = ids_tensor([self.batch_size], self.num_labels)\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, labels\n+\n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        model = TextNetBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n+        scale_h = self.image_size[0] // 32\n+        scale_w = self.image_size[1] // 32\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[1], 8 * scale_h, 8 * scale_w]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), len(config.out_features))\n+        self.parent.assertListEqual(model.channels, config.hidden_sizes[1:])\n+\n+        # verify backbone works with out_features=None\n+        config.out_features = None\n+        model = TextNetBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        scale_h = self.image_size[0] // 32\n+        scale_w = self.image_size[1] // 32\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, self.hidden_sizes[0], scale_h, scale_w]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), 1)\n+        self.parent.assertListEqual(model.channels, [config.hidden_sizes[-1]])\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, labels = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class TextNetModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some tests of test_modeling_common.py, as TextNet does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (TextNetModel, TextNetForImageClassification, TextNetBackbone) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": TextNetModel, \"image-classification\": TextNetForImageClassification}\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = TextNetModelTester(self)\n+        self.config_tester = TextNetConfigTester(self, config_class=TextNetConfig, has_text_modality=False)\n+\n+    @unittest.skip(reason=\"TextNet does not output attentions\")\n+    def test_attention_outputs(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TextNet does not have input/output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TextNet does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"TextNet does not support input and output embeddings\")\n+    def test_model_common_attributes(self):\n+        pass\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_backbone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(*config_and_inputs)\n+\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=config)\n+            for name, module in model.named_modules():\n+                if isinstance(module, (nn.BatchNorm2d, nn.GroupNorm)):\n+                    self.assertTrue(\n+                        torch.all(module.weight == 1),\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+                    self.assertTrue(\n+                        torch.all(module.bias == 0),\n+                        msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                    )\n+\n+    def test_hidden_states_output(self):\n+        def check_hidden_states_output(inputs_dict, config, model_class):\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            with torch.no_grad():\n+                outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n+\n+            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n+\n+            self.assertEqual(len(hidden_states), self.model_tester.num_stages)\n+\n+            self.assertListEqual(\n+                list(hidden_states[0].shape[-2:]),\n+                [self.model_tester.image_size[0] // 2, self.model_tester.image_size[1] // 2],\n+            )\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        layers_type = [\"preactivation\", \"bottleneck\"]\n+        for model_class in self.all_model_classes:\n+            for layer_type in layers_type:\n+                config.layer_type = layer_type\n+                inputs_dict[\"output_hidden_states\"] = True\n+                check_hidden_states_output(inputs_dict, config, model_class)\n+\n+                # check that output_hidden_states also work using config\n+                del inputs_dict[\"output_hidden_states\"]\n+                config.output_hidden_states = True\n+\n+                check_hidden_states_output(inputs_dict, config, model_class)\n+\n+    @unittest.skip(reason=\"TextNet does not use feedforward chunking\")\n+    def test_feed_forward_chunking(self):\n+        pass\n+\n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"czczup/textnet-base\"\n+        model = TextNetModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+\n+@require_torch\n+@require_vision\n+class TextNetModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_inference_no_head(self):\n+        processor = TextNetImageProcessor.from_pretrained(\"czczup/textnet-base\")\n+        model = TextNetModel.from_pretrained(\"czczup/textnet-base\").to(torch_device)\n+\n+        # prepare image\n+        url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        image = Image.open(requests.get(url, stream=True).raw)\n+        inputs = processor(images=image, return_tensors=\"pt\").to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            output = model(**inputs)\n+\n+        # verify logits\n+        self.assertEqual(output.logits.shape, torch.Size([1, 2]))\n+        expected_slice_backbone = torch.tensor(\n+            [0.9210, 0.6099, 0.0000, 0.0000, 0.0000, 0.0000, 3.2207, 2.6602, 1.8925, 0.0000],\n+            device=torch_device,\n+        )\n+        self.assertTrue(torch.allclose(output.feature_maps[-1][0][10][12][:10], expected_slice_backbone, atol=1e-3))\n+\n+\n+@require_torch\n+# Copied from tests.models.bit.test_modeling_bit.BitBackboneTest with Bit->TextNet\n+class TextNetBackboneTest(BackboneTesterMixin, unittest.TestCase):\n+    all_model_classes = (TextNetBackbone,) if is_torch_available() else ()\n+    config_class = TextNetConfig\n+\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = TextNetModelTester(self)"
        },
        {
            "sha": "b5792eaea634347e1ba9b66c3e8705bdd631f4f3",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7176e06b52bc27bf7fcaba6ca56e65ff17157e78/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=7176e06b52bc27bf7fcaba6ca56e65ff17157e78",
            "patch": "@@ -1020,6 +1020,7 @@ def find_all_documented_objects() -> List[str]:\n     \"ResNetBackbone\",\n     \"SwinBackbone\",\n     \"Swinv2Backbone\",\n+    \"TextNetBackbone\",\n     \"TimmBackbone\",\n     \"TimmBackboneConfig\",\n     \"VitDetBackbone\","
        }
    ],
    "stats": {
        "total": 1805,
        "additions": 1805,
        "deletions": 0
    }
}