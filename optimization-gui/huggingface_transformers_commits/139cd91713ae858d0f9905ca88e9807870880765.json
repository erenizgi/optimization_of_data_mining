{
    "author": "thisisiron",
    "message": "Fix: Apply `get_placeholder_mask` in Ovis2 (#40280)\n\n* Refactor special image mask\n\n* Refactor get_placeholder_mask method\n\n* Revert \"Refactor special image mask\"\n\nThis reverts commit 9eb1828ae930329656d6f323a510c5e6033e1f85.\n\n* Fix\n\n* Revert \"Refactor get_placeholder_mask method\"\n\nThis reverts commit 07aad6484bb08d6351d5b605e9db574d28edcd15.",
    "sha": "139cd91713ae858d0f9905ca88e9807870880765",
    "files": [
        {
            "sha": "b415bc810a8c51c21779a483013e34d3b7b10d9a",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 18,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/139cd91713ae858d0f9905ca88e9807870880765/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/139cd91713ae858d0f9905ca88e9807870880765/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=139cd91713ae858d0f9905ca88e9807870880765",
            "patch": "@@ -32,7 +32,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple\n from ..auto import AutoModel\n from .configuration_ovis2 import Ovis2Config, Ovis2VisionConfig\n \n@@ -682,23 +682,11 @@ def forward(\n         if pixel_values is not None:\n             image_features, visual_indicator_features = self.get_image_features(pixel_values=pixel_values)\n \n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = special_image_mask.sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            image_features = image_features.reshape(-1, image_features.shape[-1])\n-            n_image_features = image_features.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids,\n+                inputs_embeds=inputs_embeds,\n+                image_features=image_features,\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n             for i, visual_indicator_id in enumerate(self.visual_indicator_token_ids):"
        },
        {
            "sha": "fee26273d1ed8cbfec07d8f17325f848445434ec",
            "filename": "src/transformers/models/ovis2/modular_ovis2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 18,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/139cd91713ae858d0f9905ca88e9807870880765/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/139cd91713ae858d0f9905ca88e9807870880765/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodular_ovis2.py?ref=139cd91713ae858d0f9905ca88e9807870880765",
            "patch": "@@ -22,7 +22,7 @@\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling\n+from ...utils import auto_docstring, can_return_tuple\n from ..aimv2.modeling_aimv2 import Aimv2Attention, Aimv2EncoderLayer\n from ..auto import AutoModel\n from ..llama.modeling_llama import LlamaMLP, LlamaRMSNorm\n@@ -280,23 +280,11 @@ def forward(\n         if pixel_values is not None:\n             image_features, visual_indicator_features = self.get_image_features(pixel_values=pixel_values)\n \n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                special_image_mask = special_image_mask.all(-1)\n-            else:\n-                special_image_mask = input_ids == self.config.image_token_id\n-\n-            n_image_tokens = special_image_mask.sum()\n-            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            image_features = image_features.reshape(-1, image_features.shape[-1])\n-            n_image_features = image_features.shape[0]\n-            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n+            special_image_mask = self.get_placeholder_mask(\n+                input_ids,\n+                inputs_embeds=inputs_embeds,\n+                image_features=image_features,\n+            )\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n             for i, visual_indicator_id in enumerate(self.visual_indicator_token_ids):"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 12,
        "deletions": 36
    }
}