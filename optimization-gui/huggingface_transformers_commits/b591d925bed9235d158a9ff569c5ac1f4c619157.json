{
    "author": "Cyrilvallez",
    "message": "Fix Llama4 (#38222)\n\nUpdate modeling_llama4.py",
    "sha": "b591d925bed9235d158a9ff569c5ac1f4c619157",
    "files": [
        {
            "sha": "f8274bf1822b1bcbc7fdbdaf77d5c139eb3c98dd",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b591d925bed9235d158a9ff569c5ac1f4c619157/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b591d925bed9235d158a9ff569c5ac1f4c619157/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=b591d925bed9235d158a9ff569c5ac1f4c619157",
            "patch": "@@ -144,7 +144,7 @@ def __init__(self, config):\n \n     def forward(self, hidden_states):\n         batch, seq_len, hidden_dim = hidden_states.shape\n-        hidden_states = hidden_states.view(-1, self.hidden_dim)\n+        hidden_states = hidden_states.reshape(-1, self.hidden_dim)\n         router_logits = self.router(hidden_states)\n         tokens_per_expert = batch * seq_len\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}