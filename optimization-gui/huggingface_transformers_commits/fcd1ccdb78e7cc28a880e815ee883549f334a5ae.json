{
    "author": "vasqu",
    "message": "[`Docs`] Fix changed references (#41614)\n\n* fix\n\n* fix\n\n* other ln",
    "sha": "fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
    "files": [
        {
            "sha": "db9b60c15f17ab4ef63aa2ab6ccbac3b6358503f",
            "filename": "docs/source/en/models.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fen%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fen%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodels.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -139,10 +139,10 @@ with tempfile.TemporaryDirectory() as tmp_dir:\n     new_model = AutoModel.from_pretrained(tmp_dir)\n ```\n \n-Sharded checkpoints can also be directly loaded with [`~transformers.modeling_utils.load_sharded_checkpoint`].\n+Sharded checkpoints can also be directly loaded with [`~transformers.trainer_utils.load_sharded_checkpoint`].\n \n ```py\n-from transformers.modeling_utils import load_sharded_checkpoint\n+from transformers.trainer_utils import load_sharded_checkpoint\n \n with tempfile.TemporaryDirectory() as tmp_dir:\n     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")"
        },
        {
            "sha": "57b88672c7e11bccaa8bafb757f1bf6f2e4cde66",
            "filename": "docs/source/it/big_models.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fit%2Fbig_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fit%2Fbig_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fbig_models.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -106,10 +106,10 @@ La mappa dei pesi è la parte principale di questo indice, che mappa ogni nome d\n  ...\n ```\n \n-Se vuoi caricare direttamente un checkpoint frammentato in un modello senza usare [`~PreTrainedModel.from_pretrained`] (come si farebbe con `model.load_state_dict()` per un checkpoint completo) devi usare [`~modeling_utils.load_sharded_checkpoint`]:\n+Se vuoi caricare direttamente un checkpoint frammentato in un modello senza usare [`~PreTrainedModel.from_pretrained`] (come si farebbe con `model.load_state_dict()` per un checkpoint completo) devi usare [`~trainer_utils.load_sharded_checkpoint`]:\n \n ```py\n->>> from transformers.modeling_utils import load_sharded_checkpoint\n+>>> from transformers.trainer_utils import load_sharded_checkpoint\n \n >>> with tempfile.TemporaryDirectory() as tmp_dir:\n ...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
        },
        {
            "sha": "4a6b2574f986376d0e32472bb853034f69a994a3",
            "filename": "docs/source/ja/big_models.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fja%2Fbig_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fja%2Fbig_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fbig_models.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -110,11 +110,11 @@ dict_keys(['metadata', 'weight_map'])\n ```\n \n 直接モデル内で[`~PreTrainedModel.from_pretrained`]を使用せずに、\n-シャーディングされたチェックポイントをロードしたい場合（フルチェックポイントの場合に`model.load_state_dict()`を使用するように行う方法）、[`~modeling_utils.load_sharded_checkpoint`]を使用する必要があります：\n+シャーディングされたチェックポイントをロードしたい場合（フルチェックポイントの場合に`model.load_state_dict()`を使用するように行う方法）、[`~trainer_utils.load_sharded_checkpoint`]を使用する必要があります：\n \n \n ```py\n->>> from transformers.modeling_utils import load_sharded_checkpoint\n+>>> from transformers.trainer_utils import load_sharded_checkpoint\n \n >>> with tempfile.TemporaryDirectory() as tmp_dir:\n ...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
        },
        {
            "sha": "d8e228499a8b508b9badbc2f273af3df4f452cd9",
            "filename": "docs/source/ja/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmain_classes%2Fmodel.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -127,7 +127,3 @@ Pytorch の設計により、この機能は浮動小数点 dtype でのみ使\n ## Pushing to the Hub\n \n [[autodoc]] utils.PushToHubMixin\n-\n-## Sharded checkpoints\n-\n-[[autodoc]] modeling_utils.load_sharded_checkpoint"
        },
        {
            "sha": "c74df0b4efb733664ced0701179417c44d34e055",
            "filename": "docs/source/ko/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fko%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fko%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fmodel.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -46,7 +46,3 @@ rendered properly in your Markdown viewer.\n ## 허브에 저장하기\n \n [[autodoc]] utils.PushToHubMixin\n-\n-## 공유된 체크포인트\n-\n-[[autodoc]] modeling_utils.load_sharded_checkpoint"
        },
        {
            "sha": "b58a85948ec865de2a2d7e4288a171f18b30514a",
            "filename": "docs/source/ko/models.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fko%2Fmodels.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fko%2Fmodels.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodels.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -178,10 +178,10 @@ with tempfile.TemporaryDirectory() as tmp_dir:\n     new_model = AutoModel.from_pretrained(tmp_dir)\n ```\n \n-분할된 체크포인트는 [`~transformers.modeling_utils.load_sharded_checkpoint`]로도 직접 불러올 수 있습니다.\n+분할된 체크포인트는 [`~transformers.trainer_utils.load_sharded_checkpoint`]로도 직접 불러올 수 있습니다.\n \n ```py\n-from transformers.modeling_utils import load_sharded_checkpoint\n+from transformers.trainer_utils import load_sharded_checkpoint\n \n with tempfile.TemporaryDirectory() as tmp_dir:\n     model.save_pretrained(tmp_dir, max_shard_size=\"5GB\")"
        },
        {
            "sha": "5ab3d82f3863e8f1ec756fc87841c6855c752282",
            "filename": "docs/source/zh/big_models.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fzh%2Fbig_models.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fzh%2Fbig_models.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fbig_models.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -105,11 +105,11 @@ dict_keys(['metadata', 'weight_map'])\n  ...\n ```\n \n-如果您想直接在模型内部加载这样的分片`checkpoint`，而不使用 [`PreTrainedModel.from_pretrained`](就像您会为完整`checkpoint`执行 `model.load_state_dict()` 一样)，您应该使用 [`modeling_utils.load_sharded_checkpoint`]：\n+如果您想直接在模型内部加载这样的分片`checkpoint`，而不使用 [`PreTrainedModel.from_pretrained`](就像您会为完整`checkpoint`执行 `model.load_state_dict()` 一样)，您应该使用 [`trainer_utils.load_sharded_checkpoint`]：\n \n \n ```py\n->>> from transformers.modeling_utils import load_sharded_checkpoint\n+>>> from transformers.trainer_utils import load_sharded_checkpoint\n \n >>> with tempfile.TemporaryDirectory() as tmp_dir:\n ...     model.save_pretrained(tmp_dir, max_shard_size=\"200MB\")"
        },
        {
            "sha": "e93dfa6739bc720058613a9984964fe7fe19a1f6",
            "filename": "docs/source/zh/main_classes/model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fcd1ccdb78e7cc28a880e815ee883549f334a5ae/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmain_classes%2Fmodel.md?ref=fcd1ccdb78e7cc28a880e815ee883549f334a5ae",
            "patch": "@@ -109,6 +109,3 @@ model = AutoModel.from_config(config)\n \n ## 推送到 Hub\n [[autodoc]] utils.PushToHubMixin\n-\n-## 分片检查点\n-[[autodoc]] modeling_utils.load_sharded_checkpoint"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 10,
        "deletions": 21
    }
}