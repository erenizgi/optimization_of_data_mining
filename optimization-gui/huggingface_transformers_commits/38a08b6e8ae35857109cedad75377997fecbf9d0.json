{
    "author": "cyyever",
    "message": "More typing fixes (#41102)\n\n* Fix noqa\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* fix typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* remove noqa\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix chars\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "38a08b6e8ae35857109cedad75377997fecbf9d0",
    "files": [
        {
            "sha": "b125428a0550ab1cb31f94b981d1ec489afd588e",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -17,25 +17,25 @@\n from collections.abc import Mapping\n from dataclasses import dataclass\n from random import randint\n-from typing import Any, Callable, NewType, Optional, Union\n+from typing import Any, Callable, Optional, Union\n \n import numpy as np\n \n from ..tokenization_utils_base import PreTrainedTokenizerBase\n from ..utils import PaddingStrategy\n \n \n-InputDataClass = NewType(\"InputDataClass\", Any)\n+InputDataClass = Any\n \n \"\"\"\n A DataCollator is a function that takes a list of samples from a Dataset and collate them into a batch, as a dictionary\n of PyTorch tensors or NumPy arrays.\n \"\"\"\n-DataCollator = NewType(\"DataCollator\", Callable[[list[InputDataClass]], dict[str, Any]])\n+DataCollator = Callable[[list[InputDataClass]], dict[str, Any]]\n \n \n class DataCollatorMixin:\n-    def __call__(self, features, return_tensors=None):\n+    def __call__(self, features, return_tensors: Optional[str] = None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n         if return_tensors == \"pt\":"
        },
        {
            "sha": "98037256d311ffeb8f9bef90661c343f376f21fb",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -187,7 +187,7 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n             [`BatchFeature`]: The same instance after modification.\n         \"\"\"\n         requires_backends(self, [\"torch\"])\n-        import torch  # noqa\n+        import torch\n \n         device = kwargs.get(\"device\")\n         non_blocking = kwargs.get(\"non_blocking\", False)"
        },
        {
            "sha": "ee94a8e54e55fa5aedf2eef8ff29e8f1a02f6936",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -996,7 +996,7 @@ def _get_candidate_generator(\n         input_ids: torch.LongTensor,\n         inputs_tensor: torch.Tensor,\n         logits_processor: LogitsProcessorList,\n-        model_kwargs: dict,\n+        model_kwargs: dict[str, Any],\n         assistant_model: Optional[\"PreTrainedModel\"] = None,\n         target_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n         assistant_tokenizer: Optional[\"PreTrainedTokenizerBase\"] = None,\n@@ -1687,7 +1687,10 @@ def _prepare_generated_length(\n         return generation_config\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n+        self,\n+        generation_config: Optional[GenerationConfig],\n+        use_model_defaults: Optional[bool] = None,\n+        **kwargs: Any,\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         Prepares the base generation config, then applies any generation configuration options from kwargs. This\n@@ -2115,7 +2118,7 @@ def _tensor_or_none(token, device=None):\n         generation_config._pad_token_tensor = pad_token_tensor\n         generation_config._decoder_start_token_tensor = decoder_start_token_tensor\n \n-    def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: GenerationConfig) -> bool:\n+    def _valid_auto_compile_criteria(self, model_kwargs: dict[str, Any], generation_config: GenerationConfig) -> bool:\n         \"\"\"\n         Determines whether to trigger auto-compilation of the model's forward pass at generation time.\n         \"\"\""
        },
        {
            "sha": "d6d0e19afb2e933412f0bde0de5fb7e999e7ebff",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -76,7 +76,7 @@\n \n ImageInput = Union[\n     \"PIL.Image.Image\", np.ndarray, \"torch.Tensor\", list[\"PIL.Image.Image\"], list[np.ndarray], list[\"torch.Tensor\"]\n-]  # noqa\n+]\n \n \n class ChannelDimension(ExplicitEnum):"
        },
        {
            "sha": "039203441b78a723727007d91983c94261e7e398",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -36,7 +36,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size  # noqa: N811\n+    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size\n     from torch.nn.attention.flex_attention import BlockMask, create_block_mask, flex_attention\n \n "
        },
        {
            "sha": "db6b634582eebcbea2880f74352cf7a783181ab1",
            "filename": "src/transformers/integrations/integration_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fintegration_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -681,7 +681,7 @@ def __init__(self, tb_writer=None):\n             )\n         if has_tensorboard:\n             try:\n-                from torch.utils.tensorboard import SummaryWriter  # noqa: F401\n+                from torch.utils.tensorboard import SummaryWriter\n \n                 self._SummaryWriter = SummaryWriter\n             except ImportError:"
        },
        {
            "sha": "ab8f457dbba5088997a5a13d655c253f2965df45",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -26,7 +26,7 @@\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size  # noqa: N811\n+    from torch.nn.attention.flex_attention import _DEFAULT_SPARSE_BLOCK_SIZE as flex_default_block_size\n     from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n else:\n     # Register a fake type to avoid crashing for annotations and `isinstance` checks\n@@ -43,7 +43,7 @@\n logger = logging.get_logger(__name__)\n \n \n-def and_masks(*mask_functions: list[Callable]) -> Callable:\n+def and_masks(*mask_functions: Callable) -> Callable:\n     \"\"\"Returns a mask function that is the intersection of provided mask functions\"\"\"\n     if not all(callable(arg) for arg in mask_functions):\n         raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n@@ -57,7 +57,7 @@ def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n     return and_mask\n \n \n-def or_masks(*mask_functions: list[Callable]) -> Callable:\n+def or_masks(*mask_functions: Callable) -> Callable:\n     \"\"\"Returns a mask function that is the union of provided mask functions\"\"\"\n     if not all(callable(arg) for arg in mask_functions):\n         raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")"
        },
        {
            "sha": "2c7b47c04fd508319e9f13511e5621260108dc2b",
            "filename": "src/transformers/model_debugging_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodel_debugging_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodel_debugging_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -21,21 +21,22 @@\n from io import StringIO\n from typing import Optional\n \n+from .utils import logging\n from .utils.import_utils import is_torch_available, requires\n \n \n if is_torch_available():\n     import torch\n     from safetensors.torch import save_file\n \n+    _torch_distributed_available = False\n     # Note to code inspectors: this toolbox is intended for people who add models to `transformers`.\n     if torch.distributed.is_available():\n         import torch.distributed.tensor\n \n         _torch_distributed_available = True\n else:\n     _torch_distributed_available = False\n-from .utils import logging\n \n \n logger = logging.get_logger(__name__)\n@@ -224,7 +225,7 @@ def prune_intermediate_layers(node):\n         prune_intermediate_layers(child)\n \n \n-def log_model_debug_trace(debug_path, model):\n+def log_model_debug_trace(debug_path: Optional[str], model):\n     if debug_path:\n         try:\n             os.makedirs(debug_path, exist_ok=True)\n@@ -283,7 +284,7 @@ def _attach_debugger_logic(\n         debug_path (`str`): Optional directory to dump debug JSON files.\n         do_prune_layers (`bool`, *optional*, defaults to `True`): Whether to prune intermediate layers.\n         use_repr (bool, *optional*, defaults to `True`): Whether to save a `repr()`-ized version of the tensors as the\n-            `value` property in the asscoiated FULL_TENSORS.json file, or to store full tensors in separate SafeTensors\n+            `value` property in the associated FULL_TENSORS.json file, or to store full tensors in separate SafeTensors\n             files and store the relative path to that file in the `value` property.\n     \"\"\"\n     class_name = model.__class__.__name__"
        },
        {
            "sha": "4c2317d96db4c37b6797b5e31e24cf1c5c918589",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1805,7 +1805,7 @@ def get_input_embeddings(self) -> nn.Module:\n             )\n \n     def set_input_embeddings(self, value: nn.Module):\n-        \"\"\"Fallback setter that handles **~70 %** of models in the code‑base.\n+        \"\"\"Fallback setter that handles **~70%** of models in the code-base.\n \n         Order of attempts:\n         1. `self.model.embed_tokens`"
        },
        {
            "sha": "d0c3af490d715a685d978ad8b907d767a6c6e943",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1142,7 +1142,7 @@ def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n         # Otherwise we have to be creative.\n         # if model is an encoder decoder, the encoder tokenizer class is used by default\n         if isinstance(config, EncoderDecoderConfig):\n-            if type(config.decoder) is not type(config.encoder):  # noqa: E721\n+            if type(config.decoder) is not type(config.encoder):\n                 logger.warning(\n                     f\"The encoder model config class: {config.encoder.__class__} is different from the decoder model \"\n                     f\"config class: {config.decoder.__class__}. It is not recommended to use the \""
        },
        {
            "sha": "cf8bc141f5d1ddfb1c5eeea6ae9c4ce2590a8caa",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -15,7 +15,7 @@\n \n import os\n from dataclasses import dataclass\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n import torch\n import torch.nn as nn\n@@ -90,7 +90,7 @@ def _get_stopping_criteria(\n         return kept_criteria\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Any\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         This method overrides [~generation.utils.GenerationMixin._prepare_generation_config]."
        },
        {
            "sha": "88657bab435d95cf5f06014524b184939e8582c7",
            "filename": "src/transformers/models/deprecated/graphormer/collating_graphormer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgraphormer%2Fcollating_graphormer.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -14,7 +14,7 @@\n     import pyximport\n \n     pyximport.install(setup_args={\"include_dirs\": np.get_include()})\n-    from . import algos_graphormer  # noqa E402\n+    from . import algos_graphormer\n \n \n def convert_to_single_emb(x, offset: int = 512):"
        },
        {
            "sha": "c297de7203d4e5b30a189047233976d310179907",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -109,7 +109,7 @@ def _get_logits_processor(\n         return merged_processors\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: dict\n+        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Any\n     ) -> tuple[GenerationConfig, dict]:\n         generation_config, model_kwargs = super()._prepare_generation_config(\n             generation_config, use_model_defaults, **kwargs"
        },
        {
            "sha": "253b99edff0d7a557da404fa680ce8403e22ccf1",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -439,7 +439,7 @@ class DiffLlamaForTokenClassification(LlamaForTokenClassification):\n \n __all__ = [\n     \"DiffLlamaPreTrainedModel\",\n-    \"DiffLlamaModel\",  # noqa: F822\n+    \"DiffLlamaModel\",\n     \"DiffLlamaForCausalLM\",\n     \"DiffLlamaForSequenceClassification\",\n     \"DiffLlamaForQuestionAnswering\","
        },
        {
            "sha": "76c3b6130653fe8a40329f84ffd2b03dfbdfe94d",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -135,7 +135,7 @@ def to(self, *args, **kwargs) -> \"BatchFeature\":\n             [`BatchFeature`]: The same instance after modification.\n         \"\"\"\n         requires_backends(self, [\"torch\"])\n-        import torch  # noqa\n+        import torch\n \n         new_data = {}\n         device = kwargs.get(\"device\")"
        },
        {
            "sha": "e17545822da8799fafc8a6ed2b0da382e4c0ec8a",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -517,5 +517,5 @@ class GemmaForTokenClassification(LlamaForTokenClassification):\n     \"GemmaForCausalLM\",\n     \"GemmaForSequenceClassification\",\n     \"GemmaForTokenClassification\",\n-    \"GemmaPreTrainedModel\",  # noqa: F822\n+    \"GemmaPreTrainedModel\",\n ]"
        },
        {
            "sha": "0fc87b992a75887830902afea3f355aad65002a2",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -586,7 +586,7 @@ class Gemma2ForTokenClassification(GemmaForTokenClassification):\n     \"Gemma2Config\",\n     \"Gemma2ForCausalLM\",\n     \"Gemma2Model\",\n-    \"Gemma2PreTrainedModel\",  # noqa: F822\n+    \"Gemma2PreTrainedModel\",\n     \"Gemma2ForSequenceClassification\",\n     \"Gemma2ForTokenClassification\",\n ]"
        },
        {
            "sha": "13858faf06ef1e0058f12a6ec61033ce39a223bf",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1162,7 +1162,7 @@ class Gemma3TextForSequenceClassification(GenericForSequenceClassification, Gemm\n __all__ = [\n     \"Gemma3Config\",\n     \"Gemma3TextConfig\",\n-    \"Gemma3PreTrainedModel\",  # noqa: F822\n+    \"Gemma3PreTrainedModel\",\n     \"Gemma3TextModel\",\n     \"Gemma3ForCausalLM\",\n     \"Gemma3ForConditionalGeneration\","
        },
        {
            "sha": "5b778672bd33caada255d6d61355aaebd3345243",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -2674,7 +2674,7 @@ def create_masks_for_generate(self, **super_kwargs):\n     \"Gemma3nForCausalLM\",\n     \"Gemma3nForConditionalGeneration\",\n     \"Gemma3nModel\",\n-    \"Gemma3nPreTrainedModel\",  # noqa: F822\n+    \"Gemma3nPreTrainedModel\",\n     \"Gemma3nTextConfig\",\n     \"Gemma3nTextModel\",\n     \"Gemma3nVisionConfig\","
        },
        {
            "sha": "39496fe043ed144e871a2ad8c364ffcf743a699a",
            "filename": "src/transformers/models/got_ocr2/convert_got_ocr2_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fconvert_got_ocr2_weights_to_hf.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -182,7 +182,7 @@ def __init__(\n \n def write_tokenizer(tokenizer_path: str, save_dir: str, push_to_hub: bool = False):\n     model_max_length = CONTEXT_LENGTH\n-    pattern = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: W605\n+    pattern = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n     # Special tokens\n     special_tokens = (\n         [\"<|endoftext|>\", \"<|im_start|>\", \"<|im_end|>\"]"
        },
        {
            "sha": "2363a77b103154540d94597fe5c416fec5cd1deb",
            "filename": "src/transformers/models/llama4/convert_llama4_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconvert_llama4_weights_to_hf.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -662,7 +662,7 @@ def update_post_processor(self, tokenizer):\n         )\n \n \n-O200K_PATTERN = r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"  # noqa: E501\n+O200K_PATTERN = r\"\"\"[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]*[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]+(?i:'s|'t|'re|'ve|'m|'ll|'d)?|[^\\r\\n\\p{L}\\p{N}]?[\\p{Lu}\\p{Lt}\\p{Lm}\\p{Lo}\\p{M}]+[\\p{Ll}\\p{Lm}\\p{Lo}\\p{M}]*(?i:'s|'t|'re|'ve|'m|'ll|'d)?|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n/]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\"\"\n \n \n def write_tokenizer(args):"
        },
        {
            "sha": "4681cfb60e53ff6cf89a0588a2e220740bda71ac",
            "filename": "src/transformers/models/longcat_flash/modeling_longcat_flash.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodeling_longcat_flash.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -534,7 +534,7 @@ def __init__(self, config):\n         self.rotary_emb = LongcatFlashRotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n         # Each layer above has 2 sublayers, config hack to have a correct cache (to avoid a checkpoint change)\n-        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used) # noqa\n+        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used)\n \n         self.config.num_hidden_layers = 2 * config.num_layers\n "
        },
        {
            "sha": "60c93239d2c4d7eb3a4f5bc787b73d4155eb3e10",
            "filename": "src/transformers/models/longcat_flash/modular_longcat_flash.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongcat_flash%2Fmodular_longcat_flash.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -300,7 +300,7 @@ def __init__(self, config):\n             [LongcatFlashDecoderLayer(config, layer_idx) for layer_idx in range(config.num_layers)]\n         )\n         # Each layer above has 2 sublayers, config hack to have a correct cache (to avoid a checkpoint change)\n-        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used) # noqa\n+        self.head_dim = config.head_dim  # For CI happiness (we didn't convert so head_dim is not directly used)\n \n         self.config.num_hidden_layers = 2 * config.num_layers\n         self.norm = LongcatFlashRMSNorm(config.hidden_size, eps=config.rms_norm_eps)"
        },
        {
            "sha": "0a25c70d1f932b42353f4d06fdced9cd4ed2dbd4",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -250,7 +250,7 @@ def forward(self, hidden_states):\n try:\n     from apex.normalization import FusedRMSNorm\n \n-    LongT5LayerNorm = FusedRMSNorm  # noqa\n+    LongT5LayerNorm = FusedRMSNorm\n \n     logger.info(\"Discovered apex.normalization.FusedRMSNorm - will use it instead of LongT5LayerNorm\")\n except ImportError:"
        },
        {
            "sha": "6bc499d21453ef10540378c5e5e46bdced64f627",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -332,6 +332,6 @@ def forward(\n \n __all__ = [\n     \"Mistral3Model\",\n-    \"Mistral3PreTrainedModel\",  # noqa\n+    \"Mistral3PreTrainedModel\",\n     \"Mistral3ForConditionalGeneration\",\n ]"
        },
        {
            "sha": "c773d0514f81e1a95b94350472016f818f999463",
            "filename": "src/transformers/models/mllama/convert_mllama_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fconvert_mllama_weights_to_hf.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -496,7 +496,7 @@ def __init__(\n \n def write_tokenizer(tokenizer_path: str, save_dir: str, instruct: bool = False):\n     model_max_length = CONTEXT_LENGTH\n-    pattern = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"  # noqa: W605\n+    pattern = r\"(?i:'s|'t|'re|'ve|'m|'ll|'d)|[^\\r\\n\\p{L}\\p{N}]?\\p{L}+|\\p{N}{1,3}| ?[^\\s\\p{L}\\p{N}]+[\\r\\n]*|\\s*[\\r\\n]+|\\s+(?!\\S)|\\s+\"\n \n     # Special tokens\n     num_reserved_special_tokens = 256"
        },
        {
            "sha": "2815dcfa7b7ac156b533f48fa35b8c080bd22b42",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -65,13 +65,13 @@ def __call__(\n         data_format: Optional[\"ChannelDimension\"] = \"channels_first\",  # noqa: F821\n         input_data_format: Optional[Union[str, \"ChannelDimension\"]] = None,  # noqa: F821\n         text_pair: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n-        text_target: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n+        text_target: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         text_pair_target: Optional[\n             Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]\n         ] = None,\n         add_special_tokens: bool = True,\n         padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n+        truncation: Optional[Union[bool, str, TruncationStrategy]] = None,\n         max_length: Optional[int] = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,"
        },
        {
            "sha": "84aa2509007dfc196945e02d9e60fde62d157f6c",
            "filename": "src/transformers/models/olmo2/modular_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodular_olmo2.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -317,5 +317,5 @@ class Olmo2ForCausalLM(OlmoForCausalLM):\n     \"Olmo2Config\",\n     \"Olmo2ForCausalLM\",\n     \"Olmo2Model\",\n-    \"Olmo2PreTrainedModel\",  # noqa: F822\n+    \"Olmo2PreTrainedModel\",\n ]"
        },
        {
            "sha": "963b18ea0afc5cd913e428e0af9d997c2d8d56cb",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -423,5 +423,5 @@ class Olmo3ForCausalLM(Olmo2ForCausalLM):\n     \"Olmo3Config\",\n     \"Olmo3ForCausalLM\",\n     \"Olmo3Model\",\n-    \"Olmo3PreTrainedModel\",  # noqa: F822\n+    \"Olmo3PreTrainedModel\",\n ]"
        },
        {
            "sha": "bddd7cbf9a8bfdfac978ebf69cef8887a49a05ce",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -86,7 +86,7 @@ def forward(self, hidden_states):\n try:\n     from apex.normalization import FusedRMSNorm\n \n-    Pix2StructLayerNorm = FusedRMSNorm  # noqa\n+    Pix2StructLayerNorm = FusedRMSNorm\n \n     logger.info(\"Discovered apex.normalization.FusedRMSNorm - will use it instead of Pix2StructLayerNorm\")\n except ImportError:"
        },
        {
            "sha": "6bc90961154bbbefe560706052b6ab492a7adaae",
            "filename": "src/transformers/models/pop2piano/configuration_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fconfiguration_pop2piano.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -87,7 +87,7 @@ def __init__(\n         dropout_rate=0.1,\n         layer_norm_epsilon=1e-6,\n         initializer_factor=1.0,\n-        feed_forward_proj=\"gated-gelu\",  # noqa\n+        feed_forward_proj=\"gated-gelu\",\n         is_encoder_decoder=True,\n         use_cache=True,\n         pad_token_id=0,"
        },
        {
            "sha": "15fea2105b308bbf04c2390e6efd914c1a33ef8c",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -88,7 +88,7 @@ def forward(self, hidden_states):\n \n \n if not _load_pop2piano_layer_norm:\n-    Pop2PianoLayerNorm = FusedRMSNorm  # noqa\n+    Pop2PianoLayerNorm = FusedRMSNorm\n \n \n # Copied from transformers.models.t5.modeling_t5.T5DenseActDense with T5->Pop2Piano,t5->pop2piano"
        },
        {
            "sha": "b2ec54f0d8673467d630551e2e1c9cb586a30229",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -2065,10 +2065,10 @@ def forward(\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n         elif input_ids is not None:\n             self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()  # noqa: F841\n+            input_shape = input_ids.size()\n             device = input_ids.device\n         elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]  # noqa: F841\n+            input_shape = inputs_embeds.size()[:-1]\n             device = inputs_embeds.device\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")"
        },
        {
            "sha": "6c1edc74508cf398f2732736b41d646de671556d",
            "filename": "src/transformers/models/rwkv/modeling_rwkv.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frwkv%2Fmodeling_rwkv.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -532,7 +532,7 @@ def set_input_embeddings(self, new_embeddings):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,  # noqa\n+        attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         state: Optional[list[torch.FloatTensor]] = None,\n         use_cache: Optional[bool] = None,\n@@ -730,7 +730,7 @@ def prepare_inputs_for_generation(self, input_ids, state=None, inputs_embeds=Non\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        attention_mask: Optional[torch.LongTensor] = None,  # noqa\n+        attention_mask: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         state: Optional[list[torch.FloatTensor]] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "79528cf5c329b6ab863fc32cd2d2cd8121cf7a09",
            "filename": "src/transformers/models/switch_transformers/convert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fconvert_switch_transformers_original_flax_checkpoint_to_pytorch.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -36,7 +36,7 @@ def load_flax_weights_in_pytorch_model(pt_model, flax_state):\n     \"\"\"Load flax checkpoints in a PyTorch model\"\"\"\n \n     try:\n-        import torch  # noqa: F401\n+        import torch\n     except (ImportError, ModuleNotFoundError):\n         logger.error(\n             \"Loading a Flax weights in PyTorch, requires both PyTorch and Flax to be installed. Please see\""
        },
        {
            "sha": "02e4eb25c4aecbb576eb040a2b85d5f51ec213c2",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -144,7 +144,7 @@ def forward(self, hidden_states):\n try:\n     from apex.normalization import FusedRMSNorm\n \n-    T5LayerNorm = FusedRMSNorm  # noqa\n+    T5LayerNorm = FusedRMSNorm\n \n     logger.info(\"Discovered apex.normalization.FusedRMSNorm - will use it instead of T5LayerNorm\")\n except ImportError:"
        },
        {
            "sha": "d41c8796e5ee7816dd1469e27bdd5d9d1691ed90",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1378,7 +1378,7 @@ def forward(\n     \"T5GemmaForConditionalGeneration\",\n     \"T5GemmaModel\",\n     \"T5GemmaEncoderModel\",\n-    \"T5GemmaPreTrainedModel\",  # noqa: F822\n+    \"T5GemmaPreTrainedModel\",\n     \"T5GemmaForSequenceClassification\",\n     \"T5GemmaForTokenClassification\",\n ]"
        },
        {
            "sha": "46ebf7172b7377246e66516bc539540eb1fe060a",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -957,7 +957,7 @@ def save_pretrained(\n         self,\n         save_directory: Union[str, os.PathLike],\n         safe_serialization: bool = True,\n-        **kwargs,\n+        **kwargs: Any,\n     ):\n         \"\"\"\n         Save the pipeline's model and tokenizer."
        },
        {
            "sha": "5f3f455662e32dbf73307f7889aa136fa7abc4ef",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -1266,7 +1266,7 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n         used_keys = set()\n \n         # get defaults from set model processor kwargs if they exist\n-        for modality in default_kwargs:  # noqa: PLC0206\n+        for modality in default_kwargs:\n             default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n             # update defaults with arguments from tokenizer init\n             for modality_key in ModelProcessorKwargs.__annotations__[modality].__annotations__:"
        },
        {
            "sha": "30122f76f6492704ddebe1a18ff6de6085e3313a",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -922,7 +922,7 @@ def _align_special_tokens(self):\n         uses the new tokens as well.\n         \"\"\"\n         if isinstance(self.processing_class, ProcessorMixin):\n-            tokenizer = self.processing_class.tokenizer\n+            tokenizer: PreTrainedTokenizerBase = self.processing_class.tokenizer\n         else:\n             tokenizer = self.processing_class\n         model_has_generation_config = (\n@@ -2215,7 +2215,7 @@ def train(\n         resume_from_checkpoint: Optional[Union[str, bool]] = None,\n         trial: Union[\"optuna.Trial\", dict[str, Any], None] = None,\n         ignore_keys_for_eval: Optional[list[str]] = None,\n-        **kwargs,\n+        **kwargs: Any,\n     ):\n         \"\"\"\n         Main training entry point.\n@@ -2411,7 +2411,7 @@ def _inner_training_loop(\n                     \" (torchrun or torch.distributed.launch (deprecated)).\"\n                 )\n             else:\n-                debug_overflow = DebugUnderflowOverflow(self.model)  # noqa\n+                DebugUnderflowOverflow(self.model)\n \n         delay_optimizer_creation = is_sagemaker_mp_enabled() or self.is_fsdp_xla_enabled or self.is_fsdp_enabled\n "
        },
        {
            "sha": "951e7b52017ddabe93b24dc6514f666cbab8c58c",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -294,7 +294,7 @@ def default_hp_space_optuna(trial) -> dict[str, float]:\n     }\n \n \n-def default_hp_space_ray(trial) -> dict[str, float]:\n+def default_hp_space_ray(trial) -> dict[str, Any]:\n     from .integrations import is_ray_tune_available\n \n     assert is_ray_tune_available(), \"This function needs ray installed: `pip install ray[tune]`\"\n@@ -321,7 +321,7 @@ def default_hp_space_sigopt(trial):\n     ]\n \n \n-def default_hp_space_wandb(trial) -> dict[str, float]:\n+def default_hp_space_wandb(trial) -> dict[str, Any]:\n     from .integrations import is_wandb_available\n \n     if not is_wandb_available():\n@@ -474,7 +474,7 @@ def __init__(self, skip_memory_metrics=False):\n         if self.skip_memory_metrics:\n             return\n \n-        import psutil  # noqa\n+        import psutil\n \n         if is_torch_cuda_available() or is_torch_mlu_available() or is_torch_musa_available():\n             import torch"
        },
        {
            "sha": "ac67b545a5ce5aa2386420b49aaf0f7c7c23d464",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -514,11 +514,9 @@ class TrainingArguments:\n                     A list of options along the following:\n \n                     - `\"backward_pre\"` : Prefetches the next set of parameters before the current set of parameter's\n-                      gradient\n-                        computation.\n+                      gradient computation.\n                     - `\"backward_post\"` : This prefetches the next set of parameters after the current set of\n-                      parameter’s\n-                        gradient computation.\n+                      parameter's gradient computation.\n                 - forward_prefetch (`bool`, *optional*, defaults to `False`)\n                     FSDP's forward prefetch mode (useful only when `fsdp` field is passed).\n                      If `\"True\"`, then FSDP explicitly prefetches the next upcoming all-gather while executing in the\n@@ -2359,7 +2357,7 @@ def to_json_string(self):\n \n     def to_sanitized_dict(self) -> dict[str, Any]:\n         \"\"\"\n-        Sanitized serialization to use with TensorBoard’s hparams\n+        Sanitized serialization to use with TensorBoard's hparams\n         \"\"\"\n         d = self.to_dict()\n         d = {**d, **{\"train_batch_size\": self.train_batch_size, \"eval_batch_size\": self.eval_batch_size}}"
        },
        {
            "sha": "dfd5f27d2456bae2304f58b65986658d7c1b523d",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -46,7 +46,7 @@\n \n if is_torch_available():\n     # required for @can_return_tuple decorator to work with torchdynamo\n-    import torch  # noqa: F401\n+    import torch\n \n     from ..model_debugging_utils import model_addition_debugger_context\n "
        },
        {
            "sha": "430153690c7929770260fdddbf8567e59d6d48d2",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -911,7 +911,7 @@ def is_habana_gaudi1() -> bool:\n     if not is_torch_hpu_available():\n         return False\n \n-    import habana_frameworks.torch.utils.experimental as htexp  # noqa: F401\n+    import habana_frameworks.torch.utils.experimental as htexp\n \n     # Check if the device is Gaudi1 (vs Gaudi2, Gaudi3)\n     return htexp._get_device_type() == htexp.synDeviceType.synDeviceGaudi\n@@ -937,7 +937,7 @@ def is_torchdynamo_compiling() -> Union[tuple[bool, str], bool]:\n         return torch.compiler.is_compiling()\n     except Exception:\n         try:\n-            import torch._dynamo as dynamo  # noqa: F401\n+            import torch._dynamo as dynamo\n \n             return dynamo.is_compiling()\n         except Exception:\n@@ -954,7 +954,7 @@ def is_torchdynamo_exporting() -> bool:\n         return torch.compiler.is_exporting()\n     except Exception:\n         try:\n-            import torch._dynamo as dynamo  # noqa: F401\n+            import torch._dynamo as dynamo\n \n             return dynamo.is_exporting()\n         except Exception:"
        },
        {
            "sha": "e383653871bf9ef4acbc8579ae99ed206988ecf4",
            "filename": "src/transformers/utils/logging.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Flogging.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Futils%2Flogging.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Flogging.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -20,13 +20,13 @@\n import threading\n from logging import (\n     CRITICAL,  # NOQA\n-    DEBUG,  # NOQA\n-    ERROR,  # NOQA\n+    DEBUG,\n+    ERROR,\n     FATAL,  # NOQA\n-    INFO,  # NOQA\n+    INFO,\n     NOTSET,  # NOQA\n     WARN,  # NOQA\n-    WARNING,  # NOQA\n+    WARNING,\n )\n from logging import captureWarnings as _captureWarnings\n from typing import Optional"
        },
        {
            "sha": "452762418cd102db40563f88ce34421c428101ec",
            "filename": "src/transformers/video_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fvideo_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38a08b6e8ae35857109cedad75377997fecbf9d0/src%2Ftransformers%2Fvideo_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_utils.py?ref=38a08b6e8ae35857109cedad75377997fecbf9d0",
            "patch": "@@ -74,7 +74,7 @@\n     Path,\n     list[Path],\n     list[list[Path]],\n-]  # noqa\n+]\n \n \n @dataclass"
        }
    ],
    "stats": {
        "total": 146,
        "additions": 74,
        "deletions": 72
    }
}