{
    "author": "McPatate",
    "message": "refactor(serve): move `request_id` to headers (#40722)\n\n* refactor(serve): move `request_id` to headers\n\n* fix(serve): typo in middleware fn name\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "bb45d3631ec7026db04a77d33a52b31766372160",
    "files": [
        {
            "sha": "622f50378dfd59222fba9ad56a751fe1ad34638c",
            "filename": "src/transformers/commands/serving.py",
            "status": "modified",
            "additions": 24,
            "deletions": 15,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb45d3631ec7026db04a77d33a52b31766372160/src%2Ftransformers%2Fcommands%2Fserving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb45d3631ec7026db04a77d33a52b31766372160/src%2Ftransformers%2Fcommands%2Fserving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fserving.py?ref=bb45d3631ec7026db04a77d33a52b31766372160",
            "patch": "@@ -24,6 +24,7 @@\n import tempfile\n import threading\n import time\n+import uuid\n from argparse import ArgumentParser, Namespace\n from collections.abc import AsyncGenerator, Generator, Iterable\n from contextlib import asynccontextmanager\n@@ -132,7 +133,6 @@ class TransformersCompletionCreateParamsStreaming(CompletionCreateParamsStreamin\n         \"\"\"\n \n         generation_config: str\n-        request_id: str\n \n     class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total=False):\n         \"\"\"\n@@ -211,6 +211,8 @@ class TransformersTranscriptionCreateParams(TranscriptionCreateParamsBase, total\n }\n _MODELS_WITH_TOOL_SUPPORT = list(_TOOL_CALL_TOKENS.keys())\n \n+X_REQUEST_ID = \"x-request-id\"\n+\n \n class Modality(enum.Enum):\n     LLM = \"LLM\"\n@@ -688,14 +690,16 @@ async def lifespan(app: FastAPI):\n                 \"CORS allow origin is set to `*`. This is not recommended for production environments.\"\n             )\n \n+        from fastapi import Request\n+\n         @app.post(\"/v1/chat/completions\")\n-        def chat_completion(request: dict):\n-            self.validate_chat_completion_request(request=request)\n+        def chat_completion(request: Request, body: dict):\n+            self.validate_chat_completion_request(request=body)\n \n             if self.use_continuous_batching:\n-                output = self.continuous_batching_chat_completion(request)\n+                output = self.continuous_batching_chat_completion(body, request.state.request_id)\n             else:\n-                output = self.generate_chat_completion(request)\n+                output = self.generate_chat_completion(body)\n             return StreamingResponse(output, media_type=\"text/event-stream\")\n \n         @app.post(\"/v1/responses\")\n@@ -705,8 +709,6 @@ def responses(request: dict):\n             output = self.generate_response(request)\n             return StreamingResponse(output, media_type=\"text/event-stream\")\n \n-        from fastapi import Request\n-\n         @app.post(\"/v1/audio/transcriptions\")\n         async def audio_transcriptions(request: Request):\n             # Parses the multipart/form-data request into the request format used by other endpoints\n@@ -734,6 +736,14 @@ def get_all_models():\n         def healthcheck():\n             return JSONResponse({\"status\": \"ok\"})\n \n+        @app.middleware(\"http\")\n+        async def get_or_set_request_id(request: Request, call_next):\n+            request_id = request.headers.get(X_REQUEST_ID) or str(uuid.uuid4())\n+            request.state.request_id = request_id\n+            response = await call_next(request)\n+            response.headers[X_REQUEST_ID] = request_id\n+            return response\n+\n         uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n \n     @functools.cache\n@@ -782,7 +792,7 @@ def get_gen_models(self) -> list[dict[str, any]]:\n                 for model in model_infos\n             ]\n \n-    def continuous_batching_chat_completion(self, req: dict) -> AsyncGenerator[str, None]:\n+    def continuous_batching_chat_completion(self, req: dict, request_id: str) -> AsyncGenerator[str, None]:\n         \"\"\"\n         Generates an OpenAI Chat Completion using continuous batching.\n \n@@ -858,22 +868,21 @@ def stream_chat_completion(request_id, decode_stream):\n                 self.running_continuous_batching_manager.cancel_request(request_id)\n                 yield f'data: {{\"error\": \"{str(e)}\"}}'\n \n-        async def cancellation_wrapper(_inputs):\n-            request_id = None\n+        async def cancellation_wrapper(_inputs, request_id):\n             try:\n                 decode_stream = DecodeStream(_inputs.tolist(), False)\n+                # XXX: using returned request_id as safety in case it is None\n                 request_id = self.running_continuous_batching_manager.add_request(\n-                    _inputs, request_id=req.get(\"request_id\"), max_new_tokens=generation_config.max_new_tokens\n+                    _inputs, request_id=request_id, max_new_tokens=generation_config.max_new_tokens\n                 )\n                 for chunk in stream_chat_completion(request_id, decode_stream):\n                     yield chunk\n                     await asyncio.sleep(0)  # Yield control to the event loop to check for cancellations\n             except asyncio.CancelledError:\n-                if request_id is not None:\n-                    self.running_continuous_batching_manager.cancel_request(request_id)\n-                    logger.warning(f\"Request {request_id} was cancelled.\")\n+                self.running_continuous_batching_manager.cancel_request(request_id)\n+                logger.warning(f\"Request {request_id} was cancelled.\")\n \n-        return cancellation_wrapper(inputs[0])\n+        return cancellation_wrapper(inputs[0], request_id)\n \n     @staticmethod\n     def get_model_modality(model: \"PreTrainedModel\") -> Modality:"
        },
        {
            "sha": "1838e37eddcfba9c2f07c345489ef3c8a2c8fa90",
            "filename": "tests/commands/test_serving.py",
            "status": "modified",
            "additions": 26,
            "deletions": 14,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb45d3631ec7026db04a77d33a52b31766372160/tests%2Fcommands%2Ftest_serving.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb45d3631ec7026db04a77d33a52b31766372160/tests%2Fcommands%2Ftest_serving.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_serving.py?ref=bb45d3631ec7026db04a77d33a52b31766372160",
            "patch": "@@ -498,30 +498,45 @@ def _get_scheduler(serve_command):\n     cbm = getattr(serve_command, \"running_continuous_batching_manager\", None)\n     assert cbm is not None, \"ServeCommand has no running_continuous_batching_manager\"\n     bp = getattr(cbm, \"batch_processor\", None)\n-    assert bp is not None, \"CBM has no batch_processor\"\n+    assert bp is not None, \"running_continuous_batching_manager has no batch_processor\"\n     sched = getattr(bp, \"scheduler\", None)\n     assert sched is not None, \"batch_processor has no scheduler\"\n     return sched\n \n \n+def _call_healthcheck(base_url: str):\n+    response = None\n+    retries = 10\n+    while retries > 0:\n+        try:\n+            response = requests.get(f\"{base_url}/health\")\n+            break\n+        except requests.exceptions.ConnectionError:\n+            time.sleep(0.1)\n+            retries -= 1\n+    return response\n+\n+\n def _open_stream_and_cancel(base_url: str, request_id: str):\n     with requests.Session() as s:\n         with s.post(\n             f\"{base_url}/v1/chat/completions\",\n+            headers={\"X-Request-ID\": request_id},\n             json={\n                 \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n                 \"stream\": True,\n                 \"messages\": [{\"role\": \"user\", \"content\": \"Count slowly so I can cancel you.\"}],\n-                \"request_id\": request_id,\n             },\n             stream=True,\n             timeout=30,\n         ) as resp:\n             assert resp.status_code == 200\n \n-            for _ in resp.iter_content(chunk_size=None):\n-                resp.close()\n-                break\n+            wait_for_n_chunks = 3\n+            for i, _ in enumerate(resp.iter_content(chunk_size=None)):\n+                if i >= wait_for_n_chunks:\n+                    resp.close()\n+                    break\n \n \n @slow  # server startup time is slow on our push CI\n@@ -598,6 +613,11 @@ def test_request_cancellation(self):\n         base_url = f\"http://127.0.0.1:{self.port}\"\n         request_id = \"test-cancel\"\n \n+        # Ensure the server is up before sending a request\n+        response = _call_healthcheck(base_url)\n+        self.assertIsNotNone(response, \"Failed to connect to the server health endpoint.\")\n+        self.assertEqual(response.status_code, 200)\n+\n         _open_stream_and_cancel(base_url, request_id)\n \n         scheduler = _get_scheduler(self.serve_command)\n@@ -724,15 +744,7 @@ def setUpClass(cls):\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\"\n-        response = None\n-        retries = 10\n-        while retries > 0:\n-            try:\n-                response = requests.get(f\"http://localhost:{self.port}/health\")\n-                break\n-            except requests.exceptions.ConnectionError:\n-                time.sleep(0.1)\n-                retries -= 1\n+        response = _call_healthcheck(f\"http://localhost:{self.port}\")\n         self.assertIsNotNone(response, \"Failed to connect to the server health endpoint.\")\n         self.assertEqual(response.status_code, 200)\n         self.assertEqual(response.json(), {\"status\": \"ok\"})"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 50,
        "deletions": 29
    }
}