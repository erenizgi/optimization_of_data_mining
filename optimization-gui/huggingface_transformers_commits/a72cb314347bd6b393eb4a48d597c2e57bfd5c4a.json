{
    "author": "yao-matrix",
    "message": "enable utils test cases on XPU (#38005)\n\n* enable utils test cases on XPU\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n* Update tests/utils/test_skip_decorators.py\n\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>\n\n* fix comment\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Yao Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>",
    "sha": "a72cb314347bd6b393eb4a48d597c2e57bfd5c4a",
    "files": [
        {
            "sha": "243ae657c1060b8d207d95ccce92d58c29425af1",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=a72cb314347bd6b393eb4a48d597c2e57bfd5c4a",
            "patch": "@@ -28,6 +28,7 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n+    require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     slow,\n     torch_device,\n@@ -355,7 +356,7 @@ def test_dynamic_cache_hard(self):\n         self.assertIsInstance(gen_out.past_key_values, DynamicCache)  # sanity check\n \n     @parameterized.expand([(\"eager\"), (\"sdpa\")])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_static_cache_greedy_decoding_pad_left(self, attn_implementation):\n         \"\"\"Tests that different cache implementations work well with eager and SDPA inference\"\"\"\n@@ -436,30 +437,30 @@ def test_offloaded_cache_uses_less_memory_than_dynamic_cache(self):\n         offloaded_peak_memory = torch_accelerator_module.max_memory_allocated(device)\n         self.assertTrue(offloaded_peak_memory < original_peak_memory)\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_cache_copy(self):\n         \"\"\"Tests that we can manually set a cache, copy, and reuse it for generation\"\"\"\n         # TODO (joao): test for all cache implementations in `CacheIntegrationTest` after standardizing the\n         # lazy init of cache layers\n         model_name = \"microsoft/Phi-3-mini-4k-instruct\"\n         tokenizer = AutoTokenizer.from_pretrained(model_name)\n-        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"cuda\", torch_dtype=torch.bfloat16)\n+        model = AutoModelForCausalLM.from_pretrained(model_name, device_map=torch_device, torch_dtype=torch.bfloat16)\n \n         prompt_cache = StaticCache(\n-            config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16\n+            config=model.config, max_batch_size=1, max_cache_len=1024, device=torch_device, dtype=torch.bfloat16\n         )\n \n         INITIAL_PROMPT = \"You are a helpful assistant. \"\n-        inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n+        inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(torch_device)\n         # This is the common prompt cached, we need to run forward without grad to be able to copy\n         with torch.no_grad():\n             prompt_cache = model(**inputs_initial_prompt, past_key_values=prompt_cache).past_key_values\n \n         prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n         responses = []\n         for prompt in prompts:\n-            new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n+            new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(torch_device)\n             past_key_values = copy.deepcopy(prompt_cache)\n             outputs = model.generate(\n                 **new_inputs, past_key_values=past_key_values, max_new_tokens=40, disable_compile=True\n@@ -474,6 +475,7 @@ def test_cache_copy(self):\n             \"You are a helpful assistant. What is the capital of France?\\n\\n\\n## Response:Paris is the capital \"\n             \"of France.\\n\\n\\n\\n\\n\\n\\n<|endoftext|>\",\n         ]\n+\n         self.assertEqual(responses, EXPECTED_DECODED_TEXT)\n \n     @require_torch_multi_gpu\n@@ -526,11 +528,11 @@ def test_static_cache_no_cuda_graph_skips(self):\n             model.generate(**inputs, max_new_tokens=2, cache_implementation=\"static\")\n         self.assertNotIn(\"cuda\", cap.err.lower())\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n     @slow\n     @require_read_token\n-    def test_static_cache_multi_gpu(self):\n-        \"\"\"Regression test for #35164: static cache with multi-gpu\"\"\"\n+    def test_static_cache_multi_accelerator(self):\n+        \"\"\"Regression test for #35164: static cache with multi-accelerator\"\"\"\n \n         model_id = \"google/gemma-2-2b-it\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)"
        },
        {
            "sha": "81b46af37eb45377feb86cb0bfb05e547c5cc9bb",
            "filename": "tests/utils/test_deprecation.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_deprecation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_deprecation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_deprecation.py?ref=a72cb314347bd6b393eb4a48d597c2e57bfd5c4a",
            "patch": "@@ -18,7 +18,7 @@\n from parameterized import parameterized\n \n from transformers import __version__, is_torch_available\n-from transformers.testing_utils import require_torch_gpu\n+from transformers.testing_utils import require_torch_accelerator, torch_device\n from transformers.utils.deprecation import deprecate_kwarg\n \n \n@@ -174,11 +174,11 @@ def dummy_function(new_name=None, **kwargs):\n             result = dummy_function(deprecated_name=\"old_value\", new_name=\"new_value\")\n         self.assertEqual(result, \"new_value\")\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_compile_safe(self):\n         @deprecate_kwarg(\"deprecated_factor\", new_name=\"new_factor\", version=INFINITE_VERSION)\n         def dummy_function(new_factor=None, **kwargs):\n-            return new_factor * torch.ones(1, device=\"cuda\")\n+            return new_factor * torch.ones(1, device=torch_device)\n \n         compiled_function = torch.compile(dummy_function, fullgraph=True)\n "
        },
        {
            "sha": "2df33849639504152b79cc79e5a02061767dd1c8",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=a72cb314347bd6b393eb4a48d597c2e57bfd5c4a",
            "patch": "@@ -63,7 +63,6 @@\n     require_tf,\n     require_torch,\n     require_torch_accelerator,\n-    require_torch_gpu,\n     require_torch_multi_accelerator,\n     require_usr_bin_time,\n     slow,\n@@ -1896,7 +1895,7 @@ def test_unknown_quantization_config(self):\n     @parameterized.expand([(\"Qwen/Qwen2.5-3B-Instruct\", 10), (\"meta-llama/Llama-2-7b-chat-hf\", 10)])\n     @slow\n     @require_read_token\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_loading_is_fast_on_gpu(self, model_id: str, max_loading_time: float):\n         \"\"\"\n         This test is used to avoid regression on https://github.com/huggingface/transformers/pull/36380.\n@@ -1913,27 +1912,30 @@ def test_loading_is_fast_on_gpu(self, model_id: str, max_loading_time: float):\n             import time\n             import argparse\n             from transformers import AutoModelForCausalLM\n+            from transformers.utils import is_torch_accelerator_available\n \n             parser = argparse.ArgumentParser()\n             parser.add_argument(\"model_id\", type=str)\n             parser.add_argument(\"max_loading_time\", type=float)\n             args = parser.parse_args()\n \n-            device = torch.device(\"cuda:0\")\n+            device_type = torch.accelerator.current_accelerator().type if is_torch_accelerator_available() else \"cuda\"\n+            device = torch.device(f\"{device_type}:0\")\n \n-            torch.cuda.synchronize(device)\n+            torch_accelerator_module = getattr(torch, device_type, torch.cuda)\n+            torch_accelerator_module.synchronize(device)\n             t0 = time.time()\n             model = AutoModelForCausalLM.from_pretrained(args.model_id, torch_dtype=torch.float16, device_map=device)\n-            torch.cuda.synchronize(device)\n+            torch_accelerator_module.synchronize(device)\n             dt = time.time() - t0\n \n             # Assert loading is faster (it should be more than enough in both cases)\n             if dt > args.max_loading_time:\n                 raise ValueError(f\"Loading took {dt:.2f}s! It should not take more than {args.max_loading_time}s\")\n-            # Ensure everything is correctly loaded on gpu\n+            # Ensure everything is correctly loaded on accelerator\n             bad_device_params = {k for k, v in model.named_parameters() if v.device != device}\n             if len(bad_device_params) > 0:\n-                raise ValueError(f\"The following parameters are not on GPU: {bad_device_params}\")\n+                raise ValueError(f\"The following parameters are not on accelerator: {bad_device_params}\")\n             \"\"\"\n         )\n "
        },
        {
            "sha": "5ef578f0c3b1074fb252f437cf6aa032cdf44f79",
            "filename": "tests/utils/test_skip_decorators.py",
            "status": "modified",
            "additions": 14,
            "deletions": 9,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_skip_decorators.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a72cb314347bd6b393eb4a48d597c2e57bfd5c4a/tests%2Futils%2Ftest_skip_decorators.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_skip_decorators.py?ref=a72cb314347bd6b393eb4a48d597c2e57bfd5c4a",
            "patch": "@@ -33,7 +33,7 @@\n import pytest\n from parameterized import parameterized\n \n-from transformers.testing_utils import require_torch, require_torch_gpu, slow, torch_device\n+from transformers.testing_utils import require_torch, require_torch_accelerator, slow, torch_device\n \n \n # skipping in unittest tests\n@@ -59,17 +59,22 @@ def check_slow_torch_cuda():\n         assert False, \"should have been skipped\"\n \n \n+def check_slow_torch_accelerator():\n+    run_slow = bool(os.getenv(\"RUN_SLOW\", 0))\n+    assert run_slow and torch_device in [\"cuda\", \"xpu\"], \"should have been skipped\"\n+\n+\n @require_torch\n class SkipTester(unittest.TestCase):\n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     def test_2_skips_slow_first(self):\n-        check_slow_torch_cuda()\n+        check_slow_torch_accelerator()\n \n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_2_skips_slow_last(self):\n-        check_slow_torch_cuda()\n+        check_slow_torch_accelerator()\n \n     # The combination of any skip decorator, followed by parameterized fails to skip the tests\n     # 1. @slow manages to correctly skip `test_param_slow_first`\n@@ -96,15 +101,15 @@ def test_param_slow_last(self, param=None):\n \n \n @slow\n-@require_torch_gpu\n+@require_torch_accelerator\n def test_pytest_2_skips_slow_first():\n-    check_slow_torch_cuda()\n+    check_slow_torch_accelerator()\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @slow\n def test_pytest_2_skips_slow_last():\n-    check_slow_torch_cuda()\n+    check_slow_torch_accelerator()\n \n \n @slow"
        }
    ],
    "stats": {
        "total": 65,
        "additions": 37,
        "deletions": 28
    }
}