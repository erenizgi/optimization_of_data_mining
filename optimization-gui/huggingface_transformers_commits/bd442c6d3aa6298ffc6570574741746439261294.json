{
    "author": "pglorio",
    "message": "Zamba new attention standard (#35375)\n\n* updated zamba to new attention standard\n\n* make fixup fixes",
    "sha": "bd442c6d3aa6298ffc6570574741746439261294",
    "files": [
        {
            "sha": "761c799bdcdc41abe9ad818fbf6478f74829ea2d",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 102,
            "deletions": 288,
            "changes": 390,
            "blob_url": "https://github.com/huggingface/transformers/blob/bd442c6d3aa6298ffc6570574741746439261294/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bd442c6d3aa6298ffc6570574741746439261294/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=bd442c6d3aa6298ffc6570574741746439261294",
            "patch": "@@ -20,7 +20,7 @@\n \"\"\"PyTorch Zamba model.\"\"\"\n \n import math\n-from typing import Any, Dict, List, Optional, Tuple, Union\n+from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -33,18 +33,18 @@\n from ...modeling_attn_mask_utils import (\n     AttentionMaskConverter,\n )\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -113,7 +113,7 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n-class HybridMambaAttentionDynamicCache(DynamicCache):\n+class ZambaHybridDynamicCache(DynamicCache):\n     \"\"\"\n     A dynamic cache that can handle both the attention cache (which has a seq_len dimension) and the mamba cache\n     (which has a constant shape regardless of seq_len).\n@@ -131,9 +131,9 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self.dtype = dtype\n         self.layers_block_type = config.layers_block_type\n         self.has_previous_state = False  # only used by mamba\n-        intermediate_size = config.mamba_expand * config.hidden_size\n-        ssm_state_size = config.mamba_d_state\n-        conv_kernel_size = config.mamba_d_conv\n+        self.intermediate_size = config.mamba_expand * config.hidden_size\n+        self.ssm_state_size = config.mamba_d_state\n+        self.conv_kernel_size = config.mamba_d_conv\n         self.n_mamba_heads = config.n_mamba_heads\n         self.conv_states = []\n         self.ssm_states = []\n@@ -143,9 +143,14 @@ def __init__(self, config, batch_size, dtype=torch.float16, device=None):\n         self._buffers = {}\n         for i in range(config.num_hidden_layers):\n             self.conv_states += [\n-                torch.zeros(batch_size, intermediate_size, conv_kernel_size, device=device, dtype=dtype)\n+                torch.zeros(batch_size, self.intermediate_size, self.conv_kernel_size, device=device, dtype=dtype)\n             ]\n-            cache_shape = (batch_size, self.n_mamba_heads, intermediate_size // self.n_mamba_heads, ssm_state_size)\n+            cache_shape = (\n+                batch_size,\n+                self.n_mamba_heads,\n+                self.intermediate_size // self.n_mamba_heads,\n+                self.ssm_state_size,\n+            )\n             self.ssm_states += [torch.zeros(cache_shape, device=device, dtype=dtype)]\n             if self.layers_block_type[i] == \"hybrid\":\n                 self.transformer_layers.append(i)\n@@ -194,14 +199,38 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n             return 0\n         return self.key_cache[layer_idx].shape[-2]\n \n-    # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.to_legacy_cache\n     def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+        raise NotImplementedError(\"ZambaHybridDynamicCache does not have a legacy cache equivalent.\")\n \n     @classmethod\n-    # Copied from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache.from_legacy_cache\n     def from_legacy_cache(cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None) -> \"DynamicCache\":\n-        raise NotImplementedError(\"HybridMambaAttentionDynamicCache does not have a legacy cache equivalent.\")\n+        raise NotImplementedError(\"ZambaHybridDynamicCache does not have a legacy cache equivalent.\")\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n \n \n class ZambaAttention(nn.Module):\n@@ -218,277 +247,67 @@ class ZambaAttention(nn.Module):\n     attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim/2)\n     \"\"\"\n \n-    def __init__(self, config: ZambaConfig, layer_idx: Optional[int] = None):\n+    def __init__(self, config: ZambaConfig, layer_idx: int):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n \n-        self.hidden_size = config.hidden_size\n         self.attention_hidden_size = config.attention_hidden_size\n-        self.num_heads = config.num_attention_heads\n         self.head_dim = config.attention_head_dim\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.max_position_embeddings = config.max_position_embeddings\n+        self.scaling = (self.head_dim / 2) ** -0.5\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout\n \n-        if (self.head_dim * self.num_heads) != self.attention_hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-        self.q_proj = nn.Linear(self.attention_hidden_size, self.num_heads * self.head_dim, bias=False)\n-        self.k_proj = nn.Linear(self.attention_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.v_proj = nn.Linear(self.attention_hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n+        self.q_proj = nn.Linear(config.attention_hidden_size, config.num_attention_heads * self.head_dim, bias=False)\n+        self.k_proj = nn.Linear(config.attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.v_proj = nn.Linear(config.attention_hidden_size, config.num_key_value_heads * self.head_dim, bias=False)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         layer_idx: int,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[ZambaHybridDynamicCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n \n         if past_key_value is not None:\n             key_states, value_states = past_key_value.update(key_states, value_states, layer_idx)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim / 2)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.attention_hidden_size)\n-\n-        attn_output = attn_output\n-        attn_output = self.o_proj(attn_output)\n-        attn_output = attn_output\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:\n-# Added softmax_scale = 1 / (query_states.shape[-1]/2)**0.5 to the arguments of self._flash_attention_forward\n-# dropped use_sliding_windows from the arguments of self._flash_attention_forward\n-class ZambaFlashAttention2(ZambaAttention):\n-    \"\"\"\n-    Zamba flash attention module. This module inherits from `ZambaAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        layer_idx: int,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n-    ):\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, layer_idx)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-        dropout_rate = 0.0 if not self.training else self.attention_dropout\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in float16 just to be sure everything works as expected.\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-        softmax_scale = 1 / math.sqrt(self.head_dim / 2)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            softmax_scale=softmax_scale,\n-        )\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.attention_hidden_size).contiguous()\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# Adapted from transformers.models.mistral.modeling_mistral.MistralAttention:\n-# added scale = 1 / (query_states.shape[-1]/2)**0.5 to the arguments of torch.nn.functional.scaled_dot_product_attention\n-class ZambaSdpaAttention(ZambaAttention):\n-    \"\"\"\n-    Zamba attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `ZambaAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        layer_idx: int,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"ZambaModel is using ZambaSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        if past_key_value is not None:\n-            key_states, value_states = past_key_value.update(key_states, value_states, layer_idx)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        softmax_scale = 1 / math.sqrt(self.head_dim / 2)\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-            is_causal=self.is_causal and attention_mask is None and q_len > 1,\n-            scale=softmax_scale,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.attention_hidden_size)\n-\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-ZAMBA_ATTENTION_CLASSES = {\n-    \"eager\": ZambaAttention,\n-    \"flash_attention_2\": ZambaFlashAttention2,\n-    \"sdpa\": ZambaSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class ZambaMambaMixer(nn.Module):\n@@ -568,7 +387,7 @@ def __init__(self, config: ZambaConfig, layer_idx):\n             )\n \n     def cuda_kernels_forward(\n-        self, hidden_states: torch.Tensor, cache_params: HybridMambaAttentionDynamicCache = None, attention_mask=None\n+        self, hidden_states: torch.Tensor, cache_params: ZambaHybridDynamicCache = None, attention_mask=None\n     ):\n         batch_size, seq_len, _ = hidden_states.shape\n         use_precomputed_states = cache_params is not None and cache_params.has_previous_state and seq_len == 1\n@@ -664,7 +483,7 @@ def cuda_kernels_forward(\n         contextualized_states = self.out_proj(scan_outputs.transpose(1, 2))\n         return contextualized_states\n \n-    def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCache = None, attention_mask=None):\n+    def slow_forward(self, input_states, cache_params: ZambaHybridDynamicCache = None, attention_mask=None):\n         batch_size, seq_len, _ = input_states.shape\n         dtype = input_states.dtype\n         # 1. Gated linear projection\n@@ -675,7 +494,7 @@ def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCa\n         gate = gate.squeeze(2)\n         gate = gate.reshape(batch_size, self.n_mamba_heads, -1, seq_len).transpose(0, 1)\n \n-        use_cache = isinstance(cache_params, HybridMambaAttentionDynamicCache)\n+        use_cache = isinstance(cache_params, ZambaHybridDynamicCache)\n         # 2. Convolution sequence transformation\n         if use_cache and cache_params.ssm_states[self.layer_idx].shape[0] == batch_size:\n             if self.training:\n@@ -757,7 +576,7 @@ def slow_forward(self, input_states, cache_params: HybridMambaAttentionDynamicCa\n         )\n         return contextualized_states\n \n-    def forward(self, hidden_states, cache_params: HybridMambaAttentionDynamicCache = None, attention_mask=None):\n+    def forward(self, hidden_states, cache_params: ZambaHybridDynamicCache = None, attention_mask=None):\n         if self.use_fast_kernels:\n             if not is_fast_path_available or \"cuda\" not in self.x_proj_weight.device.type:\n                 raise ValueError(\n@@ -789,7 +608,7 @@ def forward(self, x):\n class ZambaAttentionDecoderLayer(nn.Module):\n     def __init__(self, config: ZambaConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n-        self.self_attn = ZAMBA_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx)\n+        self.self_attn = ZambaAttention(config, layer_idx)\n \n         self.feed_forward = ZambaMLP(config)\n         self.input_layernorm = ZambaRMSNorm(config.attention_hidden_size, eps=config.rms_norm_eps)\n@@ -802,11 +621,11 @@ def forward(\n         layer_idx: int,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_value: Optional[ZambaHybridDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -815,9 +634,11 @@ def forward(\n                 This is concatenated with `hidden_states` (which is the output of the previous (mamba) layer). The\n                 concatenated tensor is then used as input of the pre-attention RMSNorm\n                 (see fig. 2 in https://arxiv.org/pdf/2405.16712).\n+            layer_idx (`int`): layer_idx in the forward pass. Used to distinguish Zamba's tied transformer layers.\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            position_ids (`torch.LongTensor`, *optional*): token positions of shape `(batch, seq_len)`. Used for positional encodings.\n+            past_key_value (`ZambaHybridDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -829,7 +650,7 @@ def forward(\n         \"\"\"\n         hidden_states = torch.concatenate([hidden_states, original_hidden_states], dim=-1)\n         hidden_states = self.input_layernorm(hidden_states)\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             layer_idx=layer_idx,\n             attention_mask=attention_mask,\n@@ -849,9 +670,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -870,7 +688,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_value: Optional[ZambaHybridDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -881,7 +699,7 @@ def forward(\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_value (`ZambaHybridDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -923,7 +741,7 @@ def forward(\n         return outputs\n \n \n-class HybridLayer(nn.Module):\n+class ZambaHybridLayer(nn.Module):\n     def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear, mamba: ZambaMambaDecoderLayer):\n         super().__init__()\n         self.shared_transf = shared_transf\n@@ -938,7 +756,7 @@ def forward(\n         attention_mask: Optional[torch.Tensor] = None,\n         causal_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_value: Optional[ZambaHybridDynamicCache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n@@ -951,7 +769,7 @@ def forward(\n             layer_idx (`int`): layer number.\n             attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n                 `(batch, sequence_length)` where padding elements are indicated by 0.\n-            past_key_value (`HybridMambaAttentionDynamicCache`, *optional*): cached past key and value projection states\n+            past_key_value (`ZambaHybridDynamicCache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n@@ -1027,7 +845,7 @@ class ZambaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = False\n     _supports_sdpa = False\n-    _supports_cache_class = True  # Note: only supports HybridMambaAttentionDynamicCache\n+    _supports_cache_class = True  # Note: only supports ZambaHybridDynamicCache\n     _is_stateful = True\n \n     def _init_weights(self, module):\n@@ -1121,14 +939,14 @@ def _check_and_enable_flash_attn_2(\n             config.n_positions - 1]`.\n \n             [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`HybridMambaAttentionDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            A HybridMambaAttentionDynamicCache object containing pre-computed hidden-states (keys and values in the\n+        past_key_values (`ZambaHybridDynamicCache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            A ZambaHybridDynamicCache object containing pre-computed hidden-states (keys and values in the\n             self-attention blocks and convolution and ssm states in the mamba blocks) that can be used (see\n             `past_key_values` input) to speed up sequential decoding.\n             Key and value cache tensors have shape `(batch_size, num_heads, seq_len, head_dim)`.\n             Convolution and ssm states tensors have shape `(batch_size, d_inner, d_conv)` and\n             `(batch_size, d_inner, d_state)` respectively.\n-            See the `HybridMambaAttentionDynamicCache` class for more details.\n+            See the `ZambaHybridDynamicCache` class for more details.\n \n             If `past_key_values` are used, the user can optionally input only the last `input_ids` (those that\n             don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n@@ -1202,7 +1020,7 @@ def __init__(self, config: ZambaConfig):\n                     \"shared_transf.pre_ff_layernorm.weight\",\n                 ]\n                 self._tied_weights_keys = [*self._tied_weights_keys, *[prefix_name + key for key in tied_keys]]\n-                layers.append(HybridLayer(block, next(linear_layers), next(mamba_layers)))\n+                layers.append(ZambaHybridLayer(block, next(linear_layers), next(mamba_layers)))\n             else:\n                 layers.append(next(mamba_layers))\n         self.layers = nn.ModuleList(layers)\n@@ -1226,7 +1044,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[ZambaHybridDynamicCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1263,7 +1081,7 @@ def forward(\n \n         if use_cache and past_key_values is None:\n             logger.warning_once(\n-                \"Zamba requires an initialized `HybridMambaAttentionDynamicCache` to return a cache. None was \"\n+                \"Zamba requires an initialized `ZambaHybridDynamicCache` to return a cache. None was \"\n                 \"provided, so no cache will be returned.\"\n             )\n \n@@ -1324,17 +1142,13 @@ def forward(\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n-        next_cache = None if not use_cache else past_key_values\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     # Copied from transformers.models.jamba.modeling_jamba.JambaModel._update_causal_mask\n     def _update_causal_mask(self, attention_mask, input_tensor, cache_position):\n@@ -1410,7 +1224,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridMambaAttentionDynamicCache] = None,\n+        past_key_values: Optional[ZambaHybridDynamicCache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1504,7 +1318,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         **kwargs,\n     ):\n-        # Overwitten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n+        # Overwitten -- has a unique cache type, `ZambaHybridDynamicCache`\n \n         empty_past_kv = past_key_values is None\n \n@@ -1518,7 +1332,7 @@ def prepare_inputs_for_generation(\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n         else:\n-            past_key_values = HybridMambaAttentionDynamicCache(\n+            past_key_values = ZambaHybridDynamicCache(\n                 self.config, input_ids.shape[0], dtype=self.dtype, device=self.device\n             )\n "
        },
        {
            "sha": "ee47f98a1f41339fead6cb4a6c2e5ac7035f25fa",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bd442c6d3aa6298ffc6570574741746439261294/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bd442c6d3aa6298ffc6570574741746439261294/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=bd442c6d3aa6298ffc6570574741746439261294",
            "patch": "@@ -46,7 +46,7 @@\n         ZambaModel,\n     )\n     from transformers.models.zamba.modeling_zamba import (\n-        HybridMambaAttentionDynamicCache,\n+        ZambaHybridDynamicCache,\n     )\n \n \n@@ -215,9 +215,7 @@ def create_and_check_decoder_model_past_large_inputs(\n \n         # first forward pass\n         # Attention: Zamba needs the cache to be initialized to return a cache!\n-        past_key_values = HybridMambaAttentionDynamicCache(\n-            config, input_ids.shape[0], model.dtype, device=model.device\n-        )\n+        past_key_values = ZambaHybridDynamicCache(config, input_ids.shape[0], model.dtype, device=model.device)\n         outputs = model(\n             input_ids,\n             attention_mask=input_mask,"
        }
    ],
    "stats": {
        "total": 396,
        "additions": 104,
        "deletions": 292
    }
}