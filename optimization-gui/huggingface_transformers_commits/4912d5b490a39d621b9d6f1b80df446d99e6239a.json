{
    "author": "3outeille",
    "message": "fix to avoid modifying a view in place (#40162)\n\n* fix to avoid modifying a view in place\n\n* add backward test in tensor parallel\n\n* add test to test_modelig_gpt_oss.py\n\n* linting",
    "sha": "4912d5b490a39d621b9d6f1b80df446d99e6239a",
    "files": [
        {
            "sha": "2b6bdc83347c011a1ba184c56aaa19dd1b938057",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4912d5b490a39d621b9d6f1b80df446d99e6239a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4912d5b490a39d621b9d6f1b80df446d99e6239a/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=4912d5b490a39d621b9d6f1b80df446d99e6239a",
            "patch": "@@ -677,7 +677,7 @@ def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_me\n             outputs = outputs.redistribute(placements=output_layouts, async_op=True)\n         outputs = outputs.to_local()  # otherwise the `+=` op will gather\n         if hasattr(mod, \"_bias\"):\n-            outputs += mod._bias\n+            outputs = outputs + mod._bias\n         # back to local tensor if use_local_output is True\n         return outputs\n "
        },
        {
            "sha": "84389c31d502e7f1220d1e8dcddf1367e3557a7f",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/4912d5b490a39d621b9d6f1b80df446d99e6239a/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4912d5b490a39d621b9d6f1b80df446d99e6239a/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=4912d5b490a39d621b9d6f1b80df446d99e6239a",
            "patch": "@@ -397,6 +397,51 @@ def test_model_outputs(self, quantized, model, kernels, attn_impl, mode):\n     def test_model_outputs_distributed(self, quantized, model, kernels, attn_impl, mode):\n         self.run_distributed_test(quantized, model, kernels, attn_impl, mode)\n \n+    # ------------------------\n+    # Training test\n+    # ------------------------\n+    @parameterized.expand(PARAMETERS)\n+    @require_read_token\n+    def test_training_step(self, quantized, model, kernels, attn_impl, mode):\n+        if mode != \"train\":\n+            self.skipTest(\"This test is only for training mode.\")\n+\n+        if quantized:\n+            self.skipTest(\"Training test for quantized models is not supported.\")\n+\n+        model_id = f\"/fsx/vb/new-oai/gpt-oss-{model}-trfs\"\n+\n+        model_obj = AutoModelForCausalLM.from_pretrained(\n+            model_id,\n+            torch_dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+            attn_implementation=attn_impl,\n+            use_kernels=kernels,\n+        )\n+        model_obj.train()\n+\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n+        if tokenizer.pad_token is None:\n+            tokenizer.pad_token = tokenizer.eos_token\n+\n+        inputs = tokenizer(self.input_text, return_tensors=\"pt\", padding=True).to(model_obj.device)\n+        inputs[\"labels\"] = inputs[\"input_ids\"].clone()\n+\n+        outputs = model_obj(**inputs)\n+        loss = outputs.loss\n+        self.assertIsNotNone(loss)\n+\n+        loss.backward()\n+\n+        # Check that gradients were computed for all parameters that have a grad field\n+        for name, param in model_obj.named_parameters():\n+            if param.requires_grad:\n+                self.assertIsNotNone(param.grad, f\"Parameter '{name}' did not receive a gradient.\")\n+                # Check that gradients are not all zero\n+                self.assertTrue(\n+                    torch.sum(torch.abs(param.grad)).item() > 0, f\"Gradient for parameter '{name}' is all zeros.\"\n+                )\n+\n     def test_model_matches_original_20b(self):\n         input_text = \"Roses are red, violets\"\n "
        },
        {
            "sha": "15612acd740845c38cb412e5cf4d660b3d434970",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4912d5b490a39d621b9d6f1b80df446d99e6239a/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4912d5b490a39d621b9d6f1b80df446d99e6239a/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=4912d5b490a39d621b9d6f1b80df446d99e6239a",
            "patch": "@@ -126,6 +126,32 @@ def test_model_forward(self):\n         )\n         self.torchrun(script_to_run)\n \n+    def test_model_backward_pass(self):\n+        script_to_run = textwrap.dedent(\n+            \"\"\"\n+            import torch\n+            import os\n+            from transformers import AutoModelForCausalLM\n+            from torch import nn\n+\n+            model_id = \"JackFram/llama-68m\"\n+\n+            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float32, tp_plan=\"auto\")\n+            torch.distributed.barrier()\n+\n+            # Dummy forward and backward pass\n+            # Note that loss.backward() will fail if there is a bug in the TP implementation\n+            inputs = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n+            labels = torch.randint(0, model.config.vocab_size, (2, 10), device=model.device)\n+            loss = model(inputs, labels=labels).loss\n+            loss.backward()\n+\n+            torch.distributed.barrier()\n+            torch.distributed.destroy_process_group()\n+            \"\"\"\n+        )\n+        self.torchrun(script_to_run)\n+\n     def test_model_generate(self):\n         script_to_run = textwrap.dedent(\n             \"\"\""
        }
    ],
    "stats": {
        "total": 73,
        "additions": 72,
        "deletions": 1
    }
}