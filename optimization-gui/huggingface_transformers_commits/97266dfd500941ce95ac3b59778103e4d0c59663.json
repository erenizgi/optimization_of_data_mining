{
    "author": "ydshieh",
    "message": "Fix flaky `JambaModelTest.test_load_balancing_loss` (#40617)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "97266dfd500941ce95ac3b59778103e4d0c59663",
    "files": [
        {
            "sha": "91601b1d0414714e2e61dd969db656922adf0a73",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/97266dfd500941ce95ac3b59778103e4d0c59663/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/97266dfd500941ce95ac3b59778103e4d0c59663/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=97266dfd500941ce95ac3b59778103e4d0c59663",
            "patch": "@@ -24,6 +24,7 @@\n     DeviceProperties,\n     Expectations,\n     get_device_properties,\n+    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_torch,\n@@ -104,10 +105,10 @@ def __init__(\n         use_labels=True,\n         vocab_size=99,\n         hidden_size=32,\n-        num_hidden_layers=5,\n+        num_hidden_layers=2,\n         attn_layer_offset=1,\n         attn_layer_period=8,\n-        num_attention_heads=4,\n+        num_attention_heads=2,\n         num_key_value_heads=2,\n         intermediate_size=37,\n         hidden_act=\"gelu\",\n@@ -385,13 +386,15 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n+    # After #40617, we still have 0.01 % of failure rate here.\n+    @is_flaky(max_attempts=2)\n     def test_load_balancing_loss(self):\n         r\"\"\"\n         Let's make sure we can actually compute the loss and do a backward on it.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.num_labels = 3\n-        config.num_experts = 16\n+        config.num_experts = 3\n         config.output_router_logits = True\n         input_ids = input_dict[\"input_ids\"]\n         attention_mask = input_ids.ne(config.pad_token_id).to(torch_device)\n@@ -401,11 +404,13 @@ def test_load_balancing_loss(self):\n         result = model(input_ids, attention_mask=attention_mask)\n         bs, seqlen = input_ids.shape\n         self.assertEqual(result.router_logits[0].shape, (bs * seqlen, config.num_experts))\n+        # After #40617, we still have 0.01 % of failure rate here.\n         torch.testing.assert_close(result.aux_loss.cpu(), torch.tensor(2, dtype=torch.float32), rtol=1e-2, atol=1e-2)\n \n         # First, we make sure that adding padding tokens doesn't change the loss\n         # loss(input_ids, attention_mask=None) == loss(input_ids + padding, attention_mask=attention_mask_with_padding)\n-        pad_length = 1000\n+        # (This length is selected from experiments)\n+        pad_length = input_ids.shape[1] * 4\n         # Add padding tokens to input_ids\n         padding_block = config.pad_token_id * torch.ones(input_ids.shape[0], pad_length, dtype=torch.int32).to(\n             torch_device\n@@ -421,6 +426,7 @@ def test_load_balancing_loss(self):\n         include_padding_result = model(padded_input_ids, attention_mask=None)\n \n         # This is to mimic torch.testing.assert_not_close\n+        # After #40617, we still have 0.003 % of failure rate here.\n         self.assertNotAlmostEqual(include_padding_result.aux_loss.item(), result.aux_loss.item())\n \n     def test_initialization(self):"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 10,
        "deletions": 4
    }
}