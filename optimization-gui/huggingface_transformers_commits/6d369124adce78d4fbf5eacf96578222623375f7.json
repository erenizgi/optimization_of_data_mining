{
    "author": "bozheng-hit",
    "message": "Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library. (#40807)\n\n* align torch implementation of gdn with fla.\n\n* fix fla import.\n\n* fix\n\n* remove unused attr\n\n* fixes\n\n---------\n\nCo-authored-by: bozheng-hit <dsoul0621@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "6d369124adce78d4fbf5eacf96578222623375f7",
    "files": [
        {
            "sha": "990b776f0d5710e90bab7a93b70484a738f47c64",
            "filename": "src/transformers/models/qwen3_next/modeling_qwen3_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodeling_qwen3_next.py?ref=6d369124adce78d4fbf5eacf96578222623375f7",
            "patch": "@@ -446,8 +446,10 @@ def torch_chunk_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        query = F.normalize(query, p=2, dim=-1)\n-        key = F.normalize(key, p=2, dim=-1)\n+        head_dim = query.size(-1)\n+        inv_scale = head_dim**-0.5\n+        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n+        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n@@ -518,8 +520,10 @@ def torch_recurrent_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        query = F.normalize(query, p=2, dim=-1)\n-        key = F.normalize(key, p=2, dim=-1)\n+        head_dim = query.size(-1)\n+        inv_scale = head_dim**-0.5\n+        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n+        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]"
        },
        {
            "sha": "7512f4a3060ae4480dd7eb7c29ea376eb272c843",
            "filename": "src/transformers/models/qwen3_next/modular_qwen3_next.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_next%2Fmodular_qwen3_next.py?ref=6d369124adce78d4fbf5eacf96578222623375f7",
            "patch": "@@ -282,8 +282,10 @@ def torch_chunk_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        query = F.normalize(query, p=2, dim=-1)\n-        key = F.normalize(key, p=2, dim=-1)\n+        head_dim = query.size(-1)\n+        inv_scale = head_dim**-0.5\n+        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n+        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]\n@@ -354,8 +356,10 @@ def torch_recurrent_gated_delta_rule(\n ):\n     initial_dtype = query.dtype\n     if use_qk_l2norm_in_kernel:\n-        query = F.normalize(query, p=2, dim=-1)\n-        key = F.normalize(key, p=2, dim=-1)\n+        head_dim = query.size(-1)\n+        inv_scale = head_dim**-0.5\n+        query = F.rms_norm(query, (head_dim,), eps=1e-6) * inv_scale\n+        key = F.rms_norm(key, (head_dim,), eps=1e-6) * inv_scale\n     query, key, value, beta, g = [\n         x.transpose(1, 2).contiguous().to(torch.float32) for x in (query, key, value, beta, g)\n     ]"
        },
        {
            "sha": "d3c08eea59717a8336948365b3fd8f7257f4f6b8",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6d369124adce78d4fbf5eacf96578222623375f7/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=6d369124adce78d4fbf5eacf96578222623375f7",
            "patch": "@@ -592,12 +592,14 @@ def is_flash_linear_attention_available():\n \n         if not torch.cuda.is_available():\n             return False\n-        else:\n-            if _is_package_available(\"fla\"):\n-                import fla\n \n-                if version.parse(fla.__version__) >= version.parse(\"0.2.2\"):\n-                    return True\n+        try:\n+            import fla\n+\n+            if version.parse(fla.__version__) >= version.parse(\"0.2.2\"):\n+                return True\n+        except Exception:\n+            pass\n     return False\n \n "
        }
    ],
    "stats": {
        "total": 36,
        "additions": 23,
        "deletions": 13
    }
}