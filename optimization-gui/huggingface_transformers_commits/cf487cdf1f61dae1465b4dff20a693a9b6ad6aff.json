{
    "author": "yjc9696",
    "message": "HunYuan opensource (#39606)\n\n* merge opensource_hunyuan\n\n* add head_dim\n\n* fix assertion error\n\n* fix seen_tokens\n\n* ready_for_upstream (merge request !17)\n\nSquash merge branch 'ready_for_upstream' into 'main'\n\n* fix configuration type&docstring\n* fix style\n\n* ready_for_upstream (merge request !18)\n\nSquash merge branch 'ready_for_upstream' into 'main'\n* add doc\n* fix testcode\n* fix configuration type&docstring\n\n* rename base model\n\n* remove assert\n\n* update\n\n* remove tiktoken\n\n* update\n\n* fix moe and code style (#3)\n\n* update\n\n* fix format\n\n* update\n\n* revert makefile\n\n* fix moe config\n\n* fix numel()\n\n* remove prepare_inputs_for_generation\n\n* fix kv_seq_len\n\n* add docs/toctree\n\n* remove unused paramter&add licence\n\n* add licence\n\n* remove unused paramter\n\n* fix code\n\n* dense modular\n\nupdate import\n\nfix\n\nfix\n\nuse mistralmodel\n\nfix qknorm\n\nadd sliding_window\n\nmake style\n\nfix\n\ndense done\n\nhunyuan moe\n\nfix import\n\nfix modular\n\nfixup\n\nfixup\n\n* update model path\n\n* fix mlp_bias\n\n* fix modular\n\n* Fix modeling (#5)\n\n* fix attention\n\n* use llamamodel\n\n* fix code\n\n* Fix qk (#6)\n\n* fix qk_norm\n\n* fix\n\n* fix modual\n\n* Fix moe (#7)\n\n* fix some moe code\n\n* fix einsum\n\n* try top1\n\n* use top1\n\n* Fix rotary (#8)\n\n* fix rotary\n\n* fix modeling\n\n* fix modular\n\n* fix testcode\n\n* remove A13B unit test\n\n* Fix moe v1 (#9)\n\nfix moe & gate\n\n* Fix gate norm (#10)\n\n* add norm_topk_prob\n\n* Fix testcase (#11)\n\n* fix&skip test\n\n* Fix testcase (#12)\n\n\n* skip testcase\n\n* Fix norm topk (#13)\n\n* hardcode norm_topk_prob\n\n* fix testcase\n\n---------\n\nCo-authored-by: pridejcyang <pridejcyang@tencent.com>\nCo-authored-by: Mingji Han <mingjihan@tencent.com>",
    "sha": "cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
    "files": [
        {
            "sha": "f3a507c282730823bde1dd90e120176496b0b441",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -529,6 +529,12 @@\n         title: Helium\n       - local: model_doc/herbert\n         title: HerBERT\n+      - local: model_doc/hgnet_v2\n+        title: HGNet-V2\n+      - local: model_doc/hunyuan_v1_dense\n+        title: HunYuanDenseV1\n+      - local: model_doc/hunyuan_v1_moe\n+        title: HunYuanMoEV1\n       - local: model_doc/ibert\n         title: I-BERT\n       - local: model_doc/jamba"
        },
        {
            "sha": "f87ca422c8ed67eca2fc31e3ea72dd3bcc66474a",
            "filename": "docs/source/en/model_doc/hunyuan_v1_dense.md",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_dense.md?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,50 @@\n+<!--Copyright (C) 2024 THL A29 Limited, a Tencent company and The HuggingFace Inc. team. All rights reserved..\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# HunYuanDenseV1\n+\n+## Overview\n+\n+To be released with the official model launch.\n+\n+### Model Details\n+\n+To be released with the official model launch.\n+\n+\n+## Usage tips\n+\n+To be released with the official model launch.\n+\n+## HunYuanDenseV1Config\n+\n+[[autodoc]] HunYuanDenseV1Config\n+\n+## HunYuanModel\n+\n+[[autodoc]] HunYuanDenseV1Model\n+    - forward\n+\n+## HunYuanDenseV1ForCausalLM\n+\n+[[autodoc]] HunYuanDenseV1ForCausalLM\n+    - forward\n+\n+## HunYuanDenseV1ForSequenceClassification\n+\n+[[autodoc]] HunYuanDenseV1ForSequenceClassification\n+    - forward\n+"
        },
        {
            "sha": "c66846cc0881c781527df13adf942cf33e4af82e",
            "filename": "docs/source/en/model_doc/hunyuan_v1_moe.md",
            "status": "added",
            "additions": 50,
            "deletions": 0,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhunyuan_v1_moe.md?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,50 @@\n+<!--Copyright (C) 2024 THL A29 Limited, a Tencent company and The HuggingFace Inc. team. All rights reserved..\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# HunYuanMoEV1\n+\n+## Overview\n+\n+To be released with the official model launch.\n+\n+### Model Details\n+\n+To be released with the official model launch.\n+\n+\n+## Usage tips\n+\n+To be released with the official model launch.\n+\n+## HunYuanMoEV1Config\n+\n+[[autodoc]] HunYuanMoEV1Config\n+\n+## HunYuanMoEV1Model\n+\n+[[autodoc]] HunYuanMoEV1Model\n+    - forward\n+\n+## HunYuanMoEV1ForCausalLM\n+\n+[[autodoc]] HunYuanMoEV1ForCausalLM\n+    - forward\n+\n+## HunYuanMoEV1ForSequenceClassification\n+\n+[[autodoc]] HunYuanMoEV1ForSequenceClassification\n+    - forward\n+"
        },
        {
            "sha": "631c681f5dc4ecceda5ca321e591ec52f51e0df6",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -158,6 +158,8 @@\n     from .hgnet_v2 import *\n     from .hiera import *\n     from .hubert import *\n+    from .hunyuan_v1_dense import *\n+    from .hunyuan_v1_moe import *\n     from .ibert import *\n     from .idefics import *\n     from .idefics2 import *"
        },
        {
            "sha": "2126be005a4f012d002af2ee604c3951485959db",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -193,6 +193,8 @@\n         (\"hgnet_v2\", \"HGNetV2Config\"),\n         (\"hiera\", \"HieraConfig\"),\n         (\"hubert\", \"HubertConfig\"),\n+        (\"hunyuan_v1_dense\", \"HunYuanDenseV1Config\"),\n+        (\"hunyuan_v1_moe\", \"HunYuanMoEV1Config\"),\n         (\"ibert\", \"IBertConfig\"),\n         (\"idefics\", \"IdeficsConfig\"),\n         (\"idefics2\", \"Idefics2Config\"),\n@@ -613,6 +615,8 @@\n         (\"hgnet_v2\", \"HGNet-V2\"),\n         (\"hiera\", \"Hiera\"),\n         (\"hubert\", \"Hubert\"),\n+        (\"hunyuan_v1_dense\", \"HunYuanDenseV1\"),\n+        (\"hunyuan_v1_moe\", \"HunYuanMoeV1\"),\n         (\"ibert\", \"I-BERT\"),\n         (\"idefics\", \"IDEFICS\"),\n         (\"idefics2\", \"Idefics2\"),"
        },
        {
            "sha": "ffa8e7cc7440ee544f687132ec52d43e8ae31b87",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -193,6 +193,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraModel\"),\n         (\"hubert\", \"HubertModel\"),\n+        (\"hunyuan_v1_dense\", \"HunYuanDenseV1Model\"),\n+        (\"hunyuan_v1_moe\", \"HunYuanMoEV1Model\"),\n         (\"ibert\", \"IBertModel\"),\n         (\"idefics\", \"IdeficsModel\"),\n         (\"idefics2\", \"Idefics2Model\"),\n@@ -664,6 +666,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"granitemoehybrid\", \"GraniteMoeHybridForCausalLM\"),\n         (\"granitemoeshared\", \"GraniteMoeSharedForCausalLM\"),\n         (\"helium\", \"HeliumForCausalLM\"),\n+        (\"hunyuan_v1_dense\", \"HunYuanDenseV1ForCausalLM\"),\n+        (\"hunyuan_v1_moe\", \"HunYuanMoEV1ForCausalLM\"),\n         (\"jamba\", \"JambaForCausalLM\"),\n         (\"jetmoe\", \"JetMoeForCausalLM\"),\n         (\"lfm2\", \"Lfm2ForCausalLM\"),\n@@ -1209,6 +1213,8 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"gpt_oss\", \"GptOssForSequenceClassification\"),\n         (\"gptj\", \"GPTJForSequenceClassification\"),\n         (\"helium\", \"HeliumForSequenceClassification\"),\n+        (\"hunyuan_v1_dense\", \"HunYuanDenseV1ForSequenceClassification\"),\n+        (\"hunyuan_v1_moe\", \"HunYuanMoEV1ForSequenceClassification\"),\n         (\"ibert\", \"IBertForSequenceClassification\"),\n         (\"jamba\", \"JambaForSequenceClassification\"),\n         (\"jetmoe\", \"JetMoeForSequenceClassification\"),"
        },
        {
            "sha": "27de691c845369b97805fb53a8e509b7e948b386",
            "filename": "src/transformers/models/hunyuan_v1_dense/__init__.py",
            "status": "added",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,15 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_hunyuan_v1_dense import *\n+    from .modeling_hunyuan_v1_dense import *\n+    from .tokenization_hy import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "e0e32d7be7831417d447f302ce87752472fb9cfc",
            "filename": "src/transformers/models/hunyuan_v1_dense/configuration_hunyuan_v1_dense.py",
            "status": "added",
            "additions": 189,
            "deletions": 0,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fconfiguration_hunyuan_v1_dense.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,189 @@\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"HunYuanDenseV1 model configuration\"\"\"\n+\n+from transformers.configuration_utils import PretrainedConfig\n+from transformers.utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class HunYuanDenseV1Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`HunYuanDenseV1Config`]. It is used to instantiate an\n+    HunYuan model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the HunYuan-7B.\n+    Hunyuan-7B-Instruct [tencent/Hunyuan-7B-Instruct](https://huggingface.co/tencent/Hunyuan-7B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 290943):\n+            Vocabulary size of the HunYuan model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`HunYuanDenseV1Config`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations or shared MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        eod_token_id (int, *optional*, defaults to 3):\n+            Token ID representing the end-of-document marker. Used to indicate the termination of a text sequence.\n+            Example: In multi-document processing, this token helps the model distinguish between separate documents.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n+            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n+            issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n+            these scaling strategies behave:\n+            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n+            experimental feature, subject to breaking API changes in future versions.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The attention head dimension.\n+    \"\"\"\n+\n+    model_type = \"hunyuan_v1_dense\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=290943,\n+        hidden_size=4096,\n+        intermediate_size: int = 11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        eod_token_id=3,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # self._rope_scaling_validation()   # TODO: Need validation?\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def _rope_scaling_validation(self):\n+        \"\"\"\n+        Validate the `rope_scaling` configuration.\n+        \"\"\"\n+        if self.rope_scaling is None:\n+            return\n+\n+        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n+            raise ValueError(\n+                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n+                f\"got {self.rope_scaling}\"\n+            )\n+        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n+        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n+        rope_scaling_alpha = self.rope_scaling.get(\"alpha\", None)\n+        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n+            raise ValueError(\n+                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n+            )\n+        if rope_scaling_factor is None and rope_scaling_alpha is None:\n+            raise ValueError(\"`rope_scaling`'s factor or alpha field must be have one, got both of none\")\n+        if rope_scaling_factor is not None:\n+            if not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n+                raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1.0, got {rope_scaling_factor}\")\n+        if rope_scaling_alpha is not None:\n+            if not isinstance(rope_scaling_alpha, float) or rope_scaling_alpha <= 1.0:\n+                raise ValueError(f\"`rope_scaling`'s alpha field must be a float > 1.0, got {rope_scaling_alpha}\")\n+\n+\n+__all__ = [\"HunYuanDenseV1Config\"]"
        },
        {
            "sha": "4e2a7350c22657c8e39d7241a2fed2054f57e338",
            "filename": "src/transformers/models/hunyuan_v1_dense/modeling_hunyuan_v1_dense.py",
            "status": "added",
            "additions": 520,
            "deletions": 0,
            "changes": 520,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodeling_hunyuan_v1_dense.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,520 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_hunyuan_v1_dense.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.cache_utils import Cache\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_hunyuan_v1_dense import HunYuanDenseV1Config\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class HunYuanDenseV1RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        HunYuanDenseV1RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class HunYuanDenseV1MLP(nn.Module):\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx=None, is_shared_mlp=False):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+        self.layer_idx = layer_idx\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class HunYuanDenseV1Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.query_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.key_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = self.query_layernorm(query_states)\n+        key_states = self.key_layernorm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class HunYuanDenseV1DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+\n+        self.self_attn = HunYuanDenseV1Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = HunYuanDenseV1MLP(config)\n+        self.input_layernorm = HunYuanDenseV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = HunYuanDenseV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.layer_idx = layer_idx\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class HunYuanDenseV1PreTrainedModel(PreTrainedModel):\n+    config: HunYuanDenseV1Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"HunYuanDenseV1DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": HunYuanDenseV1DecoderLayer,\n+        \"attentions\": HunYuanDenseV1Attention,\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+class HunYuanDenseV1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: HunYuanDenseV1Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n+            # DynamicNTKAlphaRotary\n+            self.dim = config.head_dim\n+            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            self.attention_scaling = 1.0\n+        else:\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class HunYuanDenseV1Model(HunYuanDenseV1PreTrainedModel):\n+    def __init__(self, config: HunYuanDenseV1Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [HunYuanDenseV1DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = HunYuanDenseV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = HunYuanDenseV1RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class HunYuanDenseV1ForCausalLM(HunYuanDenseV1PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = HunYuanDenseV1Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, HunYuanDenseV1ForCausalLM\n+\n+        >>> model = HunYuanDenseV1ForCausalLM.from_pretrained(\"meta-hunyuan_v1_dense/HunYuanDenseV1-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-hunyuan_v1_dense/HunYuanDenseV1-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class HunYuanDenseV1ForSequenceClassification(GenericForSequenceClassification, HunYuanDenseV1PreTrainedModel):\n+    pass\n+\n+\n+__all__ = [\n+    \"HunYuanDenseV1ForCausalLM\",\n+    \"HunYuanDenseV1Model\",\n+    \"HunYuanDenseV1PreTrainedModel\",\n+    \"HunYuanDenseV1ForSequenceClassification\",\n+]"
        },
        {
            "sha": "ada3fbde0986b343eb027acc14a56101a429fae6",
            "filename": "src/transformers/models/hunyuan_v1_dense/modular_hunyuan_v1_dense.py",
            "status": "added",
            "additions": 194,
            "deletions": 0,
            "changes": 194,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_dense%2Fmodular_hunyuan_v1_dense.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,194 @@\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch HunYuanDenseV1 model.\"\"\"\n+\n+from typing import Callable, Optional\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from transformers.cache_utils import Cache\n+from transformers.utils import (\n+    logging,\n+)\n+\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaForSequenceClassification,\n+    LlamaMLP,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_hunyuan_v1_dense import HunYuanDenseV1Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class HunYuanDenseV1RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class HunYuanDenseV1MLP(LlamaMLP):\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx=None, is_shared_mlp=False):\n+        super().__init__(config)\n+        self.layer_idx = layer_idx\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class HunYuanDenseV1Attention(LlamaAttention):\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.query_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.key_layernorm = HunYuanDenseV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = self.query_layernorm(query_states)\n+        key_states = self.key_layernorm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class HunYuanDenseV1DecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: HunYuanDenseV1Config, layer_idx: int):\n+        super().__init__()\n+        self.layer_idx = layer_idx\n+\n+\n+class HunYuanDenseV1PreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+class HunYuanDenseV1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: HunYuanDenseV1Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n+            # DynamicNTKAlphaRotary\n+            self.dim = config.head_dim\n+            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            self.attention_scaling = 1.0\n+        else:\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class HunYuanDenseV1Model(LlamaModel):\n+    pass\n+\n+\n+class HunYuanDenseV1ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class HunYuanDenseV1ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+__all__ = [\n+    \"HunYuanDenseV1ForCausalLM\",\n+    \"HunYuanDenseV1Model\",\n+    \"HunYuanDenseV1PreTrainedModel\",\n+    \"HunYuanDenseV1ForSequenceClassification\",\n+]"
        },
        {
            "sha": "cd107ee7a3c16d2527806035f67702e9220cae51",
            "filename": "src/transformers/models/hunyuan_v1_moe/__init__.py",
            "status": "added",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,14 @@\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_hunyuan_v1_moe import *\n+    from .modeling_hunyuan import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "fb8cba72bdfc6601db055987c0fe10994f653198",
            "filename": "src/transformers/models/hunyuan_v1_moe/configuration_hunyuan_v1_moe.py",
            "status": "added",
            "additions": 204,
            "deletions": 0,
            "changes": 204,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fconfiguration_hunyuan_v1_moe.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,204 @@\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"HunYuanMoEV1 model configuration\"\"\"\n+\n+from typing import Union\n+\n+from transformers.configuration_utils import PretrainedConfig\n+from transformers.utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class HunYuanMoEV1Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`HunYuanMoEV1Model`]. It is used to instantiate an\n+    HunYuan model according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the HunYuan-7B.\n+    Hunyuan-A13B-Instruct [tencent/Hunyuan-A13B-Instruct](https://huggingface.co/tencent/Hunyuan-A13B-Instruct).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 290943):\n+            Vocabulary size of the HunYuan model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`HunYuanMoEV1Model`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations or shared MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details checkout [this\n+            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 0):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 1):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 2):\n+            End of stream token id.\n+        eod_token_id (int, *optional*, defaults to 3):\n+            Token ID representing the end-of-document marker. Used to indicate the termination of a text sequence.\n+            Example: In multi-document processing, this token helps the model distinguish between separate documents.\n+        sep_token_id (`int`, *optional*, defaults to 4):\n+            Token ID representing the separator token (`[SEP]`), used to demarcate boundaries between different text segments.\n+        pretraining_tp (`int`, *optional*, defaults to 1):\n+            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n+            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n+            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n+            issue](https://github.com/pytorch/pytorch/issues/76232).\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n+            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n+            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n+            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n+            these scaling strategies behave:\n+            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n+            experimental feature, subject to breaking API changes in future versions.\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        num_experts (`int` or `List`, *optional*, defaults to 1):\n+            The number of experts for moe. If it is a list, it will be used as the number of experts for each layer.\n+        moe_topk (int or List, *optional*, defaults to 1):\n+            Number of experts selected per token (Top-K routing). List form enables layer-wise customization.\n+        head_dim (`int`, *optional*, defaults to 128):\n+            The attention head dimension.\n+    \"\"\"\n+\n+    model_type = \"hunyuan_v1_moe\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    def __init__(\n+        self,\n+        vocab_size=290943,\n+        hidden_size=4096,\n+        intermediate_size: int = 11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-5,\n+        use_cache=True,\n+        pad_token_id=0,\n+        bos_token_id=1,\n+        eos_token_id=2,\n+        eod_token_id=3,\n+        sep_token_id=4,\n+        pretraining_tp=1,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        num_experts: Union[int, list] = 1,\n+        moe_topk: Union[int, list] = 1,\n+        head_dim=None,\n+        **kwargs,\n+    ):\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_experts = num_experts\n+        self.moe_topk = moe_topk\n+\n+        self.head_dim = head_dim\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.pretraining_tp = pretraining_tp\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        # self._rope_scaling_validation()   # TODO: Need validation?\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            sep_token_id=sep_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+\n+    def _rope_scaling_validation(self):\n+        \"\"\"\n+        Validate the `rope_scaling` configuration.\n+        \"\"\"\n+        if self.rope_scaling is None:\n+            return\n+\n+        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n+            raise ValueError(\n+                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor` or `type` and `alpha`, \"\n+                f\"got {self.rope_scaling}\"\n+            )\n+        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n+        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n+        rope_scaling_alpha = self.rope_scaling.get(\"alpha\", None)\n+        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n+            raise ValueError(\n+                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n+            )\n+        if rope_scaling_factor is None and rope_scaling_alpha is None:\n+            raise ValueError(\"`rope_scaling`'s factor or alpha field must be have one, got both of none\")\n+        if rope_scaling_factor is not None:\n+            if not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n+                raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1.0, got {rope_scaling_factor}\")\n+        if rope_scaling_alpha is not None:\n+            if not isinstance(rope_scaling_alpha, float) or rope_scaling_alpha <= 1.0:\n+                raise ValueError(f\"`rope_scaling`'s alpha field must be a float > 1.0, got {rope_scaling_alpha}\")\n+\n+\n+__all__ = [\"HunYuanMoEV1Config\"]"
        },
        {
            "sha": "0b2df41ba59e8043fc88972b7a91c38b2e73f67b",
            "filename": "src/transformers/models/hunyuan_v1_moe/modeling_hunyuan_v1_moe.py",
            "status": "added",
            "additions": 591,
            "deletions": 0,
            "changes": 591,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodeling_hunyuan_v1_moe.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,591 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_hunyuan_v1_moe.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+from torch import nn\n+\n+from transformers.cache_utils import Cache\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n+from ...modeling_layers import GenericForSequenceClassification, GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class HunYuanMoEV1RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        HunYuanMoEV1RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+class HunYuanMoEV1MLP(nn.Module):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx=None, is_shared_mlp=False):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+        self.layer_idx = layer_idx\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed, k_embed\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+class HunYuanMoEV1Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.query_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.key_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = self.query_layernorm(query_states)\n+        key_states = self.key_layernorm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class HunYuanMoEV1Gate(nn.Module):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n+        self.wg = nn.Linear(config.hidden_size, num_experts, bias=False, dtype=torch.float32)\n+\n+    def forward(self, hidden_states):\n+        bsz, seq_len, hidden_size = hidden_states.shape\n+        hidden_states = hidden_states.reshape(-1, hidden_size)\n+        if self.wg.weight.dtype == torch.float32:\n+            hidden_states = hidden_states.float()\n+        logits = self.wg(hidden_states)\n+        return logits\n+\n+\n+class HunYuanMoEV1Moe(nn.Module):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n+        self.top_k = config.moe_topk if isinstance(config.moe_topk, int) else config.moe_topk[layer_idx]\n+        self.gate = HunYuanMoEV1Gate(config, layer_idx=layer_idx)\n+        # self.wg = nn.Linear(config.hidden_size, config.num_experts, bias=False, dtype=torch.float32)\n+        self.experts = nn.ModuleList(\n+            [HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=False) for _ in range(self.num_experts)]\n+        )\n+\n+        self.shared_mlp = HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_mlp = self.shared_mlp(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states + hidden_states_mlp\n+\n+\n+class HunYuanMoEV1DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = HunYuanMoEV1Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = HunYuanMoEV1Moe(config, layer_idx=layer_idx)\n+        self.input_layernorm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.layer_idx = layer_idx\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states = self.input_layernorm(hidden_states)\n+        # Self Attention\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class HunYuanMoEV1PreTrainedModel(PreTrainedModel):\n+    config: HunYuanMoEV1Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"HunYuanMoEV1DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": HunYuanMoEV1DecoderLayer,\n+        \"attentions\": HunYuanMoEV1Attention,\n+    }\n+\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+class HunYuanMoEV1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: HunYuanMoEV1Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n+            # DynamicNTKAlphaRotary\n+            self.dim = config.head_dim\n+            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            self.attention_scaling = 1.0\n+        else:\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+@auto_docstring\n+class HunYuanMoEV1Model(HunYuanMoEV1PreTrainedModel):\n+    def __init__(self, config: HunYuanMoEV1Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [HunYuanMoEV1DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.rotary_emb = HunYuanMoEV1RotaryEmbedding(config=config)\n+        self.gradient_checkpointing = False\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n+        )\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class HunYuanMoEV1ForCausalLM(HunYuanMoEV1PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = HunYuanMoEV1Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, HunYuanMoEV1ForCausalLM\n+\n+        >>> model = HunYuanMoEV1ForCausalLM.from_pretrained(\"meta-hunyuan_v1_moe/HunYuanMoEV1-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-hunyuan_v1_moe/HunYuanMoEV1-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+class HunYuanMoEV1ForSequenceClassification(GenericForSequenceClassification, HunYuanMoEV1PreTrainedModel):\n+    pass\n+\n+\n+__all__ = [\n+    \"HunYuanMoEV1ForCausalLM\",\n+    \"HunYuanMoEV1Model\",\n+    \"HunYuanMoEV1PreTrainedModel\",\n+    \"HunYuanMoEV1ForSequenceClassification\",\n+]"
        },
        {
            "sha": "66801569efb8960aef9ef144baf3f16b2ec83703",
            "filename": "src/transformers/models/hunyuan_v1_moe/modular_hunyuan_v1_moe.py",
            "status": "added",
            "additions": 272,
            "deletions": 0,
            "changes": 272,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhunyuan_v1_moe%2Fmodular_hunyuan_v1_moe.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,272 @@\n+# coding=utf-8\n+# Copyright (C) 2025 THL A29 Limited, a Tencent company and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"PyTorch HunYuanMoEV1 model.\"\"\"\n+\n+from typing import Callable, Optional\n+\n+import torch\n+import torch.nn.functional as F\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from transformers.cache_utils import Cache\n+from transformers.utils import (\n+    logging,\n+)\n+\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaDecoderLayer,\n+    LlamaForCausalLM,\n+    LlamaForSequenceClassification,\n+    LlamaMLP,\n+    LlamaModel,\n+    LlamaPreTrainedModel,\n+    LlamaRMSNorm,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_hunyuan_v1_moe import HunYuanMoEV1Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class HunYuanMoEV1RMSNorm(LlamaRMSNorm):\n+    pass\n+\n+\n+class HunYuanMoEV1MLP(LlamaMLP):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx=None, is_shared_mlp=False):\n+        super().__init__(config)\n+        self.layer_idx = layer_idx\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class HunYuanMoEV1Attention(LlamaAttention):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.query_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+        self.key_layernorm = HunYuanMoEV1RMSNorm(self.head_dim, eps=config.rms_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, torch.Tensor]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+        query_states = self.query_layernorm(query_states)\n+        key_states = self.key_layernorm(key_states)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class HunYuanMoEV1Gate(nn.Module):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n+        self.wg = nn.Linear(config.hidden_size, num_experts, bias=False, dtype=torch.float32)\n+\n+    def forward(self, hidden_states):\n+        bsz, seq_len, hidden_size = hidden_states.shape\n+        hidden_states = hidden_states.reshape(-1, hidden_size)\n+        if self.wg.weight.dtype == torch.float32:\n+            hidden_states = hidden_states.float()\n+        logits = self.wg(hidden_states)\n+        return logits\n+\n+\n+class HunYuanMoEV1Moe(nn.Module):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.num_experts = config.num_experts if isinstance(config.num_experts, int) else config.num_experts[layer_idx]\n+        self.top_k = config.moe_topk if isinstance(config.moe_topk, int) else config.moe_topk[layer_idx]\n+        self.gate = HunYuanMoEV1Gate(config, layer_idx=layer_idx)\n+        # self.wg = nn.Linear(config.hidden_size, config.num_experts, bias=False, dtype=torch.float32)\n+        self.experts = nn.ModuleList(\n+            [HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=False) for _ in range(self.num_experts)]\n+        )\n+\n+        self.shared_mlp = HunYuanMoEV1MLP(config, layer_idx=layer_idx, is_shared_mlp=True)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        batch_size, sequence_length, hidden_dim = hidden_states.shape\n+        hidden_states_mlp = self.shared_mlp(hidden_states)\n+        router_logits = self.gate(hidden_states)\n+        hidden_states = hidden_states.view(-1, hidden_dim)\n+        # router_logits: (batch * sequence_length, n_experts)\n+\n+        routing_weights = F.softmax(router_logits, dim=1, dtype=torch.float)\n+        routing_weights, selected_experts = torch.topk(routing_weights, self.top_k, dim=-1)\n+        routing_weights /= routing_weights.sum(dim=-1, keepdim=True)\n+        # we cast back to the input dtype\n+        routing_weights = routing_weights.to(hidden_states.dtype)\n+\n+        final_hidden_states = torch.zeros(\n+            (batch_size * sequence_length, hidden_dim), dtype=hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # One hot encode the selected experts to create an expert mask\n+        # this will be used to easily index which expert is going to be sollicitated\n+        expert_mask = torch.nn.functional.one_hot(selected_experts, num_classes=self.num_experts).permute(2, 1, 0)\n+\n+        # Loop over all available experts in the model and perform the computation on each expert\n+        expert_hit = torch.greater(expert_mask.sum(dim=(-1, -2)), 0).nonzero()\n+        for expert_idx in expert_hit:\n+            expert_layer = self.experts[expert_idx]\n+            idx, top_x = torch.where(expert_mask[expert_idx].squeeze(0))\n+\n+            # Index the correct hidden states and compute the expert hidden state for\n+            # the current expert. We need to make sure to multiply the output hidden\n+            # states by `routing_weights` on the corresponding tokens (top-1 and top-2)\n+            current_state = hidden_states[None, top_x].reshape(-1, hidden_dim)\n+            current_hidden_states = expert_layer(current_state) * routing_weights[top_x, idx, None]\n+\n+            # However `index_add_` only support torch tensors for indexing so we'll use\n+            # the `top_x` tensor here.\n+            final_hidden_states.index_add_(0, top_x, current_hidden_states.to(hidden_states.dtype))\n+        final_hidden_states = final_hidden_states.reshape(batch_size, sequence_length, hidden_dim)\n+        return final_hidden_states + hidden_states_mlp\n+\n+\n+class HunYuanMoEV1DecoderLayer(LlamaDecoderLayer):\n+    def __init__(self, config: HunYuanMoEV1Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = HunYuanMoEV1Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = HunYuanMoEV1Moe(config, layer_idx=layer_idx)\n+        self.input_layernorm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_attention_layernorm = HunYuanMoEV1RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.layer_idx = layer_idx\n+\n+\n+class HunYuanMoEV1PreTrainedModel(LlamaPreTrainedModel):\n+    def _init_weights(self, module):\n+        std = self.config.initializer_range\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.padding_idx is not None:\n+                module.weight.data[module.padding_idx].zero_()\n+\n+\n+class HunYuanMoEV1RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: HunYuanMoEV1Config, device=None):\n+        super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        if self.rope_type == \"dynamic\" and config.rope_scaling[\"alpha\"]:\n+            # DynamicNTKAlphaRotary\n+            self.dim = config.head_dim\n+            base = config.rope_theta * config.rope_scaling.get(\"alpha\") ** (self.dim / (self.dim - 2))\n+            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2).float().to(device) / self.dim))\n+            self.attention_scaling = 1.0\n+        else:\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class HunYuanMoEV1Model(LlamaModel):\n+    pass\n+\n+\n+class HunYuanMoEV1ForCausalLM(LlamaForCausalLM):\n+    pass\n+\n+\n+class HunYuanMoEV1ForSequenceClassification(LlamaForSequenceClassification):\n+    pass\n+\n+\n+__all__ = [\n+    \"HunYuanMoEV1ForCausalLM\",\n+    \"HunYuanMoEV1Model\",\n+    \"HunYuanMoEV1PreTrainedModel\",\n+    \"HunYuanMoEV1ForSequenceClassification\",\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/hunyuan_v1_dense/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_dense%2F__init__.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff"
        },
        {
            "sha": "122f94e3d53514ae1c3397f23e6f3980021c245e",
            "filename": "tests/models/hunyuan_v1_dense/test_modeling_hunyuan_v1_dense.py",
            "status": "added",
            "additions": 93,
            "deletions": 0,
            "changes": 93,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_dense%2Ftest_modeling_hunyuan_v1_dense.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,93 @@\n+# Copyright (C) 2024 THL A29 Limited, a Tencent company and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch HunYuanDenseV1 model.\"\"\"\n+\n+import unittest\n+\n+from transformers import HunYuanDenseV1Config, is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    from transformers import (\n+        HunYuanDenseV1ForCausalLM,\n+        HunYuanDenseV1ForSequenceClassification,\n+        HunYuanDenseV1Model,\n+    )\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class HunYuanDenseV1ModelTester(CausalLMModelTester):\n+    config_class = HunYuanDenseV1Config\n+    if is_torch_available():\n+        base_model_class = HunYuanDenseV1Model\n+        causal_lm_class = HunYuanDenseV1ForCausalLM\n+        sequence_class = HunYuanDenseV1ForSequenceClassification\n+\n+\n+@require_torch\n+class HunYuanDenseV1ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            HunYuanDenseV1Model,\n+            HunYuanDenseV1ForCausalLM,\n+            HunYuanDenseV1ForSequenceClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = HunYuanDenseV1ModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": HunYuanDenseV1Model,\n+            \"text-generation\": HunYuanDenseV1ForCausalLM,\n+            \"text-classification\": HunYuanDenseV1ForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        return True\n+\n+\n+@require_torch\n+class HunYuanDenseV1IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_generation(self):\n+        # TODO Need new Dense Model\n+        return True"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/hunyuan_v1_moe/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2F__init__.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff"
        },
        {
            "sha": "6194d4eec8c8f6790b2521166cd3a8e7ea8d6dec",
            "filename": "tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py",
            "status": "added",
            "additions": 132,
            "deletions": 0,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cf487cdf1f61dae1465b4dff20a693a9b6ad6aff/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py?ref=cf487cdf1f61dae1465b4dff20a693a9b6ad6aff",
            "patch": "@@ -0,0 +1,132 @@\n+# Copyright (C) 2024 THL A29 Limited, a Tencent company and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch HunYuanMoEV1 model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+\n+from transformers import HunYuanMoEV1Config, is_torch_available\n+from transformers.testing_utils import (\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+\n+if is_torch_available():\n+    from transformers import (\n+        AutoModelForCausalLM,\n+        AutoTokenizer,\n+        HunYuanMoEV1ForCausalLM,\n+        HunYuanMoEV1ForSequenceClassification,\n+        HunYuanMoEV1Model,\n+    )\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+\n+\n+class HunYuanMoEV1ModelTester(CausalLMModelTester):\n+    config_class = HunYuanMoEV1Config\n+    if is_torch_available():\n+        base_model_class = HunYuanMoEV1Model\n+        causal_lm_class = HunYuanMoEV1ForCausalLM\n+        sequence_class = HunYuanMoEV1ForSequenceClassification\n+\n+\n+@require_torch\n+class HunYuanMoEV1ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (\n+        (\n+            HunYuanMoEV1Model,\n+            HunYuanMoEV1ForCausalLM,\n+            HunYuanMoEV1ForSequenceClassification,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    model_tester_class = HunYuanMoEV1ModelTester\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": HunYuanMoEV1Model,\n+            \"text-generation\": HunYuanMoEV1ForCausalLM,\n+            \"text-classification\": HunYuanMoEV1ForSequenceClassification,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+\n+    def is_pipeline_test_to_skip(\n+        self,\n+        pipeline_test_case_name,\n+        config_class,\n+        model_architecture,\n+        tokenizer_name,\n+        image_processor_name,\n+        feature_extractor_name,\n+        processor_name,\n+    ):\n+        return True\n+\n+    @unittest.skip(\"Hunyuan model Unsupported\")\n+    @pytest.mark.torch_compile_test\n+    def test_generate_compilation_all_outputs(self):\n+        pass\n+\n+    @unittest.skip(\"Hunyuan model Unsupported\")\n+    @pytest.mark.torch_compile_test\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n+    @unittest.skip(\"Hunyuan model Unsupported\")\n+    def test_generate_from_inputs_embeds_with_static_cache(self):\n+        pass\n+\n+    @unittest.skip(\"Hunyuan model Unsupported\")\n+    def test_generate_with_static_cache(self):\n+        pass\n+\n+\n+@require_torch\n+class HunYuanMoEV1IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_generation(self):\n+        # we will compele this when model file change over\n+        # pass\n+        EXPECTED_ANSWER = \"\\nOkay, I need to write a short summary about the benefits of regular exercise. Let me start by recalling what I know. First,\"\n+        prompt = \"Write a short summary of the benefits of regular exercise\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"tencent/Hunyuan-A13B-Instruct\")\n+        model = AutoModelForCausalLM.from_pretrained(\"tencent/Hunyuan-A13B-Instruct\", device_map=\"auto\")\n+        messages = [\n+            {\"role\": \"user\", \"content\": prompt},\n+        ]\n+        tokenized_chat = tokenizer.apply_chat_template(\n+            messages,\n+            tokenize=True,\n+            add_generation_prompt=True,\n+            return_tensors=\"pt\",\n+        )\n+        generated_ids = model.generate(tokenized_chat.to(model.device), max_new_tokens=30, top_k=1)\n+        text = tokenizer.decode(generated_ids[0])\n+        output = text.split(\"<think>\")[1]\n+        self.assertEqual(EXPECTED_ANSWER, output)"
        }
    ],
    "stats": {
        "total": 2342,
        "additions": 2342,
        "deletions": 0
    }
}