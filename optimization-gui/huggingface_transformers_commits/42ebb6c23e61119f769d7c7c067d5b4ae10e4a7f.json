{
    "author": "gante",
    "message": "[tests] Parameterized `test_eager_matches_sdpa_inference` (#36650)",
    "sha": "42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
    "files": [
        {
            "sha": "230bbc9575791dc8a035eadd33916f300ed21d92",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -17,11 +17,10 @@\n import unittest\n \n from packaging import version\n-from parameterized import parameterized\n \n from transformers import AlbertConfig, AutoTokenizer, is_torch_available\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_torch, require_torch_sdpa, slow, torch_device\n+from transformers.testing_utils import require_torch, slow, torch_device\n \n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, ids_tensor, random_attention_mask\n@@ -289,12 +288,6 @@ def setUp(self):\n         self.model_tester = AlbertModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=AlbertConfig, hidden_size=37)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @unittest.skip(\"Albert requires `head_mask` which is currently not done in this test.\")\n-    def test_eager_matches_sdpa_inference(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "d7d2bd9183d3ad403ea1c17a2a0be026bc43192e",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -256,11 +256,6 @@ def test_model_outputs_equivalence(self, **kwargs):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_inference(self):\n-        pass\n-\n     @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_eager_matches_sdpa_generate(self):\n         pass"
        },
        {
            "sha": "99abd4fa244dd947732449ef1f54831ad899ce58",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 195,
            "changes": 197,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -14,35 +14,28 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch BEiT model.\"\"\"\n \n-import inspect\n-import tempfile\n import unittest\n \n-import numpy as np\n from datasets import load_dataset\n from packaging import version\n-from parameterized import parameterized\n \n from transformers import BeitConfig\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_multi_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n )\n from transformers.utils import (\n     cached_property,\n     is_torch_available,\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n     is_vision_available,\n )\n \n from ...test_backbone_common import BackboneTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -119,6 +112,7 @@ def __init__(\n         # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.mask_length = self.seq_length - 1\n         self.num_masks = int(mask_ratio * self.seq_length)\n         self.attn_implementation = attn_implementation\n \n@@ -414,193 +408,6 @@ def test_model_from_pretrained(self):\n         model = BeitModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        # The common test modifies the num_hidden_layers to be 1. However, for Beit we want to\n-        # avoid that because the num_hidden_layers is generally assumed to be 4. Also, the code\n-        # related to attention masks in the original common tests is not required as the Beit\n-        # model does not handle attention masks. Furthermore, some extra code like modifying\n-        # the norm layers eps values for specialized configs and checking for the 'noise'\n-        # has been omitted to simply the test.\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            config.rms_norm_eps = 1.0\n-            config.layer_norm_eps = 1.0\n-            config.norm_eps = 1.0\n-            config.norm_epsilon = 1.0\n-            config.layer_norm_epsilon = 1.0\n-\n-            model = model_class(config)\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype, use_mask_token=True)\n-                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                    use_mask_token=True,\n-                )\n-                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n-\n-                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n-                for x in model_eager.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n-                for x in model_sdpa.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for output_attentions in [True, False]:\n-                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                                continue\n-                            # TODO: if we can also check with `batch_size=1` without being flaky?\n-                            for batch_size in [7]:\n-                                dummy_input = inputs_dict[model.main_input_name]\n-\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    dummy_input = dummy_input.to(torch_dtype)\n-\n-                                dummy_input = dummy_input[:batch_size]\n-                                for enable_kernels in [False, True]:\n-                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, enable_kernels={enable_kernels}\"\n-                                    processed_inputs = {\n-                                        model.main_input_name: dummy_input,\n-                                        \"output_hidden_states\": True,\n-                                    }\n-\n-                                    if (\n-                                        self.has_attentions\n-                                        and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                                    ):\n-                                        processed_inputs[\"output_attentions\"] = output_attentions\n-\n-                                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n-                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-                                        mask_length = self.model_tester.seq_length - 1 - dummy_mask.size(0)\n-                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                                    with torch.no_grad():\n-                                        with sdpa_kernel(\n-                                            enable_flash=enable_kernels,\n-                                            enable_math=True,\n-                                            enable_mem_efficient=enable_kernels,\n-                                        ):\n-                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                                            outputs_eager = model_eager(**prepared_inputs)\n-                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                                    logits_eager = outputs_eager.hidden_states[-1]\n-                                    logits_sdpa = outputs_sdpa.hidden_states[-1]\n-                                    if torch_device in [\"cpu\", \"cuda\"]:\n-                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                    elif torch_device == \"xpu\":\n-                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                        # which is implemented on PyTorch level using aten operators and is\n-                                        # device agnostic with respect to implementation of each aten operator.\n-                                        atol = atols[\"cuda\", False, torch_dtype]\n-                                        rtol = rtols[\"cuda\", False, torch_dtype]\n-                                    else:\n-                                        atol = 1e-7\n-                                        rtol = 1e-4\n-\n-                                    # Masked tokens output slightly deviates - we don't mind that.\n-                                    if use_mask:\n-                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                        _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                        _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                        if padding_side == \"left\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                        elif padding_side == \"right\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                        logits_sdpa = _logits_sdpa\n-                                        logits_eager = _logits_eager\n-\n-                                    results = [\n-                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                    ]\n-                                    # If 80% batch elements have matched results, it's fine\n-                                    if np.mean(results) < 0.8:\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "418cf50adab4805ac6114c90bacb75e5a4b76204",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -76,11 +76,6 @@ def test_model_outputs_equivalence(self, **kwargs):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_inference(self):\n-        pass\n-\n     @unittest.skip(\"Cohere2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_eager_matches_sdpa_generate(self):\n         pass"
        },
        {
            "sha": "70ff0ed8bd7ce2747b669fd40fb529f090c5fc5a",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -20,7 +20,6 @@\n \n import torch\n from datasets import load_dataset\n-from parameterized import parameterized\n \n from tests.test_configuration_common import ConfigTester\n from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n@@ -32,7 +31,6 @@\n from transformers.models.colpali.processing_colpali import ColPaliProcessor\n from transformers.testing_utils import (\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -271,14 +269,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @require_torch_sdpa\n-    @slow\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        self.skipTest(\n-            \"Due to custom causal mask, there is a slightly too big difference between eager and sdpa in bfloat16.\"\n-        )\n-\n     @unittest.skip(\n         reason=\"From PaliGemma: Some undefined behavior encountered with test versions of this model. Skip for now.\"\n     )"
        },
        {
            "sha": "3ec0ae6c60b25b2be672b7da5b1bf4a58804c3b4",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 197,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -14,32 +14,24 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Data2VecVision model.\"\"\"\n \n-import inspect\n-import tempfile\n import unittest\n \n-import numpy as np\n-from parameterized import parameterized\n-\n from transformers import Data2VecVisionConfig\n from transformers.testing_utils import (\n     require_torch,\n     require_torch_multi_gpu,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n )\n from transformers.utils import (\n     cached_property,\n     is_torch_available,\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n     is_vision_available,\n )\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -111,6 +103,7 @@ def __init__(\n         # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.mask_length = self.seq_length - 1\n         self.num_masks = int(mask_ratio * self.seq_length)\n         self.attn_implementation = attn_implementation\n \n@@ -319,194 +312,6 @@ def test_model_from_pretrained(self):\n         model = Data2VecVisionModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    # Copied from tests.models.beit.test_modeling_beit.BeitModelTest.test_eager_matches_sdpa_inference with Beit->Data2VecVision\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        # The common test modifies the num_hidden_layers to be 1. However, for Data2VecVision we want to\n-        # avoid that because the num_hidden_layers is generally assumed to be 4. Also, the code\n-        # related to attention masks in the original common tests is not required as the Data2VecVision\n-        # model does not handle attention masks. Furthermore, some extra code like modifying\n-        # the norm layers eps values for specialized configs and checking for the 'noise'\n-        # has been omitted to simply the test.\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            config.rms_norm_eps = 1.0\n-            config.layer_norm_eps = 1.0\n-            config.norm_eps = 1.0\n-            config.norm_epsilon = 1.0\n-            config.layer_norm_epsilon = 1.0\n-\n-            model = model_class(config)\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype, use_mask_token=True)\n-                model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                    use_mask_token=True,\n-                )\n-                model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n-\n-                # Another way to make sure norm layers have desired epsilon. (Some models don't set it from its config.)\n-                for x in model_eager.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n-                for x in model_sdpa.modules():\n-                    if isinstance(x, (nn.LayerNorm, nn.GroupNorm)):\n-                        x.eps = 1.0\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for output_attentions in [True, False]:\n-                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                                continue\n-                            # TODO: if we can also check with `batch_size=1` without being flaky?\n-                            for batch_size in [7]:\n-                                dummy_input = inputs_dict[model.main_input_name]\n-\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    dummy_input = dummy_input.to(torch_dtype)\n-\n-                                dummy_input = dummy_input[:batch_size]\n-                                for enable_kernels in [False, True]:\n-                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, enable_kernels={enable_kernels}\"\n-                                    processed_inputs = {\n-                                        model.main_input_name: dummy_input,\n-                                        \"output_hidden_states\": True,\n-                                    }\n-\n-                                    if (\n-                                        self.has_attentions\n-                                        and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                                    ):\n-                                        processed_inputs[\"output_attentions\"] = output_attentions\n-\n-                                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n-                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-                                        mask_length = self.model_tester.seq_length - 1 - dummy_mask.size(0)\n-                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                                    with torch.no_grad():\n-                                        with sdpa_kernel(\n-                                            enable_flash=enable_kernels,\n-                                            enable_math=True,\n-                                            enable_mem_efficient=enable_kernels,\n-                                        ):\n-                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                                            outputs_eager = model_eager(**prepared_inputs)\n-                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                                    logits_eager = outputs_eager.hidden_states[-1]\n-                                    logits_sdpa = outputs_sdpa.hidden_states[-1]\n-                                    if torch_device in [\"cpu\", \"cuda\"]:\n-                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                    elif torch_device == \"xpu\":\n-                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                        # which is implemented on PyTorch level using aten operators and is\n-                                        # device agnostic with respect to implementation of each aten operator.\n-                                        atol = atols[\"cuda\", False, torch_dtype]\n-                                        rtol = rtols[\"cuda\", False, torch_dtype]\n-                                    else:\n-                                        atol = 1e-7\n-                                        rtol = 1e-4\n-\n-                                    # Masked tokens output slightly deviates - we don't mind that.\n-                                    if use_mask:\n-                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                        _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                        _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                        if padding_side == \"left\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                        elif padding_side == \"right\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                        logits_sdpa = _logits_sdpa\n-                                        logits_eager = _logits_eager\n-\n-                                    results = [\n-                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                    ]\n-                                    # If 80% batch elements have matched results, it's fine\n-                                    if np.mean(results) < 0.8:\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "d6db7079cb0a17c37443f1fe655769ce7adec180",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -91,11 +91,6 @@ def test_model_outputs_equivalence(self, **kwargs):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n-    def test_eager_matches_sdpa_inference(self):\n-        pass\n-\n     @unittest.skip(\"Gemma2's eager attn/sdpa attn outputs are expected to be different\")\n     def test_eager_matches_sdpa_generate(self):\n         pass"
        },
        {
            "sha": "d09b5f7850594486813c99a48e50e4cd87430033",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -25,7 +25,6 @@\n     TestCasePlus,\n     require_bitsandbytes,\n     require_torch,\n-    require_torch_sdpa,\n     require_vision,\n     slow,\n     torch_device,\n@@ -34,7 +33,13 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+    random_attention_mask,\n+)\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -311,16 +316,12 @@ def prepare_config_and_inputs_for_common(self):\n     def prepare_pixel_values(self):\n         return floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n \n-    @require_torch_sdpa\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n-\n-    @require_torch_sdpa\n-    @slow\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_generate(self):\n-        self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @unittest.skip(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        pass\n \n \n @require_torch\n@@ -349,10 +350,11 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n         return inputs_dict\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @unittest.skip(\"Idefics requires both text and image inputs which is currently not done in this test.\")\n-    def test_eager_matches_sdpa_inference(self):\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n         pass\n \n     def test_model_outputs_equivalence(self):\n@@ -597,10 +599,11 @@ def setUp(self):\n         )\n         self.config_tester = ConfigTester(self, config_class=IdeficsConfig, hidden_size=37)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @unittest.skip(\"Idefics requires both text and image inputs which is currently not done in this test.\")\n-    def test_eager_matches_sdpa_inference(self, torch_dtype):\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n         pass\n \n     @pytest.mark.generate"
        },
        {
            "sha": "43219155c15f5e87d802bb809ab2e85539290408",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 292,
            "changes": 293,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -21,7 +21,6 @@\n \n import numpy as np\n from datasets import Audio, load_dataset\n-from parameterized import parameterized\n from pytest import mark\n \n from transformers import AutoFeatureExtractor, MimiConfig\n@@ -31,17 +30,12 @@\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n-    require_torch_sdpa,\n     slow,\n     torch_device,\n )\n-from transformers.utils import (\n-    is_torch_bf16_available_on_device,\n-    is_torch_fp16_available_on_device,\n-)\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor, sdpa_kernel\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n \n \n if is_torch_available():\n@@ -409,291 +403,6 @@ def test_identity_shortcut(self):\n         config.use_conv_shortcut = False\n         self.model_tester.create_and_check_model_forward(config, inputs_dict)\n \n-    # Overwrite to use `audio_values` as the tensors to compare.\n-    # TODO: Try to do this in the parent class.\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        if torch_dtype == \"float16\" and torch_device == \"cpu\":\n-            self.skipTest(\"`replication_pad1d` not implemented for 'Half\")\n-\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-            # FIXME: we deactivate boolean mask for models using \"use_mask_token\" in their constructors.\n-            # These models support masking only in the case `use_mask_token=True`. Otherwise they cannot consume an input mask.\n-            # This means that the class needs to be instantiated much later, after `use_mask` is set, which means a significant refactor of the code.\n-            # However masking there is not done at any layers that matters (i.e self-attention), therefore we can safely deactivate it.\n-            deactivate_mask = \"use_mask_token\" in inspect.signature(model_class).parameters\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n-\n-                for name, submodule in model_eager.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        raise ValueError(\"The eager model should not have SDPA attention layers\")\n-\n-                has_sdpa = False\n-                for name, submodule in model_sdpa.named_modules():\n-                    class_name = submodule.__class__.__name__\n-                    if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n-                        has_sdpa = True\n-                        break\n-                if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n-                    raise ValueError(\"The SDPA model should have SDPA attention layers\")\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for output_attentions in [True, False]:\n-                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                                continue\n-                            for batch_size in [7]:\n-                                dummy_input = inputs_dict[model.main_input_name]\n-\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    dummy_input = dummy_input.to(torch_dtype)\n-\n-                                dummy_input = dummy_input[:batch_size]\n-                                if dummy_input.shape[0] != batch_size:\n-                                    if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                        extension = torch.rand(\n-                                            batch_size - dummy_input.shape[0],\n-                                            *dummy_input.shape[1:],\n-                                            dtype=torch_dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                    else:\n-                                        extension = torch.randint(\n-                                            high=5,\n-                                            size=(batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                            dtype=dummy_input.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                                if not use_mask:\n-                                    dummy_attention_mask = None\n-                                else:\n-                                    dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                    if dummy_attention_mask is None:\n-                                        if is_encoder_decoder:\n-                                            seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                        else:\n-                                            seqlen = dummy_input.shape[-1]\n-                                        dummy_attention_mask = (\n-                                            torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                        )\n-\n-                                    dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                    if dummy_attention_mask.shape[0] != batch_size:\n-                                        extension = torch.ones(\n-                                            batch_size - dummy_attention_mask.shape[0],\n-                                            *dummy_attention_mask.shape[1:],\n-                                            dtype=dummy_attention_mask.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                        dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                    dummy_attention_mask[:] = 1\n-                                    if padding_side == \"left\":\n-                                        dummy_attention_mask[-1, :2] = 0\n-                                        dummy_attention_mask[-1, 2:] = 1\n-                                    elif padding_side == \"right\":\n-                                        dummy_attention_mask[-1, -2:] = 0\n-                                        dummy_attention_mask[-1, :-2] = 1\n-\n-                                for enable_kernels in [False, True]:\n-                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-                                    if is_encoder_decoder:\n-                                        decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[\n-                                            :batch_size\n-                                        ]\n-                                        if decoder_input_ids.shape[0] != batch_size:\n-                                            extension = torch.ones(\n-                                                batch_size - decoder_input_ids.shape[0],\n-                                                *decoder_input_ids.shape[1:],\n-                                                dtype=decoder_input_ids.dtype,\n-                                                device=torch_device,\n-                                            )\n-                                            decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                                            decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                                        # TODO: never an `attention_mask` arg here?\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"decoder_input_ids\": decoder_input_ids,\n-                                            \"decoder_attention_mask\": dummy_attention_mask,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-                                    else:\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-\n-                                        # Otherwise fails for e.g. WhisperEncoderModel\n-                                        if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                                            processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                                        if (\n-                                            self.has_attentions\n-                                            and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                                        ):\n-                                            processed_inputs[\"output_attentions\"] = output_attentions\n-                                    if not deactivate_mask and (\n-                                        \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters\n-                                    ):\n-                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-\n-                                        # In case of additional token (like class) we define a custom `mask_length`\n-                                        if hasattr(self.model_tester, \"mask_length\"):\n-                                            mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n-                                        else:\n-                                            mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n-                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                                    if \"noise\" in inspect.signature(model_eager.forward).parameters:\n-                                        np.random.seed(2)\n-                                        num_patches = int(\n-                                            (self.model_tester.image_size // self.model_tester.patch_size) ** 2\n-                                        )\n-                                        noise = np.random.uniform(size=(batch_size, num_patches))\n-                                        processed_inputs[\"noise\"] = torch.from_numpy(noise)\n-\n-                                    # TODO: test gradients as well (& for FA2 as well!)\n-                                    with torch.no_grad():\n-                                        with sdpa_kernel(\n-                                            enable_flash=enable_kernels,\n-                                            enable_math=True,\n-                                            enable_mem_efficient=enable_kernels,\n-                                        ):\n-                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                                            outputs_eager = model_eager(**prepared_inputs)\n-                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                                    # Ignore copy\n-                                    logits_eager = outputs_eager.audio_values\n-                                    # Ignore copy\n-                                    logits_sdpa = outputs_sdpa.audio_values\n-\n-                                    if torch_device in [\"cpu\", \"cuda\"]:\n-                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                    elif torch_device == \"xpu\":\n-                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                        # which is implemented on PyTorch level using aten operators and is\n-                                        # device agnostic with respect to implementation of each aten operator.\n-                                        atol = atols[\"cuda\", False, torch_dtype]\n-                                        rtol = rtols[\"cuda\", False, torch_dtype]\n-                                    else:\n-                                        atol = 1e-7\n-                                        rtol = 1e-4\n-\n-                                    # Masked tokens output slightly deviates - we don't mind that.\n-                                    if use_mask:\n-                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                        _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                        _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                        if padding_side == \"left\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                        elif padding_side == \"right\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                        logits_sdpa = _logits_sdpa\n-                                        logits_eager = _logits_eager\n-\n-                                    results = [\n-                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                    ]\n-                                    # If 80% batch elements have matched results, it's fine\n-                                    if np.mean(results) < 0.8:\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        },
        {
            "sha": "e6ad1b639e9271e77950a37c2c4ed7fc704b5a07",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 19,
            "deletions": 10,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -44,7 +44,12 @@\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+)\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n@@ -188,11 +193,15 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         logits_processor_kwargs = {}\n         return logits_processor_kwargs\n \n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n-    @slow\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        self.skipTest(reason=\"Moshi has no strict equivalence between two modes, skipping this test.\")\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        if use_attention_mask or (not use_attention_mask and torch_dtype == \"fp32\" and not output_attentions):\n+            self.skipTest(\"Test is failing, fix me :) \")\n+        parent_parameterized_test = getattr(ModelTesterMixin, self._testMethodName)\n+        parent_parameterized_test(self)\n \n     # Copied from tests.test_modeling_common.ModelTesterMixin.test_resize_tokens_embeddings\n     def test_resize_tokens_embeddings(self):\n@@ -620,11 +629,11 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass\n \n-    @unittest.skip(\"Adapting this test is costly. `test_eager_matches_sdpa_generate` tests this already.\")\n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    @slow\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @unittest.skip(reason=\"Unimplemented. Relies on `test_eager_matches_sdpa_generate` to check correctness.\")\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n         pass\n \n     @unittest.skip(reason=\"The Moshi model does not have support dynamic compile yet\")"
        },
        {
            "sha": "c8faac1b7d3cb7544dfcce1c646eead361dfca06",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 477,
            "changes": 478,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -21,7 +21,6 @@\n import unittest\n \n import numpy as np\n-from parameterized import parameterized\n from pytest import mark\n \n from transformers import (\n@@ -43,7 +42,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_bf16_available_on_device, is_torch_fp16_available_on_device\n+from transformers.utils import cached_property\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -452,226 +451,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for batch_size in [7]:\n-                            # Ignore copy\n-                            batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n-                            dummy_input = inputs_dict[model.main_input_name]\n-\n-                            if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                dummy_input = dummy_input.to(torch_dtype)\n-\n-                            # Ignore copy\n-                            dummy_input = dummy_input[:batch_size_input_ids]\n-                            # Ignore copy\n-                            if dummy_input.shape[0] != batch_size_input_ids:\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    # Ignore copy\n-                                    extension = torch.rand(\n-                                        batch_size_input_ids - dummy_input.shape[0],\n-                                        *dummy_input.shape[1:],\n-                                        dtype=torch_dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                else:\n-                                    # Ignore copy\n-                                    extension = torch.randint(\n-                                        high=5,\n-                                        size=(batch_size_input_ids - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                        dtype=dummy_input.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                            if not use_mask:\n-                                dummy_attention_mask = None\n-                            else:\n-                                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                if dummy_attention_mask is None:\n-                                    if is_encoder_decoder:\n-                                        seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                    else:\n-                                        seqlen = dummy_input.shape[-1]\n-                                    dummy_attention_mask = (\n-                                        torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                    )\n-\n-                                dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                if dummy_attention_mask.shape[0] != batch_size:\n-                                    extension = torch.ones(\n-                                        batch_size - dummy_attention_mask.shape[0],\n-                                        *dummy_attention_mask.shape[1:],\n-                                        dtype=dummy_attention_mask.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                    dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                dummy_attention_mask[:] = 1\n-                                if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :2] = 0\n-                                    dummy_attention_mask[-1, 2:] = 1\n-                                elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, -2:] = 0\n-                                    dummy_attention_mask[-1, :-2] = 1\n-\n-                            for enable_kernels in [False, True]:\n-                                failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-\n-                                other_inputs = {\n-                                    \"output_hidden_states\": True,\n-                                }\n-\n-                                # Otherwise fails for e.g. WhisperEncoderModel\n-                                if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                                # TODO: test gradients as well (& for FA2 as well!)\n-                                with torch.no_grad():\n-                                    with sdpa_kernel(\n-                                        enable_flash=enable_kernels,\n-                                        enable_math=True,\n-                                        enable_mem_efficient=enable_kernels,\n-                                    ):\n-                                        outputs_eager = model_eager(dummy_input, **other_inputs)\n-                                        outputs_sdpa = model_sdpa(dummy_input, **other_inputs)\n-\n-                                logits_eager = (\n-                                    outputs_eager.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_eager.decoder_hidden_states[-1]\n-                                )\n-                                logits_sdpa = (\n-                                    outputs_sdpa.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_sdpa.decoder_hidden_states[-1]\n-                                )\n-\n-                                if torch_device in [\"cpu\", \"cuda\"]:\n-                                    atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                elif torch_device == \"xpu\":\n-                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                    # which is implemented on PyTorch level using aten operators and is\n-                                    # device agnostic with respect to implementation of each aten operator.\n-                                    atol = atols[\"cuda\", False, torch_dtype]\n-                                    rtol = rtols[\"cuda\", False, torch_dtype]\n-                                else:\n-                                    atol = 1e-7\n-                                    rtol = 1e-4\n-\n-                                # Masked tokens output slightly deviates - we don't mind that.\n-                                if use_mask:\n-                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                    _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                    _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                    if padding_side == \"left\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                    elif padding_side == \"right\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                    logits_sdpa = _logits_sdpa\n-                                    logits_eager = _logits_eager\n-\n-                                results = [\n-                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                ]\n-                                # If 80% batch elements have matched results, it's fine\n-                                if np.mean(results) < 0.8:\n-                                    fail_cases.append(\n-                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                    )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     @unittest.skip(\n         reason=(\n             \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n@@ -1496,261 +1275,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n                     raise ValueError(\"The SDPA model should have SDPA attention layers\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        if hasattr(self.model_tester, \"num_hidden_layers\"):\n-            self.model_tester.num_hidden_layers = 1\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-            config.rms_norm_eps = 1.0\n-            config.layer_norm_eps = 1.0\n-            config.norm_eps = 1.0\n-            config.norm_epsilon = 1.0\n-            config.layer_norm_epsilon = 1.0\n-\n-            for attr in [\"text_config\", \"vision_config\", \"text_encoder\", \"audio_encoder\", \"decoder\"]:\n-                if hasattr(config, attr):\n-                    getattr(config, attr).rms_norm_eps = 1.0\n-                    getattr(config, attr).layer_norm_eps = 1.0\n-                    getattr(config, attr).norm_eps = 1.0\n-                    getattr(config, attr).norm_epsilon = 1.0\n-                    getattr(config, attr).layer_norm_epsilon = 1.0\n-\n-            model = model_class(config)\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                for x in model_eager.modules():\n-                    if isinstance(x, (torch.nn.LayerNorm, torch.nn.GroupNorm)):\n-                        x.eps = 1.0\n-                for x in model_sdpa.modules():\n-                    if isinstance(x, (torch.nn.LayerNorm, torch.nn.GroupNorm)):\n-                        x.eps = 1.0\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for batch_size in [7]:\n-                            dummy_input = inputs_dict[model.main_input_name]\n-\n-                            if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                dummy_input = dummy_input.to(torch_dtype)\n-\n-                            dummy_input = dummy_input[:batch_size]\n-                            if dummy_input.shape[0] != batch_size:\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    extension = torch.rand(\n-                                        batch_size - dummy_input.shape[0],\n-                                        *dummy_input.shape[1:],\n-                                        dtype=torch_dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                else:\n-                                    extension = torch.randint(\n-                                        high=5,\n-                                        size=(batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                        dtype=dummy_input.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                            if not use_mask:\n-                                dummy_attention_mask = None\n-                            else:\n-                                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                if dummy_attention_mask is None:\n-                                    # Ignore copy\n-                                    seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                    # Ignore copy\n-                                    dummy_attention_mask = (\n-                                        torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                    )\n-\n-                                dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                if dummy_attention_mask.shape[0] != batch_size:\n-                                    extension = torch.ones(\n-                                        batch_size - dummy_attention_mask.shape[0],\n-                                        *dummy_attention_mask.shape[1:],\n-                                        dtype=dummy_attention_mask.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                    dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                dummy_attention_mask[:] = 1\n-                                if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :2] = 0\n-                                    dummy_attention_mask[-1, 2:] = 1\n-                                elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, -2:] = 0\n-                                    dummy_attention_mask[-1, :-2] = 1\n-\n-                            for enable_kernels in [False, True]:\n-                                failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-                                # Ignore copy\n-                                batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n-                                # Ignore copy\n-                                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[\n-                                    :batch_size_input_ids\n-                                ]\n-                                # Ignore copy\n-                                if decoder_input_ids.shape[0] != batch_size_input_ids:\n-                                    # Ignore copy\n-                                    extension = torch.ones(\n-                                        batch_size_input_ids - decoder_input_ids.shape[0],\n-                                        *decoder_input_ids.shape[1:],\n-                                        dtype=decoder_input_ids.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                                    decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                                # TODO: never an `attention_mask` arg here?\n-                                # Ignore copy\n-                                other_inputs = {\n-                                    \"decoder_input_ids\": decoder_input_ids,\n-                                    \"decoder_attention_mask\": dummy_attention_mask,\n-                                    \"output_hidden_states\": True,\n-                                }\n-\n-                                # TODO: test gradients as well (& for FA2 as well!)\n-                                # Ignore copy\n-                                with torch.no_grad():\n-                                    with sdpa_kernel(\n-                                        enable_flash=enable_kernels,\n-                                        enable_math=True,\n-                                        enable_mem_efficient=enable_kernels,\n-                                    ):\n-                                        outputs_eager = model_eager(dummy_input, **other_inputs)\n-                                        outputs_sdpa = model_sdpa(dummy_input, **other_inputs)\n-\n-                                logits_eager = (\n-                                    outputs_eager.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_eager.decoder_hidden_states[-1]\n-                                )\n-                                logits_sdpa = (\n-                                    outputs_sdpa.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_sdpa.decoder_hidden_states[-1]\n-                                )\n-\n-                                if torch_device in [\"cpu\", \"cuda\"]:\n-                                    atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                elif torch_device == \"xpu\":\n-                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                    # which is implemented on PyTorch level using aten operators and is\n-                                    # device agnostic with respect to implementation of each aten operator.\n-                                    atol = atols[\"cuda\", False, torch_dtype]\n-                                    rtol = rtols[\"cuda\", False, torch_dtype]\n-                                else:\n-                                    atol = 1e-7\n-                                    rtol = 1e-4\n-\n-                                # Masked tokens output slightly deviates - we don't mind that.\n-                                if use_mask:\n-                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                    _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                    _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                    if padding_side == \"left\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                    elif padding_side == \"right\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                    logits_sdpa = _logits_sdpa\n-                                    logits_eager = _logits_eager\n-\n-                                results = [\n-                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                ]\n-                                # If 80% batch elements have matched results, it's fine\n-                                if np.mean(results) < 0.8:\n-                                    fail_cases.append(\n-                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                    )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "c89471d119a9f4d4e00c7ea824e1dae943f14428",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 1,
            "deletions": 465,
            "changes": 466,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -21,7 +21,6 @@\n import unittest\n \n import numpy as np\n-from parameterized import parameterized\n from pytest import mark\n \n from transformers import (\n@@ -41,13 +40,10 @@\n     require_torch_gpu,\n     require_torch_sdpa,\n     require_torchaudio,\n-    set_config_for_less_flaky_test,\n-    set_model_for_less_flaky_test,\n-    set_model_tester_for_less_flaky_test,\n     slow,\n     torch_device,\n )\n-from transformers.utils import cached_property, is_torch_bf16_available_on_device, is_torch_fp16_available_on_device\n+from transformers.utils import cached_property\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -463,232 +459,6 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n \n                 assert torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2)\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        set_model_tester_for_less_flaky_test(self)\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            set_config_for_less_flaky_test(config)\n-            model = model_class(config)\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                set_model_for_less_flaky_test(model_eager)\n-                set_model_for_less_flaky_test(model_sdpa)\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for batch_size in [7]:\n-                            # Ignore copy\n-                            batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n-                            dummy_input = inputs_dict[model.main_input_name]\n-\n-                            if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                dummy_input = dummy_input.to(torch_dtype)\n-\n-                            # Ignore copy\n-                            dummy_input = dummy_input[:batch_size_input_ids]\n-                            # Ignore copy\n-                            if dummy_input.shape[0] != batch_size_input_ids:\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    # Ignore copy\n-                                    extension = torch.rand(\n-                                        batch_size_input_ids - dummy_input.shape[0],\n-                                        *dummy_input.shape[1:],\n-                                        dtype=torch_dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                else:\n-                                    # Ignore copy\n-                                    extension = torch.randint(\n-                                        high=5,\n-                                        size=(batch_size_input_ids - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                        dtype=dummy_input.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                            if not use_mask:\n-                                dummy_attention_mask = None\n-                            else:\n-                                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                if dummy_attention_mask is None:\n-                                    if is_encoder_decoder:\n-                                        seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                    else:\n-                                        seqlen = dummy_input.shape[-1]\n-                                    dummy_attention_mask = (\n-                                        torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                    )\n-\n-                                dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                if dummy_attention_mask.shape[0] != batch_size:\n-                                    extension = torch.ones(\n-                                        batch_size - dummy_attention_mask.shape[0],\n-                                        *dummy_attention_mask.shape[1:],\n-                                        dtype=dummy_attention_mask.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                    dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                dummy_attention_mask[:] = 1\n-                                if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :2] = 0\n-                                    dummy_attention_mask[-1, 2:] = 1\n-                                elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, -2:] = 0\n-                                    dummy_attention_mask[-1, :-2] = 1\n-\n-                            for enable_kernels in [False, True]:\n-                                failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-\n-                                other_inputs = {\n-                                    \"output_hidden_states\": True,\n-                                }\n-\n-                                # Otherwise fails for e.g. WhisperEncoderModel\n-                                if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                                    other_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                                # TODO: test gradients as well (& for FA2 as well!)\n-                                with torch.no_grad():\n-                                    with sdpa_kernel(\n-                                        enable_flash=enable_kernels,\n-                                        enable_math=True,\n-                                        enable_mem_efficient=enable_kernels,\n-                                    ):\n-                                        outputs_eager = model_eager(dummy_input, **other_inputs)\n-                                        outputs_sdpa = model_sdpa(dummy_input, **other_inputs)\n-\n-                                logits_eager = (\n-                                    outputs_eager.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_eager.decoder_hidden_states[-1]\n-                                )\n-                                logits_sdpa = (\n-                                    outputs_sdpa.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_sdpa.decoder_hidden_states[-1]\n-                                )\n-\n-                                if torch_device in [\"cpu\", \"cuda\"]:\n-                                    atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                elif torch_device == \"xpu\":\n-                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                    # which is implemented on PyTorch level using aten operators and is\n-                                    # device agnostic with respect to implementation of each aten operator.\n-                                    atol = atols[\"cuda\", False, torch_dtype]\n-                                    rtol = rtols[\"cuda\", False, torch_dtype]\n-                                else:\n-                                    atol = 1e-7\n-                                    rtol = 1e-4\n-\n-                                # Masked tokens output slightly deviates - we don't mind that.\n-                                if use_mask:\n-                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                    _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                    _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                    if padding_side == \"left\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                    elif padding_side == \"right\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                    logits_sdpa = _logits_sdpa\n-                                    logits_eager = _logits_eager\n-\n-                                results = [\n-                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                ]\n-                                # If 80% batch elements have matched results, it's fine\n-                                if np.mean(results) < 0.8:\n-                                    fail_cases.append(\n-                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                    )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     @unittest.skip(\n         reason=(\n             \"MusicGen has a custom set of generation tests that rely on `GenerationTesterMixin`, controlled by \"\n@@ -1495,240 +1265,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n                     raise ValueError(\"The SDPA model should have SDPA attention layers\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n-    @require_torch_sdpa\n-    # Copied from tests.test_modeling_common.ModelTesterMixin.test_eager_matches_sdpa_inference\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n-        if not self.all_model_classes[0]._supports_sdpa:\n-            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n-\n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n-            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n-\n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n-            self.skipTest(\n-                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n-            )\n-\n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n-        atols = {\n-            (\"cpu\", False, torch.float32): 1e-6,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-6,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-6,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-6,\n-            (\"cuda\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-        rtols = {\n-            (\"cpu\", False, torch.float32): 1e-4,\n-            (\"cpu\", False, torch.float16): 5e-3,\n-            (\"cpu\", False, torch.bfloat16): 1e-2,\n-            (\"cpu\", True, torch.float32): 1e-4,\n-            (\"cpu\", True, torch.float16): 5e-3,\n-            (\"cpu\", True, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float32): 1e-4,\n-            (\"cuda\", False, torch.bfloat16): 1e-2,\n-            (\"cuda\", False, torch.float16): 5e-3,\n-            (\"cuda\", True, torch.float32): 1e-4,\n-            (\"cuda\", True, torch.bfloat16): 3e-2,\n-            (\"cuda\", True, torch.float16): 5e-3,\n-        }\n-\n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n-        set_model_tester_for_less_flaky_test(self)\n-\n-        for model_class in self.all_model_classes:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            set_config_for_less_flaky_test(config)\n-            model = model_class(config)\n-\n-            is_encoder_decoder = model.config.is_encoder_decoder\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n-                model_sdpa = model_sdpa.eval().to(torch_device)\n-\n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n-                model_eager = model_eager.eval().to(torch_device)\n-\n-                set_model_for_less_flaky_test(model_eager)\n-                set_model_for_less_flaky_test(model_sdpa)\n-\n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 8 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for batch_size in [7]:\n-                            dummy_input = inputs_dict[model.main_input_name]\n-\n-                            if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                dummy_input = dummy_input.to(torch_dtype)\n-\n-                            dummy_input = dummy_input[:batch_size]\n-                            if dummy_input.shape[0] != batch_size:\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    extension = torch.rand(\n-                                        batch_size - dummy_input.shape[0],\n-                                        *dummy_input.shape[1:],\n-                                        dtype=torch_dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                else:\n-                                    extension = torch.randint(\n-                                        high=5,\n-                                        size=(batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                        dtype=dummy_input.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                            if not use_mask:\n-                                dummy_attention_mask = None\n-                            else:\n-                                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                if dummy_attention_mask is None:\n-                                    # Ignore copy\n-                                    seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                    # Ignore copy\n-                                    dummy_attention_mask = (\n-                                        torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                    )\n-\n-                                dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                if dummy_attention_mask.shape[0] != batch_size:\n-                                    extension = torch.ones(\n-                                        batch_size - dummy_attention_mask.shape[0],\n-                                        *dummy_attention_mask.shape[1:],\n-                                        dtype=dummy_attention_mask.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                    dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                dummy_attention_mask[:] = 1\n-                                if padding_side == \"left\":\n-                                    dummy_attention_mask[-1, :2] = 0\n-                                    dummy_attention_mask[-1, 2:] = 1\n-                                elif padding_side == \"right\":\n-                                    dummy_attention_mask[-1, -2:] = 0\n-                                    dummy_attention_mask[-1, :-2] = 1\n-\n-                            for enable_kernels in [False, True]:\n-                                failcase = f\"padding_side={padding_side}, use_mask={use_mask}, batch_size={batch_size}, enable_kernels={enable_kernels}\"\n-                                # Ignore copy\n-                                batch_size_input_ids = self.model_tester.num_codebooks * batch_size\n-                                # Ignore copy\n-                                decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[\n-                                    :batch_size_input_ids\n-                                ]\n-                                # Ignore copy\n-                                if decoder_input_ids.shape[0] != batch_size_input_ids:\n-                                    # Ignore copy\n-                                    extension = torch.ones(\n-                                        batch_size_input_ids - decoder_input_ids.shape[0],\n-                                        *decoder_input_ids.shape[1:],\n-                                        dtype=decoder_input_ids.dtype,\n-                                        device=torch_device,\n-                                    )\n-                                    decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                                    decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                                # TODO: never an `attention_mask` arg here?\n-                                # Ignore copy\n-                                other_inputs = {\n-                                    \"decoder_input_ids\": decoder_input_ids,\n-                                    \"decoder_attention_mask\": dummy_attention_mask,\n-                                    \"output_hidden_states\": True,\n-                                }\n-\n-                                # TODO: test gradients as well (& for FA2 as well!)\n-                                # Ignore copy\n-                                with torch.no_grad():\n-                                    with sdpa_kernel(\n-                                        enable_flash=enable_kernels,\n-                                        enable_math=True,\n-                                        enable_mem_efficient=enable_kernels,\n-                                    ):\n-                                        outputs_eager = model_eager(dummy_input, **other_inputs)\n-                                        outputs_sdpa = model_sdpa(dummy_input, **other_inputs)\n-\n-                                logits_eager = (\n-                                    outputs_eager.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_eager.decoder_hidden_states[-1]\n-                                )\n-                                logits_sdpa = (\n-                                    outputs_sdpa.hidden_states[-1]\n-                                    if not is_encoder_decoder\n-                                    else outputs_sdpa.decoder_hidden_states[-1]\n-                                )\n-\n-                                if torch_device in [\"cpu\", \"cuda\"]:\n-                                    atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                    rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                elif torch_device == \"xpu\":\n-                                    # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                    # which is implemented on PyTorch level using aten operators and is\n-                                    # device agnostic with respect to implementation of each aten operator.\n-                                    atol = atols[\"cuda\", False, torch_dtype]\n-                                    rtol = rtols[\"cuda\", False, torch_dtype]\n-                                else:\n-                                    atol = 1e-7\n-                                    rtol = 1e-4\n-\n-                                # Masked tokens output slightly deviates - we don't mind that.\n-                                if use_mask:\n-                                    _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                    _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                    _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                    _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                    if padding_side == \"left\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                    elif padding_side == \"right\":\n-                                        _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                        _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                    logits_sdpa = _logits_sdpa\n-                                    logits_eager = _logits_eager\n-\n-                                results = [\n-                                    torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                    for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                ]\n-                                # If 80% batch elements have matched results, it's fine\n-                                if np.mean(results) < 0.8:\n-                                    fail_cases.append(\n-                                        get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                    )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n-\n     def test_requires_grad_with_frozen_encoders(self):\n         config = self.model_tester.get_config()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "4d097578023f3012cc44c0c77d52f947b17704d0",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -347,10 +347,6 @@ def test_past_key_values_format(self):\n     def test_eager_matches_sdpa_generate(self):\n         pass\n \n-    @unittest.skip(reason=\"RecurrentGemma only supports sdpa\")\n-    def test_eager_matches_sdpa_inference(self):\n-        pass\n-\n     @unittest.skip(reason=\"RecurrentGemma does not return the cache\")\n     def test_contrastive_generate_low_memory(self):\n         pass"
        },
        {
            "sha": "2d56bbd5514dec5323822b34ff38202a206b894f",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -22,7 +22,7 @@\n \n from transformers import VideoMAEConfig\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_torch, require_torch_sdpa, require_vision, slow, torch_device\n+from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n from transformers.utils import cached_property, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -214,11 +214,6 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n \n         return inputs_dict\n \n-    @unittest.skip(\"`mse_cpu` not implemented for 'BFloat16'\")\n-    @require_torch_sdpa\n-    def test_eager_matches_sdpa_inference_1_bfloat16(self):\n-        pass\n-\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        },
        {
            "sha": "e437ce75ca8126f22e3b91948f35120855a46dca",
            "filename": "tests/models/vit_msn/test_modeling_vit_msn.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -59,6 +59,7 @@ def __init__(\n         initializer_range=0.02,\n         scope=None,\n         attn_implementation=\"eager\",\n+        mask_ratio=0.5,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -82,6 +83,8 @@ def __init__(\n         # in ViT MSN, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n+        self.num_masks = int(mask_ratio * self.seq_length)\n+        self.mask_length = self.seq_length - 1\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])"
        },
        {
            "sha": "1bd5b651db96f6d291c45abf40f209f0da29cf6f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 254,
            "deletions": 224,
            "changes": 478,
            "blob_url": "https://github.com/huggingface/transformers/blob/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=42ebb6c23e61119f769d7c7c067d5b4ae10e4a7f",
            "patch": "@@ -133,6 +133,23 @@\n     import deepspeed\n \n \n+# used in other test files e.g. when overwriting the test\n+TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION = [\n+    (\n+        # test name for the test runner\n+        f\"{dtype}_pad_{padding_side}{'' if use_attention_mask else '_no_attn_mask'}\"\n+        f\"{'_output_attn' if output_attentions else ''}{'_sdpa_kernels' if enable_kernels else ''}\",\n+        # parameterization\n+        *(dtype, padding_side, use_attention_mask, output_attentions, enable_kernels),\n+    )\n+    for dtype in (\"fp16\", \"fp32\", \"bf16\")\n+    for padding_side in (\"left\", \"right\")\n+    for use_attention_mask in (True, False)\n+    for output_attentions in (True, False)\n+    for enable_kernels in (True, False)\n+]\n+\n+\n def _config_zero_init(config):\n     configs_no_init = copy.deepcopy(config)\n     for key in configs_no_init.__dict__.keys():\n@@ -3543,31 +3560,39 @@ def test_sdpa_can_dispatch_composite_models(self):\n                     ):\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n-    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+    def test_eager_matches_sdpa_inference(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        # TODO: we shouldn't need to do this skip, i.e. the test would be composable from the model tester. CLIP-like\n+        # models have a custom mixin, which we detect to skip this test.\n+        if not any(\".ModelTesterMixin\" in str(base) for base in self.__class__.__bases__):\n+            self.skipTest(reason=\"CLIP-like models have a different `test_eager_matches_sdpa_inference`\")\n+\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n         if not self.all_model_classes[0]._supports_sdpa:\n             self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n \n-        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n+        # convert shorthand name to torch.dtype\n+        if torch_dtype == \"fp16\":\n+            torch_dtype = torch.float16\n+        elif torch_dtype == \"bf16\":\n+            torch_dtype = torch.bfloat16\n+        elif torch_dtype == \"fp32\":\n+            torch_dtype = torch.float32\n+\n+        if not is_torch_fp16_available_on_device(torch_device) and torch_dtype == torch.float16:\n             self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n \n-        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n+        if not is_torch_bf16_available_on_device(torch_device) and torch_dtype == torch.bfloat16:\n             self.skipTest(\n                 f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n             )\n \n-        # Not sure whether it's fine to put torch.XXX in a decorator if torch is not available so hacking it here instead.\n-        if torch_dtype == \"float16\":\n-            torch_dtype = torch.float16\n-        elif torch_dtype == \"bfloat16\":\n-            torch_dtype = torch.bfloat16\n-        elif torch_dtype == \"float32\":\n-            torch_dtype = torch.float32\n-\n+        # Dictionary of tolerances for eager <> sdpa tests. Key = (device, sdpa_kernels_enabled, dtype)\n         atols = {\n             (\"cpu\", False, torch.float32): 1e-6,\n             (\"cpu\", False, torch.float16): 5e-3,\n@@ -3597,238 +3622,243 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n             (\"cuda\", True, torch.float16): 5e-3,\n         }\n \n-        def get_mean_reldiff(failcase, x, ref, atol, rtol):\n-            return f\"{failcase}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n-\n         set_model_tester_for_less_flaky_test(self)\n \n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             set_config_for_less_flaky_test(config)\n             model = model_class(config)\n-            # FIXME: we deactivate boolean mask for models using \"use_mask_token\" in their constructors.\n-            # These models support masking only in the case `use_mask_token=True`. Otherwise they cannot consume an input mask.\n-            # This means that the class needs to be instantiated much later, after `use_mask` is set, which means a significant refactor of the code.\n-            # However masking there is not done at any layers that matters (i.e self-attention), therefore we can safely deactivate it.\n-            deactivate_mask = \"use_mask_token\" in inspect.signature(model_class).parameters\n-            is_encoder_decoder = model.config.is_encoder_decoder\n+            # TODO: standardize the interfaces for musicgen models, see other todo in this test\n+            if model.__class__.__name__ == \"MusicgenMelodyForConditionalGeneration\":\n+                is_encoder_decoder = True\n+            else:\n+                is_encoder_decoder = model.config.is_encoder_decoder\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n+                model_from_pretrained_kwargs = {\n+                    \"pretrained_model_name_or_path\": tmpdirname,\n+                    \"torch_dtype\": torch_dtype,\n+                }\n+\n+                if (\n+                    hasattr(config, \"use_mask_token\")\n+                    or \"use_mask_token\" in inspect.signature(model.__init__).parameters\n+                ):\n+                    model_from_pretrained_kwargs[\"use_mask_token\"] = True\n+\n+                # TODO: remove this try/except, models should have a shared API\n                 try:\n                     model_sdpa = model_class.from_pretrained(\n-                        tmpdirname, torch_dtype=torch_dtype, attn_implementation=\"sdpa\"\n+                        **model_from_pretrained_kwargs, attn_implementation=\"sdpa\"\n                     )\n                 except ValueError:\n-                    model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n+                    model_sdpa = model_class.from_pretrained(**model_from_pretrained_kwargs)\n                 model_sdpa = model_sdpa.eval().to(torch_device, dtype=torch_dtype)\n \n-                model_eager = model_class.from_pretrained(\n-                    tmpdirname,\n-                    torch_dtype=torch_dtype,\n-                    attn_implementation=\"eager\",\n-                )\n+                model_eager = model_class.from_pretrained(**model_from_pretrained_kwargs, attn_implementation=\"eager\")\n                 model_eager = model_eager.eval().to(torch_device, dtype=torch_dtype)\n \n                 set_model_for_less_flaky_test(model_eager)\n                 set_model_for_less_flaky_test(model_sdpa)\n \n-                # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving 16 times the model,\n-                # but it would be nicer to have an efficient way to use parameterized.expand\n-                fail_cases = []\n-                for padding_side in [\"left\", \"right\"]:\n-                    for use_mask in [False, True]:\n-                        for output_attentions in [True, False]:\n-                            can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                            if not (self.has_attentions and can_output_attn) and output_attentions:\n-                                continue\n-                            # TODO: if we can also check with `batch_size=1` without being flaky?\n-                            for batch_size in [7]:\n-                                dummy_input = inputs_dict[model.main_input_name]\n-\n-                                if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                    dummy_input = dummy_input.to(torch_dtype)\n-\n-                                dummy_input = dummy_input[:batch_size]\n-                                if dummy_input.shape[0] != batch_size:\n-                                    if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n-                                        extension = torch.rand(\n-                                            batch_size - dummy_input.shape[0],\n-                                            *dummy_input.shape[1:],\n-                                            dtype=torch_dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-                                    else:\n-                                        extension = torch.randint(\n-                                            high=5,\n-                                            size=(batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n-                                            dtype=dummy_input.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n-\n-                                if not use_mask:\n-                                    dummy_attention_mask = None\n-                                else:\n-                                    dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n-                                    if dummy_attention_mask is None:\n-                                        if is_encoder_decoder:\n-                                            seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n-                                        else:\n-                                            seqlen = dummy_input.shape[-1]\n-                                        dummy_attention_mask = (\n-                                            torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n-                                        )\n-\n-                                    dummy_attention_mask = dummy_attention_mask[:batch_size]\n-                                    if dummy_attention_mask.shape[0] != batch_size:\n-                                        extension = torch.ones(\n-                                            batch_size - dummy_attention_mask.shape[0],\n-                                            *dummy_attention_mask.shape[1:],\n-                                            dtype=dummy_attention_mask.dtype,\n-                                            device=torch_device,\n-                                        )\n-                                        dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n-                                        dummy_attention_mask = dummy_attention_mask.to(torch_device)\n-\n-                                    dummy_attention_mask[:] = 1\n-                                    if padding_side == \"left\":\n-                                        dummy_attention_mask[-1, :2] = 0\n-                                        dummy_attention_mask[-1, 2:] = 1\n-                                    elif padding_side == \"right\":\n-                                        dummy_attention_mask[-1, -2:] = 0\n-                                        dummy_attention_mask[-1, :-2] = 1\n-\n-                                for enable_kernels in [False, True]:\n-                                    failcase = f\"padding_side={padding_side}, use_mask={use_mask}, enable_kernels={enable_kernels}\"\n-                                    if is_encoder_decoder:\n-                                        decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[\n-                                            :batch_size\n-                                        ]\n-                                        if decoder_input_ids.shape[0] != batch_size:\n-                                            extension = torch.ones(\n-                                                batch_size - decoder_input_ids.shape[0],\n-                                                *decoder_input_ids.shape[1:],\n-                                                dtype=decoder_input_ids.dtype,\n-                                                device=torch_device,\n-                                            )\n-                                            decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n-                                            decoder_input_ids = decoder_input_ids.to(torch_device)\n-\n-                                        # TODO: never an `attention_mask` arg here?\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"decoder_input_ids\": decoder_input_ids,\n-                                            \"decoder_attention_mask\": dummy_attention_mask,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-                                    else:\n-                                        processed_inputs = {\n-                                            model.main_input_name: dummy_input,\n-                                            \"output_hidden_states\": True,\n-                                        }\n-\n-                                        # Otherwise fails for e.g. WhisperEncoderModel\n-                                        if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n-                                            processed_inputs[\"attention_mask\"] = dummy_attention_mask\n-\n-                                        if (\n-                                            self.has_attentions\n-                                            and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n-                                        ):\n-                                            processed_inputs[\"output_attentions\"] = output_attentions\n-                                    if not deactivate_mask and (\n-                                        \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters\n-                                    ):\n-                                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n-\n-                                        # In case of additional token (like class) we define a custom `mask_length`\n-                                        if hasattr(self.model_tester, \"mask_length\"):\n-                                            mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n-                                        else:\n-                                            mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n-                                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n-                                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n-                                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n-\n-                                    if \"noise\" in inspect.signature(model_eager.forward).parameters:\n-                                        np.random.seed(2)\n-                                        num_patches = int(\n-                                            (self.model_tester.image_size // self.model_tester.patch_size) ** 2\n-                                        )\n-                                        noise = np.random.uniform(size=(batch_size, num_patches))\n-                                        processed_inputs[\"noise\"] = torch.from_numpy(noise)\n-\n-                                    # TODO: test gradients as well (& for FA2 as well!)\n-                                    with torch.no_grad():\n-                                        with sdpa_kernel(\n-                                            enable_flash=enable_kernels,\n-                                            enable_math=True,\n-                                            enable_mem_efficient=enable_kernels,\n-                                        ):\n-                                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n-                                            outputs_eager = model_eager(**prepared_inputs)\n-                                            outputs_sdpa = model_sdpa(**prepared_inputs)\n-\n-                                    if hasattr(outputs_eager, \"vision_hidden_states\"):\n-                                        logits_eager = outputs_eager.vision_hidden_states[-1]\n-                                        logits_sdpa = outputs_sdpa.vision_hidden_states[-1]\n-                                    else:\n-                                        logits_eager = (\n-                                            outputs_eager.hidden_states[-1]\n-                                            if not is_encoder_decoder\n-                                            else outputs_eager.decoder_hidden_states[-1]\n-                                        )\n-                                        logits_sdpa = (\n-                                            outputs_sdpa.hidden_states[-1]\n-                                            if not is_encoder_decoder\n-                                            else outputs_sdpa.decoder_hidden_states[-1]\n-                                        )\n-\n-                                    if torch_device in [\"cpu\", \"cuda\"]:\n-                                        atol = atols[torch_device, enable_kernels, torch_dtype]\n-                                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n-                                    elif torch_device == \"xpu\":\n-                                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n-                                        # which is implemented on PyTorch level using aten operators and is\n-                                        # device agnostic with respect to implementation of each aten operator.\n-                                        atol = atols[\"cuda\", False, torch_dtype]\n-                                        rtol = rtols[\"cuda\", False, torch_dtype]\n-                                    else:\n-                                        atol = 1e-7\n-                                        rtol = 1e-4\n-\n-                                    # Masked tokens output slightly deviates - we don't mind that.\n-                                    if use_mask:\n-                                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n-                                        _logits_eager = torch.zeros_like(input=logits_eager)\n-\n-                                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n-                                        _logits_eager[:-1] = logits_eager[:-1]\n-\n-                                        if padding_side == \"left\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n-\n-                                        elif padding_side == \"right\":\n-                                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n-                                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n-\n-                                        logits_sdpa = _logits_sdpa\n-                                        logits_eager = _logits_eager\n-\n-                                    results = [\n-                                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n-                                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n-                                    ]\n-                                    # If 80% batch elements have matched results, it's fine\n-                                    if np.mean(results) < 0.8:\n-                                        fail_cases.append(\n-                                            get_mean_reldiff(failcase, logits_sdpa, logits_eager, atol, rtol)\n-                                        )\n-\n-                self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n+                can_output_attn = \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                if not (self.has_attentions and can_output_attn) and output_attentions:\n+                    self.skipTest(reason=\"Model does not support output_attentions\")\n+\n+                # TODO: if we can also check with `batch_size=1` without being flaky?\n+                for batch_size in [7]:\n+                    # musicgen decoder models; TODO: find better abstraction\n+                    if hasattr(self.model_tester, \"num_codebooks\") and not hasattr(model_eager, \"text_encoder\"):\n+                        input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                    else:\n+                        input_data_batch_size = batch_size\n+\n+                    dummy_input = inputs_dict[model.main_input_name]\n+\n+                    if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n+                        dummy_input = dummy_input.to(torch_dtype)\n+\n+                    dummy_input = dummy_input[:input_data_batch_size]\n+                    if dummy_input.shape[0] != input_data_batch_size:\n+                        if dummy_input.dtype in [torch.float32, torch.bfloat16, torch.float16]:\n+                            extension = torch.rand(\n+                                input_data_batch_size - dummy_input.shape[0],\n+                                *dummy_input.shape[1:],\n+                                dtype=torch_dtype,\n+                                device=torch_device,\n+                            )\n+                            dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n+                        else:\n+                            extension = torch.randint(\n+                                high=5,\n+                                size=(input_data_batch_size - dummy_input.shape[0], *dummy_input.shape[1:]),\n+                                dtype=dummy_input.dtype,\n+                                device=torch_device,\n+                            )\n+                            dummy_input = torch.cat((dummy_input, extension), dim=0).to(torch_device)\n+\n+                    if not use_attention_mask:\n+                        dummy_attention_mask = None\n+                    else:\n+                        dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+                        if dummy_attention_mask is None:\n+                            if is_encoder_decoder:\n+                                seqlen = inputs_dict.get(\"decoder_input_ids\", dummy_input).shape[-1]\n+                            else:\n+                                seqlen = dummy_input.shape[-1]\n+                            dummy_attention_mask = torch.ones(batch_size, seqlen).to(torch.int64).to(torch_device)\n+\n+                        dummy_attention_mask = dummy_attention_mask[:batch_size]\n+                        if dummy_attention_mask.shape[0] != batch_size:\n+                            extension = torch.ones(\n+                                batch_size - dummy_attention_mask.shape[0],\n+                                *dummy_attention_mask.shape[1:],\n+                                dtype=dummy_attention_mask.dtype,\n+                                device=torch_device,\n+                            )\n+                            dummy_attention_mask = torch.cat((dummy_attention_mask, extension), dim=0)\n+                            dummy_attention_mask = dummy_attention_mask.to(torch_device)\n+\n+                        dummy_attention_mask[:] = 1\n+                        if padding_side == \"left\":\n+                            dummy_attention_mask[-1, :2] = 0\n+                            dummy_attention_mask[-1, 2:] = 1\n+                        elif padding_side == \"right\":\n+                            dummy_attention_mask[-1, -2:] = 0\n+                            dummy_attention_mask[-1, :-2] = 1\n+\n+                    if is_encoder_decoder:\n+                        # musicgen encoder-decoder models; TODO: find better abstraction\n+                        if hasattr(self.model_tester, \"num_codebooks\"):\n+                            input_data_batch_size = batch_size * self.model_tester.num_codebooks\n+                        else:\n+                            input_data_batch_size = batch_size\n+\n+                        decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", dummy_input)[:input_data_batch_size]\n+                        if decoder_input_ids.shape[0] != input_data_batch_size:\n+                            extension = torch.ones(\n+                                input_data_batch_size - decoder_input_ids.shape[0],\n+                                *decoder_input_ids.shape[1:],\n+                                dtype=decoder_input_ids.dtype,\n+                                device=torch_device,\n+                            )\n+                            decoder_input_ids = torch.cat((decoder_input_ids, extension), dim=0)\n+                            decoder_input_ids = decoder_input_ids.to(torch_device)\n+\n+                        # TODO: never an `attention_mask` arg here?\n+                        processed_inputs = {\n+                            model.main_input_name: dummy_input,\n+                            \"decoder_input_ids\": decoder_input_ids,\n+                            \"decoder_attention_mask\": dummy_attention_mask,\n+                            \"output_hidden_states\": True,\n+                        }\n+                    else:\n+                        processed_inputs = {\n+                            model.main_input_name: dummy_input,\n+                            \"output_hidden_states\": True,\n+                        }\n+\n+                        # Otherwise fails for e.g. WhisperEncoderModel\n+                        if \"attention_mask\" in inspect.signature(model_eager.forward).parameters:\n+                            processed_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                        if (\n+                            self.has_attentions\n+                            and \"output_attentions\" in inspect.signature(model_sdpa.forward).parameters\n+                        ):\n+                            processed_inputs[\"output_attentions\"] = output_attentions\n+                    if \"bool_masked_pos\" in inspect.signature(model_eager.forward).parameters:\n+                        dummy_mask = torch.ones((self.model_tester.num_masks,))\n+\n+                        # In case of additional token (like class) we define a custom `mask_length`\n+                        if hasattr(self.model_tester, \"mask_length\"):\n+                            mask_length = self.model_tester.mask_length - dummy_mask.size(0)\n+                        else:\n+                            mask_length = self.model_tester.seq_length - dummy_mask.size(0)\n+                        dummy_mask = torch.cat([dummy_mask, torch.zeros(mask_length)])\n+                        dummy_bool_masked_pos = dummy_mask.expand(batch_size, -1).bool()\n+                        processed_inputs[\"bool_masked_pos\"] = dummy_bool_masked_pos.to(torch_device)\n+\n+                    if \"noise\" in inspect.signature(model_eager.forward).parameters:\n+                        np.random.seed(2)\n+                        num_patches = int((self.model_tester.image_size // self.model_tester.patch_size) ** 2)\n+                        noise = np.random.uniform(size=(batch_size, num_patches))\n+                        processed_inputs[\"noise\"] = torch.from_numpy(noise)\n+\n+                    # TODO: test gradients as well (& for FA2 as well!)\n+                    with torch.no_grad():\n+                        with sdpa_kernel(\n+                            enable_flash=enable_kernels,\n+                            enable_math=True,\n+                            enable_mem_efficient=enable_kernels,\n+                        ):\n+                            prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+                            outputs_eager = model_eager(**prepared_inputs)\n+                            outputs_sdpa = model_sdpa(**prepared_inputs)\n+\n+                    # TODO: rename logits -> hidden_states\n+                    if hasattr(outputs_eager, \"vision_hidden_states\"):\n+                        logits_eager = outputs_eager.vision_hidden_states[-1]\n+                        logits_sdpa = outputs_sdpa.vision_hidden_states[-1]\n+                    elif hasattr(outputs_eager, \"audio_values\"):\n+                        logits_eager = outputs_eager.audio_values\n+                        logits_sdpa = outputs_sdpa.audio_values\n+                    else:\n+                        logits_eager = (\n+                            outputs_eager.decoder_hidden_states[-1]\n+                            if hasattr(outputs_eager, \"decoder_hidden_states\")\n+                            else outputs_eager.hidden_states[-1]\n+                        )\n+                        logits_sdpa = (\n+                            outputs_sdpa.decoder_hidden_states[-1]\n+                            if hasattr(outputs_sdpa, \"decoder_hidden_states\")\n+                            else outputs_sdpa.hidden_states[-1]\n+                        )\n+\n+                    if torch_device in [\"cpu\", \"cuda\"]:\n+                        atol = atols[torch_device, enable_kernels, torch_dtype]\n+                        rtol = rtols[torch_device, enable_kernels, torch_dtype]\n+                    elif torch_device == \"xpu\":\n+                        # As of PyTorch 2.5 XPU backend supports only torch.nn.attention.SDPBackend.MATH\n+                        # which is implemented on PyTorch level using aten operators and is\n+                        # device agnostic with respect to implementation of each aten operator.\n+                        atol = atols[\"cuda\", False, torch_dtype]\n+                        rtol = rtols[\"cuda\", False, torch_dtype]\n+                    else:\n+                        atol = 1e-7\n+                        rtol = 1e-4\n+\n+                    # Masked tokens output slightly deviates - we don't mind that.\n+                    if use_attention_mask:\n+                        _logits_sdpa = torch.zeros_like(input=logits_sdpa)\n+                        _logits_eager = torch.zeros_like(input=logits_eager)\n+\n+                        _logits_sdpa[:-1] = logits_sdpa[:-1]\n+                        _logits_eager[:-1] = logits_eager[:-1]\n+\n+                        if padding_side == \"left\":\n+                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, 2:]\n+                            _logits_eager[-1:, 2:] = logits_eager[-1:, 2:]\n+\n+                        elif padding_side == \"right\":\n+                            _logits_sdpa[-1:, 2:] = logits_sdpa[-1:, :-2]\n+                            _logits_eager[-1:, 2:] = logits_eager[-1:, :-2]\n+\n+                        logits_sdpa = _logits_sdpa\n+                        logits_eager = _logits_eager\n+\n+                    results = [\n+                        torch.allclose(_logits_sdpa, _logits_eager, atol=atol, rtol=rtol)\n+                        for (_logits_sdpa, _logits_eager) in zip(logits_sdpa, logits_eager)\n+                    ]\n+                    # If 80% batch elements have matched results, it's fine\n+                    if np.mean(results) < 0.8:\n+                        mean_relative_diff = ((logits_sdpa - logits_eager).abs() / (logits_eager.abs() + 1e-12)).mean()\n+                        raise ValueError(\n+                            f\"mean relative difference: {mean_relative_diff:.3e}, torch atol = {atol}, torch rtol = \"\n+                            f\"{rtol}\"\n+                        )\n \n     @require_torch_sdpa\n     @require_torch_gpu"
        }
    ],
    "stats": {
        "total": 2227,
        "additions": 306,
        "deletions": 1921
    }
}