{
    "author": "NanoCode012",
    "message": "fix: XIELU act parameters not being casted to correct dtype (#40812)",
    "sha": "f5e16418575eee9b08ae2fea8a92c83ae33a3371",
    "files": [
        {
            "sha": "60fd2adb8ef59857e884035a54ec1055b0318165",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5e16418575eee9b08ae2fea8a92c83ae33a3371/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5e16418575eee9b08ae2fea8a92c83ae33a3371/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=f5e16418575eee9b08ae2fea8a92c83ae33a3371",
            "patch": "@@ -262,8 +262,8 @@ def _xielu_cuda(self, x: Tensor) -> Tensor:\n             )\n         result = self._xielu_cuda_obj.forward(\n             x,\n-            self.alpha_p,\n-            self.alpha_n,\n+            self.alpha_p.to(x.dtype),\n+            self.alpha_n.to(x.dtype),\n             # Temporary until xIELU CUDA fully implemented -> self.{beta,eps}.item()\n             self._beta_scalar,\n             self._eps_scalar,"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}