{
    "author": "BakerBunker",
    "message": "Fix Qwen2.5 Omni `SinusoidsPositionEmbedding` precision (#38151)\n\n* Fix Qwen2.5 Omni `SinusoidsPositionEmbedding` precision\n\nfixes https://github.com/QwenLM/Qwen2.5-Omni/issues/271\n\n* Update modular_qwen2_5_omni.py",
    "sha": "57a79f51b2b90c813aeacd609a688ed53bc327ee",
    "files": [
        {
            "sha": "926b19a9b713ae93cc9bc05f6de5b0b44ab2ada6",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57a79f51b2b90c813aeacd609a688ed53bc327ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57a79f51b2b90c813aeacd609a688ed53bc327ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=57a79f51b2b90c813aeacd609a688ed53bc327ee",
            "patch": "@@ -794,7 +794,7 @@ def __init__(self, length, channels, max_timescale=10000):\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n-        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2)).float()\n+        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2).float())\n         scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n         self.register_buffer(\n             \"positional_embedding\","
        },
        {
            "sha": "ef2094e425081d9f631d11fbb0c534ff4f766533",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/57a79f51b2b90c813aeacd609a688ed53bc327ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57a79f51b2b90c813aeacd609a688ed53bc327ee/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=57a79f51b2b90c813aeacd609a688ed53bc327ee",
            "patch": "@@ -1710,7 +1710,7 @@ def __init__(self, length, channels, max_timescale=10000):\n         if channels % 2 != 0:\n             raise ValueError(\"SinusoidsPositionEmbedding needs even channels input\")\n         log_timescale_increment = np.log(max_timescale) / (channels // 2 - 1)\n-        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2)).float()\n+        inv_timescales = torch.exp(-log_timescale_increment * torch.arange(channels // 2).float())\n         scaled_time = torch.arange(length)[:, np.newaxis] * inv_timescales[np.newaxis, :]\n         self.register_buffer(\n             \"positional_embedding\","
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}