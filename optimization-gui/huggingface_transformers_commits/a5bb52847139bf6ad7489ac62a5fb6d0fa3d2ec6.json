{
    "author": "molbap",
    "message": "Fix signatures for processing kwargs (#35105)\n\n* add conversion script\r\n\r\n* remove pg2 refs\r\n\r\n* fixup style\r\n\r\n* small update\r\n\r\n* get correct scaling\r\n\r\n* add back missing bos\r\n\r\n* fix missing config keys\r\n\r\n* might revert this pos_embeddings\r\n\r\n* fixup 9b config\r\n\r\n* fix 9b\r\n\r\n* fixup 9b conversion for good + add back num_hidden_layers\r\n\r\n* add correct query scaling for 2b, 9b, 27b\r\n\r\n* fixup 27b conversion\r\n\r\n* Additional variant: 27b-896\r\n\r\n* Use CPU for conversion to reduce GPU RAM requirements\r\n\r\n* fix causal mask generation + formatting\r\n\r\n* fix in-training causal mask generation edge case\r\n\r\n* trigger CI\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* update config\r\n\r\n* move conversion file to main model dir\r\n\r\n* handle multi-images + bos token\r\n\r\n* address comments for input ids\r\n\r\n* revert ci fixes\r\n\r\n* [run-slow] paligemma\r\n\r\n* fix\r\n\r\n* [run-slow] paligemma\r\n\r\n* skip end 2 end\r\n\r\n* [run-slow] paligemma\r\n\r\n---------\r\n\r\nCo-authored-by: Pedro Cuenca <pedro@huggingface.co>\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
    "files": [
        {
            "sha": "df869fcefb2ba41e0ca726a0a5a20f408a82f05b",
            "filename": "src/transformers/models/paligemma/convert_paligemma2_weights_to_hf.py",
            "status": "added",
            "additions": 415,
            "deletions": 0,
            "changes": 415,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconvert_paligemma2_weights_to_hf.py?ref=a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
            "patch": "@@ -0,0 +1,415 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert PaliGemma2 checkpoints from the original repository.\"\"\"\n+\n+import argparse\n+import collections\n+\n+import jax.numpy as jnp\n+import ml_dtypes\n+import numpy as np\n+import torch\n+\n+from transformers import (\n+    AutoTokenizer,\n+    Gemma2Config,\n+    PaliGemmaConfig,\n+    PaliGemmaForConditionalGeneration,\n+    PaliGemmaProcessor,\n+    SiglipImageProcessor,\n+)\n+from transformers.tokenization_utils_base import AddedToken\n+from transformers.utils import logging\n+\n+\n+device = \"cpu\"\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+# TODO add sequence length variations here\n+\n+PALIGEMMA2_VARIANTS = [\"2b-224\", \"2b-448\", \"2b-896\", \"9b-224\", \"9b-448\", \"9b-896\", \"27b-224\", \"27b-448\", \"27b-896\"]\n+VARIANT_CONFIGS = {\n+    \"2b\": {\n+        \"num_positions\": 256,\n+        \"hidden_size\": 2304,\n+        \"num_hidden_layers\": 26,\n+        \"intermediate_size\": 9216,\n+        \"num_key_value_heads\": 4,\n+        \"num_attention_heads\": 8,\n+        \"head_dim\": 256,\n+        \"query_pre_attn_scalar\": 256,\n+    },\n+    \"9b\": {\n+        \"num_positions\": 1024,\n+        \"hidden_size\": 3584,\n+        \"num_hidden_layers\": 42,\n+        \"intermediate_size\": 14336,\n+        \"num_key_value_heads\": 8,\n+        \"num_attention_heads\": 16,\n+        \"head_dim\": 256,\n+        \"query_pre_attn_scalar\": 256,\n+    },\n+    \"27b\": {\n+        \"num_positions\": 4096,\n+        \"hidden_size\": 4608,\n+        \"num_hidden_layers\": 46,\n+        \"intermediate_size\": 36864,\n+        \"num_key_value_heads\": 16,\n+        \"num_attention_heads\": 32,\n+        \"head_dim\": 128,\n+        \"query_pre_attn_scalar\": 4608 // 32,  # scaling is different for the 28b\n+    },\n+}\n+\n+DTYPES = {\"float32\": torch.float32, \"bfloat16\": torch.bfloat16, \"float16\": torch.float16}\n+\n+\n+def get_paligemma2_config(variant: str, precision: str):\n+    config = {\n+        \"image_token_index\": None,\n+        \"pad_token_id\": 0,\n+        \"bos_token_id\": 2,\n+        \"eos_token_id\": 1,\n+    }\n+    base_variant = variant.split(\"-\")[0]\n+\n+    if variant in PALIGEMMA2_VARIANTS:\n+        image_size = int(variant.split(\"-\")[1])\n+        variant_config = VARIANT_CONFIGS[base_variant]\n+        patch_size = 14\n+        num_image_tokens = (image_size**2) // (patch_size**2)\n+        config[\"projection_dim\"] = variant_config[\"hidden_size\"]\n+        config[\"image_token_index\"] = 257152\n+        config[\"num_hidden_layers\"] = variant_config[\"num_hidden_layers\"]  # For generate\n+        text_config = Gemma2Config.from_pretrained(\"google/gemma-2-2b-it\").to_dict()\n+        sup_text_config = {\n+            \"model_type\": \"gemma2\",\n+            \"vocab_size\": 257152,\n+            \"num_hidden_layers\": variant_config[\"num_hidden_layers\"],\n+            \"num_key_value_heads\": variant_config[\"num_key_value_heads\"],\n+            \"head_dim\": variant_config[\"head_dim\"],\n+            \"torch_dtype\": precision,\n+            \"hidden_size\": variant_config[\"hidden_size\"],\n+            \"hidden_activation\": \"gelu_pytorch_tanh\",\n+            \"num_attention_heads\": variant_config[\"num_attention_heads\"],\n+            \"intermediate_size\": variant_config[\"intermediate_size\"],\n+            \"is_encoder_decoder\": False,\n+            \"query_pre_attn_scalar\": variant_config[\"query_pre_attn_scalar\"],\n+        }\n+        text_config.update(sup_text_config)\n+\n+        vision_config = {\n+            \"num_positions\": variant_config[\"num_positions\"],  # not useful, to remove\n+            \"torch_dtype\": precision,\n+            \"image_size\": image_size,\n+            \"patch_size\": patch_size,\n+            \"num_image_tokens\": num_image_tokens,\n+            \"hidden_size\": 1152,\n+            \"intermediate_size\": 4304,\n+            \"num_hidden_layers\": 27,\n+            \"num_attention_heads\": 16,\n+            \"projection_dim\": variant_config[\"hidden_size\"],\n+            \"hidden_act\": \"gelu_pytorch_tanh\",\n+            \"vision_use_head\": False,\n+        }\n+        final_config = PaliGemmaConfig(text_config=text_config, vision_config=vision_config, **config)\n+    else:\n+        raise ValueError(f\"Identifier {variant} not supported. Available: {PALIGEMMA2_VARIANTS}\")\n+    return final_config\n+\n+\n+def slice_state_dict(state_dict, config):\n+    # fmt: off\n+    # patch embeddings\n+    state_dict[\"vision_tower.vision_model.embeddings.patch_embedding.weight\"] = state_dict.pop(\"img/embedding/kernel\").transpose(\n+        3, 2, 0, 1\n+    )\n+    state_dict[\"vision_tower.vision_model.embeddings.patch_embedding.bias\"] = state_dict.pop(\"img/embedding/bias\")\n+    # positional embeddings\n+    state_dict[\"vision_tower.vision_model.embeddings.position_embedding.weight\"] = state_dict.pop(\"img/pos_embedding\").reshape(\n+        -1, config.vision_config.hidden_size\n+    )\n+\n+    # extract vision layers to be sliced at index 0. There are 27 layers in the base model.\n+    encoderblock_layernorm0_scale = state_dict.pop(\"img/Transformer/encoderblock/LayerNorm_0/scale\")\n+    encoderblock_layernorm0_bias = state_dict.pop(\"img/Transformer/encoderblock/LayerNorm_0/bias\")\n+    encoderblock_layernorm1_scale = state_dict.pop(\"img/Transformer/encoderblock/LayerNorm_1/scale\")\n+    encoderblock_layernorm1_bias = state_dict.pop(\"img/Transformer/encoderblock/LayerNorm_1/bias\")\n+\n+    encoderblock_mlp_dense0_kernel= state_dict.pop(\"img/Transformer/encoderblock/MlpBlock_0/Dense_0/kernel\")\n+    encoderblock_mlp_dense0_bias= state_dict.pop(\"img/Transformer/encoderblock/MlpBlock_0/Dense_0/bias\")\n+    encoderblock_mlp_dense1_kernel= state_dict.pop(\"img/Transformer/encoderblock/MlpBlock_0/Dense_1/kernel\")\n+    encoderblock_mlp_dense1_bias= state_dict.pop(\"img/Transformer/encoderblock/MlpBlock_0/Dense_1/bias\")\n+\n+    encoderblock_attention_0_key_kernel = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/key/kernel\")\n+    encoderblock_attention_0_key_bias = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/key/bias\")\n+    encoderblock_attention_0_value_kernel = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/value/kernel\")\n+    encoderblock_attention_0_value_bias = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/value/bias\")\n+    encoderblock_attention_0_query_kernel = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/query/kernel\")\n+    encoderblock_attention_0_query_bias = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/query/bias\")\n+    encoderblock_attention_0_out_kernel = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/out/kernel\")\n+    encoderblock_attention_0_out_bias = state_dict.pop(\"img/Transformer/encoderblock/MultiHeadDotProductAttention_0/out/bias\")\n+\n+    for i in range(config.vision_config.num_hidden_layers):\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.layer_norm1.weight\"] = encoderblock_layernorm0_scale[i].transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.layer_norm1.bias\"] = encoderblock_layernorm0_bias[i]\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.layer_norm2.weight\"] = encoderblock_layernorm1_scale[i].transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.layer_norm2.bias\"] = encoderblock_layernorm1_bias[i]\n+\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.mlp.fc1.weight\"] = encoderblock_mlp_dense0_kernel[i].transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.mlp.fc1.bias\"] = encoderblock_mlp_dense0_bias[i]\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.mlp.fc2.weight\"] = encoderblock_mlp_dense1_kernel[i].transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.mlp.fc2.bias\"] = encoderblock_mlp_dense1_bias[i]\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.k_proj.weight\"] = encoderblock_attention_0_key_kernel[i].reshape(-1, config.vision_config.hidden_size).transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.k_proj.bias\"] = encoderblock_attention_0_key_bias[i].reshape(-1, config.vision_config.hidden_size).reshape(-1)\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.v_proj.weight\"] = encoderblock_attention_0_value_kernel[i].reshape(-1, config.vision_config.hidden_size).transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.v_proj.bias\"] = encoderblock_attention_0_value_bias[i].reshape(-1, config.vision_config.hidden_size).reshape(-1)\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.q_proj.weight\"] = encoderblock_attention_0_query_kernel[i].reshape(-1, config.vision_config.hidden_size).transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.q_proj.bias\"] = encoderblock_attention_0_query_bias[i].reshape(-1, config.vision_config.hidden_size).reshape(-1)\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.out_proj.weight\"] = encoderblock_attention_0_out_kernel[i].reshape(-1, config.vision_config.hidden_size).transpose()\n+        state_dict[f\"vision_tower.vision_model.encoder.layers.{i}.self_attn.out_proj.bias\"] = encoderblock_attention_0_out_bias[i].reshape(-1, config.vision_config.hidden_size).reshape(-1)\n+\n+    state_dict[\"vision_tower.vision_model.post_layernorm.weight\"] = state_dict.pop(\"img/Transformer/encoder_norm/scale\").transpose()\n+    state_dict[\"vision_tower.vision_model.post_layernorm.bias\"] = state_dict.pop(\"img/Transformer/encoder_norm/bias\")\n+\n+    # multimodal projector\n+\n+    state_dict['multi_modal_projector.linear.weight'] = state_dict.pop(\"img/head/kernel\").transpose()\n+    state_dict['multi_modal_projector.linear.bias'] = state_dict.pop(\"img/head/bias\")\n+\n+    # text decoder (gemma)\n+\n+    embedding_vector = state_dict.pop(\"llm/embedder/input_embedding\")\n+    state_dict[\"language_model.model.embed_tokens.weight\"] = embedding_vector\n+\n+    # pop the einsum attention + mlp representations. There are 26 layers in gemma2-2b.\n+\n+    llm_attention_attn_vec_einsum = state_dict.pop(\"llm/layers/attn/attn_vec_einsum/w\")\n+    #  (26, 2, 4, 2304, 256) for 2b-224, 4 kv heads and 26 layers\n+    llm_attention_kv_einsum = state_dict.pop(\"llm/layers/attn/kv_einsum/w\")\n+    llm_attention_q_einsum = state_dict.pop(\"llm/layers/attn/q_einsum/w\")\n+    llm_mlp_gating_einsum = state_dict.pop(\"llm/layers/mlp/gating_einsum\")\n+    llm_mlp_linear = state_dict.pop(\"llm/layers/mlp/linear\")\n+    # TODO verify correctness of layer norm loading\n+    llm_input_layernorm = state_dict.pop(\"llm/layers/pre_attention_norm/scale\")\n+    llm_pre_feedforward_layernorm = state_dict.pop(\"llm/layers/pre_ffw_norm/scale\")\n+\n+    llm_post_attention_layernorm = state_dict.pop(\"llm/layers/post_attention_norm/scale\")\n+    llm_post_feedforward_layernorm = state_dict.pop(\"llm/layers/post_ffw_norm/scale\")\n+\n+    for i in range(config.text_config.num_hidden_layers):\n+        # llm_attention_q_einsum[i].shape = (8, 2048, 256)\n+        # q_proj_weight_reshaped = llm_attention_q_einsum[i].transpose(0, 2, 1).reshape(config.text_config.num_attention_heads * config.text_config.head_dim, config.text_config.hidden_size)\n+\n+        \"\"\"\n+        q shape (8, 2304, 256)\n+        k shape (4, 2304, 256)\n+        v shape (4, 2304, 256)\n+        o shape (8, 256, 2304)\n+\n+        \"\"\"\n+        q_transpose = (0, 2, 1)\n+        k_transpose = (0, 2, 1)\n+        v_transpose = (0, 2, 1)\n+        o_transpose = (2, 0, 1)\n+\n+        q_weight_matrices = llm_attention_q_einsum[i].transpose(*q_transpose)\n+        q_proj_weight_reshaped = q_weight_matrices\n+        q_proj_weight_reshaped = q_proj_weight_reshaped.reshape(config.text_config.num_attention_heads * config.text_config.head_dim, config.text_config.hidden_size)\n+        state_dict[f\"language_model.model.layers.{i}.self_attn.q_proj.weight\"] = q_proj_weight_reshaped\n+        # Shape: (4, 2304, 256)\n+        k_weight_matrices = llm_attention_kv_einsum[i, 0].transpose(*k_transpose)\n+        k_proj_weight_reshaped = k_weight_matrices.reshape(\n+            config.text_config.num_key_value_heads * config.text_config.head_dim,\n+            config.text_config.hidden_size\n+        )\n+        state_dict[f\"language_model.model.layers.{i}.self_attn.k_proj.weight\"] = k_proj_weight_reshaped\n+        # llm_attention_kv_einsum[i, 1].shape = (num_key_value_heads, hidden_size, head_dim)\n+        v_weight_matrices = llm_attention_kv_einsum[i, 1].transpose(*v_transpose) # Shape: (4, 2304, 256)\n+        v_proj_weight_reshaped = v_weight_matrices.reshape(\n+            config.text_config.num_key_value_heads * config.text_config.head_dim,\n+            config.text_config.hidden_size\n+        )\n+        state_dict[f\"language_model.model.layers.{i}.self_attn.v_proj.weight\"] = v_proj_weight_reshaped\n+\n+        # output projection.\n+\n+        # llm_attention_attn_vec_einsum[i].shape = (8, 256, 2304)\n+        o_proj_weight_reshaped = llm_attention_attn_vec_einsum[i].transpose(*o_transpose).reshape(config.text_config.hidden_size, config.text_config.num_attention_heads * config.text_config.head_dim)\n+        state_dict[f\"language_model.model.layers.{i}.self_attn.o_proj.weight\"] = o_proj_weight_reshaped\n+        # mlp layers\n+        gate_proj_weight = llm_mlp_gating_einsum[i, 0]\n+        state_dict[f\"language_model.model.layers.{i}.mlp.gate_proj.weight\"] = gate_proj_weight.transpose()\n+        up_proj_weight = llm_mlp_gating_einsum[i, 1]\n+        state_dict[f\"language_model.model.layers.{i}.mlp.up_proj.weight\"] = up_proj_weight.transpose()\n+        state_dict[f\"language_model.model.layers.{i}.mlp.down_proj.weight\"] = llm_mlp_linear[i].transpose()\n+        state_dict[f\"language_model.model.layers.{i}.input_layernorm.weight\"] = llm_input_layernorm[i]\n+        state_dict[f\"language_model.model.layers.{i}.post_attention_layernorm.weight\"] = llm_post_attention_layernorm[i]\n+        state_dict[f\"language_model.model.layers.{i}.pre_feedforward_layernorm.weight\"] = llm_pre_feedforward_layernorm[i]\n+        state_dict[f\"language_model.model.layers.{i}.post_feedforward_layernorm.weight\"] = llm_post_feedforward_layernorm[i]\n+    state_dict[\"language_model.model.norm.weight\"] = state_dict.pop(\"llm/final_norm/scale\")\n+    state_dict[\"language_model.lm_head.weight\"] = embedding_vector # weights are tied.\n+    [k for k in state_dict.keys() if not k.startswith('vision') and not k.startswith('language')]\n+    # fmt: on\n+    for key, value in state_dict.items():\n+        if not isinstance(value, torch.Tensor):\n+            try:\n+                if value.dtype == jnp.bfloat16:\n+                    value = jnp.array(value).astype(jnp.float32)\n+                    value = np.array(value)\n+                    state_dict[key] = torch.from_numpy(value).to(torch.bfloat16)\n+                else:\n+                    state_dict[key] = torch.from_numpy(value)\n+            except Exception as initial_exception:\n+                raise ValueError(f\"Conversion failed from jax weights with {initial_exception}. Check your inputs.\")\n+    return state_dict\n+\n+\n+def flatten_nested_dict(params, parent_key=\"\", sep=\"/\", precision: int = \"float32\"):\n+    items = []\n+\n+    for k, v in params.items():\n+        k = k.removeprefix(\"params/\")\n+        new_key = parent_key + sep + k if parent_key else k\n+\n+        if isinstance(v, collections.abc.MutableMapping):\n+            items.extend(flatten_nested_dict(v, parent_key=new_key, sep=sep, precision=precision).items())\n+        else:\n+            if precision == \"bfloat16\":\n+                try:\n+                    v = v.view(ml_dtypes.bfloat16)\n+                except Exception as initial_exception:\n+                    raise ValueError(f\"Conversion failed from bfloat16 with {initial_exception}, check your inputs.\")\n+            items.append((new_key, v))\n+    return dict(items)\n+\n+\n+@torch.no_grad()\n+def convert_paligemma2_checkpoint(\n+    checkpoint_path,\n+    pytorch_dump_folder_path,\n+    variant: str,\n+    precision: str,\n+    do_convert_weights=False,\n+):\n+    \"\"\"\n+    Read checkpoints from flax npz files, rename/reshape, send result to state dict and verify logits if needed.\n+    \"\"\"\n+    config = get_paligemma2_config(variant, precision=precision)\n+    if do_convert_weights:\n+        tokenizer_id = \"google/paligemma-3b-pt-224\"  # same tokenizer as paligemma 1\n+        tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n+        image_token = AddedToken(\"<image>\", normalized=False, special=True)\n+        tokens_to_add = {\"additional_special_tokens\": [image_token]}\n+        tokenizer.add_special_tokens(tokens_to_add)\n+\n+        # tokenizer.padding_side = 'right' # uncomment for testing purposes only.\n+\n+        image_processor = SiglipImageProcessor.from_pretrained(\"google/paligemma-3b-pt-224\")\n+        image_processor.size = {\"width\": config.vision_config.image_size, \"height\": config.vision_config.image_size}\n+        image_processor.image_seq_length = config.vision_config.num_image_tokens\n+\n+        processor = PaliGemmaProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+        data = jnp.load(checkpoint_path)\n+        state_dict = flatten_nested_dict(data, precision=precision)\n+        del data\n+        state_dict_transformers = slice_state_dict(state_dict, config)\n+        del state_dict\n+        del config.hidden_size  # this key is unused\n+        model = PaliGemmaForConditionalGeneration(config).to(device).eval()\n+        model.load_state_dict(state_dict_transformers)\n+        del state_dict_transformers\n+        model.config.text_config._attn_implementation = \"sdpa\"\n+\n+        # model expansion to get random embeds of image tokens\n+        pad_shape = 64  # for performance reasons\n+        pre_expansion_embeddings = model.language_model.model.embed_tokens.weight.data\n+        mu = torch.mean(pre_expansion_embeddings, dim=0).float()\n+        n = pre_expansion_embeddings.size()[0]\n+        sigma = ((pre_expansion_embeddings - mu).T @ (pre_expansion_embeddings - mu)) / n\n+        dist = torch.distributions.multivariate_normal.MultivariateNormal(mu, covariance_matrix=1e-5 * sigma)\n+\n+        # We add an image token so we resize the model\n+        model.resize_token_embeddings(config.text_config.vocab_size + 2, pad_shape)\n+        model.language_model.model.embed_tokens.weight.data[257152:] = torch.stack(\n+            tuple(\n+                (dist.sample() for _ in range(model.language_model.model.embed_tokens.weight.data[257152:].shape[0]))\n+            ),\n+            dim=0,\n+        )\n+        model.language_model.lm_head.weight.data[257152:] = torch.stack(\n+            tuple((dist.sample() for _ in range(model.language_model.lm_head.weight.data[257152:].shape[0]))),\n+            dim=0,\n+        )\n+        # convert to needed precision\n+\n+        model.to(DTYPES[precision])\n+        model.save_pretrained(pytorch_dump_folder_path, safe_serialization=True)\n+        processor.save_pretrained(pytorch_dump_folder_path)\n+\n+    else:\n+        processor = PaliGemmaProcessor.from_pretrained(pytorch_dump_folder_path, do_rescale=False)\n+        model = (\n+            PaliGemmaForConditionalGeneration.from_pretrained(pytorch_dump_folder_path, attn_implementation=\"sdpa\")\n+            .to(device)\n+            .eval()\n+        )\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--checkpoint_path\",\n+        required=True,\n+        type=str,\n+        help=\"Path to the .npz checkpoint\",\n+    )\n+\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        required=True,\n+        type=str,\n+        help=\"Path to the output directory where model and processor will be saved.\",\n+    )\n+\n+    parser.add_argument(\n+        \"--precision\",\n+        choices=[\"float32\", \"bfloat16\", \"float16\"],\n+        type=str,\n+        help=\"Precision identifier for model conversion - should match the base checkpoint precision.\",\n+    )\n+\n+    parser.add_argument(\n+        \"--variant\",\n+        default=\"2b-224\",\n+        choices=PALIGEMMA2_VARIANTS,\n+        type=str,\n+        help=\"String identifier of the paligemma2 variant to convert.\",\n+    )\n+\n+    parser.add_argument(\n+        \"--do_convert_weights\", action=\"store_true\", help=\"Whether or not to reload and convert the weights.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_paligemma2_checkpoint(\n+        checkpoint_path=args.checkpoint_path,\n+        pytorch_dump_folder_path=args.pytorch_dump_folder_path,\n+        variant=args.variant,\n+        precision=args.precision,\n+        do_convert_weights=args.do_convert_weights,\n+    )"
        },
        {
            "sha": "b4a231561ba791b6829e1cdcaaded79b4dc3f504",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 25,
            "deletions": 12,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
            "patch": "@@ -21,7 +21,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n@@ -341,19 +341,28 @@ def tie_weights(self):\n         return self.language_model.tie_weights()\n \n     def _update_causal_mask(\n-        self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n+        self,\n+        attention_mask,\n+        token_type_ids,\n+        past_key_values,\n+        cache_position,\n+        input_ids=None,\n+        inputs_embeds=None,\n+        is_training: bool = False,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n \n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        dtype = inputs_embeds.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = inputs_embeds.shape[1]\n+        min_dtype = torch.finfo(self.dtype).min\n+        inputs_lead_dim = input_ids.shape[0] if input_ids is not None else inputs_embeds.shape[0]\n+        sequence_length = input_ids.shape[1] if input_ids is not None else inputs_embeds.shape[1]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n+        elif isinstance(past_key_values, HybridCache):\n+            target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = (\n                 attention_mask.shape[-1]\n@@ -366,7 +375,7 @@ def _update_causal_mask(\n             return attention_mask\n \n         causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n         )\n         # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n         if sequence_length != 1:\n@@ -376,7 +385,7 @@ def _update_causal_mask(\n                 causal_mask[:, :sequence_length] = 0.0\n \n         causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_embeds.shape[0], 1, -1, -1)\n+        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n         if attention_mask is not None:\n             causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n             mask_length = attention_mask.shape[-1]\n@@ -405,7 +414,7 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         image_outputs = self.vision_tower(pixel_values)\n         selected_image_feature = image_outputs.last_hidden_state\n         image_features = self.multi_modal_projector(selected_image_feature)\n-        image_features = image_features / (self.config.hidden_size**0.5)\n+        image_features = image_features / (self.config.text_config.hidden_size**0.5)\n         return image_features\n \n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n@@ -516,9 +525,8 @@ def forward(\n             labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n \n         causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training\n+            attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n         )\n-\n         outputs = self.language_model(\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n@@ -579,6 +587,7 @@ def prepare_inputs_for_generation(\n         token_type_ids=None,\n         use_cache=True,\n         num_logits_to_keep=None,\n+        labels=None,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -598,10 +607,14 @@ def prepare_inputs_for_generation(\n         # position_ids in Paligemma are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1\n-\n         # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-\n+        is_training = token_type_ids is not None and labels is not None\n+        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n+            causal_mask = self._update_causal_mask(\n+                attention_mask, token_type_ids, past_key_values, cache_position, input_ids, inputs_embeds, is_training\n+            )\n+            model_inputs[\"attention_mask\"] = causal_mask\n         return model_inputs"
        },
        {
            "sha": "cb35aab66cba491fd2438400656cffff5bf0972b",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
            "patch": "@@ -269,7 +269,7 @@ def __call__(\n                 logger.warning(\n                     \"You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special \"\n                     \"image tokens in the text, as many tokens as there are images per each text. It is recommended to \"\n-                    \"add `<image>` tokens in the very beginning of your text and `<bos>` token after that. For this call, we will infer how many images \"\n+                    \"add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images \"\n                     \"each text has and add special tokens.\"\n                 )\n \n@@ -304,9 +304,16 @@ def __call__(\n                 ]\n                 images = make_batched_images(images)\n             else:\n-                text = [sample.replace(IMAGE_TOKEN, IMAGE_TOKEN * self.image_seq_length) for sample in text]\n-                input_strings = [f\"{sample}\\n\" for sample in text]\n-\n+                expanded_samples = []\n+                for sample in text:\n+                    expanded_sample = sample.replace(IMAGE_TOKEN, IMAGE_TOKEN * self.image_seq_length)\n+                    bos_rfind_index = expanded_sample.rfind(IMAGE_TOKEN)\n+                    bos_index = bos_rfind_index + len(IMAGE_TOKEN) if bos_rfind_index != -1 else 0\n+                    expanded_sample = (\n+                        expanded_sample[:bos_index] + self.tokenizer.bos_token + expanded_sample[bos_index:]\n+                    )\n+                    expanded_samples.append(expanded_sample)\n+                input_strings = [f\"{sample}\\n\" for sample in expanded_samples]\n         pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n         # max_length has to account for the image tokens"
        },
        {
            "sha": "5ffea7ffe5508730a6a1cd60d7ce2bf2d8a433e2",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
            "patch": "@@ -347,6 +347,11 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n+    @unittest.skip(\"PaliGemma is not compatible with end-to-end generation compilation\")\n+    def test_generate_compile_fullgraph(self):\n+        pass\n+\n \n @slow\n @require_torch"
        },
        {
            "sha": "e301bf304b1ed8da4e45d69a506ff9a01188a8c8",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=a5bb52847139bf6ad7489ac62a5fb6d0fa3d2ec6",
            "patch": "@@ -63,8 +63,8 @@ def test_text_with_image_tokens(self):\n         tokenizer = self.get_component(\"tokenizer\")\n \n         processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n-        text_multi_images = \"<image><image><bos>Dummy text!\"\n-        text_single_image = \"<image><bos>Dummy text!\"\n+        text_multi_images = \"<image><image>Dummy text!\"\n+        text_single_image = \"<image>Dummy text!\"\n         text_no_image = \"Dummy text!\"\n \n         image = self.prepare_image_inputs()\n@@ -85,7 +85,7 @@ def test_text_with_image_tokens(self):\n             self.assertTrue(out_noimage[k].tolist() == out_multiimages[k].tolist())\n \n         text_batched = [\"Dummy text!\", \"Dummy text!\"]\n-        text_batched_with_image = [\"<image><bos>Dummy text!\", \"<image><bos>Dummy text!\"]\n+        text_batched_with_image = [\"<image>Dummy text!\", \"<image>Dummy text!\"]\n         out_images = processor(text=text_batched_with_image, images=[image, image], return_tensors=\"np\")\n         out_noimage_nested = processor(text=text_batched, images=[[image], [image]], return_tensors=\"np\")\n         out_noimage = processor(text=text_batched, images=[image, image], return_tensors=\"np\")"
        }
    ],
    "stats": {
        "total": 478,
        "additions": 459,
        "deletions": 19
    }
}