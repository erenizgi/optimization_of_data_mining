{
    "author": "ivarflakstad",
    "message": "Add AMD expectation to test_gpt2_sample (#38079)",
    "sha": "7eaa90b87bcbd5013737fa183a5f7166c891fa9f",
    "files": [
        {
            "sha": "ed172fd91dda4743b553f9a4bdf9f66b942c594e",
            "filename": "tests/models/gpt2/test_modeling_gpt2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/7eaa90b87bcbd5013737fa183a5f7166c891fa9f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7eaa90b87bcbd5013737fa183a5f7166c891fa9f/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_gpt2.py?ref=7eaa90b87bcbd5013737fa183a5f7166c891fa9f",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers import GPT2Config, is_torch_available\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_flash_attn,\n     require_torch,\n@@ -817,10 +818,14 @@ def test_gpt2_sample(self):\n         output_seq_strs = tokenizer.batch_decode(output_seq, skip_special_tokens=True)\n         output_seq_tt_strs = tokenizer.batch_decode(output_seq_tt, skip_special_tokens=True)\n \n-        EXPECTED_OUTPUT_STR = (\n-            \"Today is a nice day and if you don't know anything about the state of play during your holiday\"\n-        )\n-        self.assertEqual(output_str, EXPECTED_OUTPUT_STR)\n+        expected_outputs = Expectations(\n+            {\n+                (\"rocm\", None): 'Today is a nice day and we can do this again.\"\\n\\nDana said that she will',\n+                (\"cuda\", None): \"Today is a nice day and if you don't know anything about the state of play during your holiday\",\n+            }\n+        )  # fmt: skip\n+        EXPECTED_OUTPUT = expected_outputs.get_expectation()\n+        self.assertEqual(output_str, EXPECTED_OUTPUT)\n         self.assertTrue(\n             all(output_seq_strs[idx] != output_seq_tt_strs[idx] for idx in range(len(output_seq_tt_strs)))\n         )  # token_type_ids should change output"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 9,
        "deletions": 4
    }
}