{
    "author": "cyyever",
    "message": "Remove SageMakerTrainer (#41267)\n\n* Remove SageMakerTrainer\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More removal\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "30b79effb5d13c9319c1a32aa239c1cc238f67cb",
    "files": [
        {
            "sha": "ec04af63a8cbd6c353ee6064f33158d22ec2db1a",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/30b79effb5d13c9319c1a32aa239c1cc238f67cb/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/30b79effb5d13c9319c1a32aa239c1cc238f67cb/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=30b79effb5d13c9319c1a32aa239c1cc238f67cb",
            "patch": "@@ -471,7 +471,6 @@\n         \"prune_layer\",\n         \"infer_device\",\n     ]\n-    _import_structure[\"sagemaker\"] = []\n     _import_structure[\"time_series_utils\"] = []\n     _import_structure[\"trainer\"] = [\"Trainer\"]\n     _import_structure[\"trainer_pt_utils\"] = [\"torch_distributed_zero_first\"]"
        },
        {
            "sha": "98fe38de89cd025911d03669f9e22b03ab0768bd",
            "filename": "src/transformers/sagemaker/__init__.py",
            "status": "removed",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fsagemaker%2F__init__.py?ref=aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86",
            "patch": "@@ -1,16 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from .trainer_sm import SageMakerTrainer\n-from .training_args_sm import SageMakerTrainingArguments, is_sagemaker_dp_enabled"
        },
        {
            "sha": "6ab4e01acdbcd3ade1afc2339a75850bc538bd7a",
            "filename": "src/transformers/sagemaker/trainer_sm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2Ftrainer_sm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2Ftrainer_sm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fsagemaker%2Ftrainer_sm.py?ref=aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86",
            "patch": "@@ -1,30 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import warnings\n-\n-from ..trainer import Trainer\n-from ..utils import logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-\n-class SageMakerTrainer(Trainer):\n-    def __init__(self, args=None, **kwargs):\n-        warnings.warn(\n-            \"`SageMakerTrainer` is deprecated and will be removed in v5 of Transformers. You can use `Trainer` \"\n-            \"instead.\",\n-            FutureWarning,\n-        )\n-        super().__init__(args=args, **kwargs)"
        },
        {
            "sha": "0dcd2adf931dc930714a0a9bacb49f5a514cb664",
            "filename": "src/transformers/sagemaker/training_args_sm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 137,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2Ftraining_args_sm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/src%2Ftransformers%2Fsagemaker%2Ftraining_args_sm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fsagemaker%2Ftraining_args_sm.py?ref=aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86",
            "patch": "@@ -1,137 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import importlib.util\n-import json\n-import os\n-import warnings\n-from dataclasses import dataclass, field\n-from functools import cached_property\n-\n-import torch\n-\n-from ..training_args import TrainingArguments\n-from ..utils import is_sagemaker_dp_enabled, logging\n-\n-\n-logger = logging.get_logger(__name__)\n-\n-# TODO: should be moved to `utils` after refactoring of SageMakerTrainer\n-\n-\n-def is_sagemaker_model_parallel_available():\n-    # Get the sagemaker specific mp parameters from smp_options variable.\n-    smp_options = os.getenv(\"SM_HP_MP_PARAMETERS\", \"{}\")\n-    try:\n-        # Parse it and check the field \"partitions\" is included, it is required for model parallel.\n-        smp_options = json.loads(smp_options)\n-        if \"partitions\" not in smp_options:\n-            return False\n-    except json.JSONDecodeError:\n-        return False\n-\n-    # Get the sagemaker specific framework parameters from mpi_options variable.\n-    mpi_options = os.getenv(\"SM_FRAMEWORK_PARAMS\", \"{}\")\n-    try:\n-        # Parse it and check the field \"sagemaker_distributed_dataparallel_enabled\".\n-        mpi_options = json.loads(mpi_options)\n-        if not mpi_options.get(\"sagemaker_mpi_enabled\", False):\n-            return False\n-    except json.JSONDecodeError:\n-        return False\n-    # Lastly, check if the `smdistributed` module is present.\n-    return importlib.util.find_spec(\"smdistributed\") is not None\n-\n-\n-if is_sagemaker_model_parallel_available():\n-    import smdistributed.modelparallel.torch as smp\n-\n-    smp.init()\n-\n-\n-@dataclass\n-class SageMakerTrainingArguments(TrainingArguments):\n-    mp_parameters: str = field(\n-        default=\"\",\n-        metadata={\"help\": \"Used by the SageMaker launcher to send mp-specific args. Ignored in SageMakerTrainer\"},\n-    )\n-\n-    def __post_init__(self):\n-        super().__post_init__()\n-        warnings.warn(\n-            \"`SageMakerTrainingArguments` is deprecated and will be removed in v5 of Transformers. You can use \"\n-            \"`TrainingArguments` instead.\",\n-            FutureWarning,\n-        )\n-\n-    @cached_property\n-    def _setup_devices(self) -> \"torch.device\":\n-        logger.info(\"PyTorch: setting up devices\")\n-        if torch.distributed.is_available() and torch.distributed.is_initialized() and self.local_rank == -1:\n-            logger.warning(\n-                \"torch.distributed process group is initialized, but local_rank == -1. \"\n-                \"In order to use Torch DDP, launch your script with `python -m torch.distributed.launch\"\n-            )\n-        if self.no_cuda:\n-            device = torch.device(\"cpu\")\n-            self._n_gpu = 0\n-        elif is_sagemaker_model_parallel_available():\n-            local_rank = smp.local_rank()\n-            device = torch.device(\"cuda\", local_rank)\n-            self._n_gpu = 1\n-        elif is_sagemaker_dp_enabled():\n-            import smdistributed.dataparallel.torch.torch_smddp  # noqa: F401\n-\n-            torch.distributed.init_process_group(backend=\"smddp\", timeout=self.ddp_timeout_delta)\n-            self.local_rank = int(os.getenv(\"SMDATAPARALLEL_LOCAL_RANK\"))\n-            device = torch.device(\"cuda\", self.local_rank)\n-            self._n_gpu = 1\n-        elif self.local_rank == -1:\n-            # if n_gpu is > 1 we'll use nn.DataParallel.\n-            # If you only want to use a specific subset of GPUs use `CUDA_VISIBLE_DEVICES=0`\n-            # Explicitly set CUDA to the first (index 0) CUDA device, otherwise `set_device` will\n-            # trigger an error that a device index is missing. Index 0 takes into account the\n-            # GPUs available in the environment, so `CUDA_VISIBLE_DEVICES=1,2` with `cuda:0`\n-            # will use the first GPU in that env, i.e. GPU#1\n-            device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n-            # Sometimes the line in the postinit has not been run before we end up here, so just checking we're not at\n-            # the default value.\n-            self._n_gpu = torch.cuda.device_count()\n-        else:\n-            # Here, we'll use torch.distributed.\n-            # Initializes the distributed backend which will take care of synchronizing nodes/GPUs\n-            if not torch.distributed.is_initialized():\n-                torch.distributed.init_process_group(backend=\"nccl\", timeout=self.ddp_timeout_delta)\n-            device = torch.device(\"cuda\", self.local_rank)\n-            self._n_gpu = 1\n-\n-        if device.type == \"cuda\":\n-            torch.cuda.set_device(device)\n-\n-        return device\n-\n-    @property\n-    def world_size(self):\n-        if is_sagemaker_model_parallel_available():\n-            return smp.dp_size()\n-\n-        return super().world_size\n-\n-    @property\n-    def place_model_on_device(self):\n-        return not is_sagemaker_model_parallel_available()\n-\n-    @property\n-    def _no_sync_in_gradient_accumulation(self):\n-        return False"
        },
        {
            "sha": "2074e7e3670a557d4706c681fbb7e41d0847bdd9",
            "filename": "tests/sagemaker/README.md",
            "status": "modified",
            "additions": 5,
            "deletions": 8,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/30b79effb5d13c9319c1a32aa239c1cc238f67cb/tests%2Fsagemaker%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/30b79effb5d13c9319c1a32aa239c1cc238f67cb/tests%2Fsagemaker%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2FREADME.md?ref=30b79effb5d13c9319c1a32aa239c1cc238f67cb",
            "patch": "@@ -9,7 +9,7 @@ This document explains the testing strategy for releasing the new Hugging Face D\n \n ### Run Tests:\n \n-Before we can run the tests we need to adjust the `requirements.txt` for PyTorch under `/tests/sagemaker/scripts/pytorch` and for TensorFlow under `/tests/sagemaker/scripts/pytorch`. We adjust the branch to the new RC-tag.\n+Before we can run the tests we need to adjust the `requirements.txt` for PyTorch. We adjust the branch to the new RC-tag.\n \n ```\n git+https://github.com/huggingface/transformers.git@v4.5.0.rc0 # install main or adjust it with vX.X.X for installing version specific-transforms\n@@ -28,7 +28,7 @@ After we have released the Release Candidate we need to create a PR at the [Deep\n \n **Creating the update PR:**\n \n-1. Update the two latest `buildspec.yaml` config for [PyTorch](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch) and [TensorFlow](https://github.com/aws/deep-learning-containers/tree/master/huggingface/tensorflow). The two latest `buildspec.yaml` are the `buildspec.yaml` without a version tag and the one with the highest framework version, e.g. `buildspec-1-7-1.yml` and not `buildspec-1-6.yml`.  \n+1. Update the latest `buildspec.yaml` config for [PyTorch](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch). The two latest `buildspec.yaml` are the `buildspec.yaml` without a version tag and the one with the highest framework version, e.g. `buildspec-1-7-1.yml` and not `buildspec-1-6.yml`.\n \n To update the `buildspec.yaml` we need to adjust either the `transformers_version` or the `datasets_version` or both. Example for upgrading to `transformers 4.5.0` and `datasets 1.6.0`.\n ```yaml\n@@ -73,15 +73,14 @@ images:\n ## Execute Tests\n \n ### Requirements:\n-AWS is going to release new DLCs for PyTorch and/or TensorFlow. The Tests should run on the new framework versions with current `transformers` release to validate the new framework release is compatible with the `transformers` version. To run these tests you need credentials for the HF SageMaker AWS Account. You can ask @philschmid or @n1t0 to get access. AWS will notify us with a new issue in the repository pointing to their framework upgrade PR.\n+AWS is going to release new DLCs for PyTorch. The Tests should run on the new framework versions with current `transformers` release to validate the new framework release is compatible with the `transformers` version. To run these tests you need credentials for the HF SageMaker AWS Account. You can ask @philschmid or @n1t0 to get access. AWS will notify us with a new issue in the repository pointing to their framework upgrade PR.\n \n ### Run Tests:\n \n-Before we can run the tests we need to adjust the `requirements.txt` for Pytorch under `/tests/sagemaker/scripts/pytorch` and for Tensorflow under `/tests/sagemaker/scripts/pytorch`. We add the new framework version to it.\n+Before we can run the tests we need to adjust the `requirements.txt` for Pytorch under `/tests/sagemaker/scripts/pytorch`. We add the new framework version to it.\n \n ```bash\n torch==1.8.1 # for pytorch\n-tensorflow-gpu==2.5.0 # for tensorflow\n ```\n \n After we adjusted the `requirements.txt` we can run Amazon SageMaker tests with. \n@@ -97,7 +96,7 @@ After we have successfully run tests for the new framework version we need to cr\n \n **Creating the update PR:**\n \n-1. Create a new `buildspec.yaml` config for [PyTorch](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch) and [TensorFlow](https://github.com/aws/deep-learning-containers/tree/master/huggingface/tensorflow) and rename the old `buildspec.yaml` to `buildespec-x.x.x`, where `x.x.x` is the base framework version, e.g. if pytorch 1.6.0 is the latest version in `buildspec.yaml` the file should be renamed to `buildspec-yaml-1-6.yaml`. \n+1. Create a new `buildspec.yaml` config for [PyTorch](https://github.com/aws/deep-learning-containers/tree/master/huggingface/pytorch) and rename the old `buildspec.yaml` to `buildespec-x.x.x`, where `x.x.x` is the base framework version, e.g. if pytorch 1.6.0 is the latest version in `buildspec.yaml` the file should be renamed to `buildspec-yaml-1-6.yaml`.\n \n To create the new `buildspec.yaml` we need to adjust  the `version` and the `short_version`. Example for upgrading to `pytorch 1.7.1`. \n \n@@ -144,5 +143,3 @@ images:\n | pytorch-transformers-test-2-ddp     | test bert finetuning using BERT from transformer lib+ PT DPP      | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n | pytorch-transformers-test-2-smd     | test bert finetuning using BERT from transformer lib+ PT SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |\n | pytorch-transformers-test-1-smp     | test roberta finetuning using BERT from transformer lib+ PT SM MP | SageMaker createTrainingJob | 8     | train_runtime, eval_accuracy & eval_loss |\n-| tensorflow-transformers-test-single | Test bert finetuning using BERT from transformer lib+TF           | SageMaker createTrainingJob | 1     | train_runtime, eval_accuracy & eval_loss |\n-| tensorflow-transformers-test-2-smd  | test bert finetuning using BERT from transformer lib+ TF SM DDP   | SageMaker createTrainingJob | 16    | train_runtime, eval_accuracy & eval_loss |"
        },
        {
            "sha": "525b63f1bc8888491cfbf3d2a1e6aca59f4fe529",
            "filename": "tests/sagemaker/scripts/pytorch/run_glue_model_parallelism.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/30b79effb5d13c9319c1a32aa239c1cc238f67cb/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/30b79effb5d13c9319c1a32aa239c1cc238f67cb/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Fpytorch%2Frun_glue_model_parallelism.py?ref=30b79effb5d13c9319c1a32aa239c1cc238f67cb",
            "patch": "@@ -26,7 +26,7 @@\n from datasets import load_dataset, load_metric\n \n import transformers\n-from transformers import (  # Trainer,; TrainingArguments,\n+from transformers import (\n     AutoConfig,\n     AutoModelForSequenceClassification,\n     AutoTokenizer,\n@@ -37,11 +37,9 @@\n     default_data_collator,\n     set_seed,\n )\n-\n-# Will import SageMaker Model parallelism specific Trainer\n-from transformers.sagemaker import SageMakerTrainer as Trainer\n-from transformers.sagemaker import SageMakerTrainingArguments as TrainingArguments\n+from transformers.trainer import Trainer\n from transformers.trainer_utils import get_last_checkpoint\n+from transformers.training_args import TrainingArguments\n from transformers.utils import check_min_version\n \n "
        },
        {
            "sha": "34a3d05b460e2dcac55c9cf3d186afed24c7f9fb",
            "filename": "tests/sagemaker/scripts/tensorflow/requirements.txt",
            "status": "removed",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frequirements.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frequirements.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frequirements.txt?ref=aabf0a03cb5ebf34bf17cf157c88a34a76cb8a86",
            "patch": "@@ -1 +0,0 @@\n-git+https://github.com/huggingface/transformers.git@main # install main or adjust ist with vX.X.X for installing version specific transforms\n\\ No newline at end of file"
        },
        {
            "sha": "613ceaf70f555a9f95f15a2e4e432834466b8c6a",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/30b79effb5d13c9319c1a32aa239c1cc238f67cb/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/30b79effb5d13c9319c1a32aa239c1cc238f67cb/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=30b79effb5d13c9319c1a32aa239c1cc238f67cb",
            "patch": "@@ -772,8 +772,6 @@ src/transformers/quantizers/quantizer_bnb_4bit.py\n src/transformers/quantizers/quantizer_bnb_8bit.py\n src/transformers/quantizers/quantizer_gptq.py\n src/transformers/quantizers/quantizers_utils.py\n-src/transformers/sagemaker/trainer_sm.py\n-src/transformers/sagemaker/training_args_sm.py\n src/transformers/testing_utils.py\n src/transformers/time_series_utils.py\n src/transformers/tokenization_utils.py"
        }
    ],
    "stats": {
        "total": 208,
        "additions": 8,
        "deletions": 200
    }
}