{
    "author": "vaibhav-research",
    "message": "GemmaTokenizer: remove redundant whitespace pre-tokenizer (#43106)\n\n* 43062: fixed redundant gemma pre_tokenizer\n\n* 43062: fixed redundant gemma pre_tokenizer\n\n* fixing the test for CI\n\n* removed the test case\n\n* Update src/transformers/models/gemma/tokenization_gemma.py\n\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>",
    "sha": "607d24dd0ea8ebdb80d524f854b71d6059a6f667",
    "files": [
        {
            "sha": "26323e5b953e9bb0f76ffe90f0817834c6472c0c",
            "filename": "src/transformers/models/gemma/tokenization_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/607d24dd0ea8ebdb80d524f854b71d6059a6f667/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/607d24dd0ea8ebdb80d524f854b71d6059a6f667/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Ftokenization_gemma.py?ref=607d24dd0ea8ebdb80d524f854b71d6059a6f667",
            "patch": "@@ -12,7 +12,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n+from tokenizers import Tokenizer, decoders, normalizers\n from tokenizers.models import BPE\n \n from ...tokenization_utils_tokenizers import TokenizersBackend\n@@ -93,7 +93,6 @@ def __init__(\n             [decoders.Replace(\"▁\", \" \"), decoders.ByteFallback(), decoders.Fuse()]\n         )\n         self._tokenizer.normalizer = normalizers.Replace(\" \", \"▁\")\n-        self._tokenizer.pre_tokenizer = pre_tokenizers.Split(\" \", \"merged_with_previous\")\n         super().__init__(\n             unk_token=unk_token,\n             bos_token=bos_token,"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 1,
        "deletions": 2
    }
}