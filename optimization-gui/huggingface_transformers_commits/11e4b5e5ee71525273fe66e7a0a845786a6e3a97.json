{
    "author": "yao-matrix",
    "message": "make some ut cases pass on xpu w/ latest torch (#41337)\n\n* make some ut cases pass on xpu w/ latest torch\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\n\n* Update test_modeling_llava_onevision.py\n\n* Apply style fixes\n\n---------\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>",
    "sha": "11e4b5e5ee71525273fe66e7a0a845786a6e3a97",
    "files": [
        {
            "sha": "8d25201c1e4245619534db9538c87159e0b3ad93",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=11e4b5e5ee71525273fe66e7a0a845786a6e3a97",
            "patch": "@@ -393,7 +393,7 @@ def test_small_model_integration_batched_generate(self):\n         decoded_output = processor.decode(output[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): \"Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest lake.\",\n+                (\"xpu\", 3): \"Wooden bridge stretches\\nInto still waters, mountains gleam\\nPeaceful forest scene\",\n                 # 4-bit\n                 (\"cuda\", 7): \"Wooden bridge stretches\\nMirrored lake below, mountains rise\\nPeaceful, serene\",\n                 (\"cuda\", 8): 'Wooden path to water,\\nMountains echo in stillness,\\nPeaceful forest scene.',\n@@ -412,7 +412,7 @@ def test_small_model_integration_batched_generate(self):\n \n         expected_outputs = Expectations(\n             {\n-                (\"xpu\", 3): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',\n+                (\"xpu\", 3): 'This vibrant image captures a bustling street scene in a Chinese-influenced neighborhood. The focal point is a striking red stop sign',\n                 # 4-bit\n                 (\"cuda\", 7): 'This vibrant image captures a bustling street scene in a multicultural urban area, featuring a traditional Chinese gate adorned with intricate red and',\n                 (\"cuda\", 8): 'This image captures a vibrant street scene in a bustling urban area, likely in an Asian city. The focal point is a',"
        },
        {
            "sha": "4c88538befb4b073aa13e6ad9237f638fec523dd",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=11e4b5e5ee71525273fe66e7a0a845786a6e3a97",
            "patch": "@@ -446,8 +446,8 @@ def test_model_4b_batch(self):\n             {\n                 (\"xpu\", 3):\n                     [\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach next to a turquoise ocean. It looks like a very sunny and',\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. They depict very different scenes:\\n\\n*   **Image 1** shows a cow standing on a beach.',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a',\n+                        \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a brown\",\n                     ],\n                 (\"cuda\", (8,0)):\n                     [\n@@ -567,9 +567,8 @@ def test_model_4b_batch_crops(self):\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): [\n-                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There are clouds in the blue sky above.',\n-                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a',\n-                ],\n+                    \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\",\n+                    'user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, the images are not identical. \\n\\nThe first image shows a cow on a beach, while the second image shows a street scene with a'],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", (8,0)): [\n                     \"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\",\n@@ -627,7 +626,7 @@ def test_model_4b_multiimage(self):\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_TEXTS = Expectations(\n             {\n-                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image!\\n\\nHere's a description of the scene:\\n\\n*   **Chinese Arch\"],\n+                (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a city with\"],\n                 (\"cuda\", 7): [],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a vibrant,\"],\n                 (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt appears to be a street scene in a city\"],"
        },
        {
            "sha": "9dc1a2d67059fefb1b3aa0477852fe9d355575bf",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=11e4b5e5ee71525273fe66e7a0a845786a6e3a97",
            "patch": "@@ -626,7 +626,7 @@ def test_pixtral_4bit(self):\n         EXPECTED_GENERATIONS = Expectations(\n             {\n                 (\"cuda\", 7): \"Describe the images.The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which spans the entire image. The dog appears to be a black Labrador\",\n-                (\"xpu\", 3): \"Describe the images.The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which covers the entire background. The dog appears to be the main focus\",\n+                (\"xpu\", 3): \"Describe the images.The image showcases a dog, which is prominently positioned in the center, taking up a significant portion of the frame. The dog is situated against a backdrop of a wooden surface, which spans the entire image. The dog appears to be a black Labrador\",\n                 (\"rocm\", (9, 5)): \"Describe the images.The image features a dog positioned centrally, taking up a significant portion of the frame. The dog is situated against a backdrop of rugged terrain, which includes rocky cliffs and grassy slopes. The dog appears to be in a relaxed posture, possibly looking directly\",\n             }\n         )  # fmt: skip"
        },
        {
            "sha": "082844f933a52fe9abbba2822f51843bd95876d6",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 28,
            "deletions": 6,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11e4b5e5ee71525273fe66e7a0a845786a6e3a97/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=11e4b5e5ee71525273fe66e7a0a845786a6e3a97",
            "patch": "@@ -408,12 +408,18 @@ def test_small_model_integration_test_multi_image(self):\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe images you've provided appear to be related to a graphical representation of a radar chart, which is a type of data visualization used to show the distribution of a particular variable across a geographic area. The\"  # fmt: skip\n-\n-        self.assertEqual(\n-            self.processor.decode(output[0], skip_special_tokens=True),\n-            EXPECTED_DECODED_TEXT,\n+        output_text = self.processor.decode(output[0], skip_special_tokens=True)\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", None): \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe images you've provided appear to be related to a graphical representation of a radar chart, which is a type of data visualization used to show the distribution of a particular variable across a geographic area. The\",\n+                (\"xpu\", 3): \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe images you've provided appear to be related to a graphical representation of a radar chart, which is a type of data visualization used to show the distribution of a particular variable across a geographic area. The\",\n+            }\n         )\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+        # fmt: on\n+\n+        self.assertEqual(output_text, EXPECTED_DECODED_TEXT)\n \n     @slow\n     @require_bitsandbytes\n@@ -442,7 +448,23 @@ def test_small_model_integration_test_multi_image_nested(self):\n \n         # verify generation\n         output = model.generate(**inputs, max_new_tokens=40)\n-        EXPECTED_DECODED_TEXT = [\"user\\nTell me about the french revolution.\\nassistant\\nThe French Revolution! A pivotal event in modern history that had a profound impact on the course of Western civilization. Here's a brief overview:\\n\\n**Background**\\n\\nIn the late 18th century,\", \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe first image shows a stop sign with a traditional Chinese architectural background, while the second image displays a radar chart with various algorithms and models, including BLIP-2, InstructBLIP, Q\", \"user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different\"]  # fmt: skip\n+        # fmt: off\n+        EXPECTED_DECODED_TEXTS = Expectations(\n+            {\n+                (\"cuda\", None): [\n+                    \"user\\nTell me about the french revolution.\\nassistant\\nThe French Revolution! A pivotal event in modern history that had a profound impact on the course of Western civilization. Here's a brief overview:\\n\\n**Background**\\n\\nIn the late 18th century,\",\n+                    \"user\\n\\nWhat is the difference between these images?\\nassistant\\nThe first image shows a stop sign with a traditional Chinese architectural background, while the second image displays a radar chart with various algorithms and models, including BLIP-2, InstructBLIP, Q\",\n+                    \"user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that compares the performance of different models in a specific task, likely related to natural language processing or machine learning. The chart is divided into several axes, each representing a different\"\n+                    ],\n+                (\"xpu\", 3): [\n+                    \"user\\nTell me about the french revolution.\\nassistant\\nThe French Revolution! A pivotal event in modern history that had a profound impact on the course of Western civilization. Here's a brief overview:\\n\\n**Background**\\n\\nIn the late 18th century,\",\n+                    'user\\n\\nWhat is the difference between these images?\\nassistant\\nThe image shows a traffic light with a stop sign in the foreground, while the other images show a car driving through a street intersection.',\n+                    'user\\n\\nWhat do you see in this image?\\nassistant\\nThe image is a radar chart that represents the performance of different machine learning models in terms of their ability to predict the number of users who have been infected with COVID-19. The radar chart is'\n+                    ],\n+            }\n+        )\n+        EXPECTED_DECODED_TEXT = EXPECTED_DECODED_TEXTS.get_expectation()\n+        # fmt: on\n         DECODED_TEXT = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         self.assertListEqual(DECODED_TEXT, EXPECTED_DECODED_TEXT)"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 36,
        "deletions": 15
    }
}