{
    "author": "gante",
    "message": "[tests] Stricter generate + compilation test -- no recompilations allowed (#37629)\n\n* tmp commit\n\n* stricter compilation test\n\n* trigger tests\n\n* rm todo",
    "sha": "85665a4263681d2d8eaf85de40583ad7f46bc976",
    "files": [
        {
            "sha": "2f9118b0dac2ce7364dbd6a1cafbeb88923dbd73",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -563,17 +563,17 @@ def prepare_inputs_for_generation(\n                 device = model_inputs[input_ids_key].device\n \n             # Create the causal mask with fixed shape in advance, to reduce recompilations. If the function to create\n-            # the 4D causal mask exists, it should be present in the base model (XXXModel class).\n-            base_model = getattr(self, self.base_model_prefix, None)\n-            if base_model is None:\n-                causal_mask_creation_function = getattr(\n-                    self, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n-                )\n-            else:\n+            # the 4D causal mask exists, it should be present in the base model (XXXModel class) or in its decoder.\n+            base_model = getattr(self, self.base_model_prefix, self)\n+            decoder = base_model.get_decoder() if hasattr(base_model, \"get_decoder\") else None\n+            causal_mask_creation_function = getattr(\n+                base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n+            )\n+            if causal_mask_creation_function is None and decoder is not None:  # it may be in the decoder\n                 causal_mask_creation_function = getattr(\n-                    base_model, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n+                    decoder, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n                 )\n-            if causal_mask_creation_function is None:\n+            if causal_mask_creation_function is None:  # can't be found\n                 logger.warning_once(\n                     f\"{self.__class__.__name__} has no `_prepare_4d_causal_attention_mask_with_cache_position` method \"\n                     \"defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're \""
        },
        {
            "sha": "7e097cca311c2b65d4b26fbb27d2f52cca5ae293",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -1012,7 +1012,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1091,7 +1091,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1279,7 +1279,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1398,7 +1398,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         start_positions: Optional[torch.LongTensor] = None,\n         end_positions: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "b7a454839c0d78affa10d69780291b7fa57ec9c4",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -1837,6 +1837,9 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.decoder.embed_tokens = value\n \n+    def get_decoder(self):\n+        return self.decoder\n+\n     def forward(self, *args, **kwargs):\n         return self.decoder(*args, **kwargs)\n "
        },
        {
            "sha": "2bc1c351b8f9bf26ff852f15ea3e161b9076c570",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 65,
            "deletions": 47,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -28,8 +28,9 @@\n from packaging import version\n from parameterized import parameterized\n \n-from transformers import AutoConfig, AutoProcessor, AutoTokenizer, is_torch_available, pipeline\n+from transformers import AutoConfig, AutoProcessor, AutoTokenizer, is_torch_available, logging, pipeline\n from transformers.testing_utils import (\n+    CaptureLogger,\n     is_flaky,\n     require_accelerate,\n     require_flash_attn,\n@@ -38,6 +39,7 @@\n     require_torch,\n     require_torch_accelerator,\n     require_torch_gpu,\n+    require_torch_greater_or_equal,\n     require_torch_multi_accelerator,\n     require_torch_multi_gpu,\n     require_torch_sdpa,\n@@ -81,6 +83,7 @@\n         BeamSampleEncoderDecoderOutput,\n         BeamSearchDecoderOnlyOutput,\n         BeamSearchEncoderDecoderOutput,\n+        CompileConfig,\n         DisjunctiveConstraint,\n         GenerateBeamDecoderOnlyOutput,\n         GenerateBeamEncoderDecoderOutput,\n@@ -2109,22 +2112,34 @@ def test_generate_with_quant_cache(self):\n                 model.generate(**generation_kwargs, **inputs_dict)\n \n     @pytest.mark.generate\n+    @require_torch_greater_or_equal(\"2.6\")  # Uses torch.compiler.set_stance\n     def test_generate_compile_model_forward(self):\n         \"\"\"\n-        Tests that `.generate` is compatible with torch.compile without graph breaks, keeping the same results.\n+        Tests that `.generate` is compatible with torch.compile, keeping the same results. Also confirms that\n+        `.forward` called from `.generate` sees no graph breaks or recompilations when compiled.\n+\n         ⚠️ Runs two sequential generations to ensure the cache doesn't get stuck after the first compiled run! ⚠️\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n+            # 1. Test exclusion criteria\n             if not model_class._supports_static_cache:\n                 self.skipTest(\"This model doesn't support static cache (= no expectations of compilation support)\")\n \n+            # 2. Prepares two sets of inputs\n             config, inputs_dict = self.prepare_config_and_inputs_for_generate(batch_size=4)\n-\n             model = model_class(config).to(torch_device)\n             model.eval()  # otherwise `self.training` is `True` -- this flag is used at attn mask creation time\n \n-            main_input = inputs_dict[model.main_input_name].to(torch_device)\n+            # Some composite models have a custom generate and will call an inner model's generate -> that inner model\n+            # is the one that gets compiled.\n+            # (Note for the future: if BLIP starts causing problems, let's stop testing it)\n+            if \"blip\" in model.__class__.__name__.lower():\n+                model_to_be_compiled = model.language_model\n+            else:\n+                model_to_be_compiled = model\n+\n             # creates two sets of *different* inputs with the same shape\n+            main_input = inputs_dict[model.main_input_name].to(torch_device)\n             half_batch_size = main_input.shape[0] // 2\n             input_1 = {}\n             input_2 = {}\n@@ -2140,66 +2155,69 @@ def test_generate_compile_model_forward(self):\n                 model_input_sets[0][model.main_input_name].shape == model_input_sets[1][model.main_input_name].shape\n             )\n \n-            # compilation-specific setup\n+            # 3. compilation-specific setup and generation parameterization\n             torch.compiler.reset()  # prevent cached compilation from being used in the test\n             has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n-\n-            # BLIP is the only exception with custom generate which call `self.lm.generate()`\n-            # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n-            # compatible with multimodality\n-            if \"blip\" in model.__class__.__name__.lower():\n-                model.language_model.generation_config.compile_config._compile_all_devices = True\n-            else:\n-                # force compilation (e.g. fast CI, CPU\n-                model.generation_config.compile_config._compile_all_devices = True\n+            compile_config = CompileConfig(dynamic=False)  # Error out on dynamic shapes\n+            compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n \n             generation_kwargs = {\n                 \"do_sample\": False,\n                 \"max_new_tokens\": 5,\n                 \"return_dict_in_generate\": True,\n                 \"output_scores\": True,\n+                \"compile_config\": compile_config,\n             }\n \n-            # get eager + dynamic cache results for future comparison\n+            # 4. get eager + dynamic cache results for future comparison\n             dynamic_outputs = []\n-            for model_inputs in model_input_sets:\n-                gen_out = model.generate(**model_inputs, **generation_kwargs)\n-                dynamic_outputs.append(gen_out)\n-                # sanity checks for the default cache implementation\n-                if not has_defined_cache_implementation:\n-                    decoder_cache = (\n-                        gen_out.past_key_values.self_attention_cache\n-                        if config.is_encoder_decoder\n-                        else gen_out.past_key_values\n-                    )\n-                    self.assertTrue(isinstance(decoder_cache, DynamicCache))\n-                    self.assertFalse(decoder_cache.is_compileable)\n-                    self.assertFalse(hasattr(model, \"_compiled_call\"))  # our auto compile should NOT have been called\n+            # Ignores all `torch.compile` usage, useful to test models that that have non-default compilable caches\n+            # (who would have used compilation in this section)\n+            with torch.compiler.set_stance(\"force_eager\"):\n+                for model_inputs in model_input_sets:\n+                    gen_out = model.generate(**model_inputs, **generation_kwargs)\n+                    dynamic_outputs.append(gen_out)\n+                    # sanity checks for the default cache implementation\n+                    if not has_defined_cache_implementation:\n+                        decoder_cache = (\n+                            gen_out.past_key_values.self_attention_cache\n+                            if config.is_encoder_decoder\n+                            else gen_out.past_key_values\n+                        )\n+                        self.assertTrue(isinstance(decoder_cache, DynamicCache))\n+                        self.assertFalse(decoder_cache.is_compileable)\n+                        # our auto compile should NOT have been called\n+                        self.assertFalse(hasattr(model_to_be_compiled, \"_compiled_call\"))\n \n-            # get compiled results -- relies on the automatic compilation triggered by specific \"cache_implementation\"\n+            # 5. get compiled results -- relies on the automatic compilation triggered by specific compilable caches\n             if not has_defined_cache_implementation:\n                 generation_kwargs[\"cache_implementation\"] = \"static\"\n \n             compiled_outputs = []\n-            for model_inputs in model_input_sets:\n-                gen_out = model.generate(**model_inputs, **generation_kwargs)\n-                compiled_outputs.append(gen_out)\n-                # sanity checks\n-                decoder_cache = (\n-                    gen_out.past_key_values.self_attention_cache\n-                    if config.is_encoder_decoder\n-                    else gen_out.past_key_values\n+            # Uses a context manager to catch recompilation logs. If there is any recompilation, this test fails.\n+            torch._logging.set_logs(recompiles_verbose=True)\n+            logger = logging.get_logger(\"torch._dynamo.guards\")\n+            with CaptureLogger(logger) as cl:\n+                for model_inputs in model_input_sets:\n+                    # with torch.compiler.set_stance(\"fail_on_recompile\"):\n+                    gen_out = model.generate(**model_inputs, **generation_kwargs)\n+                    compiled_outputs.append(gen_out)\n+                    # sanity checks\n+                    decoder_cache = (\n+                        gen_out.past_key_values.self_attention_cache\n+                        if config.is_encoder_decoder\n+                        else gen_out.past_key_values\n+                    )\n+                    self.assertFalse(isinstance(decoder_cache, DynamicCache))\n+                    self.assertTrue(decoder_cache.is_compileable)\n+                    # our auto compile should have been called\n+                    self.assertTrue(hasattr(model_to_be_compiled, \"_compiled_call\"))\n+\n+            if \"Recompiling\" in cl.out or (\"guard\" in cl.out and \"failure\" in cl.out):\n+                raise RuntimeError(\n+                    f\"`torch.compile` recompiled part of the forward pass in {model.__class__.__name__}. \"\n+                    \"See the test logs for more details.\"\n                 )\n-                self.assertFalse(isinstance(decoder_cache, DynamicCache))\n-                self.assertTrue(decoder_cache.is_compileable)\n-\n-                # BLIP is the only exception with custom generate which call `self.lm.generate()`\n-                # We should avoid such calls in all subsequent multimodal models and try to make `generate()`\n-                # compatible with multimodality\n-                if \"blip\" in model.__class__.__name__.lower():\n-                    self.assertTrue(hasattr(model.language_model, \"_compiled_call\"))\n-                else:\n-                    self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n             for dynamic_result, compiled_result in zip(dynamic_outputs, compiled_outputs):\n                 self._check_similar_generate_outputs(dynamic_result, compiled_result)"
        },
        {
            "sha": "63c812180a9df602cd756069eb46741bc19ff9d3",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -280,10 +280,6 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "5f6a0f1832c9cd154b616c4967d37e229ced9829",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -840,10 +840,6 @@ def test_custom_4d_attention_mask(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n-    @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n     def test_model(self):\n         pass"
        },
        {
            "sha": "48cf7ebc2f5e34bac986b0a8288445a152b00f72",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -335,6 +335,10 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                         else:\n                             pass\n \n+    @unittest.skip(\"There are recompilations in Janus\")  # TODO (joao, raushan): fix me\n+    def test_generate_compile_model_forward(self):\n+        pass\n+\n \n class JanusVQModelTester:\n     def __init__("
        },
        {
            "sha": "ea134aee9e39f1425234a71ac50e767c44e54463",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -341,10 +341,6 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"LLaVA Next has dynamic control flow in unpadding\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class LlavaNextForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "47c71d9c751bb638fe33e526ccfe07671493fa65",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -356,10 +356,6 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"LLaVA Next Video has dynamic control flow in unpadding\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class LlavaNextVideoForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "021739976b097308c19cfc32997bbd77bf37a0af",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -312,10 +312,6 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    @unittest.skip(\"LLaVA OneVision has dynamic control flow in unpadding\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n \n @require_torch\n class LlavaOnevisionForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "84b78f72642fd50afe05263255e2bcb9525d2c22",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -344,11 +344,6 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n-    @unittest.skip(\"PaliGemma is not compatible with end-to-end generation compilation\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n     def test_attention_mask_with_token_types(self):\n         \"\"\"Test that attention masking works correctly both with and without token type IDs.\"\"\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e7d60a8849bcc16d65ddcd387c1fda7bb21f23b9",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -341,11 +341,6 @@ def test_flash_attn_2_fp32_ln(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n-    # TODO (joao, raushan): fix me -- the problem is in `cache_position[0] == 0`, i.e. dynamic control flow\n-    @unittest.skip(\"PaliGemma is not compatible with end-to-end generation compilation\")\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n     @unittest.skip(\"Low memory will be removed soon so no need to fix it\")\n     def test_beam_search_low_memory(self):\n         pass"
        },
        {
            "sha": "5070383228b37331f1a69bda25688b6161bf1162",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -365,6 +365,8 @@ def test_dola_decoding_sample(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    # TODO (joao, raushan): there are multiple standardization issues in this model that prevent this test from\n+    # passing, fix me\n     @unittest.skip(\"Cannot handle 4D attention mask\")\n     def test_generate_compile_model_forward(self):\n         pass"
        },
        {
            "sha": "7d2a5e54bd56fc18ede3593cd90a0246d9047e38",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/85665a4263681d2d8eaf85de40583ad7f46bc976/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=85665a4263681d2d8eaf85de40583ad7f46bc976",
            "patch": "@@ -1431,7 +1431,7 @@ def test_labels_sequence_max_length_error_after_changing_config(self):\n             with self.assertRaises(ValueError):\n                 model(input_features=input_features, labels=labels)\n \n-    # TODO (joao, eustache): fix me :)\n+    # TODO (joao, eustache): fix me :) The model is not returning a `Cache` by default\n     @unittest.skip(reason=\"Whisper's custom generate is not consistent regarding the cache return types\")\n     def test_generate_compile_model_forward(self):\n         pass"
        }
    ],
    "stats": {
        "total": 179,
        "additions": 88,
        "deletions": 91
    }
}