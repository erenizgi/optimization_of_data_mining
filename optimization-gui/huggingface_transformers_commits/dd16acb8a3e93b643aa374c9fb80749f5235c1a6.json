{
    "author": "ydshieh",
    "message": "set `test_torchscript = False` for Blip2 testing  (#35972)\n\n* just skip\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "dd16acb8a3e93b643aa374c9fb80749f5235c1a6",
    "files": [
        {
            "sha": "17d14c848667fedf2a67607d0559f4dd2fca5e47",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 224,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd16acb8a3e93b643aa374c9fb80749f5235c1a6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd16acb8a3e93b643aa374c9fb80749f5235c1a6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=dd16acb8a3e93b643aa374c9fb80749f5235c1a6",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"Testing suite for the PyTorch BLIP-2 model.\"\"\"\n \n import inspect\n-import os\n import tempfile\n import unittest\n \n@@ -36,7 +35,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_available, is_torch_sdpa_available, is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -477,7 +476,7 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n     test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n-    test_torchscript = True\n+    test_torchscript = False\n     _is_composite = True\n \n     def setUp(self):\n@@ -494,116 +493,6 @@ def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_conditional_generation(*config_and_inputs)\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        # overwrite because BLIP requires ipnut ids and pixel values as input\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to `False`\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            for attn_implementation in [\"eager\", \"sdpa\"]:\n-                if attn_implementation == \"sdpa\" and (not model_class._supports_sdpa or not is_torch_sdpa_available()):\n-                    continue\n-\n-                configs_no_init._attn_implementation = attn_implementation\n-                model = model_class(config=configs_no_init)\n-                model.to(torch_device)\n-                model.eval()\n-                inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                main_input_name = model_class.main_input_name\n-\n-                try:\n-                    if model.config.is_encoder_decoder:\n-                        model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                        main_input = inputs[main_input_name]\n-                        input_ids = inputs[\"input_ids\"]\n-                        attention_mask = inputs[\"attention_mask\"]\n-                        decoder_input_ids = inputs[\"decoder_input_ids\"]\n-                        decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                        model(main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        traced_model = torch.jit.trace(\n-                            model, (main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        )\n-                    else:\n-                        main_input = inputs[main_input_name]\n-                        input_ids = inputs[\"input_ids\"]\n-\n-                        if model.config._attn_implementation == \"sdpa\":\n-                            trace_input = {main_input_name: main_input, \"input_ids\": input_ids}\n-\n-                            if \"attention_mask\" in inputs:\n-                                trace_input[\"attention_mask\"] = inputs[\"attention_mask\"]\n-                            else:\n-                                self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n-\n-                            model(main_input, attention_mask=inputs[\"attention_mask\"])\n-                            # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n-                            traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n-                        else:\n-                            model(main_input, input_ids)\n-                            traced_model = torch.jit.trace(model, (main_input, input_ids))\n-                except RuntimeError:\n-                    self.fail(\"Couldn't trace module.\")\n-\n-                with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                    try:\n-                        torch.jit.save(traced_model, pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't save module.\")\n-\n-                    try:\n-                        loaded_model = torch.jit.load(pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't load module.\")\n-\n-                model.to(torch_device)\n-                model.eval()\n-\n-                loaded_model.to(torch_device)\n-                loaded_model.eval()\n-\n-                model_state_dict = model.state_dict()\n-                loaded_model_state_dict = loaded_model.state_dict()\n-\n-                non_persistent_buffers = {}\n-                for key in loaded_model_state_dict.keys():\n-                    if key not in model_state_dict.keys():\n-                        non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-                loaded_model_state_dict = {\n-                    key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-                }\n-\n-                self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-                model_buffers = list(model.buffers())\n-                for non_persistent_buffer in non_persistent_buffers.values():\n-                    found_buffer = False\n-                    for i, model_buffer in enumerate(model_buffers):\n-                        if torch.equal(non_persistent_buffer, model_buffer):\n-                            found_buffer = True\n-                            break\n-\n-                    self.assertTrue(found_buffer)\n-                    model_buffers.pop(i)\n-\n-                models_equal = True\n-                for layer_name, p1 in model_state_dict.items():\n-                    if layer_name in loaded_model_state_dict:\n-                        p2 = loaded_model_state_dict[layer_name]\n-                        if p1.data.ne(p2.data).sum() > 0:\n-                            models_equal = False\n-\n-                self.assertTrue(models_equal)\n-\n-                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-                # (Even with this call, there are still memory leak by ~0.04MB)\n-                self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass\n@@ -1015,7 +904,7 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n     test_pruning = False\n     test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = True\n+    test_torchscript = False\n     _is_composite = True\n \n     # TODO: Fix the failed tests\n@@ -1049,116 +938,6 @@ def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_conditional_generation(*config_and_inputs)\n \n-    def _create_and_check_torchscript(self, config, inputs_dict):\n-        # overwrite because BLIP requires ipnut ids and pixel values as input\n-        if not self.test_torchscript:\n-            self.skipTest(reason=\"test_torchscript is set to `False`\")\n-\n-        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n-        configs_no_init.torchscript = True\n-        for model_class in self.all_model_classes:\n-            for attn_implementation in [\"eager\", \"sdpa\"]:\n-                if attn_implementation == \"sdpa\" and (not model_class._supports_sdpa or not is_torch_sdpa_available()):\n-                    continue\n-\n-                configs_no_init._attn_implementation = attn_implementation\n-                model = model_class(config=configs_no_init)\n-                model.to(torch_device)\n-                model.eval()\n-                inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-                main_input_name = model_class.main_input_name\n-\n-                try:\n-                    if model.config.is_encoder_decoder:\n-                        model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n-                        main_input = inputs[main_input_name]\n-                        input_ids = inputs[\"input_ids\"]\n-                        attention_mask = inputs[\"attention_mask\"]\n-                        decoder_input_ids = inputs[\"decoder_input_ids\"]\n-                        decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n-                        model(main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        traced_model = torch.jit.trace(\n-                            model, (main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n-                        )\n-                    else:\n-                        main_input = inputs[main_input_name]\n-                        input_ids = inputs[\"input_ids\"]\n-\n-                        if model.config._attn_implementation == \"sdpa\":\n-                            trace_input = {main_input_name: main_input, \"input_ids\": input_ids}\n-\n-                            if \"attention_mask\" in inputs:\n-                                trace_input[\"attention_mask\"] = inputs[\"attention_mask\"]\n-                            else:\n-                                self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n-\n-                            model(main_input, attention_mask=inputs[\"attention_mask\"])\n-                            # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n-                            traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n-                        else:\n-                            model(main_input, input_ids)\n-                            traced_model = torch.jit.trace(model, (main_input, input_ids))\n-                except RuntimeError:\n-                    self.fail(\"Couldn't trace module.\")\n-\n-                with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n-\n-                    try:\n-                        torch.jit.save(traced_model, pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't save module.\")\n-\n-                    try:\n-                        loaded_model = torch.jit.load(pt_file_name)\n-                    except Exception:\n-                        self.fail(\"Couldn't load module.\")\n-\n-                model.to(torch_device)\n-                model.eval()\n-\n-                loaded_model.to(torch_device)\n-                loaded_model.eval()\n-\n-                model_state_dict = model.state_dict()\n-                loaded_model_state_dict = loaded_model.state_dict()\n-\n-                non_persistent_buffers = {}\n-                for key in loaded_model_state_dict.keys():\n-                    if key not in model_state_dict.keys():\n-                        non_persistent_buffers[key] = loaded_model_state_dict[key]\n-\n-                loaded_model_state_dict = {\n-                    key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n-                }\n-\n-                self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n-\n-                model_buffers = list(model.buffers())\n-                for non_persistent_buffer in non_persistent_buffers.values():\n-                    found_buffer = False\n-                    for i, model_buffer in enumerate(model_buffers):\n-                        if torch.equal(non_persistent_buffer, model_buffer):\n-                            found_buffer = True\n-                            break\n-\n-                    self.assertTrue(found_buffer)\n-                    model_buffers.pop(i)\n-\n-                models_equal = True\n-                for layer_name, p1 in model_state_dict.items():\n-                    if layer_name in loaded_model_state_dict:\n-                        p2 = loaded_model_state_dict[layer_name]\n-                        if p1.data.ne(p2.data).sum() > 0:\n-                            models_equal = False\n-\n-                self.assertTrue(models_equal)\n-\n-                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n-                # (Even with this call, there are still memory leak by ~0.04MB)\n-                self.clear_torch_jit_class_registry()\n-\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass"
        }
    ],
    "stats": {
        "total": 227,
        "additions": 3,
        "deletions": 224
    }
}