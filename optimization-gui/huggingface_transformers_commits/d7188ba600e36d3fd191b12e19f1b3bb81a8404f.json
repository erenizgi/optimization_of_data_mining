{
    "author": "yonigozlan",
    "message": "Add support for nested images to LLava and VipLLava (#35558)\n\n* move make_flat_list_of_images and make_batched_videos to image_utils\r\n\r\n* remove unnecessary is_vision_available\r\n\r\n* move make_nested_list_of_images to image_utils\r\n\r\n* fix fast pixtral image processor\r\n\r\n* fix import mllama\r\n\r\n* fix make_nested_list_of_images\r\n\r\n* add tests\r\n\r\n* convert 4d arrays/tensors to list\r\n\r\n* add test_make_batched_videos\r\n\r\n* add support nested batch of videos\r\n\r\n* fix image processing qwen2vl",
    "sha": "d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
    "files": [
        {
            "sha": "4f8b5980a6d15d2ac9723c4857345e8de37add69",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 108,
            "deletions": 2,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -158,6 +158,10 @@ def is_valid_image(img):\n     return is_pil_image(img) or is_numpy_array(img) or is_torch_tensor(img) or is_tf_tensor(img) or is_jax_tensor(img)\n \n \n+def is_valid_list_of_images(images: List):\n+    return images and all(is_valid_image(image) for image in images)\n+\n+\n def valid_images(imgs):\n     # If we have an list of images, make sure every image is valid\n     if isinstance(imgs, (list, tuple)):\n@@ -189,7 +193,7 @@ def is_scaled_image(image: np.ndarray) -> bool:\n \n def make_list_of_images(images, expected_ndims: int = 3) -> List[ImageInput]:\n     \"\"\"\n-    Ensure that the input is a list of images. If the input is a single image, it is converted to a list of length 1.\n+    Ensure that the output is a list of images. If the input is a single image, it is converted to a list of length 1.\n     If the input is a batch of images, it is converted to a list of images.\n \n     Args:\n@@ -203,7 +207,7 @@ def make_list_of_images(images, expected_ndims: int = 3) -> List[ImageInput]:\n         return images\n \n     # Either the input is a single image, in which case we create a list of length 1\n-    if isinstance(images, PIL.Image.Image):\n+    if is_pil_image(images):\n         # PIL images are never batched\n         return [images]\n \n@@ -226,6 +230,108 @@ def make_list_of_images(images, expected_ndims: int = 3) -> List[ImageInput]:\n     )\n \n \n+def make_flat_list_of_images(\n+    images: Union[List[ImageInput], ImageInput],\n+) -> ImageInput:\n+    \"\"\"\n+    Ensure that the output is a flat list of images. If the input is a single image, it is converted to a list of length 1.\n+    If the input is a nested list of images, it is converted to a flat list of images.\n+    Args:\n+        images (`Union[List[ImageInput], ImageInput]`):\n+            The input image.\n+    Returns:\n+        list: A list of images or a 4d array of images.\n+    \"\"\"\n+    # If the input is a nested list of images, we flatten it\n+    if (\n+        isinstance(images, (list, tuple))\n+        and all(isinstance(images_i, (list, tuple)) for images_i in images)\n+        and all(is_valid_list_of_images(images_i) for images_i in images)\n+    ):\n+        return [img for img_list in images for img in img_list]\n+\n+    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n+        if is_pil_image(images[0]) or images[0].ndim == 3:\n+            return images\n+        if images[0].ndim == 4:\n+            return [img for img_list in images for img in img_list]\n+\n+    if is_valid_image(images):\n+        if is_pil_image(images) or images.ndim == 3:\n+            return [images]\n+        if images.ndim == 4:\n+            return list(images)\n+\n+    raise ValueError(f\"Could not make a flat list of images from {images}\")\n+\n+\n+def make_nested_list_of_images(\n+    images: Union[List[ImageInput], ImageInput],\n+) -> ImageInput:\n+    \"\"\"\n+    Ensure that the output is a nested list of images.\n+    Args:\n+        images (`Union[List[ImageInput], ImageInput]`):\n+            The input image.\n+    Returns:\n+        list: A list of list of images or a list of 4d array of images.\n+    \"\"\"\n+    # If it's a list of batches, it's already in the right format\n+    if (\n+        isinstance(images, (list, tuple))\n+        and all(isinstance(images_i, (list, tuple)) for images_i in images)\n+        and all(is_valid_list_of_images(images_i) for images_i in images)\n+    ):\n+        return images\n+\n+    # If it's a list of images, it's a single batch, so convert it to a list of lists\n+    if isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n+        if is_pil_image(images[0]) or images[0].ndim == 3:\n+            return [images]\n+        if images[0].ndim == 4:\n+            return [list(image) for image in images]\n+\n+    # If it's a single image, convert it to a list of lists\n+    if is_valid_image(images):\n+        if is_pil_image(images) or images.ndim == 3:\n+            return [[images]]\n+        if images.ndim == 4:\n+            return [list(images)]\n+\n+    raise ValueError(\"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\")\n+\n+\n+def make_batched_videos(videos) -> VideoInput:\n+    \"\"\"\n+    Ensure that the input is a list of videos.\n+    Args:\n+        videos (`VideoInput`):\n+            Video or videos to turn into a list of videos.\n+    Returns:\n+        list: A list of videos.\n+    \"\"\"\n+    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n+        # case 1: nested batch of videos so we flatten it\n+        if not is_pil_image(videos[0][0]) and videos[0][0].ndim == 4:\n+            videos = [video for batch_list in videos for video in batch_list]\n+        # case 2: list of videos represented as list of video frames\n+        return videos\n+\n+    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n+        if is_pil_image(videos[0]) or videos[0].ndim == 3:\n+            return [videos]\n+        elif videos[0].ndim == 4:\n+            return [list(video) for video in videos]\n+\n+    elif is_valid_image(videos):\n+        if is_pil_image(videos) or videos.ndim == 3:\n+            return [[videos]]\n+        elif videos.ndim == 4:\n+            return [list(videos)]\n+\n+    raise ValueError(f\"Could not make batched video from {videos}\")\n+\n+\n def to_numpy_array(img) -> np.ndarray:\n     if not is_valid_image(img):\n         raise ValueError(f\"Invalid image type: {type(img)}\")"
        },
        {
            "sha": "de8637eb28fa9768fd0640ffb2f7c9fa9a9f07f6",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -31,37 +31,14 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n-    is_valid_image,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType\n \n \n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n@@ -244,7 +221,7 @@ def preprocess(\n         if max_image_size not in [490, 980]:\n             raise ValueError(\"max_image_size must be either 490 or 980\")\n \n-        images = make_batched_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "5c348ae1ee56ae7a21500356030b01f82dd2e13b",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -28,6 +28,7 @@\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -58,7 +59,7 @@\n     LlamaRMSNorm,\n )\n from ..llava.modeling_llava import LlavaCausalLMOutputWithPast\n-from ..llava_next.image_processing_llava_next import divide_to_patches, make_batched_images\n+from ..llava_next.image_processing_llava_next import divide_to_patches\n \n \n logger = logging.get_logger(__name__)\n@@ -609,7 +610,7 @@ def preprocess(\n         if max_image_size not in [490, 980]:\n             raise ValueError(\"max_image_size must be either 490 or 980\")\n \n-        images = make_batched_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "df2aee157dc24430b7a2bc135f2f5c0b92bf348c",
            "filename": "src/transformers/models/blip/image_processing_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fimage_processing_blip.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -28,7 +28,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -231,8 +231,7 @@ def preprocess(\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n-\n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "c9d110ad229ae7cbce459855aa38ca8fa890194e",
            "filename": "src/transformers/models/chameleon/image_processing_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fimage_processing_chameleon.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -30,7 +30,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -44,29 +44,6 @@\n     import PIL\n \n \n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n class ChameleonImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a Chameleon image processor.\n@@ -275,7 +252,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_batched_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "2155b306bc0d05b3355633969f0d7dda574855ec",
            "filename": "src/transformers/models/clip/image_processing_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fimage_processing_clip.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -33,7 +33,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_kwargs,\n@@ -283,7 +283,7 @@ def preprocess(\n \n         validate_kwargs(captured_kwargs=kwargs.keys(), valid_processor_keys=self._valid_processor_keys)\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "2cc6dded858d69a0abd08d4c2193bfafa849cf69",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -20,11 +20,10 @@\n     IMAGE_TOKEN,\n     PaliGemmaProcessor,\n     build_string_from_input,\n-    make_batched_images,\n )\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image\n+from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n from ...processing_utils import (\n     ProcessingKwargs,\n     Unpack,\n@@ -168,7 +167,7 @@ def __call__(\n                 )\n                 for prompt, image_list in zip(texts_doc, images)\n             ]\n-            images = make_batched_images(images)\n+            images = make_flat_list_of_images(images)\n             pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n             # max_length has to account for the image tokens"
        },
        {
            "sha": "342cd0cd3d6a08e7a61eaa0bee65854ce7d37c9d",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -23,7 +23,7 @@\n from typing import ClassVar, List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image\n+from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import is_torch_available\n@@ -72,29 +72,6 @@ def build_string_from_input(prompt, bos_token, image_seq_len, image_token, num_i\n     return f\"{image_token * image_seq_len * num_images}{bos_token}{prompt}\\n\"\n \n \n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n class ColPaliProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a ColPali processor which wraps a PaliGemmaProcessor and special methods to process images and queries, as\n@@ -230,7 +207,7 @@ def __call__(\n                 )\n                 for prompt, image_list in zip(texts_doc, images)\n             ]\n-            images = make_batched_images(images)\n+            images = make_flat_list_of_images(images)\n             pixel_values = self.image_processor(images, **output_kwargs[\"images_kwargs\"])[\"pixel_values\"]\n \n             # max_length has to account for the image tokens"
        },
        {
            "sha": "927aba761c44c98bde31f11790cc27e4c4402eb6",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 35,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -29,7 +29,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_nested_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -77,39 +77,6 @@ def get_resize_output_image_size(image, size, input_data_format) -> Tuple[int, i\n     return height, width\n \n \n-def make_list_of_images(images: ImageInput) -> List[List[np.ndarray]]:\n-    \"\"\"\n-    Convert a single image or a list of images to a list of numpy arrays.\n-\n-    Args:\n-        images (`ImageInput`):\n-            A single image or a list of images.\n-\n-    Returns:\n-        A list of numpy arrays.\n-    \"\"\"\n-    # If it's a single image, convert it to a list of lists\n-    if is_valid_image(images):\n-        images = [[images]]\n-    # If it's a list of images, it's a single batch, so convert it to a list of lists\n-    elif isinstance(images, (list, tuple)) and len(images) > 0 and is_valid_image(images[0]):\n-        images = [images]\n-    # If it's a list of batches, it's already in the right format\n-    elif (\n-        isinstance(images, (list, tuple))\n-        and len(images) > 0\n-        and isinstance(images[0], (list, tuple))\n-        and len(images[0]) > 0\n-        and is_valid_image(images[0][0])\n-    ):\n-        pass\n-    else:\n-        raise ValueError(\n-            \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n-        )\n-    return images\n-\n-\n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> List[Any]:\n     \"\"\"\n@@ -504,7 +471,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_image_splitting = do_image_splitting if do_image_splitting is not None else self.do_image_splitting\n \n-        images_list = make_list_of_images(images)\n+        images_list = make_nested_list_of_images(images)\n \n         if not valid_images(images_list[0]):\n             raise ValueError("
        },
        {
            "sha": "b8b30609b844d82e296a0ba6e0fdf7b2b11885b4",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 36,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -29,7 +29,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_nested_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -141,40 +141,6 @@ def get_resize_output_image_size(\n     return height, width\n \n \n-# Copied from transformers.models.idefics2.image_processing_idefics2.make_list_of_images\n-def make_list_of_images(images: ImageInput) -> List[List[np.ndarray]]:\n-    \"\"\"\n-    Convert a single image or a list of images to a list of numpy arrays.\n-\n-    Args:\n-        images (`ImageInput`):\n-            A single image or a list of images.\n-\n-    Returns:\n-        A list of numpy arrays.\n-    \"\"\"\n-    # If it's a single image, convert it to a list of lists\n-    if is_valid_image(images):\n-        images = [[images]]\n-    # If it's a list of images, it's a single batch, so convert it to a list of lists\n-    elif isinstance(images, (list, tuple)) and len(images) > 0 and is_valid_image(images[0]):\n-        images = [images]\n-    # If it's a list of batches, it's already in the right format\n-    elif (\n-        isinstance(images, (list, tuple))\n-        and len(images) > 0\n-        and isinstance(images[0], (list, tuple))\n-        and len(images[0]) > 0\n-        and is_valid_image(images[0][0])\n-    ):\n-        pass\n-    else:\n-        raise ValueError(\n-            \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n-        )\n-    return images\n-\n-\n # Copied from transformers.models.detr.image_processing_detr.max_across_indices\n def max_across_indices(values: Iterable[Any]) -> List[Any]:\n     \"\"\"\n@@ -720,7 +686,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n         do_pad = do_pad if do_pad is not None else self.do_pad\n \n-        images_list = make_list_of_images(images)\n+        images_list = make_nested_list_of_images(images)\n \n         if not valid_images(images_list[0]):\n             raise ValueError("
        },
        {
            "sha": "37cec22a9b365817bbf4703631dc4fc3af0a405f",
            "filename": "src/transformers/models/instructblipvideo/image_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 26,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fimage_processing_instructblipvideo.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -32,40 +32,17 @@\n     VideoInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n-\n-\n-if is_vision_available():\n-    import PIL\n+from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], PIL.Image.Image):\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos):\n-        if isinstance(videos, PIL.Image.Image):\n-            return [[videos]]\n-        elif len(videos.shape) == 4:\n-            return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n # Copied from transformers.models.blip.image_processing_blip.BlipImageProcessor with Blip->InstructBlipVideo, BLIP->InstructBLIPVideo\n class InstructBlipVideoImageProcessor(BaseImageProcessor):\n     r\"\"\"\n@@ -198,7 +175,7 @@ def preprocess(\n         do_convert_rgb: bool = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> PIL.Image.Image:\n+    ) -> BatchFeature:\n         \"\"\"\n         Preprocess a video or batch of images/videos.\n "
        },
        {
            "sha": "742ed4cbabd5156bc3a1b40eb4089c7e2bfdf4ca",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -37,7 +37,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_flat_list_of_images,\n     make_list_of_images,\n     to_numpy_array,\n     valid_images,\n@@ -53,29 +53,6 @@\n     from PIL import Image\n \n \n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n     \"\"\"\n     Divides an image into patches of a specified size.\n@@ -670,7 +647,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_batched_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "3ec8d9db069d22bbcd938f9734a26193bde8f2c4",
            "filename": "src/transformers/models/llava_next_video/image_processing_llava_next_video.py",
            "status": "modified",
            "additions": 3,
            "deletions": 23,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fimage_processing_llava_next_video.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -34,37 +34,17 @@\n     VideoInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n     make_list_of_images,\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_vision_available, logging\n+from ...utils import TensorType, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], Image.Image):\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n class LlavaNextVideoImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LLaVa-NeXT-Video video processor. Based on [`CLIPImageProcessor`] with incorporation of processing each video frame.\n@@ -212,7 +192,7 @@ def _preprocess(\n         do_convert_rgb: bool = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> Image.Image:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n "
        },
        {
            "sha": "22435175045ea4be6edb6babb55d71cc315d29b1",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 26,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -36,7 +36,7 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -51,30 +51,6 @@\n     from PIL import Image\n \n \n-# Copied from transformers.models.llava_next.image_processing_llava_next.make_batched_images\n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n # Copied from transformers.models.llava_next.image_processing_llava_next.divide_to_patches\n def divide_to_patches(image: np.array, patch_size: int, input_data_format) -> List[np.array]:\n     \"\"\"\n@@ -632,7 +608,7 @@ def preprocess(\n         do_pad = do_pad if do_pad is not None else self.do_pad\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_batched_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "743e9f2df68c44e4433eab3bafcb08122a6dc3e5",
            "filename": "src/transformers/models/llava_onevision/video_processing_llava_onevision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 23,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fvideo_processing_llava_onevision.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -16,6 +16,8 @@\n \n from typing import Dict, List, Optional, Union\n \n+import numpy as np\n+\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import (\n     convert_to_rgb,\n@@ -31,37 +33,17 @@\n     VideoInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_vision_available, logging\n+from ...utils import TensorType, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], Image.Image) or len(videos[0].shape) == 3:\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n class LlavaOnevisionVideoProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a LLaVa-Onevisino-Video video processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame.\n@@ -138,7 +120,7 @@ def _preprocess(\n         do_convert_rgb: bool = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> Image.Image:\n+    ) -> list[np.ndarray]:\n         \"\"\"\n         Args:\n             images (`ImageInput`):"
        },
        {
            "sha": "9ff077f150144e0bc11e26e5e2f7e103cc9b74ac",
            "filename": "src/transformers/models/mllama/image_processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 38,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fimage_processing_mllama.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -33,8 +33,8 @@\n     ImageInput,\n     PILImageResampling,\n     infer_channel_dimension_format,\n-    is_valid_image,\n     is_vision_available,\n+    make_nested_list_of_images,\n     to_numpy_array,\n     validate_preprocess_arguments,\n )\n@@ -514,42 +514,6 @@ def convert_to_rgb(image: ImageInput) -> ImageInput:\n     return alpha_composite\n \n \n-# Modified from transformers.models.idefics2.image_processing_idefics2.make_list_of_images\n-def make_list_of_images(images: ImageInput) -> List[List[Optional[np.ndarray]]]:\n-    \"\"\"\n-    Convert a single image or a list of images to a list of numpy arrays.\n-\n-    Args:\n-        images (`ImageInput`):\n-            A single image or a list of images.\n-\n-    Returns:\n-        A list of numpy arrays.\n-    \"\"\"\n-    # If it's a single image, convert it to a list of lists\n-    if is_valid_image(images):\n-        output_images = [[images]]\n-    # If it's a list of images, it's a single batch, so convert it to a list of lists\n-    elif isinstance(images, (list, tuple)) and is_valid_list_of_images(images):\n-        output_images = [images]\n-    # If it's a list of batches, it's already in the right format\n-    elif (\n-        isinstance(images, (list, tuple))\n-        and all(isinstance(images_i, (list, tuple)) for images_i in images)\n-        and any(is_valid_list_of_images(images_i) for images_i in images)\n-    ):\n-        output_images = images\n-    else:\n-        raise ValueError(\n-            \"Invalid input type. Must be a single image, a list of images, or a list of batches of images.\"\n-        )\n-    return output_images\n-\n-\n-def is_valid_list_of_images(images: List):\n-    return images and all(is_valid_image(image) for image in images)\n-\n-\n def _validate_size(size: Dict[str, int]) -> None:\n     if not (\"height\" in size and \"width\" in size):\n         raise ValueError(f\"Argument `size` must be a dictionary with keys 'height' and 'width'. Got: {size}\")\n@@ -726,7 +690,7 @@ def preprocess(\n         # extra validation\n         _validate_mllama_preprocess_arguments(do_resize, size, do_pad, max_image_tiles)\n \n-        images_list = make_list_of_images(images)\n+        images_list = make_nested_list_of_images(images)\n \n         if self.do_convert_rgb:\n             images_list = [[convert_to_rgb(image) for image in images] for images in images_list]"
        },
        {
            "sha": "4e8f788cf70202689b98878bee36c3ff2879e044",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -20,16 +20,13 @@\n import numpy as np\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput\n+from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n     PreTokenizedInput,\n     TextInput,\n )\n \n-# TODO: Can we do it that way or its better include as \"Copied from ...\"\n-from .image_processing_mllama import make_list_of_images\n-\n \n class MllamaImagesKwargs(ImagesKwargs, total=False):\n     max_image_tiles: Optional[int]\n@@ -292,7 +289,7 @@ def __call__(\n \n         n_images_in_images = [0]\n         if images is not None:\n-            images = make_list_of_images(images)\n+            images = make_nested_list_of_images(images)\n             n_images_in_images = [len(sample) for sample in images]\n \n         if text is not None:"
        },
        {
            "sha": "ac4b98e70b001fea8e63f21b8b8c39a25e76b463",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 26,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -19,7 +19,7 @@\n from typing import List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n-from ...image_utils import ImageInput, is_valid_image\n+from ...image_utils import ImageInput, is_valid_image, make_flat_list_of_images\n from ...processing_utils import (\n     ImagesKwargs,\n     ProcessingKwargs,\n@@ -99,30 +99,6 @@ def build_string_from_input(prompt, bos_token, image_seq_len, image_token, num_i\n     return f\"{image_token * image_seq_len * num_images}{bos_token}{prompt}\\n\"\n \n \n-# Copied from transformers.models.llava_next.image_processing_llava_next.make_batched_images\n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched video from {images}\")\n-\n-\n class PaliGemmaProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a PaliGemma processor which wraps a PaliGemma image processor and a PaliGemma tokenizer into a single processor.\n@@ -297,7 +273,7 @@ def __call__(\n                     )\n                     for prompt, image_list in zip(text, images)\n                 ]\n-                images = make_batched_images(images)\n+                images = make_flat_list_of_images(images)\n             else:\n                 expanded_samples = []\n                 for sample in text:"
        },
        {
            "sha": "168995f344ec5418636e87bab6300375245b6383",
            "filename": "src/transformers/models/qwen2_5_vl/image_processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 46,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fimage_processing_qwen2_5_vl.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -41,61 +41,19 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n+    make_flat_list_of_images,\n     make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_vision_available, logging\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n+from ...utils import TensorType, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched images from {images}\")\n-\n-\n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], Image.Image):\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -398,7 +356,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n-            images = make_batched_images(images)\n+            images = make_flat_list_of_images(images)\n         if videos is not None:\n             videos = make_batched_videos(videos)\n "
        },
        {
            "sha": "51b657327c345231c04fa9a4ee0b8f2b65fe963a",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 47,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -40,62 +40,19 @@\n     get_image_size,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n+    make_flat_list_of_images,\n     make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, is_vision_available, logging\n+from ...utils import TensorType, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-def make_batched_images(images) -> List[List[ImageInput]]:\n-    \"\"\"\n-    Accepts images in list or nested list format, and makes a list of images for preprocessing.\n-\n-    Args:\n-        images (`Union[List[List[ImageInput]], List[ImageInput], ImageInput]`):\n-            The input image.\n-\n-    Returns:\n-        list: A list of images.\n-    \"\"\"\n-    if isinstance(images, (list, tuple)) and isinstance(images[0], (list, tuple)) and is_valid_image(images[0][0]):\n-        return [img for img_list in images for img in img_list]\n-\n-    elif isinstance(images, (list, tuple)) and is_valid_image(images[0]):\n-        return images\n-\n-    elif is_valid_image(images):\n-        return [images]\n-\n-    raise ValueError(f\"Could not make batched images from {images}\")\n-\n-\n-# Copied from transformers.models.llava_next_video.image_processing_llava_next_video.make_batched_videos\n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], Image.Image):\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n def smart_resize(\n     height: int, width: int, factor: int = 28, min_pixels: int = 56 * 56, max_pixels: int = 14 * 14 * 4 * 1280\n ):\n@@ -392,7 +349,7 @@ def preprocess(\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n         if images is not None:\n-            images = make_batched_images(images)\n+            images = make_flat_list_of_images(images)\n         if videos is not None:\n             videos = make_batched_videos(videos)\n "
        },
        {
            "sha": "2283da6097d2cd620ea5859ec559f6e4466483ac",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -39,6 +39,8 @@\n     get_image_size,\n     get_image_type,\n     infer_channel_dimension_format,\n+    make_batched_videos,\n+    make_flat_list_of_images,\n     make_list_of_images,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -51,7 +53,7 @@\n     is_vision_available,\n     logging,\n )\n-from .image_processing_qwen2_vl import make_batched_images, make_batched_videos, smart_resize\n+from .image_processing_qwen2_vl import smart_resize\n \n \n if is_torch_available():\n@@ -350,7 +352,7 @@ def preprocess(\n         image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n \n         if images is not None:\n-            images = make_batched_images(images)\n+            images = make_flat_list_of_images(images)\n         if videos is not None:\n             videos = make_batched_videos(videos)\n "
        },
        {
            "sha": "d5826878065393ad784a80af0370868f158d890c",
            "filename": "src/transformers/models/siglip/image_processing_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fimage_processing_siglip.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -30,7 +30,7 @@\n     PILImageResampling,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    make_list_of_images,\n+    make_flat_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n@@ -181,7 +181,7 @@ def preprocess(\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n \n-        images = make_list_of_images(images)\n+        images = make_flat_list_of_images(images)\n \n         if not valid_images(images):\n             raise ValueError("
        },
        {
            "sha": "dbb1054857706c013a87bc34c3ffbc65cb3dc9bf",
            "filename": "src/transformers/models/video_llava/image_processing_video_llava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 23,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fimage_processing_video_llava.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -34,38 +34,18 @@\n     VideoInput,\n     infer_channel_dimension_format,\n     is_scaled_image,\n-    is_valid_image,\n+    make_batched_videos,\n     make_list_of_images,\n     to_numpy_array,\n     valid_images,\n     validate_preprocess_arguments,\n )\n-from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n-if is_vision_available():\n-    import PIL\n-\n-\n-def make_batched_videos(videos) -> List[VideoInput]:\n-    if isinstance(videos, (list, tuple)) and isinstance(videos[0], (list, tuple)) and is_valid_image(videos[0][0]):\n-        return videos\n-\n-    elif isinstance(videos, (list, tuple)) and is_valid_image(videos[0]):\n-        if isinstance(videos[0], PIL.Image.Image):\n-            return [videos]\n-        elif len(videos[0].shape) == 4:\n-            return [list(video) for video in videos]\n-\n-    elif is_valid_image(videos) and len(videos.shape) == 4:\n-        return [list(videos)]\n-\n-    raise ValueError(f\"Could not make batched video from {videos}\")\n-\n-\n class VideoLlavaImageProcessor(BaseImageProcessor):\n     r\"\"\"\n     Constructs a CLIP image processor.\n@@ -208,7 +188,7 @@ def preprocess(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-    ) -> PIL.Image.Image:\n+    ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images.\n "
        },
        {
            "sha": "5e94ecaab9fc84f4ad7f7051f4f6d8ae86a75e63",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -26,7 +26,6 @@\n from tests.test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from transformers import (\n     is_torch_available,\n-    is_vision_available,\n )\n from transformers.models.colpali.configuration_colpali import ColPaliConfig\n from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliForRetrievalOutput\n@@ -43,9 +42,6 @@\n if is_torch_available():\n     import torch\n \n-if is_vision_available():\n-    pass\n-\n \n class ColPaliForRetrievalModelTester:\n     def __init__("
        },
        {
            "sha": "76c5c11de2201186b53973aa4c65696189b81f79",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -39,7 +39,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -58,10 +58,6 @@\n     from transformers import InstructBlipVideoForConditionalGeneration, InstructBlipVideoVisionModel\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class InstructBlipVideoVisionModelTester:\n     def __init__(\n         self,"
        },
        {
            "sha": "173ddc1a134f9c56f3245380c588d66388c48319",
            "filename": "tests/models/pixtral/test_modeling_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_modeling_pixtral.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -20,7 +20,6 @@\n     PixtralVisionConfig,\n     PixtralVisionModel,\n     is_torch_available,\n-    is_vision_available,\n )\n from transformers.testing_utils import (\n     require_torch,\n@@ -35,10 +34,6 @@\n     import torch\n \n \n-if is_vision_available():\n-    pass\n-\n-\n class PixtralVisionModelTester:\n     def __init__(\n         self,"
        },
        {
            "sha": "d4ce1435a14dbdb7bb7e2f5b8861338ce999b184",
            "filename": "tests/utils/test_image_utils.py",
            "status": "modified",
            "additions": 340,
            "deletions": 1,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Futils%2Ftest_image_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7188ba600e36d3fd191b12e19f1b3bb81a8404f/tests%2Futils%2Ftest_image_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_image_utils.py?ref=d7188ba600e36d3fd191b12e19f1b3bb81a8404f",
            "patch": "@@ -28,7 +28,14 @@\n \n from tests.pipelines.test_pipelines_document_question_answering import INVOICE_URL\n from transformers import is_torch_available, is_vision_available\n-from transformers.image_utils import ChannelDimension, get_channel_dimension_axis, make_list_of_images\n+from transformers.image_utils import (\n+    ChannelDimension,\n+    get_channel_dimension_axis,\n+    make_batched_videos,\n+    make_flat_list_of_images,\n+    make_list_of_images,\n+    make_nested_list_of_images,\n+)\n from transformers.testing_utils import is_flaky, require_torch, require_vision\n \n \n@@ -115,6 +122,21 @@ def test_conversion_array_to_array(self):\n         self.assertEqual(array5.shape, (3, 16, 32))\n         self.assertTrue(np.array_equal(array5, array1))\n \n+    def test_make_list_of_images_pil(self):\n+        # Test a single image is converted to a list of 1 image\n+        pil_image = get_random_image(16, 32)\n+        images_list = make_list_of_images(pil_image)\n+        self.assertIsInstance(images_list, list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertIsInstance(images_list[0], PIL.Image.Image)\n+\n+        # Test a list of images is not modified\n+        images = [get_random_image(16, 32) for _ in range(4)]\n+        images_list = make_list_of_images(images)\n+        self.assertIsInstance(images_list, list)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertIsInstance(images_list[0], PIL.Image.Image)\n+\n     def test_make_list_of_images_numpy(self):\n         # Test a single image is converted to a list of 1 image\n         images = np.random.randint(0, 256, (16, 32, 3))\n@@ -167,6 +189,323 @@ def test_make_list_of_images_torch(self):\n         self.assertTrue(np.array_equal(images_list[0], images[0]))\n         self.assertIsInstance(images_list, list)\n \n+    def test_make_flat_list_of_images_pil(self):\n+        # Test a single image is converted to a list of 1 image\n+        pil_image = get_random_image(16, 32)\n+        images_list = make_flat_list_of_images(pil_image)\n+        self.assertIsInstance(images_list, list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertIsInstance(images_list[0], PIL.Image.Image)\n+\n+        # Test a list of images is not modified\n+        images = [get_random_image(16, 32) for _ in range(4)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertIsInstance(images_list, list)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertIsInstance(images_list[0], PIL.Image.Image)\n+\n+        # Test a nested list of images is flattened\n+        images = [[get_random_image(16, 32) for _ in range(2)] for _ in range(2)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertIsInstance(images_list, list)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertIsInstance(images_list[0], PIL.Image.Image)\n+\n+    def test_make_flat_list_of_images_numpy(self):\n+        # Test a single image is converted to a list of 1 image\n+        images = np.random.randint(0, 256, (16, 32, 3))\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertTrue(np.array_equal(images_list[0], images))\n+        self.assertIsInstance(images_list, list)\n+\n+        # Test a 4d array of images is changed to a list of images\n+        images = np.random.randint(0, 256, (4, 16, 32, 3))\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertIsInstance(images_list, list)\n+        self.assertIsInstance(images_list[0], np.ndarray)\n+        self.assertTrue(np.array_equal(images_list[0], images[0]))\n+\n+        # Test a list of images is not modified\n+        images = [np.random.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertTrue(np.array_equal(images_list[0], images[0]))\n+        self.assertIsInstance(images_list, list)\n+\n+        # Test list of 4d array images is flattened\n+        images = [np.random.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 8)\n+        self.assertTrue(np.array_equal(images_list[0], images[0][0]))\n+        self.assertIsInstance(images_list, list)\n+        self.assertIsInstance(images_list[0], np.ndarray)\n+\n+        # Test nested list of images is flattened\n+        images = [[np.random.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertTrue(np.array_equal(images_list[0], images[0][0]))\n+        self.assertIsInstance(images_list, list)\n+\n+    @require_torch\n+    def test_make_flat_list_of_images_torch(self):\n+        # Test a single image is converted to a list of 1 image\n+        images = torch.randint(0, 256, (16, 32, 3))\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertTrue(np.array_equal(images_list[0], images))\n+        self.assertIsInstance(images_list, list)\n+\n+        # Test a 4d tensors of images is changed to a list of images\n+        images = torch.randint(0, 256, (4, 16, 32, 3))\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertIsInstance(images_list, list)\n+        self.assertIsInstance(images_list[0], torch.Tensor)\n+        self.assertTrue(np.array_equal(images_list[0], images[0]))\n+\n+        # Test a list of images is not modified\n+        images = [torch.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertTrue(np.array_equal(images_list[0], images[0]))\n+        self.assertIsInstance(images_list, list)\n+\n+        # Test list of 4d tensors of imagess is flattened\n+        images = [torch.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 8)\n+        self.assertTrue(np.array_equal(images_list[0], images[0][0]))\n+        self.assertIsInstance(images_list, list)\n+        self.assertIsInstance(images_list[0], torch.Tensor)\n+\n+        # Test nested list of images is flattened\n+        images = [[torch.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        images_list = make_flat_list_of_images(images)\n+        self.assertEqual(len(images_list), 4)\n+        self.assertTrue(np.array_equal(images_list[0], images[0][0]))\n+        self.assertIsInstance(images_list, list)\n+\n+    def test_make_nested_list_of_images_pil(self):\n+        # Test a single image is converted to a nested list of 1 image\n+        pil_image = get_random_image(16, 32)\n+        images_list = make_nested_list_of_images(pil_image)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list[0]), 1)\n+        self.assertIsInstance(images_list[0][0], PIL.Image.Image)\n+\n+        # Test a list of images is converted to a nested list of images\n+        images = [get_random_image(16, 32) for _ in range(4)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertIsInstance(images_list[0][0], PIL.Image.Image)\n+\n+        # Test a nested list of images is not modified\n+        images = [[get_random_image(16, 32) for _ in range(2)] for _ in range(2)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 2)\n+        self.assertEqual(len(images_list[0]), 2)\n+        self.assertIsInstance(images_list[0][0], PIL.Image.Image)\n+\n+    def test_make_nested_list_of_images_numpy(self):\n+        # Test a single image is converted to a nested list of 1 image\n+        images = np.random.randint(0, 256, (16, 32, 3))\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertTrue(np.array_equal(images_list[0][0], images))\n+\n+        # Test a 4d array of images is converted to a nested list of images\n+        images = np.random.randint(0, 256, (4, 16, 32, 3))\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertIsInstance(images_list[0][0], np.ndarray)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0]))\n+\n+        # Test a list of images is converted to a nested list of images\n+        images = [np.random.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0]))\n+\n+        # Test a nested list of images is left unchanged\n+        images = [[np.random.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 2)\n+        self.assertEqual(len(images_list[0]), 2)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0][0]))\n+\n+        # Test a list of 4d array images is converted to a nested list of images\n+        images = [np.random.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertIsInstance(images_list[0][0], np.ndarray)\n+        self.assertEqual(len(images_list), 2)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0][0]))\n+\n+    @require_torch\n+    def test_make_nested_list_of_images_torch(self):\n+        # Test a single image is converted to a nested list of 1 image\n+        images = torch.randint(0, 256, (16, 32, 3))\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list[0]), 1)\n+        self.assertTrue(np.array_equal(images_list[0][0], images))\n+\n+        # Test a 4d tensor of images is converted to a nested list of images\n+        images = torch.randint(0, 256, (4, 16, 32, 3))\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertIsInstance(images_list[0][0], torch.Tensor)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0]))\n+\n+        # Test a list of images is converted to a nested list of images\n+        images = [torch.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 1)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0]))\n+\n+        # Test a nested list of images is left unchanged\n+        images = [[torch.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertEqual(len(images_list), 2)\n+        self.assertEqual(len(images_list[0]), 2)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0][0]))\n+\n+        # Test a list of 4d tensor images is converted to a nested list of images\n+        images = [torch.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        images_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(images_list[0], list)\n+        self.assertIsInstance(images_list[0][0], torch.Tensor)\n+        self.assertEqual(len(images_list), 2)\n+        self.assertEqual(len(images_list[0]), 4)\n+        self.assertTrue(np.array_equal(images_list[0][0], images[0][0]))\n+\n+    def test_make_batched_videos_pil(self):\n+        # Test a single image is converted to a list of 1 video with 1 frame\n+        pil_image = get_random_image(16, 32)\n+        videos_list = make_batched_videos(pil_image)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list[0]), 1)\n+        self.assertIsInstance(videos_list[0][0], PIL.Image.Image)\n+\n+        # Test a list of images is converted to a list of 1 video\n+        images = [get_random_image(16, 32) for _ in range(4)]\n+        videos_list = make_batched_videos(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertIsInstance(videos_list[0][0], PIL.Image.Image)\n+\n+        # Test a nested list of images is not modified\n+        images = [[get_random_image(16, 32) for _ in range(2)] for _ in range(2)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 2)\n+        self.assertEqual(len(videos_list[0]), 2)\n+        self.assertIsInstance(videos_list[0][0], PIL.Image.Image)\n+\n+    def test_make_batched_videos_numpy(self):\n+        # Test a single image is converted to a list of 1 video with 1 frame\n+        images = np.random.randint(0, 256, (16, 32, 3))\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images))\n+\n+        # Test a 4d array of images is converted to a a list of 1 video\n+        images = np.random.randint(0, 256, (4, 16, 32, 3))\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertIsInstance(videos_list[0][0], np.ndarray)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0]))\n+\n+        # Test a list of images is converted to a list of videos\n+        images = [np.random.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0]))\n+\n+        # Test a nested list of images is left unchanged\n+        images = [[np.random.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 2)\n+        self.assertEqual(len(videos_list[0]), 2)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0][0]))\n+\n+        # Test a list of 4d array images is converted to a list of videos\n+        images = [np.random.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertIsInstance(videos_list[0][0], np.ndarray)\n+        self.assertEqual(len(videos_list), 2)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0][0]))\n+\n+    @require_torch\n+    def test_make_batched_videos_torch(self):\n+        # Test a single image is converted to a list of 1 video with 1 frame\n+        images = torch.randint(0, 256, (16, 32, 3))\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list[0]), 1)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images))\n+\n+        # Test a 4d tensor of images is converted to a list of 1 video\n+        images = torch.randint(0, 256, (4, 16, 32, 3))\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertIsInstance(videos_list[0][0], torch.Tensor)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0]))\n+\n+        # Test a list of images is converted to a list of videos\n+        images = [torch.randint(0, 256, (16, 32, 3)) for _ in range(4)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 1)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0]))\n+\n+        # Test a nested list of images is left unchanged\n+        images = [[torch.randint(0, 256, (16, 32, 3)) for _ in range(2)] for _ in range(2)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertEqual(len(videos_list), 2)\n+        self.assertEqual(len(videos_list[0]), 2)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0][0]))\n+\n+        # Test a list of 4d tensor images is converted to a list of videos\n+        images = [torch.randint(0, 256, (4, 16, 32, 3)) for _ in range(2)]\n+        videos_list = make_nested_list_of_images(images)\n+        self.assertIsInstance(videos_list[0], list)\n+        self.assertIsInstance(videos_list[0][0], torch.Tensor)\n+        self.assertEqual(len(videos_list), 2)\n+        self.assertEqual(len(videos_list[0]), 4)\n+        self.assertTrue(np.array_equal(videos_list[0][0], images[0][0]))\n+\n     @require_torch\n     def test_conversion_torch_to_array(self):\n         feature_extractor = ImageFeatureExtractionMixin()"
        }
    ],
    "stats": {
        "total": 991,
        "additions": 506,
        "deletions": 485
    }
}