{
    "author": "henryhmko",
    "message": "Multiple typo fixes in Tutorials docs (#35035)\n\n* Fixed typo in multi gpu docs and OLMoE version\r\n\r\n* Fixed typos in docs for agents, agents advanced, knowledge distillation, and image feature extraction\r\n\r\n* Fixed incorrect usage of model.image_guided_detection in zero shot object detection docs",
    "sha": "31299670cda29f25fbc655f6f166e7b8cc21c89f",
    "files": [
        {
            "sha": "56c9184980f4b2ecfe20f899041b7fe824fee21c",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -225,7 +225,7 @@ You have access to the following tools:\n To solve the task, you must plan forward to proceed in a series of steps, in a cycle of 'Thought:', 'Code:', and 'Observation:' sequences.\n \n At each step, in the 'Thought:' sequence, you should first explain your reasoning towards solving the task, then the tools that you want to use.\n-Then in the 'Code:' sequence, you shold write the code in simple Python. The code sequence must end with '/End code' sequence.\n+Then in the 'Code:' sequence, you should write the code in simple Python. The code sequence must end with '/End code' sequence.\n During each intermediate step, you can use 'print()' to save whatever important information you will then need.\n These print outputs will then be available in the 'Observation:' field, for using this information as input for the next step.\n "
        },
        {
            "sha": "c4753bf1366b09b5814ac7ebae43c39e9c86d83c",
            "filename": "docs/source/en/agents_advanced.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fagents_advanced.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fagents_advanced.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents_advanced.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -211,7 +211,7 @@ agent.run(\"How many more blocks (also denoted as layers) are in BERT base encode\n \n ## Display your agent run in a cool Gradio interface\n \n-You can leverage `gradio.Chatbot`to display your agent's thoughts using `stream_to_gradio`, here is an example:\n+You can leverage `gradio.Chatbot` to display your agent's thoughts using `stream_to_gradio`, here is an example:\n \n ```py\n import gradio as gr"
        },
        {
            "sha": "c810a18470a09e370a7d53b0250927742dc7b42d",
            "filename": "docs/source/en/perf_train_gpu_many.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_many.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -553,7 +553,7 @@ It performs a sort of 4D Parallelism over Sample-Operator-Attribute-Parameter.\n Examples:\n * Sample\n \n-Let's take 10 batches of sequence length 512. If we parallelize them by sample dimension into 2 devices, we get 10 x 512 which becomes be 5 x 2 x 512.\n+Let's take 10 batches of sequence length 512. If we parallelize them by sample dimension into 2 devices, we get 10 x 512 which becomes 5 x 2 x 512.\n \n * Operator\n "
        },
        {
            "sha": "e55a9e2379d531115fea5b2bec0267f2809deb59",
            "filename": "docs/source/en/tasks/image_feature_extraction.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -84,7 +84,7 @@ If you want to get the last hidden states before pooling, avoid passing any valu\n \n ```python\n pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-224\", device=DEVICE)\n-output = pipe(image_real)\n+outputs = pipe(image_real)\n ```\n \n Since the outputs are unpooled, we get the last hidden states where the first dimension is the batch size, and the last two are the embedding shape."
        },
        {
            "sha": "c1ccafb6fc5d2adb66caa0f68fc7d06f6fc3a52a",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -17,7 +17,7 @@ rendered properly in your Markdown viewer.\n \n [[open-in-colab]]\n \n-Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between it's outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://arxiv.org/abs/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n+Knowledge distillation is a technique used to transfer knowledge from a larger, more complex model (teacher) to a smaller, simpler model (student). To distill knowledge from one model to another, we take a pre-trained teacher model trained on a certain task (image classification for this case) and randomly initialize a student model to be trained on image classification. Next, we train the student model to minimize the difference between its outputs and the teacher's outputs, thus making it mimic the behavior. It was first introduced in [Distilling the Knowledge in a Neural Network by Hinton et al](https://arxiv.org/abs/1503.02531). In this guide, we will do task-specific knowledge distillation. We will use the [beans dataset](https://huggingface.co/datasets/beans) for this.\n \n This guide demonstrates how you can distill a [fine-tuned ViT model](https://huggingface.co/merve/vit-mobilenet-beans-224) (teacher model) to a [MobileNet](https://huggingface.co/google/mobilenet_v2_1.4_224) (student model) using the [TrainerÂ API](https://huggingface.co/docs/transformers/en/main_classes/trainer#trainer) of ðŸ¤— Transformers.\n "
        },
        {
            "sha": "defe54b1c769eb8f999bdc0d59e690daa614bb5f",
            "filename": "docs/source/en/tasks/zero_shot_object_detection.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fzero_shot_object_detection.md?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -288,7 +288,7 @@ as before except now there are no labels.\n >>> scores = results[\"scores\"].tolist()\n >>> boxes = results[\"boxes\"].tolist()\n \n->>> for box, score, label in zip(boxes, scores, labels):\n+>>> for box, score in zip(boxes, scores):\n ...     xmin, ymin, xmax, ymax = box\n ...     draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\n "
        },
        {
            "sha": "dbee3bfa0bd4d72124fdaa99459fd2495459982a",
            "filename": "src/transformers/models/olmoe/configuration_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fconfiguration_olmoe.py?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -19,7 +19,7 @@ class OlmoeConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`OlmoeModel`]. It is used to instantiate an OLMoE\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n-    defaults will yield a similar configuration to that of the [allenai/OLMoE-1B-7B-0824](https://huggingface.co/allenai/OLMoE-1B-7B-0824).\n+    defaults will yield a similar configuration to that of the [allenai/OLMoE-1B-7B-0924](https://huggingface.co/allenai/OLMoE-1B-7B-0924).\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n     documentation from [`PretrainedConfig`] for more information."
        },
        {
            "sha": "4398e2f5c9a1fd89fb6106475532163f2a14b330",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/31299670cda29f25fbc655f6f166e7b8cc21c89f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31299670cda29f25fbc655f6f166e7b8cc21c89f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=31299670cda29f25fbc655f6f166e7b8cc21c89f",
            "patch": "@@ -1249,8 +1249,8 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, OlmoeForCausalLM\n \n-        >>> model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0824\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0824\")\n+        >>> model = OlmoeForCausalLM.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"allenai/OLMoE-1B-7B-0924\")\n \n         >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 9,
        "deletions": 9
    }
}