{
    "author": "pcuenca",
    "message": "Support Llama 3.2 conversion (text models) (#33778)\n\n* Support Llama 3.2 conversion (text models)\r\n\r\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>\r\n\r\n* Fix rope factor\r\n\r\n* Update chat template\r\n\r\nInitialize from a well-known template.\r\nThe guidance is that the changes should be applied to 3.1 models as\r\nwell.\r\n\r\n* Remove import\r\n\r\n* Support Llama Guard 3 conversion\r\n\r\n* Tokenizer details\r\n\r\n* Fix eos added token in base models\r\n\r\n* Fix generation config for base models\r\n\r\n* Specify revision for known tokenizers\r\n\r\n* Style\r\n\r\n* Reuse chat templates for older models\r\n\r\n* Improve error when converting tokenizer < Llama 3\r\n\r\n---------\r\n\r\nCo-authored-by: Omar Sanseviero <osanseviero@gmail.com>",
    "sha": "7a06d07e14ce4f575ac97287d9af472bd88e9d4c",
    "files": [
        {
            "sha": "eb2862eb203d031d2cea647915a41cd43abdded8",
            "filename": "src/transformers/models/llama/convert_llama_weights_to_hf.py",
            "status": "modified",
            "additions": 338,
            "deletions": 215,
            "changes": 553,
            "blob_url": "https://github.com/huggingface/transformers/blob/7a06d07e14ce4f575ac97287d9af472bd88e9d4c/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7a06d07e14ce4f575ac97287d9af472bd88e9d4c/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fconvert_llama_weights_to_hf.py?ref=7a06d07e14ce4f575ac97287d9af472bd88e9d4c",
            "patch": "@@ -15,11 +15,12 @@\n import gc\n import json\n import os\n-import shutil\n+import tempfile\n import warnings\n from typing import List\n \n import torch\n+from tokenizers import AddedToken, processors\n \n from transformers import GenerationConfig, LlamaConfig, LlamaForCausalLM, LlamaTokenizer, PreTrainedTokenizerFast\n from transformers.convert_slow_tokenizer import TikTokenConverter\n@@ -39,7 +40,7 @@\n \n ```\n python src/transformers/models/llama/convert_llama_weights_to_hf.py \\\n-    --input_dir /path/to/downloaded/llama/weights --model_size 7B --output_dir /output/path\n+    --input_dir /path/to/downloaded/llama/weights --model_size 1B --llama_version 3.2 --output_dir /output/path\n ```\n \n Thereafter, models can be loaded via:\n@@ -75,6 +76,8 @@\n \"\"\"\n \n NUM_SHARDS = {\n+    \"1B\": 1,\n+    \"3B\": 1,\n     \"7B\": 1,\n     \"8B\": 1,\n     \"8Bf\": 1,\n@@ -90,7 +93,79 @@\n     \"405B-MP16\": 16,\n }\n \n-CONTEXT_LENGTH_FOR_VERSION = {\"3.1\": 131072, \"3\": 8192, \"2\": 4096, \"1\": 2048}\n+CONTEXT_LENGTH_FOR_VERSION = {\"Guard-3\": 131072, \"3.2\": 131072, \"3.1\": 131072, \"3\": 8192, \"2\": 4096, \"1\": 2048}\n+\n+BOS_ADDED_TOKEN = AddedToken(\n+    \"<|begin_of_text|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True\n+)\n+EOS_ADDED_TOKEN = AddedToken(\n+    \"<|end_of_text|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True\n+)\n+EOT_ADDED_TOKEN = AddedToken(\n+    \"<|eot_id|>\", single_word=False, lstrip=False, rstrip=False, normalized=False, special=True\n+)\n+\n+DEFAULT_LLAMA_SPECIAL_TOKENS = {\n+    \"3\": [\n+        \"<|begin_of_text|>\",\n+        \"<|end_of_text|>\",\n+        \"<|reserved_special_token_0|>\",\n+        \"<|reserved_special_token_1|>\",\n+        \"<|reserved_special_token_2|>\",\n+        \"<|reserved_special_token_3|>\",\n+        \"<|start_header_id|>\",\n+        \"<|end_header_id|>\",\n+        \"<|reserved_special_token_4|>\",\n+        \"<|eot_id|>\",  # end of turn\n+    ]\n+    + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)],\n+    \"3.1\": [\n+        \"<|begin_of_text|>\",\n+        \"<|end_of_text|>\",\n+        \"<|reserved_special_token_0|>\",\n+        \"<|reserved_special_token_1|>\",\n+        \"<|finetune_right_pad_id|>\",\n+        \"<|reserved_special_token_2|>\",\n+        \"<|start_header_id|>\",\n+        \"<|end_header_id|>\",\n+        \"<|eom_id|>\",  # end of message\n+        \"<|eot_id|>\",  # end of turn\n+        \"<|python_tag|>\",\n+    ]\n+    + [f\"<|reserved_special_token_{i}|>\" for i in range(3, 256 - 8)],\n+    \"3.2\": [\n+        \"<|begin_of_text|>\",\n+        \"<|end_of_text|>\",\n+        \"<|reserved_special_token_0|>\",\n+        \"<|reserved_special_token_1|>\",\n+        \"<|finetune_right_pad_id|>\",\n+        \"<|reserved_special_token_2|>\",\n+        \"<|start_header_id|>\",\n+        \"<|end_header_id|>\",\n+        \"<|eom_id|>\",  # end of message\n+        \"<|eot_id|>\",  # end of turn\n+        \"<|python_tag|>\",\n+    ]\n+    + [f\"<|reserved_special_token_{i}|>\" for i in range(3, 256 - 8)],\n+    \"Guard-3\": [\n+        \"<|begin_of_text|>\",\n+        \"<|end_of_text|>\",\n+        \"<|reserved_special_token_0|>\",\n+        \"<|reserved_special_token_1|>\",\n+        \"<|finetune_right_pad_id|>\",\n+        \"<|reserved_special_token_2|>\",\n+        \"<|start_header_id|>\",\n+        \"<|end_header_id|>\",\n+        \"<|eom_id|>\",  # end of message\n+        \"<|eot_id|>\",  # end of turn\n+        \"<|python_tag|>\",\n+    ]\n+    + [f\"<|reserved_special_token_{i}|>\" for i in range(3, 256 - 8)],\n+}\n+\n+\n+def is_llama_3(version):\n+    return version in [\"3\", \"3.1\", \"3.2\", \"Guard-3\"]\n \n \n def compute_intermediate_size(n, ffn_dim_multiplier=1, multiple_of=256):\n@@ -116,11 +191,9 @@ def write_model(\n     vocab_size=None,\n     num_shards=None,\n     instruct=False,\n+    push_to_hub=False,\n ):\n-    os.makedirs(model_path, exist_ok=True)\n-    tmp_model_path = os.path.join(model_path, \"tmp\")\n-    os.makedirs(tmp_model_path, exist_ok=True)\n-\n+    print(\"Converting the model.\")\n     params = read_json(os.path.join(input_base_path, \"params.json\"))\n     num_shards = NUM_SHARDS[model_size] if num_shards is None else num_shards\n     params = params.get(\"model\", params)\n@@ -131,7 +204,7 @@ def write_model(\n     dims_per_head = dim // n_heads\n     base = params.get(\"rope_theta\", 10000.0)\n     inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n-    if base > 10000.0 and float(llama_version) < 3:\n+    if base > 10000.0 and not is_llama_3(llama_version):\n         max_position_embeddings = 16384\n     else:\n         max_position_embeddings = CONTEXT_LENGTH_FOR_VERSION[llama_version]\n@@ -149,163 +222,183 @@ def write_model(\n     def permute(w, n_heads, dim1=dim, dim2=dim):\n         return w.view(n_heads, dim1 // n_heads // 2, 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n \n-    print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n-    # Load weights\n-    if num_shards == 1:\n-        # Not sharded\n-        # (The sharded implementation would also work, but this is simpler.)\n-        loaded = torch.load(os.path.join(input_base_path, \"consolidated.00.pth\"), map_location=\"cpu\")\n-    else:\n-        # Sharded\n-        checkpoint_list = sorted([file for file in os.listdir(input_base_path) if file.endswith(\".pth\")])\n-        print(\"Loading in order:\", checkpoint_list)\n-        loaded = [torch.load(os.path.join(input_base_path, file), map_location=\"cpu\") for file in checkpoint_list]\n-    param_count = 0\n-    index_dict = {\"weight_map\": {}}\n-    for layer_i in range(n_layers):\n-        filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n+    with tempfile.TemporaryDirectory() as tmp_model_path:\n+        print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n+        # Load weights\n         if num_shards == 1:\n-            # Unsharded\n-            state_dict = {\n-                f\"model.layers.{layer_i}.self_attn.q_proj.weight\": permute(\n-                    loaded[f\"layers.{layer_i}.attention.wq.weight\"], n_heads=n_heads\n-                ),\n-                f\"model.layers.{layer_i}.self_attn.k_proj.weight\": permute(\n-                    loaded[f\"layers.{layer_i}.attention.wk.weight\"],\n-                    n_heads=num_key_value_heads,\n-                    dim1=key_value_dim,\n-                ),\n-                f\"model.layers.{layer_i}.self_attn.v_proj.weight\": loaded[f\"layers.{layer_i}.attention.wv.weight\"],\n-                f\"model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[f\"layers.{layer_i}.attention.wo.weight\"],\n-                f\"model.layers.{layer_i}.mlp.gate_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w1.weight\"],\n-                f\"model.layers.{layer_i}.mlp.down_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w2.weight\"],\n-                f\"model.layers.{layer_i}.mlp.up_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w3.weight\"],\n-                f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[f\"layers.{layer_i}.attention_norm.weight\"],\n-                f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[f\"layers.{layer_i}.ffn_norm.weight\"],\n-            }\n+            # Not sharded\n+            # (The sharded implementation would also work, but this is simpler.)\n+            loaded = torch.load(os.path.join(input_base_path, \"consolidated.00.pth\"), map_location=\"cpu\")\n         else:\n             # Sharded\n-            # Note that attention.w{q,k,v,o}, feed_fordward.w[1,2,3], attention_norm.weight and ffn_norm.weight share\n-            # the same storage object, saving attention_norm and ffn_norm will save other weights too, which is\n-            # redundant as other weights will be stitched from multiple shards. To avoid that, they are cloned.\n-\n-            state_dict = {\n-                f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[0][\n-                    f\"layers.{layer_i}.attention_norm.weight\"\n-                ].clone(),\n-                f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[0][\n-                    f\"layers.{layer_i}.ffn_norm.weight\"\n-                ].clone(),\n-            }\n-            state_dict[f\"model.layers.{layer_i}.self_attn.q_proj.weight\"] = permute(\n-                torch.cat(\n-                    [\n-                        loaded[i][f\"layers.{layer_i}.attention.wq.weight\"].view(n_heads_per_shard, dims_per_head, dim)\n-                        for i in range(len(loaded))\n+            checkpoint_list = sorted([file for file in os.listdir(input_base_path) if file.endswith(\".pth\")])\n+            print(\"Loading in order:\", checkpoint_list)\n+            loaded = [torch.load(os.path.join(input_base_path, file), map_location=\"cpu\") for file in checkpoint_list]\n+        param_count = 0\n+        index_dict = {\"weight_map\": {}}\n+        for layer_i in range(n_layers):\n+            filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n+            if num_shards == 1:\n+                # Unsharded\n+                state_dict = {\n+                    f\"model.layers.{layer_i}.self_attn.q_proj.weight\": permute(\n+                        loaded[f\"layers.{layer_i}.attention.wq.weight\"], n_heads=n_heads\n+                    ),\n+                    f\"model.layers.{layer_i}.self_attn.k_proj.weight\": permute(\n+                        loaded[f\"layers.{layer_i}.attention.wk.weight\"],\n+                        n_heads=num_key_value_heads,\n+                        dim1=key_value_dim,\n+                    ),\n+                    f\"model.layers.{layer_i}.self_attn.v_proj.weight\": loaded[f\"layers.{layer_i}.attention.wv.weight\"],\n+                    f\"model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[f\"layers.{layer_i}.attention.wo.weight\"],\n+                    f\"model.layers.{layer_i}.mlp.gate_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w1.weight\"],\n+                    f\"model.layers.{layer_i}.mlp.down_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w2.weight\"],\n+                    f\"model.layers.{layer_i}.mlp.up_proj.weight\": loaded[f\"layers.{layer_i}.feed_forward.w3.weight\"],\n+                    f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[\n+                        f\"layers.{layer_i}.attention_norm.weight\"\n                     ],\n-                    dim=0,\n-                ).reshape(dim, dim),\n-                n_heads=n_heads,\n-            )\n-            state_dict[f\"model.layers.{layer_i}.self_attn.k_proj.weight\"] = permute(\n-                torch.cat(\n+                    f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[\n+                        f\"layers.{layer_i}.ffn_norm.weight\"\n+                    ],\n+                }\n+            else:\n+                # Sharded\n+                # Note that attention.w{q,k,v,o}, feed_fordward.w[1,2,3], attention_norm.weight and ffn_norm.weight share\n+                # the same storage object, saving attention_norm and ffn_norm will save other weights too, which is\n+                # redundant as other weights will be stitched from multiple shards. To avoid that, they are cloned.\n+\n+                state_dict = {\n+                    f\"model.layers.{layer_i}.input_layernorm.weight\": loaded[0][\n+                        f\"layers.{layer_i}.attention_norm.weight\"\n+                    ].clone(),\n+                    f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[0][\n+                        f\"layers.{layer_i}.ffn_norm.weight\"\n+                    ].clone(),\n+                }\n+                state_dict[f\"model.layers.{layer_i}.self_attn.q_proj.weight\"] = permute(\n+                    torch.cat(\n+                        [\n+                            loaded[i][f\"layers.{layer_i}.attention.wq.weight\"].view(\n+                                n_heads_per_shard, dims_per_head, dim\n+                            )\n+                            for i in range(len(loaded))\n+                        ],\n+                        dim=0,\n+                    ).reshape(dim, dim),\n+                    n_heads=n_heads,\n+                )\n+                state_dict[f\"model.layers.{layer_i}.self_attn.k_proj.weight\"] = permute(\n+                    torch.cat(\n+                        [\n+                            loaded[i][f\"layers.{layer_i}.attention.wk.weight\"].view(\n+                                num_key_value_heads_per_shard, dims_per_head, dim\n+                            )\n+                            for i in range(len(loaded))\n+                        ],\n+                        dim=0,\n+                    ).reshape(key_value_dim, dim),\n+                    num_key_value_heads,\n+                    key_value_dim,\n+                    dim,\n+                )\n+                state_dict[f\"model.layers.{layer_i}.self_attn.v_proj.weight\"] = torch.cat(\n                     [\n-                        loaded[i][f\"layers.{layer_i}.attention.wk.weight\"].view(\n+                        loaded[i][f\"layers.{layer_i}.attention.wv.weight\"].view(\n                             num_key_value_heads_per_shard, dims_per_head, dim\n                         )\n                         for i in range(len(loaded))\n                     ],\n                     dim=0,\n-                ).reshape(key_value_dim, dim),\n-                num_key_value_heads,\n-                key_value_dim,\n-                dim,\n-            )\n-            state_dict[f\"model.layers.{layer_i}.self_attn.v_proj.weight\"] = torch.cat(\n-                [\n-                    loaded[i][f\"layers.{layer_i}.attention.wv.weight\"].view(\n-                        num_key_value_heads_per_shard, dims_per_head, dim\n-                    )\n-                    for i in range(len(loaded))\n-                ],\n-                dim=0,\n-            ).reshape(key_value_dim, dim)\n-\n-            state_dict[f\"model.layers.{layer_i}.self_attn.o_proj.weight\"] = torch.cat(\n-                [loaded[i][f\"layers.{layer_i}.attention.wo.weight\"] for i in range(len(loaded))], dim=1\n-            )\n-            state_dict[f\"model.layers.{layer_i}.mlp.gate_proj.weight\"] = torch.cat(\n-                [loaded[i][f\"layers.{layer_i}.feed_forward.w1.weight\"] for i in range(len(loaded))], dim=0\n-            )\n-            state_dict[f\"model.layers.{layer_i}.mlp.down_proj.weight\"] = torch.cat(\n-                [loaded[i][f\"layers.{layer_i}.feed_forward.w2.weight\"] for i in range(len(loaded))], dim=1\n-            )\n-            state_dict[f\"model.layers.{layer_i}.mlp.up_proj.weight\"] = torch.cat(\n-                [loaded[i][f\"layers.{layer_i}.feed_forward.w3.weight\"] for i in range(len(loaded))], dim=0\n-            )\n+                ).reshape(key_value_dim, dim)\n+\n+                state_dict[f\"model.layers.{layer_i}.self_attn.o_proj.weight\"] = torch.cat(\n+                    [loaded[i][f\"layers.{layer_i}.attention.wo.weight\"] for i in range(len(loaded))], dim=1\n+                )\n+                state_dict[f\"model.layers.{layer_i}.mlp.gate_proj.weight\"] = torch.cat(\n+                    [loaded[i][f\"layers.{layer_i}.feed_forward.w1.weight\"] for i in range(len(loaded))], dim=0\n+                )\n+                state_dict[f\"model.layers.{layer_i}.mlp.down_proj.weight\"] = torch.cat(\n+                    [loaded[i][f\"layers.{layer_i}.feed_forward.w2.weight\"] for i in range(len(loaded))], dim=1\n+                )\n+                state_dict[f\"model.layers.{layer_i}.mlp.up_proj.weight\"] = torch.cat(\n+                    [loaded[i][f\"layers.{layer_i}.feed_forward.w3.weight\"] for i in range(len(loaded))], dim=0\n+                )\n+\n+            state_dict[f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\"] = inv_freq\n+            for k, v in state_dict.items():\n+                index_dict[\"weight_map\"][k] = filename\n+                param_count += v.numel()\n+            torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+\n+        filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n+        if num_shards == 1:\n+            # Unsharded\n+            state_dict = {\n+                \"model.embed_tokens.weight\": loaded[\"tok_embeddings.weight\"],\n+                \"model.norm.weight\": loaded[\"norm.weight\"],\n+                \"lm_head.weight\": loaded[\"output.weight\"],\n+            }\n+        else:\n+            concat_dim = 0 if is_llama_3(llama_version) else 1\n+            state_dict = {\n+                \"model.norm.weight\": loaded[0][\"norm.weight\"],\n+                \"model.embed_tokens.weight\": torch.cat(\n+                    [loaded[i][\"tok_embeddings.weight\"] for i in range(len(loaded))], dim=concat_dim\n+                ),\n+                \"lm_head.weight\": torch.cat([loaded[i][\"output.weight\"] for i in range(len(loaded))], dim=0),\n+            }\n \n-        state_dict[f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\"] = inv_freq\n         for k, v in state_dict.items():\n             index_dict[\"weight_map\"][k] = filename\n             param_count += v.numel()\n         torch.save(state_dict, os.path.join(tmp_model_path, filename))\n \n-    filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n-    if num_shards == 1:\n-        # Unsharded\n-        state_dict = {\n-            \"model.embed_tokens.weight\": loaded[\"tok_embeddings.weight\"],\n-            \"model.norm.weight\": loaded[\"norm.weight\"],\n-            \"lm_head.weight\": loaded[\"output.weight\"],\n-        }\n-    else:\n-        concat_dim = 0 if llama_version in [\"3\", \"3.1\"] else 1\n-        state_dict = {\n-            \"model.norm.weight\": loaded[0][\"norm.weight\"],\n-            \"model.embed_tokens.weight\": torch.cat(\n-                [loaded[i][\"tok_embeddings.weight\"] for i in range(len(loaded))], dim=concat_dim\n-            ),\n-            \"lm_head.weight\": torch.cat([loaded[i][\"output.weight\"] for i in range(len(loaded))], dim=0),\n-        }\n-\n-    for k, v in state_dict.items():\n-        index_dict[\"weight_map\"][k] = filename\n-        param_count += v.numel()\n-    torch.save(state_dict, os.path.join(tmp_model_path, filename))\n-\n-    # Write configs\n-    index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n-    write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n-    ffn_dim_multiplier = params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in params else 1\n-    multiple_of = params[\"multiple_of\"] if \"multiple_of\" in params else 256\n+        # Write configs\n+        index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n+        write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n+        ffn_dim_multiplier = params[\"ffn_dim_multiplier\"] if \"ffn_dim_multiplier\" in params else 1\n+        multiple_of = params[\"multiple_of\"] if \"multiple_of\" in params else 256\n \n-    if llama_version in [\"3\", \"3.1\"]:\n-        bos_token_id = 128000\n+        if is_llama_3(llama_version):\n+            bos_token_id = 128000\n \n-        if instruct:\n-            eos_token_id = [128001, 128008, 128009]\n+            if instruct:\n+                eos_token_id = [128001, 128008, 128009]\n+            else:\n+                eos_token_id = 128001\n         else:\n-            eos_token_id = 128001\n-    else:\n-        bos_token_id = 1\n-        eos_token_id = 2\n-\n-    config = LlamaConfig(\n-        hidden_size=dim,\n-        intermediate_size=compute_intermediate_size(dim, ffn_dim_multiplier, multiple_of),\n-        num_attention_heads=params[\"n_heads\"],\n-        num_hidden_layers=params[\"n_layers\"],\n-        rms_norm_eps=params[\"norm_eps\"],\n-        num_key_value_heads=num_key_value_heads,\n-        vocab_size=vocab_size,\n-        rope_theta=base,\n-        max_position_embeddings=max_position_embeddings,\n-        bos_token_id=bos_token_id,\n-        eos_token_id=eos_token_id,\n-    )\n-    config.save_pretrained(tmp_model_path)\n+            bos_token_id = 1\n+            eos_token_id = 2\n+\n+        if llama_version in [\"3.1\", \"3.2\", \"Guard-3\"]:\n+            rope_scaling = {\n+                \"factor\": 32.0 if llama_version == \"3.2\" else 8.0,\n+                \"low_freq_factor\": 1.0,\n+                \"high_freq_factor\": 4.0,\n+                \"original_max_position_embeddings\": 8192,\n+                \"rope_type\": \"llama3\",\n+            }\n+        else:\n+            rope_scaling = None\n+\n+        config = LlamaConfig(\n+            hidden_size=dim,\n+            intermediate_size=compute_intermediate_size(dim, ffn_dim_multiplier, multiple_of),\n+            num_attention_heads=params[\"n_heads\"],\n+            num_hidden_layers=params[\"n_layers\"],\n+            rms_norm_eps=params[\"norm_eps\"],\n+            num_key_value_heads=num_key_value_heads,\n+            vocab_size=vocab_size,\n+            rope_theta=base,\n+            rope_scaling=rope_scaling,\n+            max_position_embeddings=max_position_embeddings,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=True if llama_version in [\"3.2\"] else False,\n+        )\n+\n+        config.save_pretrained(tmp_model_path)\n \n-    if instruct:\n         generation_config = GenerationConfig(\n             do_sample=True,\n             temperature=0.6,\n@@ -315,96 +408,117 @@ def permute(w, n_heads, dim1=dim, dim2=dim):\n         )\n         generation_config.save_pretrained(tmp_model_path)\n \n-    # Make space so we can load the model properly now.\n-    del state_dict\n-    del loaded\n-    gc.collect()\n+        # Make space so we can load the model properly now.\n+        del state_dict\n+        del loaded\n+        gc.collect()\n+\n+        print(\"Loading the checkpoint in a Llama model.\")\n+        model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n \n-    print(\"Loading the checkpoint in a Llama model.\")\n-    model = LlamaForCausalLM.from_pretrained(tmp_model_path, torch_dtype=torch.bfloat16, low_cpu_mem_usage=True)\n-    # Avoid saving this as part of the config.\n-    del model.config._name_or_path\n-    model.config.torch_dtype = torch.float16\n-    print(\"Saving in the Transformers format.\")\n-    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n-    shutil.rmtree(tmp_model_path, ignore_errors=True)\n+        # Avoid saving this as part of the config.\n+        del model.config._name_or_path\n+        model.config.torch_dtype = torch.float16\n+\n+        print(\"Saving in the Transformers format.\")\n+        if push_to_hub:\n+            print(\"Pushing to the hub.\")\n+            model.push_to_hub(model_path, safe_serialization=safe_serialization, private=True, use_temp_dir=True)\n+        else:\n+            print(\"Saving to disk.\")\n+            model.save_pretrained(model_path, safe_serialization=safe_serialization)\n \n \n class Llama3Converter(TikTokenConverter):\n-    def __init__(self, vocab_file, special_tokens=None, instruct=False, model_max_length=None, **kwargs):\n+    def __init__(self, vocab_file, special_tokens=None, instruct=False, llama_version=\"3.2\", **kwargs):\n         super().__init__(vocab_file, additional_special_tokens=special_tokens, **kwargs)\n         tokenizer = self.converted()\n-        chat_template = (\n-            \"{% set loop_messages = messages %}\"\n-            \"{% for message in loop_messages %}\"\n-            \"{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}\"\n-            \"{% if loop.index0 == 0 %}\"\n-            \"{% set content = bos_token + content %}\"\n-            \"{% endif %}\"\n-            \"{{ content }}\"\n-            \"{% endfor %}\"\n-            \"{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\"\n-        )\n \n-        self.tokenizer = PreTrainedTokenizerFast(\n+        # References for chat templates in instruct models\n+        templates_for_version = {\n+            \"2\": (\"meta-llama/Llama-2-7b-chat-hf\", \"f5db02db724555f92da89c216ac04704f23d4590\"),\n+            \"3\": (\"meta-llama/Meta-Llama-3-8B-Instruct\", \"5f0b02c75b57c5855da9ae460ce51323ea669d8a\"),\n+            \"3.1\": (\"meta-llama/Llama-3.1-8B-Instruct\", \"0e9e39f249a16976918f6564b8830bc894c89659\"),\n+            \"3.2\": (\"meta-llama/Llama-3.2-1B-Instruct\", \"e9f8effbab1cbdc515c11ee6e098e3d5a9f51e14\"),\n+            \"Guard-3\": (\"meta-llama/Llama-Guard-3-1B\", \"acf7aafa60f0410f8f42b1fa35e077d705892029\"),\n+        }\n+\n+        # Add chat_template only if instruct is True.\n+        # Prevents a null chat_template, which triggers\n+        # a parsing warning in the Hub.\n+        additional_kwargs = {}\n+        if instruct or llama_version in [\"Guard-3\"]:\n+            model_id, revision = templates_for_version.get(llama_version, (None, None))\n+            if model_id is not None:\n+                from transformers import AutoTokenizer\n+\n+                t = AutoTokenizer.from_pretrained(model_id, revision=revision)\n+                additional_kwargs[\"chat_template\"] = t.chat_template\n+\n+        self.converted_tokenizer = PreTrainedTokenizerFast(\n             tokenizer_object=tokenizer,\n             bos_token=\"<|begin_of_text|>\",\n             eos_token=\"<|end_of_text|>\" if not instruct else \"<|eot_id|>\",\n-            chat_template=chat_template if instruct else None,\n             model_input_names=[\"input_ids\", \"attention_mask\"],\n-            model_max_length=model_max_length,\n+            model_max_length=CONTEXT_LENGTH_FOR_VERSION[llama_version],\n+            clean_up_tokenization_spaces=True,\n+            **additional_kwargs,\n+        )\n+        self.update_post_processor(self.converted_tokenizer)\n+        # finer special_tokens_map.json\n+        self.converted_tokenizer._bos_token = BOS_ADDED_TOKEN\n+        self.converted_tokenizer._eos_token = EOT_ADDED_TOKEN if instruct else EOS_ADDED_TOKEN\n+\n+    # We can't do this while building the tokenizer because we have no easy access to the bos token id\n+    def update_post_processor(self, tokenizer):\n+        tokenizer._tokenizer.post_processor = processors.Sequence(\n+            [\n+                processors.ByteLevel(trim_offsets=False),\n+                processors.TemplateProcessing(\n+                    single=\"<|begin_of_text|> $A\",\n+                    pair=\"<|begin_of_text|>:0 $A:0 <|begin_of_text|>:1 $B:1\",\n+                    special_tokens=[\n+                        (\"<|begin_of_text|>\", tokenizer.convert_tokens_to_ids(\"<|begin_of_text|>\")),\n+                    ],\n+                ),\n+            ]\n         )\n \n \n-def write_tokenizer(tokenizer_path, input_tokenizer_path, llama_version=\"2\", special_tokens=None, instruct=False):\n+def write_tokenizer(\n+    tokenizer_path, input_tokenizer_path, llama_version=\"2\", special_tokens=None, instruct=False, push_to_hub=False\n+):\n+    print(\"Converting the tokenizer.\")\n     tokenizer_class = LlamaTokenizer if LlamaTokenizerFast is None else LlamaTokenizerFast\n-    if llama_version in [\"3\", \"3.1\"]:\n+    if is_llama_3(llama_version):\n         tokenizer = Llama3Converter(\n-            input_tokenizer_path, special_tokens, instruct, model_max_length=CONTEXT_LENGTH_FOR_VERSION[llama_version]\n-        ).tokenizer\n+            input_tokenizer_path,\n+            special_tokens,\n+            instruct,\n+            llama_version,\n+        ).converted_tokenizer\n     else:\n-        tokenizer = tokenizer_class(input_tokenizer_path)\n-    print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n-    tokenizer.save_pretrained(tokenizer_path)\n-    return tokenizer\n-\n+        try:\n+            tokenizer = tokenizer_class(input_tokenizer_path)\n+        except Exception:\n+            raise ValueError(\n+                \"Failed to instantiate tokenizer. Please, make sure you have sentencepiece and protobuf installed.\"\n+            )\n \n-DEFAULT_LLAMA_SPECIAL_TOKENS = {\n-    \"3\": [\n-        \"<|begin_of_text|>\",\n-        \"<|end_of_text|>\",\n-        \"<|reserved_special_token_0|>\",\n-        \"<|reserved_special_token_1|>\",\n-        \"<|reserved_special_token_2|>\",\n-        \"<|reserved_special_token_3|>\",\n-        \"<|start_header_id|>\",\n-        \"<|end_header_id|>\",\n-        \"<|reserved_special_token_4|>\",\n-        \"<|eot_id|>\",  # end of turn\n-    ]\n-    + [f\"<|reserved_special_token_{i}|>\" for i in range(5, 256 - 5)],\n-    \"3.1\": [\n-        \"<|begin_of_text|>\",\n-        \"<|end_of_text|>\",\n-        \"<|reserved_special_token_0|>\",\n-        \"<|reserved_special_token_1|>\",\n-        \"<|finetune_right_pad_id|>\",\n-        \"<|reserved_special_token_2|>\",\n-        \"<|start_header_id|>\",\n-        \"<|end_header_id|>\",\n-        \"<|eom_id|>\",  # end of message\n-        \"<|eot_id|>\",  # end of turn\n-        \"<|python_tag|>\",\n-    ]\n-    + [f\"<|reserved_special_token_{i}|>\" for i in range(3, 256 - 8)],\n-}\n+    if push_to_hub:\n+        print(f\"Pushing a {tokenizer_class.__name__} to the Hub repo - {tokenizer_path}.\")\n+        tokenizer.push_to_hub(tokenizer_path, private=True, use_temp_dir=True)\n+    else:\n+        print(f\"Saving a {tokenizer_class.__name__} to {tokenizer_path}.\")\n+        tokenizer.save_pretrained(tokenizer_path)\n+    return tokenizer\n \n \n def main():\n     parser = argparse.ArgumentParser()\n     parser.add_argument(\n         \"--input_dir\",\n-        help=\"Location of LLaMA weights, which contains tokenizer.model and model folders\",\n+        help=\"Location of Llama weights, which contains tokenizer.model and model folders\",\n     )\n     parser.add_argument(\n         \"--model_size\",\n@@ -416,12 +530,18 @@ def main():\n         help=\"Location to write HF model and tokenizer\",\n     )\n     parser.add_argument(\n-        \"--safe_serialization\", default=True, type=bool, help=\"Whether or not to save using `safetensors`.\"\n+        \"--push_to_hub\",\n+        help=\"Whether or not to push the model to the hub at `output_dir` instead of saving it locally.\",\n+        action=\"store_true\",\n+        default=False,\n+    )\n+    parser.add_argument(\n+        \"--safe_serialization\", action=\"store_true\", default=True, help=\"Whether or not to save using `safetensors`.\"\n     )\n     # Different Llama versions used different default values for max_position_embeddings, hence the need to be able to specify which version is being used.\n     parser.add_argument(\n         \"--llama_version\",\n-        choices=[\"1\", \"2\", \"3\", \"3.1\"],\n+        choices=[\"1\", \"2\", \"3\", \"3.1\", \"3.2\", \"Guard-3\"],\n         default=\"1\",\n         type=str,\n         help=\"Version of the Llama model to convert. Currently supports Llama1 and Llama2. Controls the context size\",\n@@ -440,9 +560,9 @@ def main():\n     )\n     parser.add_argument(\n         \"--instruct\",\n+        action=\"store_true\",\n         default=False,\n-        type=bool,\n-        help=\"Whether the model is an instruct model or not. Will affect special tokens for llama 3.1.\",\n+        help=\"Whether the model is an instruct model or not. Will affect special tokens and chat template.\",\n     )\n     args = parser.parse_args()\n     if args.model_size is None and args.num_shards is None:\n@@ -459,8 +579,10 @@ def main():\n             llama_version=args.llama_version,\n             special_tokens=args.special_tokens,\n             instruct=args.instruct,\n+            push_to_hub=args.push_to_hub,\n         )\n     )\n+\n     if args.model_size != \"tokenizer_only\":\n         write_model(\n             model_path=args.output_dir,\n@@ -471,6 +593,7 @@ def main():\n             vocab_size=vocab_size,\n             num_shards=args.num_shards,\n             instruct=args.instruct,\n+            push_to_hub=args.push_to_hub,\n         )\n \n "
        }
    ],
    "stats": {
        "total": 553,
        "additions": 338,
        "deletions": 215
    }
}