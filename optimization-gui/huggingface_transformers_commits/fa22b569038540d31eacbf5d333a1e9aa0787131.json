{
    "author": "githubnemo",
    "message": ":rotating_light: Fix gradient checkpointing for several models and improve test robustness   (#41818)\n\n* Implement gradient checkpointing in GPTBigCode\n\nSupport for gradient checkpointing was lost in the major refactoring in PR #38635\nand this is the attempt to re-add it.\n\nI extended the tests to\n- test `use_reentrant=True` and `False`\n- make sure `model.train` is called so that gradient checkpointing works;\n  this is a limiation of the tests currently used by GPTBigCode\n- make sure that one (the first) gradient checkpointing layer is called\n- make sure that the same non-zero grads are there for normal and checkpointing\n  runs - this is something we tripped over before in PEFT due to the possibly\n  incompletely stored runtime environment in the checkpointed forward step,\n  see also peft#2826\n\nNote that the invocation of `GPTBigCodeBlock.forward` has changed:\n\n- `layer_past` is now passed as a keyword argument so that\n  `GradientCheckpointingLayer.__call__` can see and filter this parameter\n  (`use_reentrant=False` fails otherwise)\n- `{encoder_}hidden_states` are still passed as positional arguments\n  so that `torch.utils.checkpoint.checkpoint` receives them as pos. args\n  and computes gradients for these (kwargs would be filtered by\n  `GradientCheckpointingLayer`).\n\n* Improve gradient checkpointing tests\n\n- Compare that the non-zero gradients in a reference run are present in the checkpointing run\n- Make sure that the forward of at least one gradient checkpointing layer is actually called\n  more than once (as expected during gradient checkpointing backward)\n\nCurrently there are some problems with Bert-derived MultipleChoice models, when dropout is\nenabled there are scenarios during gradient checkpointing where `classifier.bias.grad` is None.\nI don't yet have a good explanation for this, disabling dropout resolves this. I would have\nunderstood, if it is dropout on the classification layer but enabling attention dropout is\nalso leading to this behavior.\n\nMoE models have selective sparsity depending on the selected experts, for this reason we\nonly compare gradients on parameters collected on the reference backward run.\n\n* Remove duplicated gradient checkpointing code\n\n* Address review comments\n\n* Make test output consistent\n\n* GradientCheckpointingLayer for xlstm, zamba, zamba2\n\n* GradientCheckpointingLayer for swiftformer\n\nalso drop janus from ignore list - only the VQVAE case is without\ngradient checkpointing and it is doubtful that it is usefule in that\ncase. Training with gradient checkpointing is not tested anyway.\n\n* Make an exception for CLVP\n\nThe implementation of GradientCheckpointingLayers is not trivial and may break behavior\nthat was previously expected. Therefore we keep it as-is for now.\n\n* Remove unneeded exceptions\n\n---------\n\nCo-authored-by: nemo <git@ningu.net>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>",
    "sha": "fa22b569038540d31eacbf5d333a1e9aa0787131",
    "files": [
        {
            "sha": "65ae3e00092dc52381994dcc61d3a257f15b57c4",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -26,6 +26,7 @@\n from ...generation import GenerationMixin\n from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import is_flash_attn_available\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n     CausalLMOutputWithCrossAttentions,\n@@ -266,7 +267,7 @@ def forward(self, hidden_states: Optional[tuple[torch.FloatTensor]]) -> torch.Fl\n         return hidden_states\n \n \n-class GPTBigCodeBlock(nn.Module):\n+class GPTBigCodeBlock(GradientCheckpointingLayer):\n     def __init__(self, config, layer_idx=None):\n         super().__init__()\n         hidden_size = config.hidden_size\n@@ -291,9 +292,9 @@ def __init__(self, config, layer_idx=None):\n     def forward(\n         self,\n         hidden_states: Optional[tuple[torch.Tensor]],\n+        encoder_hidden_states: Optional[torch.Tensor] = None,\n         layer_past: Optional[Cache] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        encoder_hidden_states: Optional[torch.Tensor] = None,\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n@@ -536,10 +537,10 @@ def forward(\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n \n             outputs = block(\n-                hidden_states,\n-                past_key_values,\n-                causal_mask,\n+                hidden_states,  # as a positional argument for gradient checkpointing\n                 encoder_hidden_states,  # as a positional argument for gradient checkpointing\n+                layer_past=past_key_values,  # as keyword argument so it can be removed by GradientCheckpointingLayer\n+                attention_mask=causal_mask,\n                 encoder_attention_mask=encoder_attention_mask,\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,"
        },
        {
            "sha": "46e522d3ac756af86ba13610299d3bc0efdb6f83",
            "filename": "src/transformers/models/swiftformer/modeling_swiftformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswiftformer%2Fmodeling_swiftformer.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -21,6 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2CLS\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithNoAttention, ImageClassifierOutputWithNoAttention\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -295,7 +296,7 @@ def forward(self, x):\n         return x\n \n \n-class SwiftFormerStage(nn.Module):\n+class SwiftFormerStage(GradientCheckpointingLayer):\n     \"\"\"\n     A Swiftformer stage consisting of a series of `SwiftFormerConvEncoder` blocks and a final\n     `SwiftFormerEncoderBlock`."
        },
        {
            "sha": "171db140bb317d8ab9247492ba8ade67e509c8b0",
            "filename": "src/transformers/models/xlstm/modeling_xlstm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 14,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlstm%2Fmodeling_xlstm.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -22,17 +22,21 @@\n from torch.nn import CrossEntropyLoss\n \n from ...generation import GenerationMixin\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import PreTrainedModel\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_xlstm_available\n from .configuration_xlstm import xLSTMConfig\n \n \n if is_xlstm_available():\n     from xlstm.xlstm_large.model import RMSNorm as xLSTMRMSNorm\n-    from xlstm.xlstm_large.model import mLSTMBlock as xLSTMBlock\n-    from xlstm.xlstm_large.model import mLSTMStateType, soft_cap\n+    from xlstm.xlstm_large.model import mLSTMBlock, mLSTMStateType, soft_cap\n \n     external_xlstm = True\n+\n+    class xLSTMBlock(GradientCheckpointingLayer, mLSTMBlock):\n+        pass\n+\n else:\n     from collections.abc import Callable\n     from functools import partial\n@@ -1164,7 +1168,7 @@ def forward(\n             y = self.out_proj(h_out)\n             return y, state\n \n-    class xLSTMBlock(nn.Module):\n+    class xLSTMBlock(GradientCheckpointingLayer):\n         def __init__(self, config: xLSTMConfig):\n             super().__init__()\n             self.config = config\n@@ -1457,17 +1461,11 @@ def forward(\n         else:\n             all_hidden_states = () if output_hidden_states else None\n             for layer_idx, xlstm_block in enumerate(self.blocks):\n-                if self.gradient_checkpointing and self.training:\n-                    hidden_states, rnn_state = self._gradient_checkpointing_func(\n-                        xlstm_block.__call__,\n-                        hidden_states,\n-                        cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n-                else:\n-                    hidden_states, rnn_state = xlstm_block(\n-                        hidden_states,\n-                        state=cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n-                    )\n+                hidden_states, rnn_state = xlstm_block(\n+                    hidden_states,\n+                    cache_params.rnn_state[layer_idx] if cache_params is not None else None,\n+                )\n+\n                 if cache_params:\n                     for state_idx in range(len(cache_params.rnn_state[layer_idx])):\n                         local_rnn_state = rnn_state[state_idx]"
        },
        {
            "sha": "6822be5d0b58aa6edceeefd22061df13db771d22",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 15,
            "deletions": 27,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -32,6 +32,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n@@ -639,7 +640,7 @@ def forward(\n         return outputs\n \n \n-class ZambaMambaDecoderLayer(nn.Module):\n+class ZambaMambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: ZambaConfig, layer_idx: int):\n         super().__init__()\n         self.mamba = ZambaMambaMixer(config=config, layer_idx=layer_idx)\n@@ -708,7 +709,7 @@ def forward(\n         return outputs\n \n \n-class ZambaHybridLayer(nn.Module):\n+class ZambaHybridLayer(GradientCheckpointingLayer):\n     def __init__(self, shared_transf: ZambaAttentionDecoderLayer, linear: nn.Linear, mamba: ZambaMambaDecoderLayer):\n         super().__init__()\n         self.shared_transf = shared_transf\n@@ -942,31 +943,18 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    cache_position,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    cache_position=cache_position,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "774a645e9f297915ccc0c2989e0ba23b49ba1ad5",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 29,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -34,6 +34,7 @@\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n@@ -1058,7 +1059,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2MambaDecoderLayer(nn.Module):\n+class Zamba2MambaDecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Zamba2Config, layer_idx: int):\n         super().__init__()\n         self.mamba = Zamba2MambaMixer(config=config, layer_idx=layer_idx)\n@@ -1127,7 +1128,7 @@ def forward(\n         return outputs\n \n \n-class Zamba2HybridLayer(nn.Module):\n+class Zamba2HybridLayer(GradientCheckpointingLayer):\n     def __init__(\n         self, shared_transformer: Zamba2AttentionDecoderLayer, linear: nn.Linear, mamba: Zamba2MambaDecoderLayer\n     ):\n@@ -1344,33 +1345,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "af76b3b5c024c5065b4abda7f6641d6848dc43a3",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 27,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -1079,33 +1079,19 @@ def forward(\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n-            if self.gradient_checkpointing and self.training:\n-                layer_outputs = self._gradient_checkpointing_func(\n-                    layer.__call__,\n-                    hidden_states,\n-                    original_hidden_states,\n-                    layer_idx,\n-                    attention_mask,\n-                    causal_mask,\n-                    past_key_values,\n-                    output_attentions,\n-                    use_cache,\n-                    position_embeddings,\n-                    position_ids,\n-                )\n-            else:\n-                layer_outputs = layer(\n-                    hidden_states,\n-                    original_hidden_states=original_hidden_states,\n-                    layer_idx=layer_idx,\n-                    attention_mask=attention_mask,\n-                    causal_mask=causal_mask,\n-                    past_key_values=past_key_values,\n-                    output_attentions=output_attentions,\n-                    use_cache=use_cache,\n-                    position_embeddings=position_embeddings,\n-                    position_ids=position_ids,\n-                )\n+            layer_outputs = layer(\n+                hidden_states,\n+                original_hidden_states,\n+                layer_idx,\n+                attention_mask,\n+                causal_mask,\n+                past_key_values=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                position_embeddings=position_embeddings,\n+                position_ids=position_ids,\n+            )\n+\n             hidden_states = layer_outputs[0]\n \n             if output_attentions:"
        },
        {
            "sha": "dc6097e67a3f8dec1b21a587e4949e0f0e921fe0",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 4,
            "deletions": 15,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -186,6 +186,10 @@ def test_training(self):\n     def test_training_gradient_checkpointing(self):\n         pass\n \n+    @unittest.skip(reason=\"ClvpEncoder does not output loss\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class ClvpDecoderTester:\n     def __init__(\n@@ -311,21 +315,6 @@ def test_training(self):\n         loss = model(**inputs).loss\n         loss.backward()\n \n-    def test_training_gradient_checkpointing(self):\n-        # we will only test the ClvpForCausalLM since it outputs loss\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.use_cache = False\n-        config.return_dict = True\n-\n-        model = ClvpForCausalLM(config)\n-        model.to(torch_device)\n-        model.gradient_checkpointing_enable()\n-        model.train()\n-        inputs = self._prepare_for_class(inputs_dict, ClvpForCausalLM, return_labels=True)\n-\n-        loss = model(**inputs).loss\n-        loss.backward()\n-\n     @unittest.skip(reason=\"Clvp `prepare_inputs_for_generation` function doesn't have cache position.\")\n     def test_generate_continue_from_inputs_embeds(self):\n         pass"
        },
        {
            "sha": "ea39a07cc1351d5d278d42c6011e3cb2b2729b04",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -307,12 +307,16 @@ def create_and_check_lm_head_model(self, config, input_ids, input_mask, token_ty\n         self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n \n     def create_and_check_forward_and_backwards(\n-        self, config, input_ids, input_mask, token_type_ids, *args, gradient_checkpointing=False\n+        self,\n+        config,\n+        input_ids,\n+        input_mask,\n+        token_type_ids,\n+        *args,\n     ):\n         model = GPTBigCodeForCausalLM(config)\n+        model.train()\n         model.to(torch_device)\n-        if gradient_checkpointing:\n-            model.gradient_checkpointing_enable()\n \n         result = model(input_ids, token_type_ids=token_type_ids, labels=input_ids)\n         self.parent.assertEqual(result.loss.shape, ())\n@@ -463,10 +467,6 @@ def test_gpt_bigcode_token_classification_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_gpt_bigcode_for_token_classification(*config_and_inputs)\n \n-    def test_gpt_bigcode_gradient_checkpointing(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs, gradient_checkpointing=True)\n-\n     def test_gpt_bigcode_scale_attn_by_inverse_layer_idx(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs(scale_attn_by_inverse_layer_idx=True)\n         self.model_tester.create_and_check_forward_and_backwards(*config_and_inputs)"
        },
        {
            "sha": "72e2a72c265f8ae5b275b46ca2953b33936fade1",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -396,6 +396,10 @@ def test_model_get_set_embeddings(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         pass\n \n+    @unittest.skip(\"Janus VQ module has no gradient checkpointing layers\")\n+    def test_gradient_checkpointing_enable_disable(self):\n+        pass\n+\n \n class JanusIntegrationTest(unittest.TestCase):\n     def setUp(self):"
        },
        {
            "sha": "1fe7b935d1467e27faf86c0e05f24ffcdd69226a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 54,
            "deletions": 5,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fa22b569038540d31eacbf5d333a1e9aa0787131/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=fa22b569038540d31eacbf5d333a1e9aa0787131",
            "patch": "@@ -20,6 +20,7 @@\n import random\n import re\n import tempfile\n+import unittest.mock\n import warnings\n from collections import defaultdict\n from contextlib import contextmanager\n@@ -46,6 +47,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.modeling_layers import GradientCheckpointingLayer\n from transformers.modeling_utils import _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n@@ -827,6 +829,12 @@ def test_gradient_checkpointing_enable_disable(self):\n             model = model_class(copy.deepcopy(config))\n             self.assertFalse(model.is_gradient_checkpointing)\n \n+            # Gradient checkpointing is implemented via GradientCheckpointingLayer, if none is present this is likely\n+            # an implementation issue. Note we exclude clvp for now since they are still not using\n+            # GradientCheckpointingLayer.\n+            if config.model_type not in [\"clvp\", \"clvp_decoder\"]:\n+                self.assertTrue([m for m in model.modules() if isinstance(m, GradientCheckpointingLayer)])\n+\n             # check enable works\n             model.gradient_checkpointing_enable()\n             self.assertTrue(model.is_gradient_checkpointing)\n@@ -1151,22 +1159,63 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n                 config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n                 config.use_cache = False\n                 config.return_dict = True\n-                model = model_class(config)\n \n+                # make sure that test runs are consistent by disabling dropout\n+                #\n+                # Note: attention_probs_dropout_prob seem to influence classifier.bias in BertForMultipleChoice\n+                # (and other Bert derived models). Sometimes classifier.bias is None when\n+                # attention_probs_dropout_prob > 0. This might indicate a bug somewhere.\n+                if hasattr(config, \"hidden_dropout_prob\"):\n+                    config.hidden_dropout_prob = 0.0\n+                if hasattr(config, \"attention_probs_dropout_prob\"):\n+                    config.attention_probs_dropout_prob = 0.0\n+\n+                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+\n+                torch.manual_seed(0)\n+                model = model_class(config)\n                 model.to(torch_device)\n-                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n                 model.train()\n \n                 # unfreeze additional layers\n                 for p in model.parameters():\n                     p.requires_grad_(True)\n \n+                # do a non-checkpointing run, so we can compare the set of non-zero gradients later. we skip None\n+                # grads here to collect a reference set of modules that have non-zero gradients (to filter layers like\n+                # MoE that drop out parts of the model).\n                 optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n-\n-                inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n+                torch.manual_seed(0)\n                 loss = model(**inputs).loss\n                 loss.backward()\n-                optimizer.step()\n+                grad_expected_params = [(n, p) for n, p in model.named_parameters() if p.grad is not None]\n+                non_zero_grads_normal = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+\n+                # reset all gradients to zero for the comparison with the gradient checkpointing run\n+                optimizer.zero_grad()\n+\n+                # now enable gradient checkpointing and compare the gradients\n+                model.gradient_checkpointing_enable(gradient_checkpointing_kwargs=gradient_checkpointing_kwargs)\n+\n+                checkpointing_layer = next(m for m in model.modules() if isinstance(m, GradientCheckpointingLayer))\n+\n+                optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n+                with unittest.mock.patch.object(\n+                    checkpointing_layer, \"forward\", wraps=checkpointing_layer.forward\n+                ) as forward_mock:\n+                    torch.manual_seed(0)\n+                    loss = model(**inputs).loss\n+                    loss.backward()\n+                    optimizer.step()\n+\n+                    # test that gradient checkpointing is active as it would call the gradient checkpointing layer's\n+                    # forward more than once.\n+                    self.assertGreater(forward_mock.call_count, 1)\n+\n+                # check that all the parameters that had non-zero gradients before, have non-zero grads with gradient\n+                # checkpointing. divergence indicates a different forward-pass environment that needs special handling.\n+                non_zero_grads_gradcp = {n for n, p in grad_expected_params if p.grad.abs().sum() > 0}\n+                self.assertEqual(non_zero_grads_gradcp, non_zero_grads_normal)\n \n                 if self.test_all_params_have_gradient:\n                     for k, v in model.named_parameters():"
        }
    ],
    "stats": {
        "total": 263,
        "additions": 133,
        "deletions": 130
    }
}