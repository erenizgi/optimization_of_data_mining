{
    "author": "ArthurZucker",
    "message": "[`clean_up_tokenization_spaces`] Pl bart was failing, updating (#33735)\n\n`clean_up_tokenization_spaces=True` for pl bart",
    "sha": "5f4420587a231e2dde3376ae2c4b6dceb7ac5627",
    "files": [
        {
            "sha": "f9648924c8e0faeb7fac616e6d70f39f64489145",
            "filename": "src/transformers/models/plbart/tokenization_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f4420587a231e2dde3376ae2c4b6dceb7ac5627/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f4420587a231e2dde3376ae2c4b6dceb7ac5627/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Ftokenization_plbart.py?ref=5f4420587a231e2dde3376ae2c4b6dceb7ac5627",
            "patch": "@@ -130,6 +130,7 @@ def __init__(\n         tgt_lang=None,\n         sp_model_kwargs: Optional[Dict[str, Any]] = None,\n         additional_special_tokens=None,\n+        clean_up_tokenization_spaces=True,\n         **kwargs,\n     ):\n         # Mask token behave like a normal word, i.e. include the space before it\n@@ -200,6 +201,7 @@ def __init__(\n             tgt_lang=tgt_lang,\n             additional_special_tokens=_additional_special_tokens,\n             sp_model_kwargs=self.sp_model_kwargs,\n+            clean_up_tokenization_spaces=clean_up_tokenization_spaces,\n             **kwargs,\n         )\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}