{
    "author": "aymeric-roucher",
    "message": "Create local Transformers Engine (#33218)\n\n* Create local Transformers Engine",
    "sha": "c79bfc71b8c98a0b757e6bde0108758ed77bc650",
    "files": [
        {
            "sha": "992e75ebe5b5de2aec718233a852b640803ac612",
            "filename": "docs/source/en/agents.md",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fen%2Fagents.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fen%2Fagents.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fagents.md?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -126,12 +126,13 @@ Additionally, `llm_engine` can also take a `grammar` argument. In the case where\n \n You will also need a `tools` argument which accepts a list of `Tools` - it can be an empty list. You can also add the default toolbox on top of your `tools` list by defining the optional argument `add_base_tools=True`.\n \n-Now you can create an agent, like [`CodeAgent`], and run it. For convenience, we also provide the [`HfEngine`] class that uses `huggingface_hub.InferenceClient` under the hood.\n+Now you can create an agent, like [`CodeAgent`], and run it. You can also create a [`TransformersEngine`] with a pre-initialized pipeline to run inference on your local machine using `transformers`.\n+For convenience, since agentic behaviours generally require stronger models such as `Llama-3.1-70B-Instruct` that are harder to run locally for now, we also provide the [`HfApiEngine`] class that initializes a `huggingface_hub.InferenceClient` under the hood. \n \n ```python\n-from transformers import CodeAgent, HfEngine\n+from transformers import CodeAgent, HfApiEngine\n \n-llm_engine = HfEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n+llm_engine = HfApiEngine(model=\"meta-llama/Meta-Llama-3-70B-Instruct\")\n agent = CodeAgent(tools=[], llm_engine=llm_engine, add_base_tools=True)\n \n agent.run(\n@@ -141,7 +142,7 @@ agent.run(\n ```\n \n This will be handy in case of emergency baguette need!\n-You can even leave the argument `llm_engine` undefined, and an [`HfEngine`] will be created by default.\n+You can even leave the argument `llm_engine` undefined, and an [`HfApiEngine`] will be created by default.\n \n ```python\n from transformers import CodeAgent\n@@ -521,14 +522,14 @@ import gradio as gr\n from transformers import (\n     load_tool,\n     ReactCodeAgent,\n-    HfEngine,\n+    HfApiEngine,\n     stream_to_gradio,\n )\n \n # Import tool from Hub\n image_generation_tool = load_tool(\"m-ric/text-to-image\")\n \n-llm_engine = HfEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n+llm_engine = HfApiEngine(\"meta-llama/Meta-Llama-3-70B-Instruct\")\n \n # Initialize the agent with the image generation tool\n agent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)"
        },
        {
            "sha": "a9fb944c627927b2d5ffebde11a561c3c425a3d5",
            "filename": "docs/source/en/main_classes/agent.md",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fagent.md?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -87,25 +87,46 @@ These engines have the following specification:\n 1. Follow the [messages format](../chat_templating.md) for its input (`List[Dict[str, str]]`) and return a string.\n 2. Stop generating outputs *before* the sequences passed in the argument `stop_sequences`\n \n-### HfEngine\n+### TransformersEngine\n \n-For convenience, we have added a `HfEngine` that implements the points above and uses an inference endpoint for the execution of the LLM.\n+For convenience, we have added a `TransformersEngine` that implements the points above, taking a pre-initialized `Pipeline` as input.\n \n ```python\n->>> from transformers import HfEngine\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline, TransformersEngine\n+\n+>>> model_name = \"HuggingFaceTB/SmolLM-135M-Instruct\"\n+>>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n+>>> model = AutoModelForCausalLM.from_pretrained(model_name)\n+\n+>>> pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n+\n+>>> engine = TransformersEngine(pipe)\n+>>> engine([{\"role\": \"user\", \"content\": \"Ok!\"}], stop_sequences=[\"great\"])\n+\n+\"What a \"\n+```\n+\n+[[autodoc]] TransformersEngine\n+\n+### HfApiEngine\n+\n+The `HfApiEngine` is an engine that wraps an [HF Inference API](https://huggingface.co/docs/api-inference/index) client for the execution of the LLM.\n+\n+```python\n+>>> from transformers import HfApiEngine\n \n >>> messages = [\n ...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n ...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n ...   {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n ... ]\n \n->>> HfEngine()(messages, stop_sequences=[\"conversation\"])\n+>>> HfApiEngine()(messages, stop_sequences=[\"conversation\"])\n \n \"That's very kind of you to say! It's always nice to have a relaxed \"\n ```\n \n-[[autodoc]] HfEngine\n+[[autodoc]] HfApiEngine\n \n \n ## Agent Types"
        },
        {
            "sha": "d0ef630e2cdf77f4d5755acdc06666c1bd78e88d",
            "filename": "docs/source/ko/main_classes/agent.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmain_classes%2Fagent.md?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -83,25 +83,25 @@ API나 기반 모델이 자주 업데이트되므로, 에이전트가 제공하\n 1. 입력(`List[Dict[str, str]]`)에 대한 [메시지 형식](../chat_templating.md)을 따르고 문자열을 반환해야 합니다.\n 2. 인수 `stop_sequences`에 시퀀스가 전달되기 *전에* 출력을 생성하는 것을 중지해야 합니다.\n \n-### HfEngine [[hfengine]]\n+### HfApiEngine [[HfApiEngine]]\n \n-편의를 위해, 위의 사항을 구현하고 대규모 언어 모델 실행을 위해 추론 엔드포인트를 사용하는 `HfEngine`을 추가했습니다.\n+편의를 위해, 위의 사항을 구현하고 대규모 언어 모델 실행을 위해 추론 엔드포인트를 사용하는 `HfApiEngine`을 추가했습니다.\n \n ```python\n->>> from transformers import HfEngine\n+>>> from transformers import HfApiEngine\n \n >>> messages = [\n ...   {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n ...   {\"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\"},\n ...   {\"role\": \"user\", \"content\": \"No need to help, take it easy.\"},\n ... ]\n \n->>> HfEngine()(messages, stop_sequences=[\"conversation\"])\n+>>> HfApiEngine()(messages, stop_sequences=[\"conversation\"])\n \n \"That's very kind of you to say! It's always nice to have a relaxed \"\n ```\n \n-[[autodoc]] HfEngine\n+[[autodoc]] HfApiEngine\n \n \n ## 에이전트 유형 [[agent-types]]"
        },
        {
            "sha": "efbfb2bb933371cf1e4d1e66ccdec2042a3238a7",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -57,14 +57,15 @@\n     \"agents\": [\n         \"Agent\",\n         \"CodeAgent\",\n-        \"HfEngine\",\n+        \"HfApiEngine\",\n         \"PipelineTool\",\n         \"ReactAgent\",\n         \"ReactCodeAgent\",\n         \"ReactJsonAgent\",\n         \"Tool\",\n         \"Toolbox\",\n         \"ToolCollection\",\n+        \"TransformersEngine\",\n         \"launch_gradio_demo\",\n         \"load_tool\",\n         \"stream_to_gradio\",\n@@ -4806,14 +4807,15 @@\n     from .agents import (\n         Agent,\n         CodeAgent,\n-        HfEngine,\n+        HfApiEngine,\n         PipelineTool,\n         ReactAgent,\n         ReactCodeAgent,\n         ReactJsonAgent,\n         Tool,\n         Toolbox,\n         ToolCollection,\n+        TransformersEngine,\n         launch_gradio_demo,\n         load_tool,\n         stream_to_gradio,"
        },
        {
            "sha": "f447d165800a01348731fd9bd46280df1010168c",
            "filename": "src/transformers/agents/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2F__init__.py?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -25,7 +25,7 @@\n \n _import_structure = {\n     \"agents\": [\"Agent\", \"CodeAgent\", \"ReactAgent\", \"ReactCodeAgent\", \"ReactJsonAgent\", \"Toolbox\"],\n-    \"llm_engine\": [\"HfEngine\"],\n+    \"llm_engine\": [\"HfApiEngine\", \"TransformersEngine\"],\n     \"monitoring\": [\"stream_to_gradio\"],\n     \"tools\": [\"PipelineTool\", \"Tool\", \"ToolCollection\", \"launch_gradio_demo\", \"load_tool\"],\n }\n@@ -45,7 +45,7 @@\n \n if TYPE_CHECKING:\n     from .agents import Agent, CodeAgent, ReactAgent, ReactCodeAgent, ReactJsonAgent, Toolbox\n-    from .llm_engine import HfEngine\n+    from .llm_engine import HfApiEngine, TransformersEngine\n     from .monitoring import stream_to_gradio\n     from .tools import PipelineTool, Tool, ToolCollection, launch_gradio_demo, load_tool\n "
        },
        {
            "sha": "8152b3213b8f71ef23aa14e0f4a1ed34feb48701",
            "filename": "src/transformers/agents/agents.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2Fagents.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2Fagents.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fagents.py?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -24,7 +24,7 @@\n from ..utils.import_utils import is_pygments_available\n from .agent_types import AgentAudio, AgentImage, AgentText\n from .default_tools import BASE_PYTHON_TOOLS, FinalAnswerTool, setup_default_tools\n-from .llm_engine import HfEngine, MessageRole\n+from .llm_engine import HfApiEngine, MessageRole\n from .prompts import (\n     DEFAULT_CODE_SYSTEM_PROMPT,\n     DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n@@ -327,7 +327,7 @@ class Agent:\n     def __init__(\n         self,\n         tools: Union[List[Tool], Toolbox],\n-        llm_engine: Callable = HfEngine(),\n+        llm_engine: Callable = HfApiEngine(),\n         system_prompt=DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n         tool_description_template=None,\n         additional_args={},\n@@ -532,7 +532,7 @@ class CodeAgent(Agent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfEngine(),\n+        llm_engine: Callable = HfApiEngine(),\n         system_prompt: str = DEFAULT_CODE_SYSTEM_PROMPT,\n         tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n         grammar: Dict[str, str] = None,\n@@ -655,7 +655,7 @@ class ReactAgent(Agent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfEngine(),\n+        llm_engine: Callable = HfApiEngine(),\n         system_prompt: str = DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n         tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n         grammar: Dict[str, str] = None,\n@@ -886,7 +886,7 @@ class ReactJsonAgent(ReactAgent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfEngine(),\n+        llm_engine: Callable = HfApiEngine(),\n         system_prompt: str = DEFAULT_REACT_JSON_SYSTEM_PROMPT,\n         tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n         grammar: Dict[str, str] = None,\n@@ -992,7 +992,7 @@ class ReactCodeAgent(ReactAgent):\n     def __init__(\n         self,\n         tools: List[Tool],\n-        llm_engine: Callable = HfEngine(),\n+        llm_engine: Callable = HfApiEngine(),\n         system_prompt: str = DEFAULT_REACT_CODE_SYSTEM_PROMPT,\n         tool_description_template: str = DEFAULT_TOOL_DESCRIPTION_TEMPLATE,\n         grammar: Dict[str, str] = None,"
        },
        {
            "sha": "5c36c2922fa2a11f21f73017e53ac6594ca7e5fa",
            "filename": "src/transformers/agents/llm_engine.py",
            "status": "modified",
            "additions": 35,
            "deletions": 1,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c79bfc71b8c98a0b757e6bde0108758ed77bc650/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fllm_engine.py?ref=c79bfc71b8c98a0b757e6bde0108758ed77bc650",
            "patch": "@@ -20,6 +20,8 @@\n \n from huggingface_hub import InferenceClient\n \n+from ..pipelines.base import Pipeline\n+\n \n class MessageRole(str, Enum):\n     USER = \"user\"\n@@ -65,7 +67,9 @@ def get_clean_message_list(message_list: List[Dict[str, str]], role_conversions:\n }\n \n \n-class HfEngine:\n+class HfApiEngine:\n+    \"\"\"This engine leverages Hugging Face's Inference API service, either serverless or with a dedicated endpoint.\"\"\"\n+\n     def __init__(self, model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n         self.model = model\n         self.client = InferenceClient(self.model, timeout=120)\n@@ -93,6 +97,36 @@ def __call__(\n         return response\n \n \n+class TransformersEngine:\n+    \"\"\"This engine uses a pre-initialized local text-generation pipeline.\"\"\"\n+\n+    def __init__(self, pipeline: Pipeline):\n+        self.pipeline = pipeline\n+\n+    def __call__(\n+        self, messages: List[Dict[str, str]], stop_sequences: Optional[List[str]] = None, grammar: Optional[str] = None\n+    ) -> str:\n+        # Get clean message list\n+        messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n+\n+        # Get LLM output\n+        output = self.pipeline(\n+            messages,\n+            stop_strings=stop_sequences,\n+            max_length=1500,\n+            tokenizer=self.pipeline.tokenizer,\n+        )\n+\n+        response = output[0][\"generated_text\"][-1][\"content\"]\n+\n+        # Remove stop sequences from LLM output\n+        if stop_sequences is not None:\n+            for stop_seq in stop_sequences:\n+                if response[-len(stop_seq) :] == stop_seq:\n+                    response = response[: -len(stop_seq)]\n+        return response\n+\n+\n DEFAULT_JSONAGENT_REGEX_GRAMMAR = {\n     \"type\": \"regex\",\n     \"value\": 'Thought: .+?\\\\nAction:\\\\n\\\\{\\\\n\\\\s{4}\"action\":\\\\s\"[^\"\\\\n]+\",\\\\n\\\\s{4}\"action_input\":\\\\s\"[^\"\\\\n]+\"\\\\n\\\\}\\\\n<end_action>',"
        }
    ],
    "stats": {
        "total": 112,
        "additions": 85,
        "deletions": 27
    }
}