{
    "author": "ArthurZucker",
    "message": "Fix red CI: benchmark script (#34351)\n\n* dont'trigger always\r\n\r\n* fux\r\n\r\n* oups\r\n\r\n* update\r\n\r\n* ??\r\n\r\n* ?\r\n\r\n* aie",
    "sha": "e50bf61decf741c6d59e4ba633b7392712673bda",
    "files": [
        {
            "sha": "79f0652e192f2a3616b280a775f04650b2fde595",
            "filename": ".github/workflows/benchmark.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e50bf61decf741c6d59e4ba633b7392712673bda/.github%2Fworkflows%2Fbenchmark.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e50bf61decf741c6d59e4ba633b7392712673bda/.github%2Fworkflows%2Fbenchmark.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbenchmark.yml?ref=e50bf61decf741c6d59e4ba633b7392712673bda",
            "patch": "@@ -18,21 +18,17 @@ jobs:\n     name: Benchmark\r\n     runs-on:\r\n       group: aws-g5-4xlarge-cache\r\n+    if: |\r\n+      (github.event_name == 'pull_request' && contains( github.event.pull_request.labels.*.name, 'run-benchmark') )||\r\n+      (github.event_name == 'push' && github.ref == 'refs/heads/main')\r\n     container:\r\n       image: huggingface/transformers-pytorch-gpu\r\n       options: --gpus all --privileged --ipc host\r\n     steps:\r\n       - name: Get repo\r\n-        if: github.event_name == 'pull_request'\r\n         uses: actions/checkout@v4\r\n         with:\r\n-          ref: ${{ github.event.pull_request.head.sha }}\r\n-\r\n-      - name: Get repo\r\n-        if: github.event_name == 'push'\r\n-        uses: actions/checkout@v4\r\n-        with:\r\n-          ref: ${{ github.sha }}\r\n+          ref: ${{ github.event.pull_request.head.sha || github.sha }}\r\n \r\n       - name: Install libpq-dev & psql\r\n         run: |\r"
        },
        {
            "sha": "b910d8de3f52b5ec66bbfad1b91dea26c940d521",
            "filename": "scripts/deberta_scrtipt.py",
            "status": "added",
            "additions": 82,
            "deletions": 0,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/e50bf61decf741c6d59e4ba633b7392712673bda/scripts%2Fdeberta_scrtipt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e50bf61decf741c6d59e4ba633b7392712673bda/scripts%2Fdeberta_scrtipt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/scripts%2Fdeberta_scrtipt.py?ref=e50bf61decf741c6d59e4ba633b7392712673bda",
            "patch": "@@ -0,0 +1,82 @@\n+import torch\n+from transformers import pipeline, AutoTokenizer, AutoModel, AutoModelForMaskedLM\n+import time\n+\n+test_sentence = 'Do you [MASK] the muffin man?'\n+\n+# for comparison\n+bert = pipeline('fill-mask', model = 'bert-base-uncased')\n+print('\\n'.join([d['sequence'] for d in bert(test_sentence)]))\n+\n+\n+deberta = pipeline('fill-mask', model = 'microsoft/deberta-v3-base', model_kwargs={\"legacy\": False})\n+print('\\n'.join([d['sequence'] for d in deberta(test_sentence)]))\n+\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\")\n+\n+tokenized_dict = tokenizer(\n+    [\"Is this working\",], [\"Not yet\",],\n+    return_tensors=\"pt\"\n+)\n+\n+deberta.model.forward = torch.compile(deberta.model.forward)\n+start=time.time()\n+deberta.model(**tokenized_dict)\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+deberta.model(**tokenized_dict)\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+deberta.model(**tokenized_dict)\n+end=time.time()\n+print(end-start)\n+\n+\n+model = AutoModel.from_pretrained('microsoft/deberta-base')\n+model.config.return_dict = False\n+model.config.output_hidden_states=False\n+input_tuple = (tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n+\n+\n+start=time.time()\n+traced_model = torch.jit.trace(model, input_tuple)\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n+end=time.time()\n+print(end-start)\n+\n+\n+start=time.time()\n+traced_model(tokenized_dict['input_ids'], tokenized_dict['attention_mask'])\n+end=time.time()\n+print(end-start)\n+\n+\n+torch.jit.save(traced_model, \"compiled_deberta.pt\")\n+\n+\n+\n+# my_script_module = torch.jit.script(model)"
        }
    ],
    "stats": {
        "total": 94,
        "additions": 86,
        "deletions": 8
    }
}