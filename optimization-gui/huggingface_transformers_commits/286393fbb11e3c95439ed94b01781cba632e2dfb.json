{
    "author": "jiqing-feng",
    "message": "enable tp on CPU (#36299)\n\n* enable tp on CPU\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* get rank from cpu\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* enable TP tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix comment\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* em print\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix model id\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix conflict\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix index and add doc\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "286393fbb11e3c95439ed94b01781cba632e2dfb",
    "files": [
        {
            "sha": "3aa1f09be55b49eb8f965f7c615e04b374c3a7bf",
            "filename": "docs/source/en/perf_infer_gpu_multi.md",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/286393fbb11e3c95439ed94b01781cba632e2dfb/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/286393fbb11e3c95439ed94b01781cba632e2dfb/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_multi.md?ref=286393fbb11e3c95439ed94b01781cba632e2dfb",
            "patch": "@@ -44,11 +44,6 @@ import os\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-# initialize distributed environment\n-rank = int(os.environ[\"RANK\"])\n-device = torch.device(f\"cuda:{rank}\")\n-torch.cuda.set_device(device)\n-torch.distributed.init_process_group(\"nccl\", device_id=device)\n \n # enable tensor parallelism\n model = AutoModelForCausalLM.from_pretrained(\n@@ -59,7 +54,7 @@ model = AutoModelForCausalLM.from_pretrained(\n # prepare input tokens\n tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n prompt = \"Can I help\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n+inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n \n # distributed run\n outputs = model(inputs)\n@@ -71,6 +66,13 @@ Launch the inference script above on [torchrun](https://pytorch.org/docs/stable/\n torchrun --nproc-per-node 4 demo.py\n ```\n \n+For CPU, please binding different socket on each rank. For example, if you are using Intel 4th Gen Xeon:\n+```bash\n+export OMP_NUM_THREADS=56\n+numactl -C 0-55 -m 0 torchrun --nnodes=2 --node_rank=0 --master_addr=\"127.0.0.1\" --master_port=29500 --nproc-per-node 1 demo.py & numactl -C 56-111 -m 1 torchrun --nnodes=2 --node_rank=1 --master_addr=\"127.0.0.1\" --master_port=29500 --nproc-per-node 1 demo.py & wait\n+```\n+The CPU benchmark data will be released soon.\n+\n You can benefit from considerable speed ups for inference, especially for inputs with large batch size or long sequences.\n \n For a single forward pass on [Llama](./model_doc/llama) with a sequence length of 512 and various batch sizes, you can expect the following speed ups."
        },
        {
            "sha": "5b27f0c3dbee0226980d9b28f5c0e06200a70b1a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/286393fbb11e3c95439ed94b01781cba632e2dfb/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/286393fbb11e3c95439ed94b01781cba632e2dfb/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=286393fbb11e3c95439ed94b01781cba632e2dfb",
            "patch": "@@ -774,7 +774,8 @@ def _load_state_dict_into_meta_model(\n     \"\"\"\n     tensor_device = \"cpu\"\n     if device_map is not None and device_map.get(\"\", None) is not None:\n-        tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n+        if device_map[\"\"] not in (\"cpu\", torch.device(\"cpu\")):\n+            tensor_device = device_map[\"\"].index if isinstance(device_map[\"\"], torch.device) else device_map[\"\"]\n     if device_map is not None:\n         device_map_regex = \"|\".join([re.escape(k) for k in sorted(device_map.keys(), reverse=True)])\n \n@@ -4110,24 +4111,34 @@ def from_pretrained(\n         if tp_plan is not None:\n             if not is_torch_greater_or_equal(\"2.5\"):\n                 raise EnvironmentError(\"tensor parallel is only supported for `torch>=2.5`.\")\n+\n+            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n+            device_type = torch._C._get_accelerator().type\n+\n             if not torch.distributed.is_initialized():\n                 try:\n                     rank = int(os.environ[\"RANK\"])\n                     world_size = int(os.environ[\"WORLD_SIZE\"])\n-                    torch.distributed.init_process_group(\n-                        \"nccl\", rank=rank, world_size=world_size, init_method=\"env://\"\n-                    )\n-                    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n+                    if device_type == \"cuda\":\n+                        torch.distributed.init_process_group(\n+                            \"nccl\", rank=rank, world_size=world_size, init_method=\"env://\"\n+                        )\n+                        torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n+                    elif device_type == \"cpu\":\n+                        cpu_backend = \"ccl\" if int(os.environ.get(\"CCL_WORKER_COUNT\", 0)) else \"gloo\"\n+                        torch.distributed.init_process_group(cpu_backend, rank=rank, world_size=world_size)\n+\n                 except Exception as e:\n                     raise EnvironmentError(\n                         \"We tried to initialize torch.distributed for you, but it failed, make\"\n                         \"sure you init torch distributed in your script to use `tp_plan='auto'`\"\n                     ) from e\n \n-            # Detect the accelerator on the machine. If no accelerator is available, it returns CPU.\n-            device_type = torch._C._get_accelerator().type\n-            tp_device = torch.device(device_type, torch.cuda.current_device())\n-            if tp_device.index > 0:\n+            # Get device with index assuming equal number of devices per host\n+            index = None if device_type == \"cpu\" else torch.cuda.current_device()\n+            tp_device = torch.device(device_type, index)\n+\n+            if index is not None and index > 0:\n                 import sys\n \n                 sys.stdout = open(os.devnull, \"w\")"
        },
        {
            "sha": "7276869d76427200a7fac0ccc0420d6cc37f15e0",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 28,
            "deletions": 98,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/286393fbb11e3c95439ed94b01781cba632e2dfb/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/286393fbb11e3c95439ed94b01781cba632e2dfb/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=286393fbb11e3c95439ed94b01781cba632e2dfb",
            "patch": "@@ -12,18 +12,13 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import os\n import subprocess\n import tempfile\n import textwrap\n \n-#  TORCH_LOGS=+dtensor CUDA_LAUNCH_BLOCKING=1 TORCH_USE_CUDA_DSA=1 PYTHONPATH=\"src\" python -m torch.distributed.run --nproc_per_node 2 ./tests/tp/test_tp.py\n from transformers import is_torch_available\n-from transformers.models.llama.configuration_llama import LlamaConfig\n-from transformers.models.llama.modeling_llama import LlamaModel\n from transformers.testing_utils import (\n     TestCasePlus,\n-    execute_subprocess_async,\n     get_torch_dist_unique_port,\n     require_torch_multi_gpu,\n )\n@@ -33,15 +28,18 @@\n     import torch\n \n \n+# RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n class TestTensorParallel(TestCasePlus):\n+    nproc_per_node = 2\n+\n     def torchrun(self, script: str):\n         \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n         with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n             tmp.write(script)\n             tmp.flush()\n             tmp.seek(0)\n             cmd = (\n-                f\"torchrun --nproc_per_node {torch.cuda.device_count()} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n+                f\"torchrun --nproc_per_node {self.nproc_per_node} --master_port {get_torch_dist_unique_port()} {tmp.name}\"\n             ).split()\n \n             # Note that the subprocess will be waited for here, and raise an error if not successful\n@@ -50,44 +48,39 @@ def torchrun(self, script: str):\n             except subprocess.CalledProcessError as e:\n                 raise Exception(f\"The following error was captured: {e.stderr}\")\n \n-    @require_torch_multi_gpu\n-    def test_tp(self):\n-        distributed_args = f\"\"\"--nproc_per_node={torch.cuda.device_count()}\n-            --master_port={get_torch_dist_unique_port()}\n-            {self.test_file_dir}/test_tp.py\n-        \"\"\".split()\n-        output_dir = self.get_auto_remove_tmp_dir()\n-        args = f\"--output_dir {output_dir} --report_to none\".split()\n-        cmd = [\"torchrun\"] + distributed_args + args\n-        print(cmd)\n-        execute_subprocess_async(cmd, env=self.get_env())\n-        # successful return here == success - any errors would have caused an error in the sub-call\n-\n-    @require_torch_multi_gpu\n-    def test_loading_memory_consumption(self):\n+    def test_model_forward(self):\n         script_to_run = textwrap.dedent(\n             \"\"\"\n             import torch\n             import os\n-            from transformers import AutoModelForCausalLM\n+            from transformers import AutoModelForCausalLM, AutoTokenizer\n \n-            model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n+            model_id = \"JackFram/llama-68m\"\n \n             rank = int(os.environ[\"RANK\"])\n             world_size = int(os.environ[\"WORLD_SIZE\"])\n-            device = torch.device(f\"cuda:{rank}\")\n-            torch.distributed.init_process_group(\"nccl\", device_id=device)\n \n-            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, tp_plan=\"auto\")\n+            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", tp_plan=\"auto\")\n             torch.distributed.barrier()\n \n-            # The expected model memory footprint. We add 1 as not all the modules are split (e.g. the embeddings)\n-            expected_model_memory_per_device = (16 / world_size) + 1\n-            overhead_factor = 1.2\n+            has_dtensor = 0\n+            for name, parameter in model.named_parameters():\n+                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+                    has_dtensor = 1\n+                    break\n+\n+            assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n+            tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            prompt = \"Can I help\"\n+\n+            inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n+            outputs = model(inputs)\n \n-            # Check that we do not use more than the expected sharded size during initialization\n-            if torch.cuda.max_memory_allocated(device) / 1024**3 > expected_model_memory_per_device * overhead_factor:\n-                raise ValueError(\"Loading the model used more than the expected fraction of model size per device\")\n+            next_token_logits = outputs[0][:, -1, :]\n+            next_token = torch.argmax(next_token_logits, dim=-1)\n+            response = tokenizer.decode(next_token)\n+            assert response == \"with\"\n \n             torch.distributed.barrier()\n             torch.distributed.destroy_process_group()\n@@ -96,69 +89,6 @@ def test_loading_memory_consumption(self):\n         self.torchrun(script_to_run)\n \n \n-if __name__ == \"__main__\":\n-    # The script below is meant to be run under torch.distributed, on a machine with multiple GPUs:\n-    # CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/tp/test_tp.py\n-    # or\n-    # PYTHONPATH=\"src\" python -m torch.distributed.run --nproc_per_node 2 ./tests/tp/test_tp.py\n-\n-    if not is_torch_available():\n-        exit(0)\n-\n-    # Test settings\n-    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n-    bs = 1\n-    seqlen = 4096\n-    # Get distributed settings\n-    rank = int(os.environ[\"RANK\"])\n-    world_size = int(os.environ[\"WORLD_SIZE\"])\n-\n-    # Initialize distributed\n-    device = torch.device(f\"cuda:{rank}\")\n-    torch.distributed.init_process_group(\"nccl\", device_id=device)\n-    device_mesh = torch.distributed.init_device_mesh(\"cuda\", (world_size,))\n-\n-    # Get model config\n-    config = LlamaConfig.from_pretrained(model_id)\n-    config.hidden_size = 2048\n-    config.attention_bias = False\n-    # Instantiate model\n-    with device:\n-        model = LlamaModel(config).to(dtype=torch.float16)\n-\n-    model.eval()\n-    # Tensor Parallel\n-    if world_size > 1:\n-        model.tensor_parallel(device_mesh)\n-    # Run model\n-\n-    inputs = torch.randint(config.vocab_size, (bs, seqlen), device=device)\n-\n-    # Test cuda graphing explicitly\n-    with torch.cuda.device(device):\n-        print(\"Cuda graphing\")\n-        with torch.no_grad():\n-            inputs = torch.randint(config.vocab_size, (bs, seqlen), device=device)\n-            # CUDA Graph setup\n-            s = torch.cuda.Stream(device=device)\n-            s.wait_stream(torch.cuda.current_stream())\n-            with torch.cuda.stream(s):\n-                for i in range(3):\n-                    out = model(inputs)\n-            torch.cuda.current_stream().wait_stream(s)\n-            g = torch.cuda.CUDAGraph()\n-            with torch.cuda.graph(g):\n-                out = model(inputs)\n-\n-            for _ in range(2):\n-                g.replay()\n-            s.synchronize()\n-\n-    assert out.last_hidden_state.shape == torch.Size([bs, seqlen, config.hidden_size])\n-\n-    # Test compile\n-    with torch.no_grad():\n-        out = model(inputs)\n-        model.forward = torch.compile(model.forward, mode=\"reduce-overhead\")\n-        out = model(inputs)\n-        out = model(inputs)\n+@require_torch_multi_gpu\n+class TestTensorParallelCuda(TestTensorParallel):\n+    nproc_per_node = torch.cuda.device_count()"
        }
    ],
    "stats": {
        "total": 169,
        "additions": 56,
        "deletions": 113
    }
}