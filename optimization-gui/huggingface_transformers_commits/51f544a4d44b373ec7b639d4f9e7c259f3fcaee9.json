{
    "author": "keetrap",
    "message": "Add Fast Conditional-DETR Processor (#37071)\n\n* Add Fast Conditional-DETR Processor\n\n* Update image_processing_conditional_detr_fast.py\n\n* Add modular_conditional_detr.py\n\n* Update image_processing_conditional_detr_fast.py\n\n* Update tests\n\n* make fix\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
    "files": [
        {
            "sha": "52de280ce84a77e7184756b0b667fed4388d937b",
            "filename": "docs/source/en/model_doc/conditional_detr.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fconditional_detr.md?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -48,6 +48,11 @@ This model was contributed by [DepuMeng](https://huggingface.co/DepuMeng). The o\n \n [[autodoc]] ConditionalDetrImageProcessor\n     - preprocess\n+\n+## ConditionalDetrImageProcessorFast\n+\n+[[autodoc]] ConditionalDetrImageProcessorFast\n+    - preprocess\n     - post_process_object_detection\n     - post_process_instance_segmentation\n     - post_process_semantic_segmentation"
        },
        {
            "sha": "e2ce65bdcaf04f12388b894131856976e9da44fb",
            "filename": "docs/source/ja/model_doc/conditional_detr.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fconditional_detr.md?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -43,6 +43,11 @@ alt=\"æç”»\" width=\"600\"/>\n \n [[autodoc]] ConditionalDetrImageProcessor\n     - preprocess\n+\n+## ConditionalDetrImageProcessorFast\n+\n+[[autodoc]] ConditionalDetrImageProcessorFast\n+    - preprocess\n     - post_process_object_detection\n     - post_process_instance_segmentation\n     - post_process_semantic_segmentation"
        },
        {
            "sha": "5de1cd9260b0fcb18cb5f85c3e517b9d29b1d0f5",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -67,7 +67,7 @@\n             (\"chinese_clip\", (\"ChineseCLIPImageProcessor\", \"ChineseCLIPImageProcessorFast\")),\n             (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"conditional_detr\", (\"ConditionalDetrImageProcessor\",)),\n+            (\"conditional_detr\", (\"ConditionalDetrImageProcessor\", \"ConditionalDetrImageProcessorFast\")),\n             (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n             (\"cvt\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),"
        },
        {
            "sha": "2979f2bd028fad29f97427754e25969d60683425",
            "filename": "src/transformers/models/conditional_detr/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2F__init__.py?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_conditional_detr import *\n     from .feature_extraction_conditional_detr import *\n     from .image_processing_conditional_detr import *\n+    from .image_processing_conditional_detr_fast import *\n     from .modeling_conditional_detr import *\n else:\n     import sys"
        },
        {
            "sha": "efa1d9476ec3146f9fff5ff25e180a00883ca844",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "added",
            "additions": 1072,
            "deletions": 0,
            "changes": 1072,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -0,0 +1,1072 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/conditional_detr/modular_conditional_detr.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_conditional_detr.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+import pathlib\n+from typing import Any, Dict, List, Optional, Set, Tuple, Union\n+\n+from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    SizeDict,\n+    get_image_size_for_max_height_width,\n+    get_max_height_width,\n+    safe_squeeze,\n+)\n+from ...image_transforms import center_to_corners_format, corners_to_center_format\n+from ...image_utils import (\n+    IMAGENET_DEFAULT_MEAN,\n+    IMAGENET_DEFAULT_STD,\n+    AnnotationFormat,\n+    AnnotationType,\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    get_image_size,\n+    validate_annotations,\n+)\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+)\n+from ...utils.import_utils import requires\n+from .image_processing_conditional_detr import (\n+    compute_segments,\n+    convert_segmentation_to_rle,\n+    get_size_with_aspect_ratio,\n+    remove_low_and_no_objects,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+if is_torch_available():\n+    from torch import nn\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.io import read_image\n+    from torchvision.transforms import functional as F\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ConditionalDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    format: Optional[Union[str, AnnotationFormat]]\n+    do_convert_annotations: Optional[bool]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[Dict[str, int]]\n+    return_segmentation_masks: Optional[bool]\n+\n+\n+SUPPORTED_ANNOTATION_FORMATS = (AnnotationFormat.COCO_DETECTION, AnnotationFormat.COCO_PANOPTIC)\n+\n+\n+# inspired by https://github.com/facebookresearch/conditional_detr/blob/master/datasets/coco.py#L33\n+def convert_coco_poly_to_mask(segmentations, height: int, width: int, device: torch.device) -> torch.Tensor:\n+    \"\"\"\n+    Convert a COCO polygon annotation to a mask.\n+\n+    Args:\n+        segmentations (`List[List[float]]`):\n+            List of polygons, each polygon represented by a list of x-y coordinates.\n+        height (`int`):\n+            Height of the mask.\n+        width (`int`):\n+            Width of the mask.\n+    \"\"\"\n+    try:\n+        from pycocotools import mask as coco_mask\n+    except ImportError:\n+        raise ImportError(\"Pycocotools is not installed in your environment.\")\n+\n+    masks = []\n+    for polygons in segmentations:\n+        rles = coco_mask.frPyObjects(polygons, height, width)\n+        mask = coco_mask.decode(rles)\n+        if len(mask.shape) < 3:\n+            mask = mask[..., None]\n+        mask = torch.as_tensor(mask, dtype=torch.uint8, device=device)\n+        mask = torch.any(mask, axis=2)\n+        masks.append(mask)\n+    if masks:\n+        masks = torch.stack(masks, axis=0)\n+    else:\n+        masks = torch.zeros((0, height, width), dtype=torch.uint8, device=device)\n+\n+    return masks\n+\n+\n+# inspired by https://github.com/facebookresearch/conditional_detr/blob/master/datasets/coco.py#L50\n+def prepare_coco_detection_annotation(\n+    image,\n+    target,\n+    return_segmentation_masks: bool = False,\n+    input_data_format: Optional[Union[ChannelDimension, str]] = None,\n+):\n+    \"\"\"\n+    Convert the target in COCO format into the format expected by CONDITIONAL_DETR.\n+    \"\"\"\n+    image_height, image_width = image.size()[-2:]\n+\n+    image_id = target[\"image_id\"]\n+    image_id = torch.as_tensor([image_id], dtype=torch.int64, device=image.device)\n+\n+    # Get all COCO annotations for the given image.\n+    annotations = target[\"annotations\"]\n+    classes = []\n+    area = []\n+    boxes = []\n+    keypoints = []\n+    for obj in annotations:\n+        if \"iscrowd\" not in obj or obj[\"iscrowd\"] == 0:\n+            classes.append(obj[\"category_id\"])\n+            area.append(obj[\"area\"])\n+            boxes.append(obj[\"bbox\"])\n+            if \"keypoints\" in obj:\n+                keypoints.append(obj[\"keypoints\"])\n+\n+    classes = torch.as_tensor(classes, dtype=torch.int64, device=image.device)\n+    area = torch.as_tensor(area, dtype=torch.float32, device=image.device)\n+    iscrowd = torch.zeros_like(classes, dtype=torch.int64, device=image.device)\n+    # guard against no boxes via resizing\n+    boxes = torch.as_tensor(boxes, dtype=torch.float32, device=image.device).reshape(-1, 4)\n+    boxes[:, 2:] += boxes[:, :2]\n+    boxes[:, 0::2] = boxes[:, 0::2].clip(min=0, max=image_width)\n+    boxes[:, 1::2] = boxes[:, 1::2].clip(min=0, max=image_height)\n+\n+    keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n+\n+    new_target = {\n+        \"image_id\": image_id,\n+        \"class_labels\": classes[keep],\n+        \"boxes\": boxes[keep],\n+        \"area\": area[keep],\n+        \"iscrowd\": iscrowd[keep],\n+        \"orig_size\": torch.as_tensor([int(image_height), int(image_width)], dtype=torch.int64, device=image.device),\n+    }\n+\n+    if keypoints:\n+        keypoints = torch.as_tensor(keypoints, dtype=torch.float32, device=image.device)\n+        # Apply the keep mask here to filter the relevant annotations\n+        keypoints = keypoints[keep]\n+        num_keypoints = keypoints.shape[0]\n+        keypoints = keypoints.reshape((-1, 3)) if num_keypoints else keypoints\n+        new_target[\"keypoints\"] = keypoints\n+\n+    if return_segmentation_masks:\n+        segmentation_masks = [obj[\"segmentation\"] for obj in annotations]\n+        masks = convert_coco_poly_to_mask(segmentation_masks, image_height, image_width, device=image.device)\n+        new_target[\"masks\"] = masks[keep]\n+\n+    return new_target\n+\n+\n+def masks_to_boxes(masks: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    Compute the bounding boxes around the provided panoptic segmentation masks.\n+\n+    Args:\n+        masks: masks in format `[number_masks, height, width]` where N is the number of masks\n+\n+    Returns:\n+        boxes: bounding boxes in format `[number_masks, 4]` in xyxy format\n+    \"\"\"\n+    if masks.numel() == 0:\n+        return torch.zeros((0, 4), device=masks.device)\n+\n+    h, w = masks.shape[-2:]\n+    y = torch.arange(0, h, dtype=torch.float32, device=masks.device)\n+    x = torch.arange(0, w, dtype=torch.float32, device=masks.device)\n+    # see https://github.com/pytorch/pytorch/issues/50276\n+    y, x = torch.meshgrid(y, x, indexing=\"ij\")\n+\n+    x_mask = masks * torch.unsqueeze(x, 0)\n+    x_max = x_mask.view(x_mask.shape[0], -1).max(-1)[0]\n+    x_min = (\n+        torch.where(masks, x.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    y_mask = masks * torch.unsqueeze(y, 0)\n+    y_max = y_mask.view(y_mask.shape[0], -1).max(-1)[0]\n+    y_min = (\n+        torch.where(masks, y.unsqueeze(0), torch.tensor(1e8, device=masks.device)).view(masks.shape[0], -1).min(-1)[0]\n+    )\n+\n+    return torch.stack([x_min, y_min, x_max, y_max], 1)\n+\n+\n+# 2 functions below adapted from https://github.com/cocodataset/panopticapi/blob/master/panopticapi/utils.py\n+# Copyright (c) 2018, Alexander Kirillov\n+# All rights reserved.\n+def rgb_to_id(color):\n+    \"\"\"\n+    Converts RGB color to unique ID.\n+    \"\"\"\n+    if isinstance(color, torch.Tensor) and len(color.shape) == 3:\n+        if color.dtype == torch.uint8:\n+            color = color.to(torch.int32)\n+        return color[:, :, 0] + 256 * color[:, :, 1] + 256 * 256 * color[:, :, 2]\n+    return int(color[0] + 256 * color[1] + 256 * 256 * color[2])\n+\n+\n+def prepare_coco_panoptic_annotation(\n+    image: torch.Tensor,\n+    target: Dict,\n+    masks_path: Union[str, pathlib.Path],\n+    return_masks: bool = True,\n+    input_data_format: Union[ChannelDimension, str] = None,\n+) -> Dict:\n+    \"\"\"\n+    Prepare a coco panoptic annotation for CONDITIONAL_DETR.\n+    \"\"\"\n+    image_height, image_width = get_image_size(image, channel_dim=input_data_format)\n+    annotation_path = pathlib.Path(masks_path) / target[\"file_name\"]\n+\n+    new_target = {}\n+    new_target[\"image_id\"] = torch.as_tensor(\n+        [target[\"image_id\"] if \"image_id\" in target else target[\"id\"]], dtype=torch.int64, device=image.device\n+    )\n+    new_target[\"size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+    new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n+\n+    if \"segments_info\" in target:\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(dtype=torch.int32, device=image.device)\n+        masks = rgb_to_id(masks)\n+\n+        ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)\n+        masks = masks == ids[:, None, None]\n+        masks = masks.to(torch.bool)\n+        if return_masks:\n+            new_target[\"masks\"] = masks\n+        new_target[\"boxes\"] = masks_to_boxes(masks)\n+        new_target[\"class_labels\"] = torch.as_tensor(\n+            [segment_info[\"category_id\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"iscrowd\"] = torch.as_tensor(\n+            [segment_info[\"iscrowd\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.int64,\n+            device=image.device,\n+        )\n+        new_target[\"area\"] = torch.as_tensor(\n+            [segment_info[\"area\"] for segment_info in target[\"segments_info\"]],\n+            dtype=torch.float32,\n+            device=image.device,\n+        )\n+\n+    return new_target\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast ConditionalDetr image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+    \"\"\",\n+)\n+@requires(backends=(\"torchvision\", \"torch\"))\n+class ConditionalDetrImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    image_mean = IMAGENET_DEFAULT_MEAN\n+    image_std = IMAGENET_DEFAULT_STD\n+    format = AnnotationFormat.COCO_DETECTION\n+    do_resize = True\n+    do_rescale = True\n+    do_normalize = True\n+    do_pad = True\n+    size = {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+    default_to_square = False\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n+    valid_kwargs = ConditionalDetrFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs]) -> None:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+\n+        size = kwargs.pop(\"size\", None)\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` parameter is deprecated and will be removed in v4.26. \"\n+                \"Please specify in `size['longest_edge'] instead`.\",\n+            )\n+            max_size = kwargs.pop(\"max_size\")\n+        else:\n+            max_size = None if size is None else 1333\n+\n+        size = size if size is not None else {\"shortest_edge\": 800, \"longest_edge\": 1333}\n+        self.size = get_size_dict(size, max_size=max_size, default_to_square=False)\n+\n+        # Backwards compatibility\n+        do_convert_annotations = kwargs.get(\"do_convert_annotations\", None)\n+        do_normalize = kwargs.get(\"do_normalize\", None)\n+        if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n+            self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n+\n+        super().__init__(**kwargs)\n+\n+    @classmethod\n+    def from_dict(cls, image_processor_dict: Dict[str, Any], **kwargs):\n+        \"\"\"\n+        Overrides the `from_dict` method from the base class to make sure parameters are updated if image processor is\n+        created using from_dict and kwargs e.g. `ConditionalDetrImageProcessorFast.from_pretrained(checkpoint, size=600,\n+        max_size=800)`\n+        \"\"\"\n+        image_processor_dict = image_processor_dict.copy()\n+        if \"max_size\" in kwargs:\n+            image_processor_dict[\"max_size\"] = kwargs.pop(\"max_size\")\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            image_processor_dict[\"pad_and_return_pixel_mask\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+        return super().from_dict(image_processor_dict, **kwargs)\n+\n+    def prepare_annotation(\n+        self,\n+        image: torch.Tensor,\n+        target: Dict,\n+        format: Optional[AnnotationFormat] = None,\n+        return_segmentation_masks: Optional[bool] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> Dict:\n+        \"\"\"\n+        Prepare an annotation for feeding into CONDITIONAL_DETR model.\n+        \"\"\"\n+        format = format if format is not None else self.format\n+\n+        if format == AnnotationFormat.COCO_DETECTION:\n+            return_segmentation_masks = False if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_detection_annotation(\n+                image, target, return_segmentation_masks, input_data_format=input_data_format\n+            )\n+        elif format == AnnotationFormat.COCO_PANOPTIC:\n+            return_segmentation_masks = True if return_segmentation_masks is None else return_segmentation_masks\n+            target = prepare_coco_panoptic_annotation(\n+                image,\n+                target,\n+                masks_path=masks_path,\n+                return_masks=return_segmentation_masks,\n+                input_data_format=input_data_format,\n+            )\n+        else:\n+            raise ValueError(f\"Format {format} is not supported.\")\n+        return target\n+\n+    def resize(\n+        self,\n+        image: torch.Tensor,\n+        size: SizeDict,\n+        interpolation: \"F.InterpolationMode\" = None,\n+        **kwargs,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize the image to the given size. Size can be `min_size` (scalar) or `(height, width)` tuple. If size is an\n+        int, smaller edge of the image will be matched to this number.\n+\n+        Args:\n+            image (`torch.Tensor`):\n+                Image to resize.\n+            size (`SizeDict`):\n+                Size of the image's `(height, width)` dimensions after resizing. Available options are:\n+                    - `{\"height\": int, \"width\": int}`: The image will be resized to the exact size `(height, width)`.\n+                        Do NOT keep the aspect ratio.\n+                    - `{\"shortest_edge\": int, \"longest_edge\": int}`: The image will be resized to a maximum size respecting\n+                        the aspect ratio and keeping the shortest edge less or equal to `shortest_edge` and the longest edge\n+                        less or equal to `longest_edge`.\n+                    - `{\"max_height\": int, \"max_width\": int}`: The image will be resized to the maximum size respecting the\n+                        aspect ratio and keeping the height less or equal to `max_height` and the width less or equal to\n+                        `max_width`.\n+            interpolation (`InterpolationMode`, *optional*, defaults to `InterpolationMode.BILINEAR`):\n+                Resampling filter to use if resizing the image.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.BILINEAR\n+        if size.shortest_edge and size.longest_edge:\n+            # Resize the image so that the shortest edge or the longest edge is of the given size\n+            # while maintaining the aspect ratio of the original image.\n+            new_size = get_size_with_aspect_ratio(\n+                image.size()[-2:],\n+                size[\"shortest_edge\"],\n+                size[\"longest_edge\"],\n+            )\n+        elif size.max_height and size.max_width:\n+            new_size = get_image_size_for_max_height_width(image.size()[-2:], size[\"max_height\"], size[\"max_width\"])\n+        elif size.height and size.width:\n+            new_size = (size[\"height\"], size[\"width\"])\n+        else:\n+            raise ValueError(\n+                \"Size must contain 'height' and 'width' keys or 'shortest_edge' and 'longest_edge' keys. Got\"\n+                f\" {size.keys()}.\"\n+            )\n+\n+        image = F.resize(\n+            image,\n+            size=new_size,\n+            interpolation=interpolation,\n+            **kwargs,\n+        )\n+        return image\n+\n+    def resize_annotation(\n+        self,\n+        annotation: Dict[str, Any],\n+        orig_size: Tuple[int, int],\n+        target_size: Tuple[int, int],\n+        threshold: float = 0.5,\n+        interpolation: \"F.InterpolationMode\" = None,\n+    ):\n+        \"\"\"\n+        Resizes an annotation to a target size.\n+\n+        Args:\n+            annotation (`Dict[str, Any]`):\n+                The annotation dictionary.\n+            orig_size (`Tuple[int, int]`):\n+                The original size of the input image.\n+            target_size (`Tuple[int, int]`):\n+                The target size of the image, as returned by the preprocessing `resize` step.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The threshold used to binarize the segmentation masks.\n+            resample (`InterpolationMode`, defaults to `InterpolationMode.NEAREST`):\n+                The resampling filter to use when resizing the masks.\n+        \"\"\"\n+        interpolation = interpolation if interpolation is not None else F.InterpolationMode.NEAREST\n+        ratio_height, ratio_width = [target / orig for target, orig in zip(target_size, orig_size)]\n+\n+        new_annotation = {}\n+        new_annotation[\"size\"] = target_size\n+\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                scaled_boxes = boxes * torch.as_tensor(\n+                    [ratio_width, ratio_height, ratio_width, ratio_height], dtype=torch.float32, device=boxes.device\n+                )\n+                new_annotation[\"boxes\"] = scaled_boxes\n+            elif key == \"area\":\n+                area = value\n+                scaled_area = area * (ratio_width * ratio_height)\n+                new_annotation[\"area\"] = scaled_area\n+            elif key == \"masks\":\n+                masks = value[:, None]\n+                masks = [F.resize(mask, target_size, interpolation=interpolation) for mask in masks]\n+                masks = torch.stack(masks).to(torch.float32)\n+                masks = masks[:, 0] > threshold\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = target_size\n+            else:\n+                new_annotation[key] = value\n+\n+        return new_annotation\n+\n+    def normalize_annotation(self, annotation: Dict, image_size: Tuple[int, int]) -> Dict:\n+        image_height, image_width = image_size\n+        norm_annotation = {}\n+        for key, value in annotation.items():\n+            if key == \"boxes\":\n+                boxes = value\n+                boxes = corners_to_center_format(boxes)\n+                boxes /= torch.as_tensor(\n+                    [image_width, image_height, image_width, image_height], dtype=torch.float32, device=boxes.device\n+                )\n+                norm_annotation[key] = boxes\n+            else:\n+                norm_annotation[key] = value\n+        return norm_annotation\n+\n+    def _update_annotation_for_padded_image(\n+        self,\n+        annotation: Dict,\n+        input_image_size: Tuple[int, int],\n+        output_image_size: Tuple[int, int],\n+        padding,\n+        update_bboxes,\n+    ) -> Dict:\n+        \"\"\"\n+        Update the annotation for a padded image.\n+        \"\"\"\n+        new_annotation = {}\n+        new_annotation[\"size\"] = output_image_size\n+        ratio_height, ratio_width = (input / output for output, input in zip(output_image_size, input_image_size))\n+\n+        for key, value in annotation.items():\n+            if key == \"masks\":\n+                masks = value\n+                masks = F.pad(\n+                    masks,\n+                    padding,\n+                    fill=0,\n+                )\n+                masks = safe_squeeze(masks, 1)\n+                new_annotation[\"masks\"] = masks\n+            elif key == \"boxes\" and update_bboxes:\n+                boxes = value\n+                boxes *= torch.as_tensor([ratio_width, ratio_height, ratio_width, ratio_height], device=boxes.device)\n+                new_annotation[\"boxes\"] = boxes\n+            elif key == \"size\":\n+                new_annotation[\"size\"] = output_image_size\n+            else:\n+                new_annotation[key] = value\n+        return new_annotation\n+\n+    def pad(\n+        self,\n+        image: torch.Tensor,\n+        padded_size: Tuple[int, int],\n+        annotation: Optional[Dict[str, Any]] = None,\n+        update_bboxes: bool = True,\n+        fill: int = 0,\n+    ):\n+        original_size = image.size()[-2:]\n+        padding_bottom = padded_size[0] - original_size[0]\n+        padding_right = padded_size[1] - original_size[1]\n+        if padding_bottom < 0 or padding_right < 0:\n+            raise ValueError(\n+                f\"Padding dimensions are negative. Please make sure that the padded size is larger than the \"\n+                f\"original size. Got padded size: {padded_size}, original size: {original_size}.\"\n+            )\n+        if original_size != padded_size:\n+            padding = [0, 0, padding_right, padding_bottom]\n+            image = F.pad(image, padding, fill=fill)\n+            if annotation is not None:\n+                annotation = self._update_annotation_for_padded_image(\n+                    annotation, original_size, padded_size, padding, update_bboxes\n+                )\n+\n+        # Make a pixel mask for the image, where 1 indicates a valid pixel and 0 indicates padding.\n+        pixel_mask = torch.zeros(padded_size, dtype=torch.int64, device=image.device)\n+        pixel_mask[: original_size[0], : original_size[1]] = 1\n+\n+        return image, pixel_mask, annotation\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+        annotations (`AnnotationType` or `List[AnnotationType]`, *optional*):\n+            List of annotations associated with the image or batch of images. If annotation is for object\n+            detection, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"annotations\" (`List[Dict]`): List of annotations for an image. Each annotation should be a\n+                dictionary. An image can have no annotations, in which case the list should be empty.\n+            If annotation is for segmentation, the annotations should be a dictionary with the following keys:\n+            - \"image_id\" (`int`): The image id.\n+            - \"segments_info\" (`List[Dict]`): List of segments for an image. Each segment should be a dictionary.\n+                An image can have no segments, in which case the list should be empty.\n+            - \"file_name\" (`str`): The file name of the image.\n+        format (`str`, *optional*, defaults to `AnnotationFormat.COCO_DETECTION`):\n+            Data format of the annotations. One of \"coco_detection\" or \"coco_panoptic\".\n+        do_convert_annotations (`bool`, *optional*, defaults to `True`):\n+            Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n+            bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n+            Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n+            method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n+            If `pad_size` is provided, the image will be padded to the specified dimensions.\n+            Otherwise, the image will be padded to the maximum height and width of the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+            provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+            height and width in the batch.\n+        return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n+            Whether to return segmentation masks.\n+        masks_path (`str` or `pathlib.Path`, *optional*):\n+            Path to the directory containing the segmentation masks.\n+        \"\"\",\n+    )\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]] = None,\n+        masks_path: Optional[Union[str, pathlib.Path]] = None,\n+        **kwargs: Unpack[ConditionalDetrFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n+        if \"pad_and_return_pixel_mask\" in kwargs:\n+            kwargs[\"do_pad\"] = kwargs.pop(\"pad_and_return_pixel_mask\")\n+            logger.warning_once(\n+                \"The `pad_and_return_pixel_mask` argument is deprecated and will be removed in a future version, \"\n+                \"use `do_pad` instead.\"\n+            )\n+\n+        if \"max_size\" in kwargs:\n+            logger.warning_once(\n+                \"The `max_size` argument is deprecated and will be removed in a future version, use\"\n+                \" `size['longest_edge']` instead.\"\n+            )\n+            kwargs[\"size\"] = kwargs.pop(\"max_size\")\n+\n+        return super().preprocess(images, annotations=annotations, masks_path=masks_path, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        annotations: Optional[Union[AnnotationType, List[AnnotationType]]],\n+        return_segmentation_masks: bool,\n+        masks_path: Optional[Union[str, pathlib.Path]],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        do_center_crop: bool,\n+        crop_size: SizeDict,\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_normalize: bool,\n+        do_convert_annotations: bool,\n+        image_mean: Optional[Union[float, List[float]]],\n+        image_std: Optional[Union[float, List[float]]],\n+        do_pad: bool,\n+        pad_size: Optional[Dict[str, int]],\n+        format: Optional[Union[str, AnnotationFormat]],\n+        return_tensors: Optional[Union[str, TensorType]],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or a batch of images so that it can be used by the model.\n+        \"\"\"\n+        if annotations is not None and isinstance(annotations, dict):\n+            annotations = [annotations]\n+\n+        if annotations is not None and len(images) != len(annotations):\n+            raise ValueError(\n+                f\"The number of images ({len(images)}) and annotations ({len(annotations)}) do not match.\"\n+            )\n+\n+        format = AnnotationFormat(format)\n+        if annotations is not None:\n+            validate_annotations(format, SUPPORTED_ANNOTATION_FORMATS, annotations)\n+\n+        if (\n+            masks_path is not None\n+            and format == AnnotationFormat.COCO_PANOPTIC\n+            and not isinstance(masks_path, (pathlib.Path, str))\n+        ):\n+            raise ValueError(\n+                \"The path to the directory containing the mask PNG files should be provided as a\"\n+                f\" `pathlib.Path` or string object, but is {type(masks_path)} instead.\"\n+            )\n+\n+        data = {}\n+\n+        processed_images = []\n+        processed_annotations = []\n+        pixel_masks = []  # Initialize pixel_masks here\n+        for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+            # prepare (COCO annotations as a list of Dict -> CONDITIONAL_DETR target as a single Dict per image)\n+            if annotations is not None:\n+                annotation = self.prepare_annotation(\n+                    image,\n+                    annotation,\n+                    format,\n+                    return_segmentation_masks=return_segmentation_masks,\n+                    masks_path=masks_path,\n+                    input_data_format=ChannelDimension.FIRST,\n+                )\n+\n+            if do_resize:\n+                resized_image = self.resize(image, size=size, interpolation=interpolation)\n+                if annotations is not None:\n+                    annotation = self.resize_annotation(\n+                        annotation,\n+                        orig_size=image.size()[-2:],\n+                        target_size=resized_image.size()[-2:],\n+                    )\n+                image = resized_image\n+            # Fused rescale and normalize\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n+            if do_convert_annotations and annotations is not None:\n+                annotation = self.normalize_annotation(annotation, get_image_size(image, ChannelDimension.FIRST))\n+\n+            processed_images.append(image)\n+            processed_annotations.append(annotation)\n+        images = processed_images\n+        annotations = processed_annotations if annotations is not None else None\n+\n+        if do_pad:\n+            # depends on all resized image shapes so we need another loop\n+            if pad_size is not None:\n+                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            else:\n+                padded_size = get_max_height_width(images)\n+\n+            padded_images = []\n+            padded_annotations = []\n+            for image, annotation in zip(images, annotations if annotations is not None else [None] * len(images)):\n+                # Pads images and returns their mask: {'pixel_values': ..., 'pixel_mask': ...}\n+                if padded_size == image.size()[-2:]:\n+                    padded_images.append(image)\n+                    pixel_masks.append(torch.ones(padded_size, dtype=torch.int64, device=image.device))\n+                    padded_annotations.append(annotation)\n+                    continue\n+                image, pixel_mask, annotation = self.pad(\n+                    image, padded_size, annotation=annotation, update_bboxes=do_convert_annotations\n+                )\n+                padded_images.append(image)\n+                padded_annotations.append(annotation)\n+                pixel_masks.append(pixel_mask)\n+            images = padded_images\n+            annotations = padded_annotations if annotations is not None else None\n+            data.update({\"pixel_mask\": torch.stack(pixel_masks, dim=0)})\n+\n+        data.update({\"pixel_values\": torch.stack(images, dim=0)})\n+        encoded_inputs = BatchFeature(data, tensor_type=return_tensors)\n+        if annotations is not None:\n+            encoded_inputs[\"labels\"] = [\n+                BatchFeature(annotation, tensor_type=return_tensors) for annotation in annotations\n+            ]\n+        return encoded_inputs\n+\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the Pascal VOC format (xmin, ymin, xmax, ymax).\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n+                image size (before any data augmentation). For visualization, this should be the image size after data\n+                augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logging.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`ConditionalDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+    def post_process_semantic_segmentation(self, outputs, target_sizes: List[Tuple[int, int]] = None):\n+        \"\"\"\n+        Converts the output of [`ConditionalDetrForSegmentation`] into semantic segmentation maps. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrForSegmentation`]):\n+                Raw outputs of the model.\n+            target_sizes (`List[Tuple[int, int]]`, *optional*):\n+                A list of tuples (`Tuple[int, int]`) containing the target size (height, width) of each image in the\n+                batch. If unset, predictions will not be resized.\n+        Returns:\n+            `List[torch.Tensor]`:\n+                A list of length `batch_size`, where each item is a semantic segmentation map of shape (height, width)\n+                corresponding to the target_sizes entry (if `target_sizes` is specified). Each entry of each\n+                `torch.Tensor` correspond to a semantic class id.\n+        \"\"\"\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        # Remove the null class `[..., :-1]`\n+        masks_classes = class_queries_logits.softmax(dim=-1)[..., :-1]\n+        masks_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Semantic segmentation logits of shape (batch_size, num_classes, height, width)\n+        segmentation = torch.einsum(\"bqc, bqhw -> bchw\", masks_classes, masks_probs)\n+        batch_size = class_queries_logits.shape[0]\n+\n+        # Resize logits and compute semantic segmentation maps\n+        if target_sizes is not None:\n+            if batch_size != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+            semantic_segmentation = []\n+            for idx in range(batch_size):\n+                resized_logits = nn.functional.interpolate(\n+                    segmentation[idx].unsqueeze(dim=0), size=target_sizes[idx], mode=\"bilinear\", align_corners=False\n+                )\n+                semantic_map = resized_logits[0].argmax(dim=0)\n+                semantic_segmentation.append(semantic_map)\n+        else:\n+            semantic_segmentation = segmentation.argmax(dim=1)\n+            semantic_segmentation = [semantic_segmentation[i] for i in range(semantic_segmentation.shape[0])]\n+\n+        return semantic_segmentation\n+\n+    def post_process_instance_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        target_sizes: Optional[List[Tuple[int, int]]] = None,\n+        return_coco_annotation: Optional[bool] = False,\n+    ) -> List[Dict]:\n+        \"\"\"\n+        Converts the output of [`ConditionalDetrForSegmentation`] into instance segmentation predictions. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrForSegmentation`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction. If unset, predictions will not be resized.\n+            return_coco_annotation (`bool`, *optional*):\n+                Defaults to `False`. If set to `True`, segmentation maps are returned in COCO run-length encoding (RLE)\n+                format.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- A tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+              `List[List]` run-length encoding (RLE) of the segmentation map if return_coco_annotation is set to\n+              `True`. Set to `None` if no mask if found above `threshold`.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- An integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: List[Dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=[],\n+                target_size=target_size,\n+            )\n+\n+            # Return segmentation map in run-length encoding (RLE) format\n+            if return_coco_annotation:\n+                segmentation = convert_segmentation_to_rle(segmentation)\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+    def post_process_panoptic_segmentation(\n+        self,\n+        outputs,\n+        threshold: float = 0.5,\n+        mask_threshold: float = 0.5,\n+        overlap_mask_area_threshold: float = 0.8,\n+        label_ids_to_fuse: Optional[Set[int]] = None,\n+        target_sizes: Optional[List[Tuple[int, int]]] = None,\n+    ) -> List[Dict]:\n+        \"\"\"\n+        Converts the output of [`ConditionalDetrForSegmentation`] into image panoptic segmentation predictions. Only supports\n+        PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrForSegmentation`]):\n+                The outputs from [`ConditionalDetrForSegmentation`].\n+            threshold (`float`, *optional*, defaults to 0.5):\n+                The probability score threshold to keep predicted instance masks.\n+            mask_threshold (`float`, *optional*, defaults to 0.5):\n+                Threshold to use when turning the predicted masks into binary values.\n+            overlap_mask_area_threshold (`float`, *optional*, defaults to 0.8):\n+                The overlap mask area threshold to merge or discard small disconnected parts within each binary\n+                instance mask.\n+            label_ids_to_fuse (`Set[int]`, *optional*):\n+                The labels in this state will have all their instances be fused together. For instance we could say\n+                there can only be one sky in an image, but several persons, so the label ID for sky would be in that\n+                set, but not the one for person.\n+            target_sizes (`List[Tuple]`, *optional*):\n+                List of length (batch_size), where each list item (`Tuple[int, int]]`) corresponds to the requested\n+                final size (height, width) of each prediction in batch. If unset, predictions will not be resized.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, one per image, each dictionary containing two keys:\n+            - **segmentation** -- a tensor of shape `(height, width)` where each pixel represents a `segment_id` or\n+              `None` if no mask if found above `threshold`. If `target_sizes` is specified, segmentation is resized to\n+              the corresponding `target_sizes` entry.\n+            - **segments_info** -- A dictionary that contains additional information on each segment.\n+                - **id** -- an integer representing the `segment_id`.\n+                - **label_id** -- An integer representing the label / semantic class id corresponding to `segment_id`.\n+                - **was_fused** -- a boolean, `True` if `label_id` was in `label_ids_to_fuse`, `False` otherwise.\n+                  Multiple instances of the same class / label were fused and assigned a single `segment_id`.\n+                - **score** -- Prediction score of segment with `segment_id`.\n+        \"\"\"\n+\n+        if label_ids_to_fuse is None:\n+            logger.warning_once(\"`label_ids_to_fuse` unset. No instance will be fused.\")\n+            label_ids_to_fuse = set()\n+\n+        class_queries_logits = outputs.logits  # [batch_size, num_queries, num_classes+1]\n+        masks_queries_logits = outputs.pred_masks  # [batch_size, num_queries, height, width]\n+\n+        batch_size = class_queries_logits.shape[0]\n+        num_labels = class_queries_logits.shape[-1] - 1\n+\n+        mask_probs = masks_queries_logits.sigmoid()  # [batch_size, num_queries, height, width]\n+\n+        # Predicted label and score of each query (batch_size, num_queries)\n+        pred_scores, pred_labels = nn.functional.softmax(class_queries_logits, dim=-1).max(-1)\n+\n+        # Loop over items in batch size\n+        results: List[Dict[str, TensorType]] = []\n+\n+        for i in range(batch_size):\n+            mask_probs_item, pred_scores_item, pred_labels_item = remove_low_and_no_objects(\n+                mask_probs[i], pred_scores[i], pred_labels[i], threshold, num_labels\n+            )\n+\n+            # No mask found\n+            if mask_probs_item.shape[0] <= 0:\n+                height, width = target_sizes[i] if target_sizes is not None else mask_probs_item.shape[1:]\n+                segmentation = torch.zeros((height, width)) - 1\n+                results.append({\"segmentation\": segmentation, \"segments_info\": []})\n+                continue\n+\n+            # Get segmentation map and segment information of batch item\n+            target_size = target_sizes[i] if target_sizes is not None else None\n+            segmentation, segments = compute_segments(\n+                mask_probs=mask_probs_item,\n+                pred_scores=pred_scores_item,\n+                pred_labels=pred_labels_item,\n+                mask_threshold=mask_threshold,\n+                overlap_mask_area_threshold=overlap_mask_area_threshold,\n+                label_ids_to_fuse=label_ids_to_fuse,\n+                target_size=target_size,\n+            )\n+\n+            results.append({\"segmentation\": segmentation, \"segments_info\": segments})\n+        return results\n+\n+\n+__all__ = [\"ConditionalDetrImageProcessorFast\"]"
        },
        {
            "sha": "878e52e14e23feb91a52c9f7fcad1f29e88d6c42",
            "filename": "src/transformers/models/conditional_detr/modular_conditional_detr.py",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fmodular_conditional_detr.py?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -0,0 +1,137 @@\n+from typing import List, Tuple, Union\n+\n+from transformers.models.detr.image_processing_detr_fast import DetrImageProcessorFast\n+\n+from ...image_transforms import (\n+    center_to_corners_format,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    logging,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class ConditionalDetrImageProcessorFast(DetrImageProcessorFast):\n+    def post_process(self, outputs, target_sizes):\n+        \"\"\"\n+        Converts the output of [`ConditionalDetrForObjectDetection`] into the format expected by the Pascal VOC format (xmin, ymin, xmax, ymax).\n+        Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            target_sizes (`torch.Tensor` of shape `(batch_size, 2)`):\n+                Tensor containing the size (h, w) of each image of the batch. For evaluation, this must be the original\n+                image size (before any data augmentation). For visualization, this should be the image size after data\n+                augment, but before padding.\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        logging.warning_once(\n+            \"`post_process` is deprecated and will be removed in v5 of Transformers, please use\"\n+            \" `post_process_object_detection` instead, with `threshold=0.` for equivalent results.\",\n+        )\n+\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if len(out_logits) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n+        if target_sizes.shape[1] != 2:\n+            raise ValueError(\"Each element of target_sizes must contain the size (h, w) of each image of the batch\")\n+\n+        prob = out_logits.sigmoid()\n+        topk_values, topk_indexes = torch.topk(prob.view(out_logits.shape[0], -1), 300, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        img_h, img_w = target_sizes.unbind(1)\n+        scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1)\n+        boxes = boxes * scale_fct[:, None, :]\n+\n+        results = [{\"scores\": s, \"labels\": l, \"boxes\": b} for s, l, b in zip(scores, labels, boxes)]\n+\n+        return results\n+\n+    def post_process_object_detection(\n+        self, outputs, threshold: float = 0.5, target_sizes: Union[TensorType, List[Tuple]] = None, top_k: int = 100\n+    ):\n+        \"\"\"\n+        Converts the raw output of [`ConditionalDetrForObjectDetection`] into final bounding boxes in (top_left_x,\n+        top_left_y, bottom_right_x, bottom_right_y) format. Only supports PyTorch.\n+\n+        Args:\n+            outputs ([`ConditionalDetrObjectDetectionOutput`]):\n+                Raw outputs of the model.\n+            threshold (`float`, *optional*):\n+                Score threshold to keep object detection predictions.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                (height, width) of each image in the batch. If left to None, predictions will not be resized.\n+            top_k (`int`, *optional*, defaults to 100):\n+                Keep only top k bounding boxes before filtering by thresholding.\n+\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n+            in the batch as predicted by the model.\n+        \"\"\"\n+        out_logits, out_bbox = outputs.logits, outputs.pred_boxes\n+\n+        if target_sizes is not None:\n+            if len(out_logits) != len(target_sizes):\n+                raise ValueError(\n+                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n+                )\n+\n+        prob = out_logits.sigmoid()\n+        prob = prob.view(out_logits.shape[0], -1)\n+        k_value = min(top_k, prob.size(1))\n+        topk_values, topk_indexes = torch.topk(prob, k_value, dim=1)\n+        scores = topk_values\n+        topk_boxes = torch.div(topk_indexes, out_logits.shape[2], rounding_mode=\"floor\")\n+        labels = topk_indexes % out_logits.shape[2]\n+        boxes = center_to_corners_format(out_bbox)\n+        boxes = torch.gather(boxes, 1, topk_boxes.unsqueeze(-1).repeat(1, 1, 4))\n+\n+        # and from relative [0, 1] to absolute [0, height] coordinates\n+        if target_sizes is not None:\n+            if isinstance(target_sizes, List):\n+                img_h = torch.Tensor([i[0] for i in target_sizes])\n+                img_w = torch.Tensor([i[1] for i in target_sizes])\n+            else:\n+                img_h, img_w = target_sizes.unbind(1)\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n+            boxes = boxes * scale_fct[:, None, :]\n+\n+        results = []\n+        for s, l, b in zip(scores, labels, boxes):\n+            score = s[s > threshold]\n+            label = l[s > threshold]\n+            box = b[s > threshold]\n+            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+\n+        return results\n+\n+    def post_process_segmentation():\n+        raise NotImplementedError(\"Segmentation post-processing is not implemented for Conditional DETR yet.\")\n+\n+    def post_process_instance():\n+        raise NotImplementedError(\"Instance post-processing is not implemented for Conditional DETR yet.\")\n+\n+    def post_process_panoptic():\n+        raise NotImplementedError(\"Panoptic post-processing is not implemented for Conditional DETR yet.\")\n+\n+\n+__all__ = [\"ConditionalDetrImageProcessorFast\"]"
        },
        {
            "sha": "4b02a1257844904f08c41f09d82918a7577d5a49",
            "filename": "tests/models/conditional_detr/test_image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 95,
            "deletions": 86,
            "changes": 181,
            "blob_url": "https://github.com/huggingface/transformers/blob/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51f544a4d44b373ec7b639d4f9e7c259f3fcaee9/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py?ref=51f544a4d44b373ec7b639d4f9e7c259f3fcaee9",
            "patch": "@@ -20,7 +20,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision, slow\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import AnnotationFormatTestMixin, ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -33,6 +33,9 @@\n \n     from transformers import ConditionalDetrImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import ConditionalDetrImageProcessorFast\n+\n \n class ConditionalDetrImageProcessingTester:\n     def __init__(\n@@ -132,6 +135,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class ConditionalDetrImageProcessingTest(AnnotationFormatTestMixin, ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = ConditionalDetrImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = ConditionalDetrImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -142,23 +146,25 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n-        self.assertEqual(image_processor.do_pad, True)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 18, \"longest_edge\": 1333})\n+            self.assertEqual(image_processor.do_pad, True)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n-        )\n-        self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n-        self.assertEqual(image_processor.do_pad, False)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, size=42, max_size=84, pad_and_return_pixel_mask=False\n+            )\n+            self.assertEqual(image_processor.size, {\"shortest_edge\": 42, \"longest_edge\": 84})\n+            self.assertEqual(image_processor.do_pad, False)\n \n     @slow\n     def test_call_pytorch_with_coco_detection_annotations(self):\n@@ -169,40 +175,41 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n \n         target = {\"image_id\": 39769, \"annotations\": target}\n \n-        # encode them\n-        image_processing = ConditionalDetrImageProcessor.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n-        encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n-\n-        # verify area\n-        expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class.from_pretrained(\"microsoft/conditional-detr-resnet-50\")\n+            encoding = image_processing(images=image, annotations=target, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+            # verify area\n+            expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -215,43 +222,45 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n \n         masks_path = pathlib.Path(\"./tests/fixtures/tests_samples/COCO/coco_panoptic\")\n \n-        # encode them\n-        image_processing = ConditionalDetrImageProcessor(format=\"coco_panoptic\")\n-        encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n-\n-        # verify pixel values\n-        expected_shape = torch.Size([1, 3, 800, 1066])\n-        self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n-\n-        expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n-\n-        # verify area\n-        expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n-        # verify boxes\n-        expected_boxes_shape = torch.Size([6, 4])\n-        self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n-        expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n-        # verify image_id\n-        expected_image_id = torch.tensor([39769])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n-        # verify is_crowd\n-        expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n-        # verify class_labels\n-        expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n-        # verify masks\n-        expected_masks_sum = 822873\n-        self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n-        # verify orig_size\n-        expected_orig_size = torch.tensor([480, 640])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n-        # verify size\n-        expected_size = torch.tensor([800, 1066])\n-        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n+        for image_processing_class in self.image_processor_list:\n+            # encode them\n+            image_processing = image_processing_class(format=\"coco_panoptic\")\n+            encoding = image_processing(images=image, annotations=target, masks_path=masks_path, return_tensors=\"pt\")\n+\n+            # verify pixel values\n+            expected_shape = torch.Size([1, 3, 800, 1066])\n+            self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n+\n+            expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+            # verify area\n+            expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n+            # verify boxes\n+            expected_boxes_shape = torch.Size([6, 4])\n+            self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n+            expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n+            # verify image_id\n+            expected_image_id = torch.tensor([39769])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n+            # verify is_crowd\n+            expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n+            # verify class_labels\n+            expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n+            # verify masks\n+            expected_masks_sum = 822873\n+            relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n+            self.assertTrue(relative_error < 1e-3)\n+            # verify orig_size\n+            expected_orig_size = torch.tensor([480, 640])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n+            # verify size\n+            expected_size = torch.tensor([800, 1066])\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->ConditionalDetr, facebook/detr-resnet-50 ->microsoft/conditional-detr-resnet-50"
        }
    ],
    "stats": {
        "total": 1403,
        "additions": 1316,
        "deletions": 87
    }
}