{
    "author": "LysandreJik",
    "message": "Transformers serve fix (#42570)\n\n* Fix: lacking EOS token + failing AutoProcessor\n\n* Tests\n\n* Tests",
    "sha": "52b988d83d8737f25b714128bf6f015338f629a1",
    "files": [
        {
            "sha": "d351894811a5792fb6a32dbd1b26e5beae1cba62",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 47,
            "deletions": 17,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/52b988d83d8737f25b714128bf6f015338f629a1/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52b988d83d8737f25b714128bf6f015338f629a1/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=52b988d83d8737f25b714128bf6f015338f629a1",
            "patch": "@@ -36,7 +36,7 @@\n from tqdm import tqdm\n \n import transformers\n-from transformers import BitsAndBytesConfig, GenerationConfig\n+from transformers import AutoTokenizer, BitsAndBytesConfig, GenerationConfig, PreTrainedTokenizerBase\n from transformers.utils.import_utils import (\n     is_fastapi_available,\n     is_librosa_available,\n@@ -823,9 +823,9 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Str\n             self.running_continuous_batching_manager.start()\n \n         # TODO (Joao, Lysandre): this should also work with tool support\n-        inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n-            model.device\n-        )[\"input_ids\"][0]\n+        inputs = processor.apply_chat_template(\n+            req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True, return_dict=True\n+        ).to(model.device)[\"input_ids\"][0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             from ..generation.continuous_batching import RequestStatus\n@@ -841,8 +841,13 @@ def stream_chat_completion(request_id, decode_stream):\n \n                     if result.status == RequestStatus.FINISHED:\n                         generated_all_tokens = n_tokens_generated >= generation_config.max_new_tokens\n-                        final_token_is_eos = result == tokenizer.eos_token\n-                        reason = \"length\" if (generated_all_tokens and not final_token_is_eos) else \"stop\"\n+\n+                        # If the tokenizer has an eos_token, we can have a more robust check.\n+                        if hasattr(tokenizer, \"eos_token\"):\n+                            final_token_is_eos = result == tokenizer.eos_token\n+                            generated_all_tokens = generated_all_tokens and not final_token_is_eos\n+\n+                        reason = \"length\" if generated_all_tokens else \"stop\"\n \n                         yield self.build_chat_completion_chunk(\n                             request_id,\n@@ -921,7 +926,11 @@ def cancellation_wrapper_buffer(_request_id):\n             return JSONResponse(json_chunk, media_type=\"application/json\")\n \n     @staticmethod\n-    def get_model_modality(model: \"PreTrainedModel\") -> Modality:\n+    def get_model_modality(model: \"PreTrainedModel\", processor=None) -> Modality:\n+        if processor is not None:\n+            if isinstance(processor, PreTrainedTokenizerBase):\n+                return Modality.LLM\n+\n         from transformers.models.auto.modeling_auto import (\n             MODEL_FOR_CAUSAL_LM_MAPPING_NAMES,\n             MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES,\n@@ -1011,7 +1020,7 @@ def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONRespons\n         self.last_model = model_id_and_revision\n         model, processor = self.load_model_and_processor(model_id_and_revision)\n \n-        modality = self.get_model_modality(model)\n+        modality = self.get_model_modality(model, processor=processor)\n         processor_inputs = self.get_processor_inputs_from_inbound_messages(messages, modality)\n \n         # ====== TOOL PREPROCESSING LOGIC ======\n@@ -1184,8 +1193,14 @@ def generate_with_cache(**kwargs):\n                         )\n \n                 generated_all_tokens = n_tokens_generated >= generation_config.max_new_tokens\n-                final_token_is_eos = result == streamer.tokenizer.eos_token\n-                reason = \"length\" if (generated_all_tokens and not final_token_is_eos) else \"stop\"\n+\n+                # If the tokenizer has an eos_token, we can have a more robust check.\n+                if hasattr(streamer.tokenizer, \"eos_token\"):\n+                    final_token_is_eos = result == streamer.tokenizer.eos_token\n+                    generated_all_tokens = generated_all_tokens and not final_token_is_eos\n+\n+                reason = \"length\" if generated_all_tokens else \"stop\"\n+\n                 yield self.build_chat_completion_chunk(_request_id, finish_reason=reason, model=model_id_and_revision)\n \n                 thread.join()\n@@ -1272,7 +1287,9 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         else:\n             raise TypeError(\"inputs should be a list, dict, or str\")\n \n-        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")[\"input_ids\"]\n+        inputs = processor.apply_chat_template(\n+            inputs, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n+        )[\"input_ids\"]\n         inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n@@ -1576,7 +1593,9 @@ def generate_response_non_streaming(self, req: dict) -> dict:\n         else:\n             raise ValueError(\"inputs should be a list, dict, or str\")\n \n-        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")[\"input_ids\"]\n+        inputs = processor.apply_chat_template(\n+            inputs, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True\n+        )[\"input_ids\"]\n         inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n@@ -1775,11 +1794,22 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n         else:\n             model_id, revision = model_id_and_revision, \"main\"\n \n-        data_processor = AutoProcessor.from_pretrained(\n-            model_id,\n-            revision=revision,\n-            trust_remote_code=self.trust_remote_code,\n-        )\n+        try:\n+            data_processor = AutoProcessor.from_pretrained(\n+                model_id,\n+                revision=revision,\n+                trust_remote_code=self.trust_remote_code,\n+            )\n+        except OSError:\n+            try:\n+                data_processor = AutoTokenizer.from_pretrained(\n+                    model_id,\n+                    revision=revision,\n+                    trust_remote_code=self.trust_remote_code,\n+                )\n+            except OSError:\n+                raise OSError(\"Failed to load processor with `AutoProcessor` and `AutoTokenizer`.\")\n+\n         dtype = self.dtype if self.dtype in [\"auto\", None] else getattr(torch, self.dtype)\n         quantization_config = self.get_quantization_config()\n "
        },
        {
            "sha": "c6db19f815a75f53dcd0a75c1dcd462ce6192072",
            "filename": "tests/cli/test_serve.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/52b988d83d8737f25b714128bf6f015338f629a1/tests%2Fcli%2Ftest_serve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52b988d83d8737f25b714128bf6f015338f629a1/tests%2Fcli%2Ftest_serve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_serve.py?ref=52b988d83d8737f25b714128bf6f015338f629a1",
            "patch": "@@ -329,7 +329,7 @@ def test_generation_config_in_request(self):\n \n     def test_early_return_due_to_length(self):\n         request = {\n-            \"model\": \"Qwen/Qwen3-0.6B\",\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n             \"messages\": [{\"role\": \"user\", \"content\": \"Hello, how are you?\"}],\n             \"stream\": True,\n             \"max_tokens\": 3,\n@@ -339,8 +339,17 @@ def test_early_return_due_to_length(self):\n         last_payload = all_payloads[-1]\n         self.assertTrue(last_payload.choices[0][\"finish_reason\"] == \"length\")\n \n-    # TODO: one test for each request flag, to confirm it is working as expected\n-    # TODO: speed-based test to confirm that KV cache is working across requests\n+    def test_continues_until_stop(self):\n+        request = {\n+            \"model\": \"Qwen/Qwen2.5-0.5B-Instruct\",\n+            \"messages\": [{\"role\": \"user\", \"content\": 'Please only answer with \"Hi.\"'}],\n+            \"stream\": True,\n+            \"max_tokens\": 30,\n+        }\n+\n+        all_payloads = self.run_server(request)\n+        last_payload = all_payloads[-1]\n+        self.assertTrue(last_payload.choices[0][\"finish_reason\"] == \"stop\")\n \n \n class ServeCompletionsGenerateMockTests(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 79,
        "additions": 59,
        "deletions": 20
    }
}