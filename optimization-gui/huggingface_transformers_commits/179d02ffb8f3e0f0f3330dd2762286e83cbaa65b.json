{
    "author": "gante",
    "message": "[generate] ✨ vectorized beam search ✨ (#35802)",
    "sha": "179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
    "files": [
        {
            "sha": "98adb32df0c0ef891a2137ea702e9bb487d7b290",
            "filename": "src/transformers/generation/tf_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fgeneration%2Ftf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_utils.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -2118,7 +2118,7 @@ def beam_search(\n         a greedy approach, otherwise does multinomial sampling without replacement.\n \n         Parameters:\n-            input_ids (`tf.Tensor` of shape `(batch_size, sequence_length)`):\n+            input_ids (`tf.Tensor` of shape `(batch_size, num_beams, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n             do_sample (`bool`, *optional*, defaults to `False`):\n                 Whether or not to use sampling ; use greedy decoding otherwise."
        },
        {
            "sha": "3165ad1c770e4c9b9a472e19e55502e152cd6035",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 388,
            "deletions": 162,
            "changes": 550,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -2322,29 +2322,16 @@ def generate(\n             )\n \n         elif generation_mode in (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n-            # 11. prepare beam search scorer\n-            beam_scorer = BeamSearchScorer(\n-                batch_size=batch_size,\n-                num_beams=generation_config.num_beams,\n-                device=inputs_tensor.device,\n-                length_penalty=generation_config.length_penalty,\n-                do_early_stopping=generation_config.early_stopping,\n-                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n-                max_length=generation_config.max_length,\n-            )\n-\n-            # 12. interleave input_ids with `num_beams` additional sequences per batch\n+            # 11. interleave input_ids with `num_beams` additional sequences per batch\n             input_ids, model_kwargs = self._expand_inputs_for_generation(\n                 input_ids=input_ids,\n                 expand_size=generation_config.num_beams,\n                 is_encoder_decoder=self.config.is_encoder_decoder,\n                 **model_kwargs,\n             )\n-\n-            # 13. run beam sample\n+            # 12. run beam sample\n             result = self._beam_search(\n                 input_ids,\n-                beam_scorer,\n                 logits_processor=prepared_logits_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,\n@@ -3396,6 +3383,7 @@ def _sample(\n         else:\n             return input_ids\n \n+    # Auxiliary functions for beam search\n     def _temporary_reorder_cache(self, past_key_values, beam_idx):\n         \"\"\"\n         Temporary function to handle the different types of cache reordering processes while we roll out `Cache`.\n@@ -3422,10 +3410,208 @@ def _temporary_reorder_cache(self, past_key_values, beam_idx):\n             past_key_values.reorder_cache(beam_idx)\n         return past_key_values\n \n+    @staticmethod\n+    def _flatten_beam_dim(tensor: torch.Tensor) -> torch.Tensor:\n+        \"\"\"[batch_size, num_beams, ...] -> [batch_size * num_beams, ...]\"\"\"\n+        shape = list(tensor.shape)\n+        return torch.reshape(tensor, [shape[0] * shape[1]] + shape[2:])\n+\n+    @staticmethod\n+    def _unflatten_beam_dim(tensor: torch.Tensor, batch_size: int, num_beams: int) -> torch.Tensor:\n+        \"\"\"[batch_size * num_beams, ...] -> [batch_size, num_beams, ...]\"\"\"\n+        shape = list(tensor.shape)\n+        return torch.reshape(tensor, [batch_size, num_beams] + shape[1:])\n+\n+    @staticmethod\n+    def _gather_beams(tensor: torch.Tensor, beam_indices: torch.Tensor) -> torch.Tensor:\n+        \"\"\"\n+        Gathers the beam slices indexed by beam_indices into new beam array.\n+\n+        Args:\n+            tensor (`torch.Tensor`): A tensor containing data to be gathered. The tensor is a 2D or a 3D tensor\n+                with the two first dimensions depicting the batch and the beam dimensions.\n+            beam_indices (`torch.Tensor` of shape `(batch_size, num_beams_to_select)`): The indices of the beams to\n+                select .\n+\n+        Returns:\n+            A tensor with the selected beams\n+        \"\"\"\n+        # `take_along_dim` requires its indices arg to have the same number of dims as `input`\n+        while len(beam_indices.shape) < len(tensor.shape):\n+            beam_indices = beam_indices.unsqueeze(-1)\n+        gathered_tensor = torch.take_along_dim(input=tensor, indices=beam_indices, dim=1)\n+        return gathered_tensor\n+\n+    @staticmethod\n+    def _beam_search_has_unfinished_sequences(\n+        running_beam_scores: torch.Tensor,\n+        beam_scores: torch.Tensor,\n+        is_sent_finished: torch.Tensor,\n+        next_token_hits_stopping_criteria: torch.Tensor,\n+        cur_len: int,\n+        max_length: int,\n+        decoder_prompt_len: int,\n+        early_stopping: Union[bool, str],\n+        length_penalty: float,\n+    ):\n+        \"\"\"\n+        Beam Search stopping condition -- halts the generation loop if any of these conditions becomes False\n+        \"\"\"\n+        # a. Can the open beams improve the top completed scores?\n+        # early_stopping == False -> apply heuristic = always get the best score from\n+        #   `cur_len - decoder_prompt_len`. See the discussion below for more details.\n+        #   https://github.com/huggingface/transformers/pull/20901#issuecomment-1369845565\n+        # early_stopping == \"never\" -> compute the best score from `max_length` or `cur_len`, depending on the\n+        #   sign of `length_penalty`. Positive `length_penalty` favors longer sequences, thus we use\n+        #   `max_length` there.\n+        if early_stopping == \"never\" and length_penalty > 0.0:\n+            best_hypothetical_length = max_length - decoder_prompt_len\n+        else:\n+            best_hypothetical_length = cur_len - decoder_prompt_len\n+        best_possible_running_score = running_beam_scores[:, :1] / (best_hypothetical_length**length_penalty)\n+        worst_finished_score = torch.where(is_sent_finished, torch.min(beam_scores, dim=1, keepdim=True)[0], -1.0e9)\n+        improvement_possible = torch.any(best_possible_running_score > worst_finished_score)\n+\n+        # b. Is there still a beam without fully completed sequences? This is only relevant if early_stopping is\n+        # enabled, where we want to finish as soon as all beams have a completed sequence.\n+        exists_open_beam = ~(torch.all(is_sent_finished) & (early_stopping is True))\n+\n+        # c. Have we hit a stopping criteria with all running sequences and have no way to continue? e.g. we have\n+        # reached `max_length``\n+        valid_continuations = ~torch.all(next_token_hits_stopping_criteria)\n+\n+        return improvement_possible & exists_open_beam & valid_continuations\n+\n+    def _get_top_k_continuations(\n+        self,\n+        accumulated_log_probs: torch.Tensor,\n+        running_sequences: torch.Tensor,\n+        running_beam_indices: torch.Tensor,\n+        cur_len: int,\n+        decoder_prompt_len: int,\n+        do_sample: bool,\n+        beams_to_keep: int,\n+        num_beams: int,\n+        vocab_size: int,\n+        batch_size: int,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Get top-K continuations given the accumulated log probs on the next token.\n+\n+        A few notes to understand what's going on:\n+        1. Each item in batch has `num_beams` * `vocab_size` candidate continuations. For each item, get the\n+        top K [K = (number of EOS tokens + 1) * `num_beams`] candidates with the highest accumulated\n+        log-probabilities, or sample them without replacement using the accumulated scores\n+        2. We gather the top K (as opposed to `num_beams`, or any number lower than K) here so that we have at\n+        least `num_beams` sequences remaining to continue the live beam search.\n+        3. Note that other stopping criteria might result in impossible to continue beams, i.e. all continuations\n+        selected in this step hit the stopping criteria.\n+        \"\"\"\n+        # TODO (joao): This function should take an optional beam scorer function, to manipulate the scores after\n+        # token selection. The function should be an argument exposed, so that custom scoring functions can be\n+        # defined.\n+\n+        # Gather the top K scores from _all_ beams.\n+        if do_sample:\n+            topk_indices = torch.multinomial(\n+                nn.functional.softmax(accumulated_log_probs, dim=-1), num_samples=beams_to_keep\n+            )\n+            topk_log_probs = torch.gather(input=accumulated_log_probs, dim=1, index=topk_indices)\n+        else:\n+            topk_log_probs, topk_indices = torch.topk(accumulated_log_probs, k=beams_to_keep)\n+\n+        # Gather K top beams, recover the beam index by floor division and token id by modulo division\n+        topk_current_beam_indices = topk_indices // vocab_size\n+        topk_running_beam_indices = self._gather_beams(running_beam_indices, topk_current_beam_indices)\n+        topk_running_sequences = self._gather_beams(running_sequences, topk_current_beam_indices)\n+        topk_ids = topk_indices % vocab_size\n+\n+        # Update sequences for the K top-k new sequences.\n+        topk_running_sequences[:, :, cur_len] = topk_ids\n+\n+        # we want to store the beam indices with batch information -> real beam index = beam index % num beams\n+        batch_offset = torch.arange(batch_size, device=topk_ids.device).view(-1, 1) * num_beams\n+        batch_modified_indices = topk_current_beam_indices + batch_offset\n+        topk_running_beam_indices[:, :, cur_len - decoder_prompt_len] = batch_modified_indices\n+\n+        return topk_log_probs, topk_running_sequences, topk_running_beam_indices\n+\n+    def _get_running_beams_for_next_iteration(\n+        self,\n+        topk_log_probs: torch.Tensor,\n+        topk_running_sequences: torch.Tensor,\n+        topk_running_beam_indices: torch.Tensor,\n+        next_token_hits_stopping_criteria: torch.Tensor,\n+        num_beams: int,\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Given the top-K continuations, their scores, and whether they hit a stopping criteria, select the\n+        best non-finished beams to continue beam search in the next iteration.\n+        \"\"\"\n+        # To prevent these just finished sequences from being used in subsequent iterations, set their log probs\n+        # to a very large negative value\n+        topk_running_log_probs = topk_log_probs + next_token_hits_stopping_criteria.to(torch.float32) * -1.0e9\n+\n+        next_topk_indices = torch.topk(topk_running_log_probs, k=num_beams)[1]\n+        running_sequences = self._gather_beams(topk_running_sequences, next_topk_indices)\n+        running_beam_scores = self._gather_beams(topk_running_log_probs, next_topk_indices)\n+        running_beam_indices = self._gather_beams(topk_running_beam_indices, next_topk_indices)\n+        return running_sequences, running_beam_scores, running_beam_indices\n+\n+    def _update_finished_beams(\n+        self,\n+        sequences: torch.Tensor,\n+        topk_running_sequences: torch.Tensor,\n+        beam_scores: torch.Tensor,\n+        topk_log_probs: torch.Tensor,\n+        beam_indices: torch.Tensor,\n+        topk_running_beam_indices: torch.Tensor,\n+        is_sent_finished: torch.Tensor,\n+        next_token_hits_stopping_criteria: torch.Tensor,\n+        top_num_beam_mask: torch.Tensor,\n+        num_beams: int,\n+        cur_len: int,\n+        decoder_prompt_len: int,\n+        length_penalty: float,\n+        early_stopping: Union[bool, str],\n+    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n+        \"\"\"\n+        Updates the finished beams if (and only if) there are new completed sequences that have a higher score than\n+        the current finished sequences.\n+        \"\"\"\n+        # Only the top `num_beam` sequences can be considered for the final returned sequences. Remember: the\n+        # remaining sequences only exist as a backup to ensure that we have at least `num_beams` sequences to\n+        # continue.\n+        did_top_num_beams_just_finished = next_token_hits_stopping_criteria & top_num_beam_mask[None, :]\n+\n+        # Further process topk logits for the finished beams\n+        # - add length penalty\n+        topk_log_probs = topk_log_probs / ((cur_len + 1 - decoder_prompt_len) ** length_penalty)\n+        # - make sure no scores can be added anymore if beam is full and early stopping is on\n+        beams_in_batch_are_full = torch.all(is_sent_finished, axis=-1, keepdims=True) & (early_stopping is True)\n+        topk_log_probs += beams_in_batch_are_full.to(torch.float32) * -1.0e9\n+        # - make sure still running sequences cannot be chosen as finalized beam\n+        topk_log_probs += (~did_top_num_beams_just_finished) * -1.0e9\n+\n+        # Get finalized  `num_beam` sequences for the next generation step -- combine the previous finalized\n+        # data with the new finalized sequences (if any, non-finalized sequences have a very large negative score\n+        # in this step), and keep the best `num_beams` sequences.\n+        merged_sequences = torch.cat((sequences, topk_running_sequences), dim=1)\n+        merged_scores = torch.cat((beam_scores, topk_log_probs), dim=1)\n+        merged_beam_indices = torch.cat((beam_indices, topk_running_beam_indices), dim=1)\n+        merged_is_sent_finished = torch.cat((is_sent_finished, did_top_num_beams_just_finished), dim=1)\n+        topk_merged_indices = torch.topk(merged_scores, k=num_beams)[1]\n+        sequences = self._gather_beams(merged_sequences, topk_merged_indices)\n+        beam_scores = self._gather_beams(merged_scores, topk_merged_indices)\n+        beam_indices = self._gather_beams(merged_beam_indices, topk_merged_indices)\n+        is_sent_finished = self._gather_beams(merged_is_sent_finished, topk_merged_indices)\n+        return sequences, beam_scores, beam_indices, is_sent_finished\n+\n+    # end of auxiliary functions for beam search\n+\n     def _beam_search(\n         self,\n         input_ids: torch.LongTensor,\n-        beam_scorer: BeamScorer,\n         logits_processor: LogitsProcessorList,\n         stopping_criteria: StoppingCriteriaList,\n         generation_config: GenerationConfig,\n@@ -3436,12 +3622,15 @@ def _beam_search(\n         Generates sequences of token ids for models with a language modeling head using **beam search decoding** and\n         can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n \n+        If it's the first time you're diving into Beam Search, we recommend you read the following blog post:\n+        https://huggingface.co/blog/how-to-generate (especially the beam search section).\n+\n+        You can recompute the sequence scores from the individual scores using the `compute_transition_scores` function\n+        (https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationMixin.compute_transition_scores)\n+\n         Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            input_ids (`torch.LongTensor` of shape `(batch_size*num_beams, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n-            beam_scorer (`BeamScorer`):\n-                An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n-                sorted during generation. For more information, the documentation of [`BeamScorer`] should be read.\n             logits_processor (`LogitsProcessorList`):\n                 An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n                 used to modify the prediction scores of the language modeling head applied at each generation step.\n@@ -3464,34 +3653,60 @@ def _beam_search(\n             `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n             `model.config.is_encoder_decoder=True`.\n         \"\"\"\n-        # init values\n+\n+        # 1. init beam_search values\n         pad_token_id = generation_config._pad_token_tensor\n         eos_token_id = generation_config._eos_token_tensor\n         output_attentions = generation_config.output_attentions\n         output_hidden_states = generation_config.output_hidden_states\n         output_scores = generation_config.output_scores\n         output_logits = generation_config.output_logits\n         return_dict_in_generate = generation_config.return_dict_in_generate\n-        sequential = generation_config.low_memory\n         do_sample = generation_config.do_sample\n+        early_stopping = generation_config.early_stopping\n+        length_penalty = generation_config.length_penalty\n+        max_length = generation_config.max_length\n+        num_beams = generation_config.num_beams\n+        num_return_sequences = generation_config.num_return_sequences\n+\n+        batch_size_unflattened, cur_len = input_ids.shape\n+        batch_size = batch_size_unflattened // num_beams\n+        # TODO (joao): standardize special cases\n+        if self.__class__.__name__ == \"MoshiDepthDecoder\":\n+            vocab_size = self.config.audio_vocab_size\n+        elif self.__class__.__name__ == \"ImageGPTForCausalImageModeling\":\n+            vocab_size = self.get_output_embeddings().out_features\n+        else:\n+            vocab_size = self.config.get_text_config().vocab_size\n+        decoder_prompt_len = cur_len\n+        this_peer_finished = False\n \n-        batch_size = len(beam_scorer._beam_hyps)\n-        num_beams = beam_scorer.num_beams\n+        # At each beam search step, we want to keep top K [K = (number of EOS tokens + 1) * `num_beams`] candidates\n+        # with the highest log-probabilities, or sample K continuations without replacement. We gather the top K\n+        # (as opposed to `num_beams`, or any number lower than K) so that we have at least `num_beams` sequences\n+        # non-finished to continue the live beam search, in case the top `num_beams` all select an EOS token.\n+        n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n+        beams_to_keep = max(2, 1 + n_eos_tokens) * num_beams\n+        top_num_beam_mask = torch.cat(\n+            (torch.ones((num_beams), dtype=torch.bool), torch.zeros((beams_to_keep - num_beams), dtype=torch.bool)),\n+            dim=0,\n+        ).to(input_ids.device)\n \n-        batch_beam_size, cur_len = input_ids.shape\n         model_kwargs = self._get_initial_cache_position(input_ids, model_kwargs)\n \n-        if num_beams * batch_size != batch_beam_size:\n+        # (joao) feature lost in the refactor. Probably won't implement, hurts readbility with minimal gains (there\n+        # are newer low-memory alternatives like the offloaded cache)\n+        sequential = generation_config.low_memory\n+        if sequential:\n             raise ValueError(\n-                f\"Batch dimension of `input_ids` should be {num_beams * batch_size}, but is {batch_beam_size}.\"\n+                \"`low_memory=True` is not supported after the beam search refactor. Please check the discussion in \"\n+                \"#35802 *after the PR got merged*, and add a comment there if your questions are not yet answered.\"\n             )\n \n-        # init attention / hidden states / scores tuples\n-        scores = () if (return_dict_in_generate and output_scores) else None\n+        # 2. init output tuples\n+        all_scores = () if (return_dict_in_generate and output_scores) else None\n         raw_logits = () if (return_dict_in_generate and output_logits) else None\n-        beam_indices = (\n-            tuple(() for _ in range(batch_beam_size)) if (return_dict_in_generate and output_scores) else None\n-        )\n+        beam_indices = () if (return_dict_in_generate and output_logits) else None\n         decoder_attentions = () if (return_dict_in_generate and output_attentions) else None\n         cross_attentions = () if (return_dict_in_generate and output_attentions) else None\n         decoder_hidden_states = () if (return_dict_in_generate and output_hidden_states) else None\n@@ -3503,184 +3718,195 @@ def _beam_search(\n                 model_kwargs[\"encoder_outputs\"].get(\"hidden_states\") if output_hidden_states else None\n             )\n \n+        # 3. init running tensors and static-shaped placeholders\n+\n+        # per batch, beam-item holding current token in loop and completed sequences\n+        output_fill_value = pad_token_id or eos_token_id[0] if eos_token_id is not None else -1\n+        running_sequences = torch.full(\n+            (batch_size, num_beams, max_length),\n+            fill_value=output_fill_value,\n+            dtype=torch.int64,\n+            device=input_ids.device,\n+        )\n+        running_sequences[:, :, :cur_len] = self._unflatten_beam_dim(input_ids, batch_size, num_beams)\n+        sequences = running_sequences.clone().detach()\n+\n+        # per batch, beam-item score, logprobs\n         # initialise score of first beam with 0 and the rest with -1e9. This makes sure that only tokens\n         # of the first beam are considered to avoid sampling the exact same tokens across all beams.\n-        beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n-        beam_scores[:, 1:] = -1e9\n-        beam_scores = beam_scores.view((batch_size * num_beams,))\n+        running_beam_scores = torch.zeros((batch_size, num_beams), dtype=torch.float, device=input_ids.device)\n+        running_beam_scores[:, 1:] = -1e9\n+        beam_scores = torch.full((batch_size, num_beams), fill_value=-1e9, dtype=torch.float, device=input_ids.device)\n \n-        this_peer_finished = False\n+        # per batch, beam-item state bit indicating if sentence has finished.\n+        is_sent_finished = torch.zeros((batch_size, num_beams), dtype=torch.bool, device=input_ids.device)\n \n-        decoder_prompt_len = input_ids.shape[-1]  # record the prompt length of decoder\n+        # per batch, beam-item state bit indicating if there are valid continuations.\n+        next_token_hits_stopping_criteria = torch.zeros(\n+            (batch_size, num_beams), dtype=torch.bool, device=input_ids.device\n+        )\n \n+        # per batch selected beam indices\n+        running_beam_indices = torch.full(\n+            (batch_size, num_beams, max_length - cur_len), fill_value=-1, dtype=torch.int32, device=input_ids.device\n+        )\n+        beam_indices = running_beam_indices.clone().detach()\n+\n+        # 4. run the generation loop\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+            # a. Forward current tokens, obtain the logits\n+            flat_running_sequences = self._flatten_beam_dim(running_sequences[:, :, :cur_len])\n+            model_inputs = self.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n \n             # prepare variable output controls (note: some models won't accept all output controls)\n             model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n             model_inputs.update({\"output_hidden_states\": output_hidden_states} if output_hidden_states else {})\n \n-            # if sequential is True, split the input to batches of batch_size and run sequentially\n-            if sequential:\n-                if any(\n-                    model_name in self.__class__.__name__.lower()\n-                    for model_name in [\n-                        \"fsmt\",\n-                        \"reformer\",\n-                        \"ctrl\",\n-                        \"gpt_bigcode\",\n-                        \"transo_xl\",\n-                        \"xlnet\",\n-                        \"cpm\",\n-                        \"jamba\",\n-                    ]\n-                ):\n-                    raise RuntimeError(\n-                        f\"Currently generation for {self.__class__.__name__} is not supported \"\n-                        f\"for `low_memory beam_search`. Please open an issue on GitHub if you need this feature.\"\n-                    )\n-\n-                inputs_per_sub_batches = _split_model_inputs(\n-                    model_inputs,\n-                    split_size=batch_size,\n-                    full_batch_size=batch_beam_size,\n-                    config=self.config.get_text_config(),\n-                )\n-                outputs_per_sub_batch = [\n-                    self(**inputs_per_sub_batch, return_dict=True) for inputs_per_sub_batch in inputs_per_sub_batches\n-                ]\n-\n-                outputs = stack_model_outputs(outputs_per_sub_batch, self.config.get_text_config())\n-\n-            else:  # Unchanged original behavior\n-                outputs = self(**model_inputs, return_dict=True)\n+            model_outputs = self(**model_inputs, return_dict=True)\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n             model_kwargs = self._update_model_kwargs_for_generation(\n-                outputs,\n+                model_outputs,\n                 model_kwargs,\n                 is_encoder_decoder=self.config.is_encoder_decoder,\n             )\n             if synced_gpus and this_peer_finished:\n-                cur_len = cur_len + 1\n                 continue\n \n-            # Clone is needed to avoid keeping a hanging ref to outputs.logits which may be very large for first iteration\n-            # (the clone itself is always small)\n-            # .float() is needed to retain precision for later logits manipulations\n-            next_token_logits = outputs.logits[:, -1, :].clone().float()\n-            next_token_logits = next_token_logits.to(input_ids.device)\n-            next_token_scores = nn.functional.log_softmax(\n-                next_token_logits, dim=-1\n-            )  # (batch_size * num_beams, vocab_size)\n+            logits = model_outputs.logits[:, -1, :].clone().float()  # Clone is needed to avoid keeping a hanging ref\n+            logits = logits.to(input_ids.device)\n \n-            next_token_scores_processed = logits_processor(input_ids, next_token_scores)\n-            next_token_scores = next_token_scores_processed + beam_scores[:, None].expand_as(\n-                next_token_scores_processed\n-            )\n+            # b. Compute log probs -- get log probabilities from logits, process logits with processors (*e.g.*\n+            # `temperature`, ...), and add new logprobs to existing running logprobs scores.\n+            log_probs = nn.functional.log_softmax(logits, dim=-1)\n+            log_probs = logits_processor(flat_running_sequences, log_probs)\n \n-            # Store scores, attentions and hidden_states when required\n+            # Store logits, attentions and hidden_states when required\n             if return_dict_in_generate:\n-                if output_scores:\n-                    scores += (next_token_scores_processed,)\n                 if output_logits:\n-                    raw_logits += (next_token_logits,)\n+                    raw_logits += (logits.clone(),)\n+                if return_dict_in_generate and output_scores:\n+                    all_scores += (log_probs.clone(),)\n+\n                 if output_attentions:\n                     decoder_attentions += (\n-                        (outputs.decoder_attentions,) if self.config.is_encoder_decoder else (outputs.attentions,)\n+                        (model_outputs.decoder_attentions,)\n+                        if self.config.is_encoder_decoder\n+                        else (model_outputs.attentions,)\n                     )\n                     if self.config.is_encoder_decoder:\n-                        cross_attentions += (outputs.cross_attentions,)\n+                        cross_attentions += (model_outputs.cross_attentions,)\n+\n                 if output_hidden_states:\n                     decoder_hidden_states += (\n-                        (outputs.decoder_hidden_states,)\n+                        (model_outputs.decoder_hidden_states,)\n                         if self.config.is_encoder_decoder\n-                        else (outputs.hidden_states,)\n+                        else (model_outputs.hidden_states,)\n                     )\n \n-            # reshape for beam search\n-            vocab_size = next_token_scores.shape[-1]\n-            next_token_scores = next_token_scores.view(batch_size, num_beams * vocab_size)\n+            # This is needed to properly delete logits which may be very large for first iteration\n+            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n+            del model_outputs\n+\n+            log_probs = self._unflatten_beam_dim(log_probs, batch_size, num_beams)\n+            log_probs = log_probs + running_beam_scores[:, :, None]\n+            log_probs = torch.reshape(log_probs, (batch_size, num_beams * vocab_size))\n+\n+            # c. Retrieve top-K continuations, i.e. select the next token (greedy or sampling) and then keep the best\n+            # continuations among all beams based on the accumulated scores.\n+            topk_log_probs, topk_running_sequences, topk_running_beam_indices = self._get_top_k_continuations(\n+                accumulated_log_probs=log_probs,\n+                running_sequences=running_sequences,\n+                running_beam_indices=running_beam_indices,\n+                cur_len=cur_len,\n+                decoder_prompt_len=decoder_prompt_len,\n+                do_sample=do_sample,\n+                beams_to_keep=beams_to_keep,\n+                num_beams=num_beams,\n+                vocab_size=vocab_size,\n+                batch_size=batch_size,\n+            )\n \n-            # Beam token selection: pick 1 + eos_token_id.shape[0] next tokens for each beam so we have at least 1\n-            # non eos token per beam.\n-            n_eos_tokens = eos_token_id.shape[0] if eos_token_id is not None else 0\n-            n_tokens_to_keep = max(2, 1 + n_eos_tokens) * num_beams\n-            if do_sample:\n-                probs = nn.functional.softmax(next_token_scores, dim=-1)\n-                next_tokens = torch.multinomial(probs, num_samples=n_tokens_to_keep)\n-                next_token_scores = torch.gather(next_token_scores, -1, next_tokens)\n-                next_token_scores, _indices = torch.sort(next_token_scores, descending=True, dim=1)\n-                next_tokens = torch.gather(next_tokens, -1, _indices)\n-            else:\n-                next_token_scores, next_tokens = torch.topk(\n-                    next_token_scores, n_tokens_to_keep, dim=1, largest=True, sorted=True\n-                )\n+            # d. Check which running sequences have finished\n+            next_token_hits_stopping_criteria = stopping_criteria(\n+                self._flatten_beam_dim(topk_running_sequences[:, :, : cur_len + 1]),  # remove unfilled token indexes\n+                all_scores,\n+            )\n+            next_token_hits_stopping_criteria = self._unflatten_beam_dim(\n+                next_token_hits_stopping_criteria, batch_size, beams_to_keep\n+            )\n \n-            next_indices = torch.div(next_tokens, vocab_size, rounding_mode=\"floor\")\n-            next_tokens = next_tokens % vocab_size\n+            # e. Get the non-finished running `num_beams` sequences for the next generation step\n+            running_sequences, running_beam_scores, running_beam_indices = self._get_running_beams_for_next_iteration(\n+                topk_log_probs=topk_log_probs,\n+                topk_running_sequences=topk_running_sequences,\n+                topk_running_beam_indices=topk_running_beam_indices,\n+                next_token_hits_stopping_criteria=next_token_hits_stopping_criteria,\n+                num_beams=num_beams,\n+            )\n \n-            # stateless\n-            beam_outputs = beam_scorer.process(\n-                input_ids,\n-                next_token_scores,\n-                next_tokens,\n-                next_indices,\n-                pad_token_id=pad_token_id,\n-                eos_token_id=eos_token_id,\n+            # f. Update the completed beams if a new high score in a finished sequence is found\n+            sequences, beam_scores, beam_indices, is_sent_finished = self._update_finished_beams(\n+                sequences=sequences,\n+                topk_running_sequences=topk_running_sequences,\n+                beam_scores=beam_scores,\n+                topk_log_probs=topk_log_probs,\n                 beam_indices=beam_indices,\n+                topk_running_beam_indices=topk_running_beam_indices,\n+                is_sent_finished=is_sent_finished,\n+                next_token_hits_stopping_criteria=next_token_hits_stopping_criteria,\n+                top_num_beam_mask=top_num_beam_mask,\n+                num_beams=num_beams,\n+                cur_len=cur_len,\n                 decoder_prompt_len=decoder_prompt_len,\n+                length_penalty=length_penalty,\n+                early_stopping=early_stopping,\n             )\n \n-            beam_scores = beam_outputs[\"next_beam_scores\"]\n-            beam_next_tokens = beam_outputs[\"next_beam_tokens\"]\n-            beam_idx = beam_outputs[\"next_beam_indices\"]\n-\n-            input_ids = torch.cat([input_ids[beam_idx, :], beam_next_tokens.unsqueeze(-1)], dim=-1)\n-\n-            # This is needed to properly delete outputs.logits which may be very large for first iteration\n-            # Otherwise a reference to outputs is kept which keeps the logits alive in the next iteration\n-            # IMPORTANT: Note that this should appear BEFORE the call to _reorder_cache() to save the maximum memory\n-            # (that way the memory peak does not include outputs.logits)\n-            del outputs\n+            # g. Prepare remaining data for the next iteration, including computing the stopping condition for\n+            # beam search as a whole (as opposed to individual beams, i.e. `stopping_criteria`)\n \n+            # pluck the cache from the beam indices that will be used in the next iteration\n             if model_kwargs.get(\"past_key_values\", None) is not None:\n                 model_kwargs[\"past_key_values\"] = self._temporary_reorder_cache(\n-                    model_kwargs[\"past_key_values\"], beam_idx\n+                    past_key_values=model_kwargs[\"past_key_values\"],\n+                    beam_idx=self._flatten_beam_dim(running_beam_indices[..., cur_len - decoder_prompt_len]),\n                 )\n \n-            if return_dict_in_generate and output_scores:\n-                beam_indices = tuple((beam_indices[beam_idx[i]] + (beam_idx[i],) for i in range(len(beam_indices))))\n-\n-            # increase cur_len\n             cur_len = cur_len + 1\n+            this_peer_finished = not self._beam_search_has_unfinished_sequences(\n+                running_beam_scores,\n+                beam_scores,\n+                is_sent_finished,\n+                next_token_hits_stopping_criteria,\n+                cur_len,\n+                max_length,\n+                decoder_prompt_len,\n+                early_stopping,\n+                length_penalty,\n+            )\n \n-            if beam_scorer.is_done or all(stopping_criteria(input_ids, scores)):\n-                this_peer_finished = True\n+        # 5. prepare outputs\n+        # Take best beams for each batch (the score is sorted in descending order)\n+        sequences = self._flatten_beam_dim(sequences[:, :num_return_sequences, :])\n+        beam_scores = self._flatten_beam_dim(beam_scores[:, :num_return_sequences])\n+        beam_indices = self._flatten_beam_dim(beam_indices[:, :num_return_sequences, :])\n \n-        sequence_outputs = beam_scorer.finalize(\n-            input_ids,\n-            beam_scores,\n-            next_tokens,\n-            next_indices,\n-            pad_token_id=pad_token_id,\n-            eos_token_id=eos_token_id,\n-            max_length=stopping_criteria.max_length,\n-            beam_indices=beam_indices,\n-            decoder_prompt_len=decoder_prompt_len,\n-        )\n+        # Crop the static-shaped tensors to the actual size\n+        sequences = sequences[:, :cur_len]\n+        beam_indices = beam_indices[:, : cur_len - decoder_prompt_len]\n \n         if return_dict_in_generate:\n             if not output_scores:\n-                sequence_outputs[\"sequence_scores\"] = None\n+                beam_scores = None\n \n             if self.config.is_encoder_decoder:\n                 return GenerateBeamEncoderDecoderOutput(\n-                    sequences=sequence_outputs[\"sequences\"],\n-                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n-                    scores=scores,\n+                    sequences=sequences,\n+                    sequences_scores=beam_scores,\n+                    scores=all_scores,\n                     logits=raw_logits,\n-                    beam_indices=sequence_outputs[\"beam_indices\"],\n+                    beam_indices=beam_indices,\n                     encoder_attentions=encoder_attentions,\n                     encoder_hidden_states=encoder_hidden_states,\n                     decoder_attentions=decoder_attentions,\n@@ -3690,17 +3916,17 @@ def _beam_search(\n                 )\n             else:\n                 return GenerateBeamDecoderOnlyOutput(\n-                    sequences=sequence_outputs[\"sequences\"],\n-                    sequences_scores=sequence_outputs[\"sequence_scores\"],\n-                    scores=scores,\n+                    sequences=sequences,\n+                    sequences_scores=beam_scores,\n+                    scores=all_scores,\n                     logits=raw_logits,\n-                    beam_indices=sequence_outputs[\"beam_indices\"],\n+                    beam_indices=beam_indices,\n                     attentions=decoder_attentions,\n                     hidden_states=decoder_hidden_states,\n                     past_key_values=model_kwargs.get(\"past_key_values\"),\n                 )\n         else:\n-            return sequence_outputs[\"sequences\"]\n+            return sequences\n \n     def _group_beam_search(\n         self,\n@@ -3717,7 +3943,7 @@ def _group_beam_search(\n         decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n \n         Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            input_ids (`torch.LongTensor` of shape `(batch_size*num_beams, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n             beam_scorer (`BeamScorer`):\n                 An derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and\n@@ -4008,7 +4234,7 @@ def _constrained_beam_search(\n         decoding** and can be used for text-decoder, text-to-text, speech-to-text, and vision-to-text models.\n \n         Parameters:\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            input_ids (`torch.LongTensor` of shape `(batch_size*num_beams, sequence_length)`):\n                 The sequence used as a prompt for the generation.\n             constrained_beam_scorer (`ConstrainedBeamSearchScorer`):\n                 A derived instance of [`BeamScorer`] that defines how beam hypotheses are constructed, stored and"
        },
        {
            "sha": "17a1f44aa1b434711753e8c7e4c8bf60e89f56aa",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...configuration_utils import PretrainedConfig\n-from ...generation import BeamSearchScorer, GenerationConfig, LogitsProcessorList, StoppingCriteriaList\n+from ...generation import GenerationConfig, LogitsProcessorList, StoppingCriteriaList\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n@@ -1563,18 +1563,8 @@ def extend_enc_output(tensor, num_beams=None):\n         elif generation_config.num_beams > 1:\n             if generation_config.num_return_sequences > generation_config.num_beams:\n                 raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n-            beam_scorer = BeamSearchScorer(\n-                batch_size=batch_size,\n-                num_beams=generation_config.num_beams,\n-                device=self.device,\n-                length_penalty=generation_config.length_penalty,\n-                do_early_stopping=generation_config.early_stopping,\n-                num_beam_hyps_to_keep=generation_config.num_return_sequences,\n-                max_length=generation_config.max_length,\n-            )\n             return self._beam_search(\n                 input_ids,\n-                beam_scorer,\n                 logits_processor=pre_processor,\n                 stopping_criteria=prepared_stopping_criteria,\n                 generation_config=generation_config,"
        },
        {
            "sha": "c166bbeec1290a4142354705d3ee7da138d5e338",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 77,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -1099,70 +1099,6 @@ def test_contrastive_generate_low_memory(self):\n             )\n             self.assertListEqual(low_output.tolist(), high_output.tolist())\n \n-    @pytest.mark.generate\n-    def test_beam_search_low_memory(self):\n-        # Check that choosing 'low_memory' does not change the model output\n-        for model_class in self.all_generative_model_classes:\n-            if model_class._is_stateful:\n-                self.skipTest(reason=\"May fix in the future: need custom cache handling\")\n-            if any(model_name in model_class.__name__.lower() for model_name in [\"fsmt\", \"reformer\"]):\n-                self.skipTest(reason=\"Won't fix: old model with different cache format\")\n-            if any(\n-                model_name in model_class.__name__.lower()\n-                for model_name in [\n-                    \"ctrl\",\n-                    \"gptbigcode\",\n-                    \"transo_xl\",\n-                    \"xlnet\",\n-                    \"cpm\",\n-                    \"jamba\",\n-                ]\n-            ):\n-                self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n-\n-            set_model_tester_for_less_flaky_test(self)\n-\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            set_config_for_less_flaky_test(config)\n-            # batch_size=1 is ok, but batch_size>1 will cause non-identical output\n-\n-            config.use_cache = True\n-            config.is_decoder = True\n-\n-            # test output equality of low versus high memory\n-            model = model_class(config).to(torch_device).eval()\n-            set_model_for_less_flaky_test(model)\n-\n-            logits_processor_kwargs = self._get_logits_processor_kwargs(config=model.config)\n-\n-            low_output = model.generate(\n-                **inputs_dict,\n-                max_new_tokens=8,\n-                num_beams=5,\n-                early_stopping=True,\n-                low_memory=True,\n-                use_cache=True,\n-                output_scores=True,\n-                output_logits=True,\n-                return_dict_in_generate=True,\n-                **logits_processor_kwargs,\n-            )\n-\n-            high_output = model.generate(\n-                **inputs_dict,\n-                max_new_tokens=8,\n-                num_beams=5,\n-                early_stopping=True,\n-                low_memory=False,\n-                use_cache=True,\n-                output_scores=True,\n-                output_logits=True,\n-                return_dict_in_generate=True,\n-                **logits_processor_kwargs,\n-            )\n-            # The two outputs must match and their shape must be as expected\n-            self._check_similar_generate_outputs(low_output, high_output)\n-\n     @parameterized.expand([(\"random\",), (\"same\",)])\n     @pytest.mark.generate\n     def test_assisted_decoding_matches_greedy_search(self, assistant_type):\n@@ -2964,19 +2900,6 @@ def test_transition_scores_group_beam_search_encoder_decoder(self):\n \n         torch.testing.assert_close(transition_scores_sum, outputs.sequences_scores, rtol=1e-3, atol=1e-3)\n \n-    def test_beam_search_low_memory(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-        model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        tokenizer.pad_token_id = tokenizer.eos_token_id\n-        model_inputs = tokenizer(\"I\", return_tensors=\"pt\")[\"input_ids\"]\n-\n-        low_output = model.generate(model_inputs, max_new_tokens=40, num_beams=5, early_stopping=True, low_memory=True)\n-\n-        high_output = model.generate(\n-            model_inputs, max_new_tokens=40, num_beams=5, early_stopping=True, low_memory=False\n-        )\n-        self.assertListEqual(low_output.tolist(), high_output.tolist())\n-\n     @slow\n     def test_green_red_watermark_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n@@ -4311,6 +4234,42 @@ def test_assisted_generation_early_exit(self):\n         self.assertEqual(decoded_assisted, [expected_output])\n \n     @slow\n+    def test_beam_search_advanced_stopping_criteria(self):\n+        \"\"\"\n+        Tests that beam search works with a stopping criteria that is not max length or EOS token. Prior to the beam\n+        search vectorization PR (#35802), beam search was not accepting other stopping criteria. Test inspired on\n+        the original issue (#34843).\n+        \"\"\"\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\")\n+        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-0.5B-Instruct\").to(torch_device)\n+\n+        prompt = (\n+            \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. \"\n+            \"How many clips did Natalia sell altogether in April and May?\"\n+        )\n+        tokens = tokenizer(prompt, return_tensors=\"pt\").to(torch_device)\n+        generation_config = GenerationConfig(num_beams=3, do_sample=False, length_penalty=1.0, max_new_tokens=100)\n+\n+        # This particular prompt should result in a \":\" being present in the answer\n+        out = model.generate(**tokens, generation_config=generation_config, tokenizer=tokenizer)\n+        output_text = tokenizer.decode(out[0], skip_special_tokens=True)\n+        last_non_special_token_decoded = tokenizer.decode(out[out != tokenizer.pad_token_id][-1])\n+        self.assertTrue(\":\" in output_text)\n+        self.assertFalse(\":\" in output_text[-5:])\n+        self.assertFalse(\":\" in last_non_special_token_decoded)\n+\n+        # Adding an advanced stopping criteria: text generation should stop when a \":\" is generated.\n+        # Note that:\n+        # 1 - the text up to \":\" doesn't have to be the same, it can belong to a different beam\n+        # 2 - \":\" may not be the last char, but it must be in the last non-special token\n+        generation_config.stop_strings = \":\"\n+        out = model.generate(**tokens, generation_config=generation_config, tokenizer=tokenizer)\n+        output_text = tokenizer.decode(out[0], skip_special_tokens=True)\n+        last_non_special_token_decoded = tokenizer.decode(out[out != tokenizer.pad_token_id][-1])\n+        self.assertTrue(\":\" in output_text)\n+        self.assertTrue(\":\" in output_text[-5:])\n+        self.assertTrue(\":\" in last_non_special_token_decoded)\n+\n     def test_max_time(self):\n         tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n         model = GPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")"
        },
        {
            "sha": "789649b832a8b48a806a76ffa888d6f04c5689eb",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -104,10 +104,6 @@ def test_dola_decoding_sample(self):\n     def test_generate_continue_from_past_key_values(self):\n         pass\n \n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @unittest.skip(\"Cohere2 has HybridCache and doesn't support contrastive generation\")\n     def test_contrastive_generate(self):\n         pass"
        },
        {
            "sha": "cf27d90a9b02e11beb160e9e800607c08588064d",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -119,10 +119,6 @@ def test_dola_decoding_sample(self):\n     def test_generate_continue_from_past_key_values(self):\n         pass\n \n-    @unittest.skip(\"Gemma2 has HybridCache and doesn't support low_memory generation\")\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @unittest.skip(\"Gemma2 has HybridCache and doesn't support contrastive generation\")\n     def test_contrastive_generate(self):\n         pass"
        },
        {
            "sha": "4b5bb61eb8f921f3dcf49ad588f938b723bc27d4",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -332,12 +332,6 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"Qwen2.5-VL can't do low-memory generation because position IDs have extra dimension and split function doesn't work for that\"\n-    )\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the tes for VLMs\"\n     )"
        },
        {
            "sha": "e1eafbf69385ebae279026ca59bc24104af19c6f",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/179d02ffb8f3e0f0f3330dd2762286e83cbaa65b/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=179d02ffb8f3e0f0f3330dd2762286e83cbaa65b",
            "patch": "@@ -344,12 +344,6 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"Qwen2-VL can't do low-memory generation because position IDs have extra dimension and split function doesn't work for that\"\n-    )\n-    def test_beam_search_low_memory(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the test for VLMs\"\n     )"
        }
    ],
    "stats": {
        "total": 697,
        "additions": 426,
        "deletions": 271
    }
}