{
    "author": "jiqing-feng",
    "message": "Enable xpu allocator on caching_allocator_warmup (#39654)\n\n* add xpu allocator\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix typo\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix variable name\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* rm useless default value\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "8db4d791618d396dba72742534ac456e5b9b5318",
    "files": [
        {
            "sha": "9db73955b069ebf8e8f7bd788b1767abd94c1ff3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 6,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8db4d791618d396dba72742534ac456e5b9b5318/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8db4d791618d396dba72742534ac456e5b9b5318/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8db4d791618d396dba72742534ac456e5b9b5318",
            "patch": "@@ -6021,19 +6021,22 @@ def caching_allocator_warmup(model: PreTrainedModel, expanded_device_map: dict,\n \n     # This will kick off the caching allocator to avoid having to Malloc afterwards\n     for device, byte_count in total_byte_count.items():\n-        if device.type == \"cuda\":\n-            index = device.index if device.index is not None else torch.cuda.current_device()\n-            device_memory = torch.cuda.mem_get_info(index)[0]\n+        if device.type in [\"cuda\", \"xpu\"]:\n+            torch_accelerator_module = getattr(torch, device.type)\n+            index = device.index if device.index is not None else torch_accelerator_module.current_device()\n+            device_memory = torch_accelerator_module.mem_get_info(index)[0]\n             # Allow up to (max device memory - 1.2 GiB) in resource-constrained hardware configurations. Trying to reserve more\n-            # than that amount might sometimes lead to unnecessary cuda OOM, if the last parameter to be loaded on the device is large,\n+            # than that amount might sometimes lead to unnecessary cuda/xpu OOM, if the last parameter to be loaded on the device is large,\n             # and the remaining reserved memory portion is smaller than the param size -> torch will then try to fully re-allocate all\n             # the param size, instead of using the remaining reserved part, and allocating only the difference, which can lead\n             # to OOM. See https://github.com/huggingface/transformers/issues/37436#issuecomment-2808982161 for more details.\n             # Note that we use an absolute value instead of device proportion here, as a 8GiB device could still allocate too much\n             # if using e.g. 90% of device size, while a 140GiB device would allocate too little\n             byte_count = min(byte_count, max(0, int(device_memory - 1.2 * 1024**3)))\n-            # If there is *unused* reserved cuda memory, we can skip/reduce the allocation.\n-            unused_memory = torch.cuda.memory_reserved(index) - torch.cuda.memory_allocated(index)\n+            # If there is *unused* reserved cuda/xpu memory, we can skip/reduce the allocation.\n+            unused_memory = torch_accelerator_module.memory_reserved(\n+                index\n+            ) - torch_accelerator_module.memory_allocated(index)\n             byte_count = max(0, byte_count - unused_memory)\n         # Allocate memory\n         _ = torch.empty(byte_count // factor, dtype=torch.float16, device=device, requires_grad=False)"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 9,
        "deletions": 6
    }
}