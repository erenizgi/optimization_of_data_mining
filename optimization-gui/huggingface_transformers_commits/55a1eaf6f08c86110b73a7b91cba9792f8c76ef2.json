{
    "author": "zucchini-nlp",
    "message": "Fix Qwen video tests (#41049)\n\nfix test",
    "sha": "55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
    "files": [
        {
            "sha": "5fcbb0c535f92b912e4638a7b2b5f6fd6bb58f81",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -62,8 +62,10 @@ class Qwen2_5OmniProcessorKwargs(ProcessingKwargs, total=False):\n             \"seconds_per_chunk\": 2.0,\n             \"position_id_per_seconds\": 25,\n             \"use_audio_in_video\": False,\n-            \"min_pixels\": 128 * 28 * 28,\n-            \"max_pixels\": 768 * 28 * 28,\n+            \"size\": {\n+                \"shortest_edge\": 128 * 28 * 28,\n+                \"longest_edge\": 768 * 28 * 28,\n+            },\n         },\n         \"audio_kwargs\": {\n             \"sampling_rate\": 16000,"
        },
        {
            "sha": "3fb020443f35553e1b21394f5f38f63cf8d10965",
            "filename": "src/transformers/models/qwen2_vl/video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 6,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fvideo_processing_qwen2_vl.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -186,7 +186,6 @@ def sample_frames(\n     def _preprocess(\n         self,\n         videos: list[\"torch.Tensor\"],\n-        do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n@@ -195,13 +194,10 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n-        min_pixels: Optional[int] = None,\n-        max_pixels: Optional[int] = None,\n         patch_size: Optional[int] = None,\n         temporal_patch_size: Optional[int] = None,\n         merge_size: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        device: Optional[\"torch.Tensor\"] = None,\n         **kwargs,\n     ):\n         # Group videos by size for batched resizing\n@@ -215,8 +211,8 @@ def _preprocess(\n                     height,\n                     width,\n                     factor=patch_size * merge_size,\n-                    min_pixels=min_pixels,\n-                    max_pixels=max_pixels,\n+                    min_pixels=size[\"shortest_edge\"],\n+                    max_pixels=size[\"longest_edge\"],\n                 )\n                 stacked_videos = self.resize(\n                     image=stacked_videos,"
        },
        {
            "sha": "28347f03a6aa9d5a45b46c8ec261347b7e02d8b6",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -2717,15 +2717,15 @@ def __call__(\n             audio_lengths = iter([])\n \n         if images is not None:\n-            images_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n+            images_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             image_grid_thw = iter(images_inputs[\"image_grid_thw\"])\n         else:\n             images_inputs = {}\n             image_grid_thw = iter([])\n \n         if videos is not None:\n             videos = make_batched_videos(videos)\n-            videos_inputs = self.video_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n             fps = [fps] * len(videos)\n             videos_inputs[\"video_second_per_grid\"] = [\n                 self.video_processor.temporal_patch_size / fps[i] for i in range(len(fps))"
        },
        {
            "sha": "86041fc3de161941b7e17c3105f9d6cee0c920c8",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -186,15 +186,15 @@ def __call__(\n             audio_lengths = iter([])\n \n         if images is not None:\n-            images_inputs = self.image_processor(images=images, videos=None, **output_kwargs[\"images_kwargs\"])\n+            images_inputs = self.image_processor(images=images, **output_kwargs[\"images_kwargs\"])\n             image_grid_thw = iter(images_inputs[\"image_grid_thw\"])\n         else:\n             images_inputs = {}\n             image_grid_thw = iter([])\n \n         if videos is not None:\n             videos = make_batched_videos(videos)\n-            videos_inputs = self.video_processor(images=None, videos=videos, **output_kwargs[\"videos_kwargs\"])\n+            videos_inputs = self.video_processor(videos=videos, **output_kwargs[\"videos_kwargs\"])\n             fps = [fps] * len(videos)\n             videos_inputs[\"video_second_per_grid\"] = [\n                 self.video_processor.temporal_patch_size / fps[i] for i in range(len(fps))"
        },
        {
            "sha": "6a510c89aafaeda1659bf0b4855acb68c615f86e",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -219,7 +219,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n                         {\n                             \"type\": \"video\",\n                             \"url\": url_to_local_path(\n-                                \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\"\n+                                \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/tiny_video.mp4\"\n                             ),\n                         },\n                         {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n@@ -251,7 +251,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_tensors=\"pt\",\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 300)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 11)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video"
        },
        {
            "sha": "c988e2d729177bf0dc9df2a05eb9f3f4c96e514d",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -213,6 +213,8 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         processor = Qwen2_5OmniProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+        processor.image_processor.size = {\"shortest_edge\": 28 * 28, \"longest_edge\": 56 * 56}\n+        processor.video_processor.size = {\"shortest_edge\": 28 * 28, \"longest_edge\": 56 * 56}\n         processor.save_pretrained(cls.tmpdirname)\n \n     def get_tokenizer(self, **kwargs):"
        },
        {
            "sha": "4d6026a0628995bcbaa99d910cb11ccd5541e013",
            "filename": "tests/models/qwen2_vl/test_video_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_video_processing_qwen2_vl.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -48,8 +48,6 @@ def __init__(\n         max_resolution=80,\n         do_resize=True,\n         size=None,\n-        do_center_crop=True,\n-        crop_size=None,\n         do_normalize=True,\n         image_mean=OPENAI_CLIP_MEAN,\n         image_std=OPENAI_CLIP_STD,\n@@ -61,7 +59,6 @@ def __init__(\n         merge_size=2,\n     ):\n         size = size if size is not None else {\"shortest_edge\": 20}\n-        crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_frames = num_frames\n@@ -70,8 +67,6 @@ def __init__(\n         self.max_resolution = max_resolution\n         self.do_resize = do_resize\n         self.size = size\n-        self.do_center_crop = do_center_crop\n-        self.crop_size = crop_size\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n@@ -85,8 +80,6 @@ def __init__(\n     def prepare_video_processor_dict(self):\n         return {\n             \"do_resize\": self.do_resize,\n-            \"do_center_crop\": self.do_center_crop,\n-            \"crop_size\": self.crop_size,\n             \"do_normalize\": self.do_normalize,\n             \"image_mean\": self.image_mean,\n             \"image_std\": self.image_std,\n@@ -149,8 +142,6 @@ def test_video_processor_properties(self):\n         video_processing = self.fast_video_processing_class(**self.video_processor_dict)\n         self.assertTrue(hasattr(video_processing, \"do_resize\"))\n         self.assertTrue(hasattr(video_processing, \"size\"))\n-        self.assertTrue(hasattr(video_processing, \"do_center_crop\"))\n-        self.assertTrue(hasattr(video_processing, \"center_crop\"))\n         self.assertTrue(hasattr(video_processing, \"do_normalize\"))\n         self.assertTrue(hasattr(video_processing, \"image_mean\"))\n         self.assertTrue(hasattr(video_processing, \"image_std\"))"
        },
        {
            "sha": "4c370e9286edfddb2114f5e64ce9fefaca0712e9",
            "filename": "tests/models/qwen3_omni_moe/test_processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_processing_qwen3_omni_moe.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -214,6 +214,8 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         processor = Qwen3OmniMoeProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n+        processor.image_processor.size = {\"shortest_edge\": 28 * 28, \"longest_edge\": 56 * 56}\n+        processor.video_processor.size = {\"shortest_edge\": 28 * 28, \"longest_edge\": 56 * 56}\n         processor.save_pretrained(cls.tmpdirname)\n \n     def get_tokenizer(self, **kwargs):\n@@ -463,7 +465,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             {\n                 \"type\": \"video\",\n                 \"url\": url_to_local_path(\n-                    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\"\n+                    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/tiny_video.mp4\"\n                 ),\n             }\n         )\n@@ -476,7 +478,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             num_frames=num_frames,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 9568)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 7728)\n \n         # Load with `fps` arg\n         fps = 1\n@@ -488,7 +490,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             fps=fps,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 23920)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 7728)\n \n         # Load with `fps` and `num_frames` args, should raise an error\n         with self.assertRaises(ValueError):\n@@ -509,7 +511,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 717600)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 23184)\n \n         # Load video as a list of frames (i.e. images). NOTE: each frame should have same size\n         # because we assume they come from one video\n@@ -527,7 +529,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n             return_dict=True,\n         )\n         self.assertTrue(self.videos_input_name in out_dict_with_video)\n-        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 11408)\n+        self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 7600)\n \n         # When the inputs are frame URLs/paths we expect that those are already\n         # sampled and will raise an error is asked to sample again."
        },
        {
            "sha": "d6d1938ccd572e0fc6e40d2c7d9e915c744a6fb7",
            "filename": "tests/models/qwen3_vl/test_processing_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/55a1eaf6f08c86110b73a7b91cba9792f8c76ef2/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl%2Ftest_processing_qwen3_vl.py?ref=55a1eaf6f08c86110b73a7b91cba9792f8c76ef2",
            "patch": "@@ -302,7 +302,7 @@ def test_apply_chat_template_video_frame_sampling(self):\n         # Add video URL for return dict and load with `num_frames` arg\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n-            \"url\": \"https://test-videos.co.uk/vids/bigbuckbunny/mp4/h264/720/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+            \"url\": \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/tiny_video.mp4\",\n         }\n         num_frames = 3\n         out_dict_with_video = processor.apply_chat_template("
        }
    ],
    "stats": {
        "total": 51,
        "additions": 22,
        "deletions": 29
    }
}