{
    "author": "yao-matrix",
    "message": "remove ipex_optimize_model usage (#38632)\n\n* remove ipex_optimize_model usage\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* update Dockerfile\n\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nSigned-off-by: root <root@a4bf01945cfe.jf.intel.com>\nCo-authored-by: root <root@a4bf01945cfe.jf.intel.com>",
    "sha": "dc76eff12b4fc19149edf44b23bc6b8050c5e566",
    "files": [
        {
            "sha": "e38b220624fb67564feaf978e3e67abbe7fa4acf",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -10,8 +10,6 @@ SHELL [\"sh\", \"-lc\"]\n # to be used as arguments for docker build (so far).\n \n ARG PYTORCH='2.6.0'\n-# (not always a valid torch version)\n-ARG INTEL_TORCH_EXT='2.3.0'\n # Example: `cu102`, `cu113`, etc.\n ARG CUDA='cu121'\n # Disable kernel mapping for now until all tests pass\n@@ -32,8 +30,6 @@ RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] &&\n \n RUN python3 -m pip uninstall -y flax jax\n \n-RUN python3 -m pip install --no-cache-dir intel_extension_for_pytorch==$INTEL_TORCH_EXT -f https://developer.intel.com/ipex-whl-stable-cpu\n-\n RUN python3 -m pip install --no-cache-dir git+https://github.com/facebookresearch/detectron2.git pytesseract\n RUN python3 -m pip install -U \"itsdangerous<2.1.0\"\n "
        },
        {
            "sha": "af93d3cabd69b7cfa9d95935fe231432057df3e2",
            "filename": "docs/source/en/perf_infer_cpu.md",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_infer_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_cpu.md?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -78,26 +78,3 @@ python examples/pytorch/question-answering/run_qa.py \\\n --no_cuda \\\n --jit_mode_eval\n ```\n-\n-## IPEX\n-\n-[Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/getting_started.html) (IPEX) offers additional optimizations for PyTorch on Intel CPUs. IPEX further optimizes TorchScript with [graph optimization](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/graph_optimization.html) which fuses operations like Multi-head attention, Concat Linear, Linear + Add, Linear + Gelu, Add + LayerNorm, and more, into single kernels for faster execution.\n-\n-Make sure IPEX is installed, and set the `--use_opex` and `--jit_mode_eval` flags in [`Trainer`] to enable IPEX graph optimization and TorchScript.\n-\n-```bash\n-!pip install intel_extension_for_pytorch\n-```\n-\n-```bash\n-python examples/pytorch/question-answering/run_qa.py \\\n---model_name_or_path csarron/bert-base-uncased-squad-v1 \\\n---dataset_name squad \\\n---do_eval \\\n---max_seq_length 384 \\\n---doc_stride 128 \\\n---output_dir /tmp/ \\\n---no_cuda \\\n---use_ipex \\\n---jit_mode_eval\n-```"
        },
        {
            "sha": "29641a99843fe74d7b86b0ff9ce2603a744da10a",
            "filename": "docs/source/en/perf_train_cpu.md",
            "status": "modified",
            "additions": 2,
            "deletions": 25,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_train_cpu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_train_cpu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu.md?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -17,30 +17,9 @@ rendered properly in your Markdown viewer.\n \n A modern CPU is capable of efficiently training large models by leveraging the underlying optimizations built into the hardware and training on fp16 or bf16 data types.\n \n-This guide focuses on how to train large models on an Intel CPU using mixed precision and the [Intel Extension for PyTorch (IPEX)](https://intel.github.io/intel-extension-for-pytorch/index.html) library.\n+This guide focuses on how to train large models on an Intel CPU using mixed precision. AMP is enabled for CPU backends training with PyTorch.\n \n-You can Find your PyTorch version by running the command below.\n-\n-```bash\n-pip list | grep torch\n-```\n-\n-Install IPEX with the PyTorch version from above.\n-\n-```bash\n-pip install intel_extension_for_pytorch==<version_name> -f https://developer.intel.com/ipex-whl-stable-cpu\n-```\n-\n-> [!TIP]\n-> Refer to the IPEX [installation](https://intel.github.io/intel-extension-for-pytorch/index.html#installation) guide for more details.\n-\n-IPEX provides additional performance optimizations for Intel CPUs. These include additional CPU instruction level architecture (ISA) support such as [Intel AVX512-VNNI](https://en.wikichip.org/wiki/x86/avx512_vnni) and [Intel AMX](https://www.intel.com/content/www/us/en/products/docs/accelerator-engines/what-is-intel-amx.html). Both of these features are designed to accelerate matrix multiplication. Older AMD and Intel CPUs with only Intel AVX2, however, aren't guaranteed better performance with IPEX.\n-\n-IPEX also supports [Auto Mixed Precision (AMP)](https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/features/amp.html) training with the fp16 and bf16 data types. Reducing precision speeds up training and reduces memory usage because it requires less computation. The loss in accuracy from using full-precision is minimal. 3rd, 4th, and 5th generation Intel Xeon Scalable processors natively support bf16, and the 6th generation processor also natively supports fp16 in addition to bf16.\n-\n-AMP is enabled for CPU backends training with PyTorch.\n-\n-[`Trainer`] supports AMP training with a CPU by adding the `--use_cpu`, `--use_ipex`, and `--bf16` parameters. The example below demonstrates the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script.\n+[`Trainer`] supports AMP training with CPU by adding the `--use_cpu`, and `--bf16` parameters. The example below demonstrates the [run_qa.py](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering) script.\n \n ```bash\n python run_qa.py \\\n@@ -54,7 +33,6 @@ python run_qa.py \\\n  --max_seq_length 384 \\\n  --doc_stride 128 \\\n  --output_dir /tmp/debug_squad/ \\\n- --use_ipex \\\n  --bf16 \\\n  --use_cpu\n ```\n@@ -65,7 +43,6 @@ These parameters can also be added to [`TrainingArguments`] as shown below.\n training_args = TrainingArguments(\n     output_dir=\"./outputs\",\n     bf16=True,\n-    use_ipex=True,\n     use_cpu=True,\n )\n ```"
        },
        {
            "sha": "0de9f895a78ad86e1ca0c388989de1ded3690ea5",
            "filename": "docs/source/en/perf_train_cpu_many.md",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_cpu_many.md?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -75,8 +75,7 @@ python3 run_qa.py \\\n  --doc_stride 128  \\\n  --output_dir /tmp/debug_squad/ \\\n  --no_cuda \\\n- --ddp_backend ccl \\\n- --use_ipex\n+ --ddp_backend ccl\n ```\n \n </hfoption>\n@@ -115,7 +114,6 @@ python3 run_qa.py \\\n  --output_dir /tmp/debug_squad/ \\\n  --no_cuda \\\n  --ddp_backend ccl \\\n- --use_ipex \\\n  --bf16\n ```\n \n@@ -201,8 +199,7 @@ spec:\n                     --output_dir /tmp/pvc-mount/output_$(date +%Y%m%d_%H%M%S) \\\n                     --no_cuda \\\n                     --ddp_backend ccl \\\n-                    --bf16 \\\n-                    --use_ipex;\n+                    --bf16;\n               env:\n               - name: LD_PRELOAD\n                 value: \"/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4.5.9:/usr/local/lib/libiomp5.so\""
        },
        {
            "sha": "533f5f5ae6e34b4484569502ffeda2e658958c5d",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -157,7 +157,6 @@\n     is_galore_torch_available,\n     is_grokadamw_available,\n     is_in_notebook,\n-    is_ipex_available,\n     is_liger_kernel_available,\n     is_lomo_available,\n     is_peft_available,\n@@ -1916,29 +1915,6 @@ def torch_jit_model_eval(self, model, dataloader, training=False):\n \n         return model\n \n-    def ipex_optimize_model(self, model, training=False, dtype=torch.float32):\n-        if not is_ipex_available():\n-            raise ImportError(\n-                \"Using IPEX but IPEX is not installed or IPEX's version does not match current PyTorch, please refer\"\n-                \" to https://github.com/intel/intel-extension-for-pytorch.\"\n-            )\n-\n-        import intel_extension_for_pytorch as ipex\n-\n-        if not training:\n-            model.eval()\n-            dtype = torch.bfloat16 if not self.is_in_train and self.args.bf16_full_eval else dtype\n-            # conv_bn_folding is disabled as it fails in symbolic tracing, resulting in ipex warnings\n-            model = ipex.optimize(model, dtype=dtype, level=\"O1\", conv_bn_folding=False, inplace=not self.is_in_train)\n-        else:\n-            if not model.training:\n-                model.train()\n-            model, self.optimizer = ipex.optimize(\n-                model, dtype=dtype, optimizer=self.optimizer, inplace=True, level=\"O1\"\n-            )\n-\n-        return model\n-\n     def compare_trainer_and_checkpoint_args(self, training_args, trainer_state):\n         attributes_map = {\n             \"logging_steps\": \"logging_steps\",\n@@ -1968,10 +1944,6 @@ def compare_trainer_and_checkpoint_args(self, training_args, trainer_state):\n             logger.warning_once(warning_str)\n \n     def _wrap_model(self, model, training=True, dataloader=None):\n-        if self.args.use_ipex:\n-            dtype = torch.bfloat16 if self.use_cpu_amp else torch.float32\n-            model = self.ipex_optimize_model(model, training, dtype=dtype)\n-\n         if is_sagemaker_mp_enabled():\n             # Wrapping the base model twice in a DistributedModel will raise an error.\n             if isinstance(self.model_wrapped, smp.model.DistributedModel):"
        },
        {
            "sha": "bbb1f22350eaa51d7d1708135cdeb451ea6d3a74",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -1581,6 +1581,12 @@ def __post_init__(self):\n                 FutureWarning,\n             )\n             self.use_cpu = self.no_cuda\n+        if self.use_ipex:\n+            warnings.warn(\n+                \"using `use_ipex` is deprecated and will be removed in version 4.54 of ðŸ¤— Transformers. \"\n+                \"You only need PyTorch for the needed optimizations on Intel CPU and XPU.\",\n+                FutureWarning,\n+            )\n \n         self.eval_strategy = IntervalStrategy(self.eval_strategy)\n         self.logging_strategy = IntervalStrategy(self.logging_strategy)"
        },
        {
            "sha": "65a16066c979c3d8d071f80b70b0cd65d7b448b6",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 146,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/dc76eff12b4fc19149edf44b23bc6b8050c5e566/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dc76eff12b4fc19149edf44b23bc6b8050c5e566/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=dc76eff12b4fc19149edf44b23bc6b8050c5e566",
            "patch": "@@ -79,7 +79,6 @@\n     require_deepspeed,\n     require_galore_torch,\n     require_grokadamw,\n-    require_intel_extension_for_pytorch,\n     require_liger_kernel,\n     require_lomo,\n     require_non_hpu,\n@@ -1325,37 +1324,6 @@ def test_number_of_steps_in_training(self):\n         train_output = trainer.train()\n         self.assertEqual(train_output.global_step, 10)\n \n-    @require_torch_bf16\n-    @require_intel_extension_for_pytorch\n-    def test_number_of_steps_in_training_with_ipex(self):\n-        for mix_bf16 in [True, False]:\n-            tmp_dir = self.get_auto_remove_tmp_dir()\n-            # Regular training has n_epochs * len(train_dl) steps\n-            trainer = get_regression_trainer(\n-                learning_rate=0.1, use_ipex=True, bf16=mix_bf16, use_cpu=True, output_dir=tmp_dir\n-            )\n-            train_output = trainer.train()\n-            self.assertEqual(train_output.global_step, self.n_epochs * 64 / trainer.args.train_batch_size)\n-\n-            # Check passing num_train_epochs works (and a float version too):\n-            trainer = get_regression_trainer(\n-                learning_rate=0.1,\n-                num_train_epochs=1.5,\n-                use_ipex=True,\n-                bf16=mix_bf16,\n-                use_cpu=True,\n-                output_dir=tmp_dir,\n-            )\n-            train_output = trainer.train()\n-            self.assertEqual(train_output.global_step, int(1.5 * 64 / trainer.args.train_batch_size))\n-\n-            # If we pass a max_steps, num_train_epochs is ignored\n-            trainer = get_regression_trainer(\n-                learning_rate=0.1, max_steps=10, use_ipex=True, bf16=mix_bf16, use_cpu=True, output_dir=tmp_dir\n-            )\n-            train_output = trainer.train()\n-            self.assertEqual(train_output.global_step, 10)\n-\n     def test_torch_compile_loss_func_compatibility(self):\n         config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n         tiny_llama = LlamaForCausalLM(config)\n@@ -2628,69 +2596,6 @@ def test_evaluate_with_jit(self):\n             expected_acc = AlmostAccuracy()((pred + 1, y))[\"accuracy\"]\n             self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n \n-    @require_torch_bf16\n-    @require_intel_extension_for_pytorch\n-    def test_evaluate_with_ipex(self):\n-        for mix_bf16 in [True, False]:\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                trainer = get_regression_trainer(\n-                    a=1.5,\n-                    b=2.5,\n-                    use_ipex=True,\n-                    compute_metrics=AlmostAccuracy(),\n-                    bf16=mix_bf16,\n-                    use_cpu=True,\n-                    output_dir=tmp_dir,\n-                )\n-                results = trainer.evaluate()\n-\n-                x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-                pred = 1.5 * x + 2.5\n-                expected_loss = ((pred - y) ** 2).mean()\n-                self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-                expected_acc = AlmostAccuracy()((pred, y))[\"accuracy\"]\n-                self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n-                # With a number of elements not a round multiple of the batch size\n-                trainer = get_regression_trainer(\n-                    a=1.5,\n-                    b=2.5,\n-                    use_ipex=True,\n-                    eval_len=66,\n-                    compute_metrics=AlmostAccuracy(),\n-                    bf16=mix_bf16,\n-                    use_cpu=True,\n-                    output_dir=tmp_dir,\n-                )\n-                results = trainer.evaluate()\n-\n-                x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-                pred = 1.5 * x + 2.5\n-                expected_loss = ((pred - y) ** 2).mean()\n-                self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-                expected_acc = AlmostAccuracy()((pred, y))[\"accuracy\"]\n-                self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n-                # With logits preprocess\n-                trainer = get_regression_trainer(\n-                    a=1.5,\n-                    b=2.5,\n-                    use_ipex=True,\n-                    compute_metrics=AlmostAccuracy(),\n-                    preprocess_logits_for_metrics=lambda logits, labels: logits + 1,\n-                    bf16=mix_bf16,\n-                    use_cpu=True,\n-                    output_dir=tmp_dir,\n-                )\n-                results = trainer.evaluate()\n-\n-                x, y = trainer.eval_dataset.x, trainer.eval_dataset.ys[0]\n-                pred = 1.5 * x + 2.5\n-                expected_loss = ((pred - y) ** 2).mean()\n-                self.assertAlmostEqual(results[\"eval_loss\"], expected_loss)\n-                expected_acc = AlmostAccuracy()((pred + 1, y))[\"accuracy\"]\n-                self.assertAlmostEqual(results[\"eval_accuracy\"], expected_acc)\n-\n     def test_predict(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             trainer = get_regression_trainer(a=1.5, b=2.5, output_dir=tmp_dir)\n@@ -2830,57 +2735,6 @@ def test_predict_with_jit(self):\n             self.assertTrue(np.array_equal(labels[0], trainer.eval_dataset.ys[0]))\n             self.assertTrue(np.array_equal(labels[1], trainer.eval_dataset.ys[1]))\n \n-    @require_torch_bf16\n-    @require_intel_extension_for_pytorch\n-    def test_predict_with_ipex(self):\n-        for mix_bf16 in [True, False]:\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                trainer = get_regression_trainer(\n-                    a=1.5, b=2.5, use_ipex=True, bf16=mix_bf16, use_cpu=True, output_dir=tmp_dir\n-                )\n-                preds = trainer.predict(trainer.eval_dataset).predictions\n-                x = trainer.eval_dataset.x\n-                self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n-\n-                # With a number of elements not a round multiple of the batch size\n-                trainer = get_regression_trainer(\n-                    a=1.5, b=2.5, eval_len=66, use_ipex=True, bf16=mix_bf16, use_cpu=True, output_dir=tmp_dir\n-                )\n-                preds = trainer.predict(trainer.eval_dataset).predictions\n-                x = trainer.eval_dataset.x\n-                self.assertTrue(np.allclose(preds, 1.5 * x + 2.5))\n-\n-                # With more than one output of the model\n-                trainer = get_regression_trainer(\n-                    a=1.5, b=2.5, double_output=True, use_ipex=True, bf16=mix_bf16, use_cpu=True, output_dir=tmp_dir\n-                )\n-                preds = trainer.predict(trainer.eval_dataset).predictions\n-                x = trainer.eval_dataset.x\n-                self.assertEqual(len(preds), 2)\n-                self.assertTrue(np.allclose(preds[0], 1.5 * x + 2.5))\n-                self.assertTrue(np.allclose(preds[1], 1.5 * x + 2.5))\n-\n-                # With more than one output/label of the model\n-                trainer = get_regression_trainer(\n-                    a=1.5,\n-                    b=2.5,\n-                    double_output=True,\n-                    label_names=[\"labels\", \"labels_2\"],\n-                    use_ipex=True,\n-                    bf16=mix_bf16,\n-                    use_cpu=True,\n-                    output_dir=tmp_dir,\n-                )\n-                outputs = trainer.predict(trainer.eval_dataset)\n-                preds = outputs.predictions\n-                labels = outputs.label_ids\n-                x = trainer.eval_dataset.x\n-                self.assertEqual(len(preds), 2)\n-                self.assertTrue(np.allclose(preds[0], 1.5 * x + 2.5))\n-                self.assertTrue(np.allclose(preds[1], 1.5 * x + 2.5))\n-                self.assertTrue(np.array_equal(labels[0], trainer.eval_dataset.ys[0]))\n-                self.assertTrue(np.array_equal(labels[1], trainer.eval_dataset.ys[1]))\n-\n     def test_dynamic_shapes(self):\n         eval_dataset = DynamicShapesDataset(batch_size=self.batch_size)\n         model = RegressionModel(a=2, b=1)"
        }
    ],
    "stats": {
        "total": 241,
        "additions": 10,
        "deletions": 231
    }
}