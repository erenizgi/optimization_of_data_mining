{
    "author": "dvrogozh",
    "message": "tests: fix pytorch tensor placement errors (#33485)\n\nThis commit fixes the following errors:\r\n* Fix \"expected all tensors to be on the same device\" error\r\n* Fix \"can't convert device type tensor to numpy\"\r\n\r\nAccording to pytorch documentation torch.Tensor.numpy(force=False)\r\nperforms conversion only if tensor is on CPU (plus few other restrictions)\r\nwhich is not the case. For our case we need force=True since we just\r\nneed a data and don't care about tensors coherency.\r\n\r\nFixes: #33517\r\nSee: https://pytorch.org/docs/2.4/generated/torch.Tensor.numpy.html\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
    "files": [
        {
            "sha": "8bbd8587b683f47cd5ff73371836d810379f1e33",
            "filename": "src/transformers/modeling_flax_pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_pytorch_utils.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -163,7 +163,7 @@ def convert_pytorch_state_dict_to_flax(pt_state_dict, flax_model):\n             # numpy currently does not support bfloat16, need to go over float32 in this case to not lose precision\n             if v.dtype == bfloat16:\n                 v = v.float()\n-            pt_state_dict[k] = v.numpy()\n+            pt_state_dict[k] = v.cpu().numpy()\n \n     model_prefix = flax_model.base_model_prefix\n "
        },
        {
            "sha": "bd9664dd15fd325ed39865dea2cc605c555fc9fe",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -848,6 +848,7 @@ def test_equivalence_pt_to_flax(self):\n             with self.subTest(model_class.__name__):\n                 # load PyTorch class\n                 pt_model = model_class(config).eval()\n+                pt_model.to(torch_device)\n                 # Flax models don't use the `use_cache` option and cache is not returned as a default.\n                 # So we disable `use_cache` here for PyTorch model.\n                 pt_model.config.use_cache = False\n@@ -881,7 +882,7 @@ def test_equivalence_pt_to_flax(self):\n                 fx_outputs = fx_model(**fx_inputs).to_tuple()\n                 self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n                 for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n+                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n \n                 with tempfile.TemporaryDirectory() as tmpdirname:\n                     pt_model.save_pretrained(tmpdirname)\n@@ -892,7 +893,7 @@ def test_equivalence_pt_to_flax(self):\n                     len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\"\n                 )\n                 for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 4e-2)\n+                    self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n \n     # overwrite from common since FlaxCLIPModel returns nested output\n     # which is not supported in the common test\n@@ -921,6 +922,7 @@ def test_equivalence_flax_to_pt(self):\n                 fx_input_keys = inspect.signature(fx_model.__call__).parameters.keys()\n \n                 pt_model = load_flax_weights_in_pytorch_model(pt_model, fx_model.params)\n+                pt_model.to(torch_device)\n \n                 # make sure weights are tied in PyTorch\n                 pt_model.tie_weights()\n@@ -940,11 +942,12 @@ def test_equivalence_flax_to_pt(self):\n                 self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n \n                 for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n+                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n \n                 with tempfile.TemporaryDirectory() as tmpdirname:\n                     fx_model.save_pretrained(tmpdirname)\n                     pt_model_loaded = model_class.from_pretrained(tmpdirname, from_flax=True)\n+                    pt_model_loaded.to(torch_device)\n \n                 with torch.no_grad():\n                     pt_outputs_loaded = pt_model_loaded(**pt_inputs).to_tuple()\n@@ -953,7 +956,7 @@ def test_equivalence_flax_to_pt(self):\n                     len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\"\n                 )\n                 for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-                    self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n+                    self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "35434a280e9ae0cf6ef34c823f92ec93735d2326",
            "filename": "tests/models/encoder_decoder/test_modeling_flax_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -297,15 +297,15 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         # prepare inputs\n         flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n+        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n \n         with torch.no_grad():\n             pt_outputs = pt_model(**pt_inputs).to_tuple()\n \n         fx_outputs = fx_model(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n \n         # PT -> Flax\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -315,7 +315,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n         fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n \n         # Flax -> PT\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -330,7 +330,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n \n     def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n         encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)"
        },
        {
            "sha": "10cb2b71824e99be536e66588562af75ce9a362f",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -170,7 +170,7 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n \n         embed_positions = InformerSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n-        )\n+        ).to(torch_device)\n         self.parent.assertTrue(torch.equal(model.encoder.embed_positions.weight, embed_positions.weight))\n         self.parent.assertTrue(torch.equal(model.decoder.embed_positions.weight, embed_positions.weight))\n "
        },
        {
            "sha": "8f210a07d278a64b98ebe1754760e847af893450",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -412,15 +412,15 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         # prepare inputs\n         flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n+        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n \n         with torch.no_grad():\n             pt_outputs = pt_model(**pt_inputs).to_tuple()\n \n         fx_outputs = fx_model(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n \n         # PT -> Flax\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -430,7 +430,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n         fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n \n         # Flax -> PT\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -445,7 +445,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n \n     def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n         encoder_decoder_config = SpeechEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)"
        },
        {
            "sha": "fabef4b8c6de04d9e4008f57abdc7862f5668f7d",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_flax_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -241,15 +241,15 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         # prepare inputs\n         flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n+        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n \n         with torch.no_grad():\n             pt_outputs = pt_model(**pt_inputs).to_tuple()\n \n         fx_outputs = fx_model(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output in zip(fx_outputs, pt_outputs):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 1e-5)\n \n         # PT -> Flax\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -259,7 +259,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n         fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output_loaded, pt_output in zip(fx_outputs_loaded, pt_outputs):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 1e-5)\n \n         # Flax -> PT\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -274,7 +274,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output_loaded in zip(fx_outputs, pt_outputs_loaded):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(), 1e-5)\n+            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 1e-5)\n \n     def check_equivalence_pt_to_flax(self, config, decoder_config, inputs_dict):\n         encoder_decoder_config = VisionEncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)"
        },
        {
            "sha": "e1e8eb4076c13750a0219bea09a1dcbfe6282900",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_flax_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -160,15 +160,15 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         # prepare inputs\n         flax_inputs = inputs_dict\n-        pt_inputs = {k: torch.tensor(v.tolist()) for k, v in flax_inputs.items()}\n+        pt_inputs = {k: torch.tensor(v.tolist()).to(torch_device) for k, v in flax_inputs.items()}\n \n         with torch.no_grad():\n             pt_outputs = pt_model(**pt_inputs).to_tuple()\n \n         fx_outputs = fx_model(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n \n         # PT -> Flax\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -178,7 +178,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n         fx_outputs_loaded = fx_model_loaded(**inputs_dict).to_tuple()\n         self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n \n         # Flax -> PT\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -193,7 +193,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, inputs_dict):\n \n         self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output_loaded in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 4e-2)\n \n     def check_equivalence_pt_to_flax(self, vision_config, text_config, inputs_dict):\n         config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)"
        },
        {
            "sha": "d935c0d27d1ccba7d3d2c96262594555cbdb24ab",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e2916bc143d615ea22bcafee9d3a37c1de0ae1e/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py?ref=5e2916bc143d615ea22bcafee9d3a37c1de0ae1e",
            "patch": "@@ -179,15 +179,15 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, input_ids, attention_mas\n         # prepare inputs\n         inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"pixel_values\": pixel_values}\n         pt_inputs = inputs_dict\n-        flax_inputs = {k: v.numpy() for k, v in pt_inputs.items()}\n+        flax_inputs = {k: v.numpy(force=True) for k, v in pt_inputs.items()}\n \n         with torch.no_grad():\n             pt_outputs = pt_model(**pt_inputs).to_tuple()\n \n         fx_outputs = fx_model(**flax_inputs).to_tuple()\n         self.assertEqual(len(fx_outputs), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output in zip(fx_outputs[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output, pt_output.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output, pt_output.numpy(force=True), 4e-2)\n \n         # PT -> Flax\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -197,7 +197,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, input_ids, attention_mas\n         fx_outputs_loaded = fx_model_loaded(**flax_inputs).to_tuple()\n         self.assertEqual(len(fx_outputs_loaded), len(pt_outputs), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output_loaded, pt_output in zip(fx_outputs_loaded[:4], pt_outputs[:4]):\n-            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output_loaded, pt_output.numpy(force=True), 4e-2)\n \n         # Flax -> PT\n         with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -212,7 +212,7 @@ def check_pt_flax_equivalence(self, pt_model, fx_model, input_ids, attention_mas\n \n         self.assertEqual(len(fx_outputs), len(pt_outputs_loaded), \"Output lengths differ between Flax and PyTorch\")\n         for fx_output, pt_output_loaded in zip(fx_outputs[:4], pt_outputs_loaded[:4]):\n-            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(), 4e-2)\n+            self.assert_almost_equals(fx_output, pt_output_loaded.numpy(force=True), 4e-2)\n \n     def check_equivalence_pt_to_flax(self, vision_config, text_config, inputs_dict):\n         config = VisionTextDualEncoderConfig.from_vision_text_configs(vision_config, text_config)"
        }
    ],
    "stats": {
        "total": 55,
        "additions": 29,
        "deletions": 26
    }
}