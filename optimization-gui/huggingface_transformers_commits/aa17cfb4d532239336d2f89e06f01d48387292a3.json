{
    "author": "FightingZhen",
    "message": "[Bugfix] Fix flash-attention func param mismatch and softmax_scale default value mistake on Ascend NPU (#37575)\n\n[Bugfix] fix flash-attention func param mismatch and softmax_scale default value mistake on Ascend NPU\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>",
    "sha": "aa17cfb4d532239336d2f89e06f01d48387292a3",
    "files": [
        {
            "sha": "f913731299e8c85dedb3a0acf89e009b58899bcc",
            "filename": "src/transformers/integrations/npu_flash_attention.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa17cfb4d532239336d2f89e06f01d48387292a3/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa17cfb4d532239336d2f89e06f01d48387292a3/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fnpu_flash_attention.py?ref=aa17cfb4d532239336d2f89e06f01d48387292a3",
            "patch": "@@ -19,6 +19,8 @@\n \n \n if is_torch_npu_available():\n+    import math\n+\n     import torch_npu\n     from einops import rearrange, repeat\n \n@@ -162,6 +164,9 @@ def npu_flash_attn_func(\n ):\n     keep_prob = 1.0 - dropout_p\n \n+    if softmax_scale is None:\n+        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n+\n     if not causal:\n         head_num = q.shape[2]\n         output = torch_npu.npu_fusion_attention(q, k, v, head_num, \"BSND\", keep_prob=keep_prob, scale=softmax_scale)[0]\n@@ -189,13 +194,18 @@ def npu_flash_attn_varlen_func(\n     v,\n     cu_seqlens_q,\n     cu_seqlens_k,\n+    max_seqlen_q=None,  # defined for aligning params order with corresponding function in `flash-attn`\n+    max_seqlen_k=None,  # defined for aligning params order with corresponding function in `flash-attn`\n     dropout_p=0.0,\n     softmax_scale=None,\n     causal=False,\n     **kwargs,\n ):\n     keep_prob = 1.0 - dropout_p\n \n+    if softmax_scale is None:\n+        softmax_scale = 1.0 / math.sqrt(q.shape[-1])\n+\n     if not causal:\n         head_num = q.shape[1]\n         output = torch_npu.npu_fusion_attention("
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}