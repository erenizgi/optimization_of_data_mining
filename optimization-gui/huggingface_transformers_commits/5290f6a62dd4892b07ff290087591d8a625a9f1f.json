{
    "author": "stevhliu",
    "message": "[docs] Fix FlashAttention link (#35171)\n\nfix link",
    "sha": "5290f6a62dd4892b07ff290087591d8a625a9f1f",
    "files": [
        {
            "sha": "b9b51082f29e5b3f0ec436178b675524b9d745a0",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=5290f6a62dd4892b07ff290087591d8a625a9f1f",
            "patch": "@@ -141,7 +141,7 @@ Do note that when training Idefics2 on multi-turn conversations between a user a\n \n ## Model optimizations: Flash Attention\n \n-The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one.md#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n+The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n \n First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n "
        },
        {
            "sha": "cc3a61aae6c736d21070ab2ec172cda7ad9b392b",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=5290f6a62dd4892b07ff290087591d8a625a9f1f",
            "patch": "@@ -240,7 +240,7 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\"llava-hf/LLaVA-N\n \n ### Flash-Attention 2 to speed-up generation\n \n-Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one.md#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n+Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n \n First, make sure to install the latest version of Flash Attention 2:\n "
        },
        {
            "sha": "cfa2af3678137a719af30898bc41ff94a170b548",
            "filename": "docs/source/en/model_doc/mistral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral.md?ref=5290f6a62dd4892b07ff290087591d8a625a9f1f",
            "patch": "@@ -91,7 +91,7 @@ As can be seen, the instruction-tuned model requires a [chat template](../chat_t\n \n ## Speeding up Mistral by using Flash Attention\n \n-The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one.md#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n+The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n \n First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n "
        },
        {
            "sha": "b5451702e44a1683d68c388964c479112194b23b",
            "filename": "docs/source/en/model_doc/mixtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmixtral.md?ref=5290f6a62dd4892b07ff290087591d8a625a9f1f",
            "patch": "@@ -93,7 +93,7 @@ As can be seen, the instruction-tuned model requires a [chat template](../chat_t\n \n ## Speeding up Mixtral by using Flash Attention\n \n-The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one.md#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n+The code snippets above showcase inference without any optimization tricks. However, one can drastically speed up the model by leveraging [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n \n First, make sure to install the latest version of Flash Attention 2 to include the sliding window attention feature.\n "
        },
        {
            "sha": "a3ba1258ecfa061fc63bb7971e5dbb1e8833c16b",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5290f6a62dd4892b07ff290087591d8a625a9f1f/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=5290f6a62dd4892b07ff290087591d8a625a9f1f",
            "patch": "@@ -174,7 +174,7 @@ model = VideoLlavaForConditionalGeneration.from_pretrained(\"LanguageBind/Video-L\n \n ### Flash-Attention 2 to speed-up generation\n \n-Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one.md#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n+Additionally, we can greatly speed-up model inference by using [Flash Attention](../perf_train_gpu_one#flash-attention-2), which is a faster implementation of the attention mechanism used inside the model.\n \n First, make sure to install the latest version of Flash Attention 2:\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 5,
        "deletions": 5
    }
}