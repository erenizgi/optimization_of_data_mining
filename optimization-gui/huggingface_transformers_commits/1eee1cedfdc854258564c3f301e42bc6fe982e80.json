{
    "author": "SunMarc",
    "message": "Fix loading with only state dict and low_cpu_mem_usage = True (#35217)\n\n* fix loading with only state dict and config\n\n* style\n\n* add tests\n\n---------\n\nCo-authored-by: Sayak Paul <spsayakpaul@gmail.com>",
    "sha": "1eee1cedfdc854258564c3f301e42bc6fe982e80",
    "files": [
        {
            "sha": "2ea88fb9b05b9071b544001adf15308bf514ec01",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/1eee1cedfdc854258564c3f301e42bc6fe982e80/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1eee1cedfdc854258564c3f301e42bc6fe982e80/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1eee1cedfdc854258564c3f301e42bc6fe982e80",
            "patch": "@@ -4022,8 +4022,11 @@ def from_pretrained(\n                 loaded_state_dict_keys = sharded_metadata[\"all_checkpoint_keys\"]\n             else:\n                 loaded_state_dict_keys = list(state_dict.keys())\n-\n-            if gguf_path is None and (low_cpu_mem_usage or (use_keep_in_fp32_modules and is_accelerate_available())):\n+            if (\n+                gguf_path is None\n+                and (low_cpu_mem_usage or (use_keep_in_fp32_modules and is_accelerate_available()))\n+                and pretrained_model_name_or_path is not None\n+            ):\n                 # In case some weights need to be kept in float32 and accelerate is not installed,\n                 # we later on want to take the path where state_dict is not None, that is the one\n                 # that do not require accelerate.\n@@ -4679,7 +4682,7 @@ def _find_mismatched_keys(\n             )\n \n             # For GGUF models `state_dict` is never set to None as the state dict is always small\n-            if gguf_path:\n+            if gguf_path or low_cpu_mem_usage:\n                 fixed_state_dict = cls._fix_state_dict_keys_on_load(state_dict)\n                 error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                     model_to_load,"
        },
        {
            "sha": "31c0d01af776ac8b58606d3ceb1cc56f3eda0376",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 0,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1eee1cedfdc854258564c3f301e42bc6fe982e80/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1eee1cedfdc854258564c3f301e42bc6fe982e80/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=1eee1cedfdc854258564c3f301e42bc6fe982e80",
            "patch": "@@ -1750,6 +1750,26 @@ def test_save_and_load_config_with_custom_generation(self):\n                 new_model.generate(random_ids, max_new_tokens=3)\n             self.assertTrue(len(w) == 0)\n \n+    def test_load_model_with_state_dict_only(self):\n+        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n+        state_dict = model.state_dict()\n+        config = model.config\n+\n+        model_loaded = BertModel.from_pretrained(\n+            pretrained_model_name_or_path=None, config=config, state_dict=state_dict\n+        )\n+        self.assertTrue(check_models_equal(model, model_loaded))\n+\n+    def test_load_model_with_state_dict_only_low_cpu_mem_usage(self):\n+        model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n+        state_dict = model.state_dict()\n+        config = model.config\n+\n+        model_loaded = BertModel.from_pretrained(\n+            pretrained_model_name_or_path=None, config=config, state_dict=state_dict, low_cpu_mem_usage=True\n+        )\n+        self.assertTrue(check_models_equal(model, model_loaded))\n+\n \n @slow\n @require_torch"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 26,
        "deletions": 3
    }
}