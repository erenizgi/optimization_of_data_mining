{
    "author": "zucchini-nlp",
    "message": "[VLMs] fix flash-attention tests (#37603)\n\n* fix one test\n\n* fa2 ln test\n\n* remove keys from config recursively\n\n* fix\n\n* fixup",
    "sha": "1cfcbfcab8da6681a73b18e3e198d56417815223",
    "files": [
        {
            "sha": "c50f9a6506817bbee88728109c81f1b0ebc71cdc",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -843,29 +843,16 @@ def to_diff_dict(self) -> dict[str, Any]:\n             ):\n                 serializable_config_dict[key] = value\n \n+        self._remove_keys_not_serialized(serializable_config_dict)\n+\n         if hasattr(self, \"quantization_config\"):\n             serializable_config_dict[\"quantization_config\"] = (\n                 self.quantization_config.to_dict()\n                 if not isinstance(self.quantization_config, dict)\n                 else self.quantization_config\n             )\n-            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n-            _ = serializable_config_dict.pop(\"_pre_quantization_dtype\", None)\n-\n         self.dict_torch_dtype_to_str(serializable_config_dict)\n \n-        if \"_attn_implementation_internal\" in serializable_config_dict:\n-            del serializable_config_dict[\"_attn_implementation_internal\"]\n-        # Do not serialize `base_model_tp_plan` for now\n-        if \"base_model_tp_plan\" in serializable_config_dict:\n-            del serializable_config_dict[\"base_model_tp_plan\"]\n-        # Do not serialize `base_model_pp_plan` for now\n-        if \"base_model_pp_plan\" in serializable_config_dict:\n-            del serializable_config_dict[\"base_model_pp_plan\"]\n-\n-        if \"_name_or_path\" in serializable_config_dict:\n-            del serializable_config_dict[\"_name_or_path\"]\n-\n         return serializable_config_dict\n \n     def to_dict(self) -> dict[str, Any]:\n@@ -878,18 +865,6 @@ def to_dict(self) -> dict[str, Any]:\n         output = copy.deepcopy(self.__dict__)\n         if hasattr(self.__class__, \"model_type\"):\n             output[\"model_type\"] = self.__class__.model_type\n-        if \"_auto_class\" in output:\n-            del output[\"_auto_class\"]\n-        if \"_commit_hash\" in output:\n-            del output[\"_commit_hash\"]\n-        if \"_attn_implementation_internal\" in output:\n-            del output[\"_attn_implementation_internal\"]\n-        # Do not serialize `base_model_tp_plan` for now\n-        if \"base_model_tp_plan\" in output:\n-            del output[\"base_model_tp_plan\"]\n-        # Do not serialize `base_model_pp_plan` for now\n-        if \"base_model_pp_plan\" in output:\n-            del output[\"base_model_pp_plan\"]\n \n         # Transformers version when serializing the model\n         output[\"transformers_version\"] = __version__\n@@ -902,16 +877,14 @@ def to_dict(self) -> dict[str, Any]:\n \n             output[key] = value\n \n+        self._remove_keys_not_serialized(output)\n+\n         if hasattr(self, \"quantization_config\"):\n             output[\"quantization_config\"] = (\n                 self.quantization_config.to_dict()\n                 if not isinstance(self.quantization_config, dict)\n                 else self.quantization_config\n             )\n-\n-            # pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n-            _ = output.pop(\"_pre_quantization_dtype\", None)\n-\n         self.dict_torch_dtype_to_str(output)\n \n         return output\n@@ -1011,6 +984,33 @@ def dict_torch_dtype_to_str(self, d: dict[str, Any]) -> None:\n             if isinstance(value, dict):\n                 self.dict_torch_dtype_to_str(value)\n \n+    def _remove_keys_not_serialized(self, d: dict[str, Any]) -> None:\n+        \"\"\"\n+        Checks and removes if there are any keys in the dict that should not be serialized when saving the config.\n+        Runs recursive check on the dict, to remove from all sub configs.\n+        \"\"\"\n+        if hasattr(self, \"quantization_config\"):\n+            # Pop the `_pre_quantization_dtype` as torch.dtypes are not serializable.\n+            _ = d.pop(\"_pre_quantization_dtype\", None)\n+\n+        if \"_auto_class\" in d:\n+            del d[\"_auto_class\"]\n+        if \"_commit_hash\" in d:\n+            del d[\"_commit_hash\"]\n+        if \"_attn_implementation_internal\" in d:\n+            del d[\"_attn_implementation_internal\"]\n+        # Do not serialize `base_model_tp_plan` for now\n+        if \"base_model_tp_plan\" in d:\n+            del d[\"base_model_tp_plan\"]\n+        # Do not serialize `base_model_pp_plan` for now\n+        if \"base_model_pp_plan\" in d:\n+            del d[\"base_model_pp_plan\"]\n+        if \"_name_or_path\" in d:\n+            del d[\"_name_or_path\"]\n+        for value in d.values():\n+            if isinstance(value, dict):\n+                self._remove_keys_not_serialized(value)\n+\n     @classmethod\n     def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n         \"\"\""
        },
        {
            "sha": "6999d46e4924791aeeb48b92c22a8fc8a87983a3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -4444,7 +4444,16 @@ def from_pretrained(\n             # once the weights have been quantized\n             # Note that once you have loaded a quantized model, you can't change its dtype so this will\n             # remain a single source of truth\n-            config._pre_quantization_dtype = torch_dtype if torch_dtype is not None else torch.get_default_dtype()\n+            original_dtype = torch_dtype if torch_dtype is not None else torch.get_default_dtype()\n+\n+            def _assign_original_dtype(module):\n+                for child in module.children():\n+                    if isinstance(child, PreTrainedModel):\n+                        child.config._pre_quantization_dtype = original_dtype\n+                    _assign_original_dtype(child)\n+\n+            config._pre_quantization_dtype = original_dtype\n+            _assign_original_dtype(model)\n \n         # Prepare the full device map\n         if device_map is not None:"
        },
        {
            "sha": "c181976e0ec219f3f7cfbf4fca6f33a521b123d3",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -125,6 +125,9 @@ def __init__(self, config: InternVLVisionConfig):\n         proj_dropout = config.projection_dropout\n         qk_norm = config.use_qk_norm\n \n+        # Needed for flash attention\n+        self.is_causal = False\n+\n         self.q_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n         self.k_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n         self.v_proj = nn.Linear(self.embed_dim, self.num_heads * self.head_dim, bias=config.attention_bias)\n@@ -134,9 +137,6 @@ def __init__(self, config: InternVLVisionConfig):\n         self.q_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n         self.k_norm = InternVLVisionRMSNorm(self.embed_dim) if qk_norm else nn.Identity()\n \n-        # Needed for flash attention\n-        self.is_causal = False\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,"
        },
        {
            "sha": "6a30954345a7ad758607fde967e3eabfb0d25dd8",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -344,6 +344,7 @@ def __init__(self, config: JanusVisionConfig):\n         self.attention_dropout = config.attention_dropout\n         proj_dropout = config.projection_dropout\n         qk_norm = config.use_qk_norm\n+        self.is_causal = False\n \n         # Janus has no MHA, hence for `eager_attention_forward` call setting `num_key_value_groups` to 1.\n         self.num_key_value_groups = 1\n@@ -398,7 +399,7 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scale,\n-            is_causal=False,\n+            is_causal=self.is_causal,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)"
        },
        {
            "sha": "36499e43eae0c45160384ac45b65164e2be6e1ad",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -509,6 +509,7 @@ def __init__(self, config: JanusVisionConfig):\n         self.attention_dropout = config.attention_dropout\n         proj_dropout = config.projection_dropout\n         qk_norm = config.use_qk_norm\n+        self.is_causal = False\n \n         # Janus has no MHA, hence for `eager_attention_forward` call setting `num_key_value_groups` to 1.\n         self.num_key_value_groups = 1\n@@ -563,7 +564,7 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scale,\n-            is_causal=False,\n+            is_causal=self.is_causal,\n             **kwargs,\n         )\n         attn_output = attn_output.reshape(batch_size, seq_len, self.embed_dim)"
        },
        {
            "sha": "5858321ee43b960639837f4f2024ce87cdd66c21",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -316,10 +316,6 @@ def test_initialization(self):\n     def test_sdpa_can_compile_dynamic(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     # todo: yoni - fix or improve the test\n     @unittest.skip(\"Difference is slightly higher than the threshold\")\n     def test_batching_equivalence(self):"
        },
        {
            "sha": "ed0a25f7b19952fa547fb7054a4f73f5869ef68b",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -222,8 +222,10 @@ def test_inputs_embeds_matches_input_ids(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n+    @unittest.skip(\n+        reason=\"GotOcr2's language backbone is Qwen2 which uses GQA so the KV cache is a non standard format\"\n+    )\n+    def test_past_key_values_format(self):\n         pass\n \n "
        },
        {
            "sha": "325d9714345104440a986dd8067d2719f7c5606c",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -407,10 +407,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n-    @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @pytest.mark.generate\n     @require_torch_sdpa\n     @slow"
        },
        {
            "sha": "69a0f85acefa869cede10d772640b01634952921",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -367,10 +367,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         pass\n \n-    @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @pytest.mark.generate\n     @require_torch_sdpa\n     @slow"
        },
        {
            "sha": "1072d9043ea0e6ad4aaaedb85a2d99f0b6931aa5",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -302,10 +302,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "3b9fc36521e801763c56ab417d3a71bf379af6c3",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -331,10 +331,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "dfb3b01395fbb7841fcb8e9734b8c21fbee8e674",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -302,10 +302,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "dee84a53f36ac08e20aade92e3cafd3d8e127b80",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -334,10 +334,6 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "6938e50a2660a37fe655093c5245d3438bb762ee",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -331,10 +331,6 @@ def test_save_load_low_cpu_mem_usage_no_safetensors(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "cdeb0d95ec1e5270a66f49d3a35484d95f7db7e7",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -378,10 +378,6 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n     def test_generate_methods_with_logits_to_keep(self):\n         super().test_generate_methods_with_logits_to_keep()\n \n-    @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip\n     def test_training_gradient_checkpointing(self):\n         pass"
        },
        {
            "sha": "bda728b391978a1e11bf0380ea763b3c6179922e",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -225,10 +225,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "60460ddfb5b5b2f37d634bd7d8b6fe708b401bee",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1cfcbfcab8da6681a73b18e3e198d56417815223/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=1cfcbfcab8da6681a73b18e3e198d56417815223",
            "patch": "@@ -305,10 +305,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(\"FlashAttention only support fp16 and bf16 data type\")\n-    def test_flash_attn_2_fp32_ln(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        }
    ],
    "stats": {
        "total": 135,
        "additions": 52,
        "deletions": 83
    }
}