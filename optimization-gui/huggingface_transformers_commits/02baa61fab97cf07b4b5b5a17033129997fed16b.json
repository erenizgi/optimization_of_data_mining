{
    "author": "MekkCyber",
    "message": "Make sure torch_is_available before using torch.distributed (#37693)\n\nfix",
    "sha": "02baa61fab97cf07b4b5b5a17033129997fed16b",
    "files": [
        {
            "sha": "05a99069399da6fa7736b8e13b703dfb4190e35c",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/02baa61fab97cf07b4b5b5a17033129997fed16b/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02baa61fab97cf07b4b5b5a17033129997fed16b/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=02baa61fab97cf07b4b5b5a17033129997fed16b",
            "patch": "@@ -997,7 +997,7 @@ def __init__(\n         else:\n             self.device = device if device is not None else -1\n \n-        if torch.distributed.is_initialized():\n+        if is_torch_available() and torch.distributed.is_initialized():\n             self.device = self.model.device\n         logger.warning(f\"Device set to use {self.device}\")\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}