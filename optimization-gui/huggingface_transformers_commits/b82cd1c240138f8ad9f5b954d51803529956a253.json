{
    "author": "zucchini-nlp",
    "message": "Processor load with multi-processing (#40786)\n\npush",
    "sha": "b82cd1c240138f8ad9f5b954d51803529956a253",
    "files": [
        {
            "sha": "a9ff39b0cc199694bbbae119b9b078a3bb4f2181",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 15,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=b82cd1c240138f8ad9f5b954d51803529956a253",
            "patch": "@@ -44,7 +44,7 @@\n     logging,\n     requires_backends,\n )\n-from .utils.hub import cached_files\n+from .utils.hub import cached_file\n \n \n if TYPE_CHECKING:\n@@ -506,20 +506,27 @@ def get_feature_extractor_dict(\n             feature_extractor_file = FEATURE_EXTRACTOR_NAME\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n-                resolved_feature_extractor_files = cached_files(\n-                    pretrained_model_name_or_path,\n-                    filenames=[feature_extractor_file, PROCESSOR_NAME],\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    resume_download=resume_download,\n-                    local_files_only=local_files_only,\n-                    subfolder=subfolder,\n-                    token=token,\n-                    user_agent=user_agent,\n-                    revision=revision,\n-                    _raise_exceptions_for_missing_entries=False,\n-                )\n+                resolved_feature_extractor_files = [\n+                    resolved_file\n+                    for filename in [feature_extractor_file, PROCESSOR_NAME]\n+                    if (\n+                        resolved_file := cached_file(\n+                            pretrained_model_name_or_path,\n+                            filename=filename,\n+                            cache_dir=cache_dir,\n+                            force_download=force_download,\n+                            proxies=proxies,\n+                            resume_download=resume_download,\n+                            local_files_only=local_files_only,\n+                            subfolder=subfolder,\n+                            token=token,\n+                            user_agent=user_agent,\n+                            revision=revision,\n+                            _raise_exceptions_for_missing_entries=False,\n+                        )\n+                    )\n+                    is not None\n+                ]\n                 resolved_feature_extractor_file = resolved_feature_extractor_files[0]\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to"
        },
        {
            "sha": "dfe94ffd0df794588b91544ffe9764ee5717c327",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 22,
            "deletions": 15,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=b82cd1c240138f8ad9f5b954d51803529956a253",
            "patch": "@@ -34,7 +34,7 @@\n     is_remote_url,\n     logging,\n )\n-from .utils.hub import cached_files\n+from .utils.hub import cached_file\n \n \n ImageProcessorType = TypeVar(\"ImageProcessorType\", bound=\"ImageProcessingMixin\")\n@@ -330,20 +330,27 @@ def get_image_processor_dict(\n             image_processor_file = image_processor_filename\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n-                resolved_image_processor_files = cached_files(\n-                    pretrained_model_name_or_path,\n-                    filenames=[image_processor_file, PROCESSOR_NAME],\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    resume_download=resume_download,\n-                    local_files_only=local_files_only,\n-                    token=token,\n-                    user_agent=user_agent,\n-                    revision=revision,\n-                    subfolder=subfolder,\n-                    _raise_exceptions_for_missing_entries=False,\n-                )\n+                resolved_image_processor_files = [\n+                    resolved_file\n+                    for filename in [image_processor_file, PROCESSOR_NAME]\n+                    if (\n+                        resolved_file := cached_file(\n+                            pretrained_model_name_or_path,\n+                            filename=filename,\n+                            cache_dir=cache_dir,\n+                            force_download=force_download,\n+                            proxies=proxies,\n+                            resume_download=resume_download,\n+                            local_files_only=local_files_only,\n+                            token=token,\n+                            user_agent=user_agent,\n+                            revision=revision,\n+                            subfolder=subfolder,\n+                            _raise_exceptions_for_missing_entries=False,\n+                        )\n+                    )\n+                    is not None\n+                ]\n                 resolved_image_processor_file = resolved_image_processor_files[0]\n             except OSError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to"
        },
        {
            "sha": "43d9e2bfd26e8a7adce1d15ecc32bdf471d8a024",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 15,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b82cd1c240138f8ad9f5b954d51803529956a253/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=b82cd1c240138f8ad9f5b954d51803529956a253",
            "patch": "@@ -50,7 +50,7 @@\n     is_torchvision_v2_available,\n     logging,\n )\n-from .utils.hub import cached_files\n+from .utils.hub import cached_file\n from .utils.import_utils import requires\n from .video_utils import (\n     VideoInput,\n@@ -683,20 +683,27 @@ def get_video_processor_dict(\n             try:\n                 # Try to load with a new config name first and if not successful try with the old file name\n                 # NOTE: we will gradually change to saving all processor configs as nested dict in PROCESSOR_NAME\n-                resolved_video_processor_files = cached_files(\n-                    pretrained_model_name_or_path,\n-                    filenames=[VIDEO_PROCESSOR_NAME, IMAGE_PROCESSOR_NAME, PROCESSOR_NAME],\n-                    cache_dir=cache_dir,\n-                    force_download=force_download,\n-                    proxies=proxies,\n-                    resume_download=resume_download,\n-                    local_files_only=local_files_only,\n-                    token=token,\n-                    user_agent=user_agent,\n-                    revision=revision,\n-                    subfolder=subfolder,\n-                    _raise_exceptions_for_missing_entries=False,\n-                )\n+                resolved_video_processor_files = [\n+                    resolved_file\n+                    for filename in [VIDEO_PROCESSOR_NAME, IMAGE_PROCESSOR_NAME, PROCESSOR_NAME]\n+                    if (\n+                        resolved_file := cached_file(\n+                            pretrained_model_name_or_path,\n+                            filename=filename,\n+                            cache_dir=cache_dir,\n+                            force_download=force_download,\n+                            proxies=proxies,\n+                            resume_download=resume_download,\n+                            local_files_only=local_files_only,\n+                            token=token,\n+                            user_agent=user_agent,\n+                            revision=revision,\n+                            subfolder=subfolder,\n+                            _raise_exceptions_for_missing_entries=False,\n+                        )\n+                    )\n+                    is not None\n+                ]\n                 resolved_video_processor_file = resolved_video_processor_files[0]\n             except OSError:\n                 # Raise any OS error raise by `cached_file`. It will have a helpful error message adapted to"
        }
    ],
    "stats": {
        "total": 111,
        "additions": 66,
        "deletions": 45
    }
}