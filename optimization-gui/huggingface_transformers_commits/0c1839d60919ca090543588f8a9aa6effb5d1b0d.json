{
    "author": "Cyrilvallez",
    "message": "[cache] Only use scalars in `get_mask_sizes` (#40907)\n\n* remove tensor ops\n\n* style\n\n* style",
    "sha": "0c1839d60919ca090543588f8a9aa6effb5d1b0d",
    "files": [
        {
            "sha": "1e08144c414d315742bee71c3d0f99d42ba73d78",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/0c1839d60919ca090543588f8a9aa6effb5d1b0d/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0c1839d60919ca090543588f8a9aa6effb5d1b0d/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=0c1839d60919ca090543588f8a9aa6effb5d1b0d",
            "patch": "@@ -122,8 +122,7 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         \"\"\"Return the length and offset of the cache, used to generate the mask\"\"\"\n         kv_offset = 0\n         query_length = cache_position.shape[0]\n-        past_seen_tokens = self.get_seq_length()\n-        kv_length = query_length + past_seen_tokens\n+        kv_length = self.get_seq_length() + query_length\n         return kv_length, kv_offset\n \n     def get_seq_length(self) -> int:\n@@ -212,14 +211,13 @@ def update(\n     def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         \"\"\"Return the length and offset of the cache, used to generate the attention mask\"\"\"\n         query_length = cache_position.shape[0]\n-        first_cache_position = cache_position[0]\n+        is_full = self.cumulative_length >= self.sliding_window\n \n-        kv_offset = torch.clamp(first_cache_position - self.sliding_window + 1, min=0)\n-\n-        if self.get_seq_length() >= self.sliding_window:\n+        kv_offset = max(self.cumulative_length - self.sliding_window + 1, 0)\n+        if is_full:\n             kv_length = self.sliding_window - 1 + query_length\n         else:\n-            kv_length = self.get_seq_length() + query_length\n+            kv_length = self.cumulative_length + query_length\n \n         return kv_length, kv_offset\n \n@@ -461,9 +459,10 @@ def get_mask_sizes(self, cache_position: torch.Tensor) -> tuple[int, int]:\n         # Not yet full, but becoming full on this update\n         elif self.cumulative_length + query_length > sliding_window:\n             kv_length = self.cumulative_length + query_length\n+        # Here the Cache is still smaller than the local size, but we return the local size as it's static\n         else:\n-            # Here the Cache is still smaller than the local size, but we return the local size as it's static\n             kv_length = sliding_window\n+\n         return kv_length, kv_offset\n \n     def get_seq_length(self) -> int:"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 7,
        "deletions": 8
    }
}