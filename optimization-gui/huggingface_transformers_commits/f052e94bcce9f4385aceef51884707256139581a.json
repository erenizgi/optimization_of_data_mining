{
    "author": "LysandreJik",
    "message": "Fix flax failures (#33912)\n\n* Few fixes here and there\r\n\r\n* Remove typos\r\n\r\n* Remove typos",
    "sha": "f052e94bcce9f4385aceef51884707256139581a",
    "files": [
        {
            "sha": "3178085c9135be8a9721d2f24dfd3b427dbe5259",
            "filename": "docs/source/en/main_classes/executorch.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f052e94bcce9f4385aceef51884707256139581a/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f052e94bcce9f4385aceef51884707256139581a/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fexecutorch.md?ref=f052e94bcce9f4385aceef51884707256139581a",
            "patch": "@@ -27,7 +27,7 @@ ExecuTorch introduces well defined entry points to perform model, device, and/or\n \n An integration point is being developed to ensure that ðŸ¤— Transformers can be exported using `torch.export`. The goal of this integration is not only to enable export but also to ensure that the exported artifact can be further lowered and optimized to run efficiently in `ExecuTorch`, particularly for mobile and edge use cases.\n \n-[[autodoc]] integrations.executorch.TorchExportableModuleWithStaticCache\n+[[autodoc]] TorchExportableModuleWithStaticCache\n     - forward\n \n-[[autodoc]] integrations.executorch.convert_and_export_with_cache\n+[[autodoc]] convert_and_export_with_cache"
        },
        {
            "sha": "574e4c75a6ac8a748af0d0538bada1913820fdb4",
            "filename": "docs/source/en/main_classes/text_generation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f052e94bcce9f4385aceef51884707256139581a/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f052e94bcce9f4385aceef51884707256139581a/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Ftext_generation.md?ref=f052e94bcce9f4385aceef51884707256139581a",
            "patch": "@@ -45,17 +45,17 @@ like token streaming.\n \n ## GenerationMixin\n \n-[[autodoc]] generation.GenerationMixin\n+[[autodoc]] GenerationMixin\n \t- generate\n \t- compute_transition_scores\n \n ## TFGenerationMixin\n \n-[[autodoc]] generation.TFGenerationMixin\n+[[autodoc]] TFGenerationMixin\n \t- generate\n \t- compute_transition_scores\n \n ## FlaxGenerationMixin\n \n-[[autodoc]] generation.FlaxGenerationMixin\n+[[autodoc]] FlaxGenerationMixin\n \t- generate"
        },
        {
            "sha": "258017f14180dfb6cc2b544041ddc8b05611afd4",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f052e94bcce9f4385aceef51884707256139581a/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f052e94bcce9f4385aceef51884707256139581a/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=f052e94bcce9f4385aceef51884707256139581a",
            "patch": "@@ -12,11 +12,15 @@\n \n import torch\n \n-from transformers import (\n-    PreTrainedModel,\n-    StaticCache,\n-)\n-from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n+from ..utils.import_utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    from transformers import (\n+        PreTrainedModel,\n+        StaticCache,\n+    )\n+    from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_3\n \n \n class TorchExportableModuleWithStaticCache(torch.nn.Module):"
        },
        {
            "sha": "fbc248824a4b45299764ef0670c2ea2233585d5c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f052e94bcce9f4385aceef51884707256139581a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f052e94bcce9f4385aceef51884707256139581a/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=f052e94bcce9f4385aceef51884707256139581a",
            "patch": "@@ -1751,9 +1751,7 @@ def __dir__(self):\n     def __getattr__(self, name: str) -> Any:\n         if name in self._objects:\n             return self._objects[name]\n-        if name in self._modules:\n-            value = self._get_module(name)\n-        elif name in self._object_missing_backend.keys():\n+        if name in self._object_missing_backend.keys():\n             missing_backends = self._object_missing_backend[name]\n \n             class Placeholder(metaclass=DummyObject):\n@@ -1769,6 +1767,8 @@ def __init__(self, *args, **kwargs):\n         elif name in self._class_to_module.keys():\n             module = self._get_module(self._class_to_module[name])\n             value = getattr(module, name)\n+        elif name in self._modules:\n+            value = self._get_module(name)\n         else:\n             raise AttributeError(f\"module {self.__name__} has no attribute {name}\")\n "
        },
        {
            "sha": "98f96bcc78a339c6e87e85dc968978da66317840",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 23,
            "deletions": 7,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/f052e94bcce9f4385aceef51884707256139581a/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f052e94bcce9f4385aceef51884707256139581a/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=f052e94bcce9f4385aceef51884707256139581a",
            "patch": "@@ -1089,18 +1089,34 @@ def check_public_method_exists(documented_methods_map):\n             for submodule_name in nested_submodules:\n                 if submodule_name == \"transformers\":\n                     continue\n-                submodule = getattr(submodule, submodule_name)\n+\n+                try:\n+                    submodule = getattr(submodule, submodule_name)\n+                except AttributeError:\n+                    failures.append(f\"Could not parse {submodule_name}. Are the required dependencies installed?\")\n+                continue\n+\n         class_name = nested_path[-1]\n-        obj_class = getattr(submodule, class_name)\n+\n+        try:\n+            obj_class = getattr(submodule, class_name)\n+        except AttributeError:\n+            failures.append(f\"Could not parse {submodule_name}. Are the required dependencies installed?\")\n+            continue\n+\n         # Checks that all explicitly documented methods are defined in the class\n         for method in methods:\n             if method == \"all\":  # Special keyword to document all public methods\n                 continue\n-            if not hasattr(obj_class, method):\n-                failures.append(\n-                    \"The following public method is explicitly documented but not defined in the corresponding \"\n-                    f\"class. class: {obj}, method: {method}\"\n-                )\n+            try:\n+                if not hasattr(obj_class, method):\n+                    failures.append(\n+                        \"The following public method is explicitly documented but not defined in the corresponding \"\n+                        f\"class. class: {obj}, method: {method}. If the method is defined, this error can be due to \"\n+                        f\"lacking dependencies.\"\n+                    )\n+            except ImportError:\n+                pass\n \n     if len(failures) > 0:\n         raise Exception(\"\\n\".join(failures))"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 40,
        "deletions": 20
    }
}