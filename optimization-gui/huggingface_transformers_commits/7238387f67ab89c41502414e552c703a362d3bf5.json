{
    "author": "MekkCyber",
    "message": "Fix typo in EETQ Tests (#35160)\n\nfix",
    "sha": "7238387f67ab89c41502414e552c703a362d3bf5",
    "files": [
        {
            "sha": "f14fa076e4bb76614bde2d9f886733fe98d78621",
            "filename": "tests/quantization/eetq_integration/test_eetq.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7238387f67ab89c41502414e552c703a362d3bf5/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7238387f67ab89c41502414e552c703a362d3bf5/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Feetq_integration%2Ftest_eetq.py?ref=7238387f67ab89c41502414e552c703a362d3bf5",
            "patch": "@@ -119,7 +119,7 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears - 1, nb_eetq_linear)\n \n-        # Try with `linear_weights_not_to_quantize`\n+        # Try with `modules_to_not_convert`\n         with init_empty_weights():\n             model = OPTForCausalLM(config)\n         quantization_config = EetqConfig(modules_to_not_convert=[\"fc1\"])\n@@ -128,7 +128,7 @@ def test_quantized_model_conversion(self):\n         for module in model.modules():\n             if isinstance(module, EetqLinear):\n                 nb_eetq_linear += 1\n-\n+        # 25 corresponds to the lm_head along with 24 fc1 layers.\n         self.assertEqual(nb_linears - 25, nb_eetq_linear)\n \n     def test_quantized_model(self):"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}