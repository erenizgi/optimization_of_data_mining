{
    "author": "Deep-unlearning",
    "message": "more tts pipeline exampel (#42484)\n\n* more tts pipeline exampel\n\n* remove duplicate code\n\n* group example by models\n\n* nit",
    "sha": "f6dcac65600d99e6cf2015ceb7e0a85b9ad0f5f1",
    "files": [
        {
            "sha": "297f936cd6b094ada6c4f545fa2b5060d231123c",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 43,
            "deletions": 8,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/f6dcac65600d99e6cf2015ceb7e0a85b9ad0f5f1/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f6dcac65600d99e6cf2015ceb7e0a85b9ad0f5f1/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=f6dcac65600d99e6cf2015ceb7e0a85b9ad0f5f1",
            "patch": "@@ -22,16 +22,14 @@ Text-to-speech (TTS) is the task of creating natural-sounding speech from text,\n languages and for multiple speakers. Several text-to-speech models are currently available in ðŸ¤— Transformers, such as [Dia](../model_doc/dia), [CSM](../model_doc/csm),\n [Bark](../model_doc/bark), [MMS](../model_doc/mms), [VITS](../model_doc/vits) and [SpeechT5](../model_doc/speecht5).\n \n-You can easily generate audio using the `\"text-to-audio\"` pipeline (or its alias - `\"text-to-speech\"`). Some models, like Dia,\n-can also be conditioned to generate non-verbal communications such as laughing, sighing and crying, or even add music.\n-Here's an example of how you would use the `\"text-to-speech\"` pipeline with Dia:\n+You can easily generate audio using the `\"text-to-audio\"` pipeline (or its alias - `\"text-to-speech\"`).\n+Here's an example of how you would use the `\"text-to-speech\"` pipeline with [CSM](https://huggingface.co/sesame/csm-1b):\n \n-```py\n+```python\n >>> from transformers import pipeline\n \n->>> pipe = pipeline(\"text-to-speech\", model=\"nari-labs/Dia-1.6B-0626\")\n->>> text = \"[S1] (clears throat) Hello! How are you? [S2] I'm good, thanks! How about you?\"\n->>> output = pipe(text)\n+>>> pipe = pipeline(\"text-to-audio\", model=\"sesame/csm-1b\")\n+>>> output = pipe(\"Hello from Sesame.\")\n ```\n \n Here's a code snippet you can use to listen to the resulting audio in a notebook:\n@@ -41,7 +39,44 @@ Here's a code snippet you can use to listen to the resulting audio in a notebook\n >>> Audio(output[\"audio\"], rate=output[\"sampling_rate\"])\n ```\n \n-For more examples on what Bark and other pretrained TTS models can do, refer to our\n+By default, CSM uses a random voice. You can do voice cloning by providing a reference audio as part of a chat template dictionary:\n+\n+```python\n+>>> import soundfile as sf\n+>>> import torch\n+>>> from datasets import Audio, load_dataset\n+>>> from transformers import pipeline\n+\n+>>> pipe = pipeline(\"text-to-audio\", model=\"sesame/csm-1b\")\n+\n+>>> ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n+>>> ds = ds.cast_column(\"audio\", Audio(sampling_rate=24000))\n+>>> conversation = [\n+...     {\n+...         \"role\": \"0\",\n+...         \"content\": [\n+...             {\"type\": \"text\", \"text\": \"What are you working on?\"},\n+...             {\"type\": \"audio\", \"path\": ds[0][\"audio\"][\"array\"]},\n+...         ],\n+...     },\n+...     {\"role\": \"0\", \"content\": [{\"type\": \"text\", \"text\": \"How much money can you spend?\"}]},\n+... ]\n+>>> output = pipe(conversation)\n+```\n+\n+Some models, like [Dia](https://huggingface.co/nari-labs/Dia-1.6B-0626), can also be conditioned to generate non-verbal communications such as laughing, sighing and crying, or even add music. Below is such an example:\n+\n+```python\n+>>> from transformers import pipeline\n+\n+>>> pipe = pipeline(\"text-to-speech\", model=\"nari-labs/Dia-1.6B-0626\")\n+>>> text = \"[S1] (clears throat) Hello! How are you? [S2] I'm good, thanks! How about you?\"\n+>>> output = pipe(text)\n+```\n+\n+Note that Dia also accepts speaker tags such as [S1] and [S2] to generate a conversation between unique voices.\n+\n+For more examples on what CSM and other pretrained TTS models can do, refer to our\n [Audio course](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models).\n \n If you are looking to fine-tune a TTS model, the only text-to-speech models currently available in ðŸ¤— Transformers"
        }
    ],
    "stats": {
        "total": 51,
        "additions": 43,
        "deletions": 8
    }
}