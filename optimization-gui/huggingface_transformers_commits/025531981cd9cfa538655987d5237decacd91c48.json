{
    "author": "vasqu",
    "message": "[`FA3`] Fix masking and loading logic in same process (#41217)\n\nfix loading and fa3 masking",
    "sha": "025531981cd9cfa538655987d5237decacd91c48",
    "files": [
        {
            "sha": "3349caed41d8dd1b8145a9f7193e10d4f7a6e424",
            "filename": "src/transformers/integrations/hub_kernels.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fhub_kernels.py?ref=025531981cd9cfa538655987d5237decacd91c48",
            "patch": "@@ -191,7 +191,7 @@ def load_and_register_kernel(attn_implementation: str) -> None:\n         if attention_wrapper is None:\n             attention_wrapper = flash_attention_forward\n         kernel_function = partial(attention_wrapper, implementation=kernel)\n-        lazy_import_flash_attention(kernel)\n+        lazy_import_flash_attention(kernel, force_import=True)\n     elif kernel_name is not None:\n         kernel_function = getattr(kernel, kernel_name)\n     # Register the kernel as a valid attention"
        },
        {
            "sha": "0abf0043b4c04fc46428b6a7d286f3af892df2be",
            "filename": "src/transformers/masking_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=025531981cd9cfa538655987d5237decacd91c48",
            "patch": "@@ -625,6 +625,7 @@ class AttentionMaskInterface(GeneralInterface):\n         \"sdpa\": sdpa_mask,\n         \"eager\": eager_mask,\n         \"flash_attention_2\": flash_attention_mask,\n+        \"flash_attention_3\": flash_attention_mask,\n         \"flex_attention\": flex_attention_mask,\n     }\n "
        },
        {
            "sha": "5312b0dd9cd0b40f6c7b64356de203f579e8b263",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=025531981cd9cfa538655987d5237decacd91c48",
            "patch": "@@ -124,19 +124,19 @@ def _lazy_define_process_function(flash_function):\n     return partial(_process_flash_attention_kwargs, supports_mapping=supports_mapping)\n \n \n-def lazy_import_flash_attention(implementation: Optional[str]):\n+def lazy_import_flash_attention(implementation: Optional[str], force_import: Optional[bool] = False):\n     \"\"\"\n     Lazily import flash attention and return the respective functions + flags.\n \n     NOTE: For fullgraph, this needs to be called before compile, while no fullgraph can\n     work without preloading. See `load_and_register_kernel` in `integrations.hub_kernels`.\n     \"\"\"\n     global _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn\n-    if any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):\n+    if force_import or any(k is None for k in [_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn]):\n         _flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn = _lazy_imports(implementation)\n \n     global _process_flash_kwargs_fn\n-    if _process_flash_kwargs_fn is None:\n+    if force_import or _process_flash_kwargs_fn is None:\n         _process_flash_kwargs_fn = _lazy_define_process_function(_flash_varlen_fn)\n \n     return (_flash_fn, _flash_varlen_fn, _pad_fn, _unpad_fn), _process_flash_kwargs_fn"
        },
        {
            "sha": "e42e5daeed80686de6fb56852be5baaf72e2b7b8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/025531981cd9cfa538655987d5237decacd91c48/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=025531981cd9cfa538655987d5237decacd91c48",
            "patch": "@@ -2557,7 +2557,7 @@ def _check_and_adjust_attn_implementation(\n             )\n             # preload flash attention here to allow compile with fullgraph\n             if applicable_attn_implementation.startswith(\"flash_attention\"):\n-                lazy_import_flash_attention(applicable_attn_implementation)\n+                lazy_import_flash_attention(applicable_attn_implementation, force_import=True)\n \n         return applicable_attn_implementation\n "
        },
        {
            "sha": "969cdddcd38d8b9588bbf3c4430a23daed074e65",
            "filename": "tests/generation/test_flash_attention_parity.py",
            "status": "modified",
            "additions": 27,
            "deletions": 30,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/025531981cd9cfa538655987d5237decacd91c48/tests%2Fgeneration%2Ftest_flash_attention_parity.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/025531981cd9cfa538655987d5237decacd91c48/tests%2Fgeneration%2Ftest_flash_attention_parity.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_flash_attention_parity.py?ref=025531981cd9cfa538655987d5237decacd91c48",
            "patch": "@@ -81,62 +81,59 @@ def _benchmark_generation(self, model, inputs, n_warmup=3, n_runs=5):\n     @slow\n     def test_flash_attention_2_3_parity(self):\n         model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n-        prompt = \"The ETH AI Center is\"\n+        prompt = [\"The ETH AI Center is\", \"What is life?\"]\n \n-        # 1. Load FA2 model and tokenizer\n-        model_2 = AutoModelForCausalLM.from_pretrained(\n+        # 1. Load model and tokenizer\n+        model = AutoModelForCausalLM.from_pretrained(\n             model_id,\n             dtype=torch.bfloat16,\n             attn_implementation=\"flash_attention_2\",\n         ).to(\"cuda\")\n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n+        tokenizer.pad_token_id = tokenizer.eos_token_id\n \n-        # 2. Load FA3 model\n-        try:\n-            model_3 = AutoModelForCausalLM.from_pretrained(\n-                model_id,\n-                dtype=torch.bfloat16,\n-                attn_implementation=\"flash_attention_3\",\n-            ).to(\"cuda\")\n-        except (ValueError, ImportError) as e:\n-            pytest.skip(f\"Could not load Flash Attention 3 model, skipping test. Error: {e}\")\n-\n-        # 3. Generate with both models\n-        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+        # 2. Generate with both models\n+        inputs = tokenizer(prompt, padding=True, padding_side=\"left\", return_tensors=\"pt\").to(\"cuda\")\n \n         with torch.no_grad():\n-            output_2 = model_2.generate(\n+            output_2 = model.generate(\n                 **inputs, max_new_tokens=20, do_sample=False, output_scores=True, return_dict_in_generate=True\n             )\n-            output_3 = model_3.generate(\n+            model.set_attn_implementation(\"flash_attention_3\")\n+            output_3 = model.generate(\n                 **inputs, max_new_tokens=20, do_sample=False, output_scores=True, return_dict_in_generate=True\n             )\n \n-        # 4. Correctness check\n-        # 4a. Logits\n+        # 3. Correctness check\n+        # 3a. Logits\n         logits_2 = torch.stack(output_2.scores)\n         logits_3 = torch.stack(output_3.scores)\n         torch.testing.assert_close(logits_2, logits_3, atol=1e-3, rtol=1e-3)\n         logprobs_2 = torch.nn.functional.log_softmax(logits_2, dim=-1)\n         logprobs_3 = torch.nn.functional.log_softmax(logits_3, dim=-1)\n         max_logprob_diff = torch.max(torch.abs(logprobs_2 - logprobs_3)).item()\n \n-        # 4b. Generated text\n-        text_2 = tokenizer.decode(output_2.sequences[0], skip_special_tokens=True)\n-        text_3 = tokenizer.decode(output_3.sequences[0], skip_special_tokens=True)\n-        rouge_score = self._calculate_rouge_l([text_2], [text_3])[0]\n-        assert rouge_score > 0.99, f\"Generated texts do not match (ROUGE-L: {rouge_score})\"\n+        # 3b. Generated text\n+        text_2s, text_3s = [], []\n+        for i in range(len(prompt)):\n+            text_2s.append(tokenizer.decode(output_2.sequences[i], skip_special_tokens=True))\n+            text_3s.append(tokenizer.decode(output_3.sequences[i], skip_special_tokens=True))\n+\n+        rouge_scores = self._calculate_rouge_l(text_2s, text_3s)\n+        for i in range(len(rouge_scores)):\n+            assert rouge_scores[i] > 0.99, f\"Generated texts at prompt {i} do not match (ROUGE-L: {rouge_scores[i]})\"\n \n-        # 5. Performance check\n+        # 4. Performance check\n         with torch.no_grad():\n-            time_2 = self._benchmark_generation(model_2, inputs)\n-            time_3 = self._benchmark_generation(model_3, inputs)\n+            time_3 = self._benchmark_generation(model, inputs)\n+            model.set_attn_implementation(\"flash_attention_2\")\n+            time_2 = self._benchmark_generation(model, inputs)\n \n         print(f\"\\n--- Flash Attention {2, 3} Parity Test on {model_id} ---\")\n         print(f\"Prompt: '{prompt}'\")\n-        print(f\"Generated text with Flash Attention 2: {text_2}\")\n-        print(f\"Generated text with Flash Attention 3: {text_3}\")\n-        print(f\"ROUGE-L: {rouge_score}\")\n+        print(f\"Generated text with Flash Attention 2: {text_2s}\")\n+        print(f\"Generated text with Flash Attention 3: {text_3s}\")\n+        print(f\"ROUGE-L: {rouge_scores}\")\n         print(f\"Max absolute difference in logprobs: {max_logprob_diff:.5e}\")\n         print(f\"Flash Attention 2 latency: {time_2:.2f} ms\")\n         print(f\"Flash Attention 3 latency: {time_3:.2f} ms\")"
        }
    ],
    "stats": {
        "total": 68,
        "additions": 33,
        "deletions": 35
    }
}