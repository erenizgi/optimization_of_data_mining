{
    "author": "molbap",
    "message": "Fix attention vizualizer (#40285)\n\n* make visualizer rely on create causal mask\n\n* format\n\n* fixup\n\n* fixup\n\n* read token\n\n* read token, duh\n\n* what is up with that token\n\n* small tests?\n\n* adjust\n\n* try with flush\n\n* normalize for ANSI\n\n* buffer shenanigans",
    "sha": "04b751f07d6151401311de50c3dad7af6e9acdb1",
    "files": [
        {
            "sha": "30d0f0cc6dd5aabf3b0517a8097a76d1e696e8d0",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 17,
            "deletions": 6,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/04b751f07d6151401311de50c3dad7af6e9acdb1/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04b751f07d6151401311de50c3dad7af6e9acdb1/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=04b751f07d6151401311de50c3dad7af6e9acdb1",
            "patch": "@@ -16,6 +16,7 @@\n import requests\n from PIL import Image\n \n+from ..masking_utils import create_causal_mask\n from ..models.auto.auto_factory import _get_model_class\n from ..models.auto.configuration_auto import AutoConfig\n from ..models.auto.modeling_auto import MODEL_FOR_PRETRAINING_MAPPING, MODEL_MAPPING\n@@ -207,13 +208,23 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n \n         model.config._attn_implementation = \"eager\"\n         model.train()\n-        attention_mask = ~model._update_causal_mask(\n+\n+        batch_size, seq_length = attention_mask.shape\n+        input_embeds = torch.zeros((batch_size, seq_length, model.config.hidden_size), dtype=self.model.dtype)\n+        cache_position = torch.arange(seq_length)\n+\n+        causal_mask = create_causal_mask(\n+            config=model.config,\n+            input_embeds=input_embeds,\n             attention_mask=attention_mask,\n-            input_tensor=attention_mask.to(self.model.dtype),\n-            cache_position=torch.arange(attention_mask.shape[1]),\n+            cache_position=cache_position,\n             past_key_values=None,\n-            **kwargs,\n-        ).bool()\n+        )\n+\n+        if causal_mask is not None:\n+            attention_mask = ~causal_mask.bool()\n+        else:\n+            attention_mask = attention_mask.unsqueeze(1).unsqueeze(1).expand(batch_size, 1, seq_length, seq_length)\n         top_bottom_border = \"##\" * (\n             len(f\"Attention visualization for {self.config.model_type} | {self.mapped_cls}\") + 4\n         )  # Box width adjusted to text length\n@@ -225,7 +236,7 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n                 len(top_bottom_border)\n             )\n             + \"    \"\n-            + side_border\n+            + side_border,\n         )\n         print(f\"{top_bottom_border}\")\n         f_string = generate_attention_matrix_from_mask("
        },
        {
            "sha": "21a11a66511cdde104dc0f159cf934e1e7935bcf",
            "filename": "tests/utils/test_attention_visualizer.py",
            "status": "added",
            "additions": 127,
            "deletions": 0,
            "changes": 127,
            "blob_url": "https://github.com/huggingface/transformers/blob/04b751f07d6151401311de50c3dad7af6e9acdb1/tests%2Futils%2Ftest_attention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04b751f07d6151401311de50c3dad7af6e9acdb1/tests%2Futils%2Ftest_attention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_attention_visualizer.py?ref=04b751f07d6151401311de50c3dad7af6e9acdb1",
            "patch": "@@ -0,0 +1,127 @@\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import builtins\n+import io\n+import re\n+import unittest\n+\n+from transformers.testing_utils import require_read_token, require_torch\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n+\n+\n+ANSI_RE = re.compile(r\"\\x1b\\[[0-9;]*m\")\n+\n+\n+def _normalize(s: str) -> str:\n+    # drop ANSI (colors may be disabled on CI), normalize line endings,\n+    # and strip trailing spaces without touching alignment inside lines\n+    s = ANSI_RE.sub(\"\", s)\n+    s = s.replace(\"\\r\\n\", \"\\n\").replace(\"\\r\", \"\\n\")\n+    return \"\\n\".join(line.rstrip() for line in s.split(\"\\n\")).strip()\n+\n+\n+@require_torch\n+class AttentionMaskVisualizerTester(unittest.TestCase):\n+    \"\"\"Test suite for AttentionMaskVisualizer\"\"\"\n+\n+    @require_read_token\n+    def test_paligemma_multimodal_visualization(self):\n+        \"\"\"Test AttentionMaskVisualizer with PaliGemma multimodal model\"\"\"\n+        model_name = \"hf-internal-testing/namespace_google_repo_name_paligemma-3b-pt-224\"\n+        input_text = \"<img> What is in this image?\"\n+\n+        buf = io.StringIO()\n+        orig_print = builtins.print\n+\n+        def _print(*args, **kwargs):\n+            kwargs.setdefault(\"file\", buf)\n+            orig_print(*args, **kwargs)\n+\n+        try:\n+            builtins.print = _print\n+            visualizer = AttentionMaskVisualizer(model_name)\n+            visualizer(input_text)\n+        finally:\n+            builtins.print = orig_print\n+        output = buf.getvalue()\n+\n+        expected_output = \"\"\"\n+##########################################################################################################################################################################################################################################\n+##                                                      Attention visualization for \\033[1mpaligemma:hf-internal-testing/namespace_google_repo_name_paligemma-3b-pt-224\\033[0m PaliGemmaModel                                                         ##\n+##########################################################################################################################################################################################################################################\n+ \\033[92m■\\033[0m: i == j (diagonal)   \\033[93m■\\033[0m: token_type_ids\n+              Attention Matrix  \n+\n+\n+\\033[93m'<image>'\\033[0m:  0 \\033[93m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+\\033[93m'<image>'\\033[0m:  1 \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+\\033[93m'<image>'\\033[0m:  2 \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+\\033[93m'<image>'\\033[0m:  3 \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+\\033[93m'<image>'\\033[0m:  4 \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m \\033[93m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'<bos>'  :  5 ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁What'  :  6 ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁is'    :  7 ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁in'    :  8 ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁this'  :  9 ■ ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚    |    \n+'▁image' : 10 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚    |    \n+'?'      : 11 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚    |    \n+'\\\\n'     : 12 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚    |    \n+'<eos>'  : 13 ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m    |    \n+##########################################################################################################################################################################################################################################\n+\"\"\"  # noqa\n+\n+        self.assertEqual(_normalize(output), _normalize(expected_output))\n+\n+    @require_read_token\n+    def test_llama_text_only_visualization(self):\n+        \"\"\"Test AttentionMaskVisualizer with Llama text-only model\"\"\"\n+        model_name = \"hf-internal-testing/namespace_meta-llama_repo_name_Llama-2-7b-hf\"\n+        input_text = \"Plants create energy through a process known as\"\n+\n+        buf = io.StringIO()\n+        orig_print = builtins.print\n+\n+        def _print(*args, **kwargs):\n+            kwargs.setdefault(\"file\", buf)\n+            orig_print(*args, **kwargs)\n+\n+        try:\n+            builtins.print = _print\n+            visualizer = AttentionMaskVisualizer(model_name)\n+            visualizer(input_text)\n+        finally:\n+            builtins.print = orig_print\n+        output = buf.getvalue()\n+\n+        expected_output = \"\"\"\n+##########################################################################################################################################################################################################\n+##                                           Attention visualization for \\033[1mllama:hf-internal-testing/namespace_meta-llama_repo_name_Llama-2-7b-hf\\033[0m LlamaModel                                              ##\n+##########################################################################################################################################################################################################\n+ \\033[92m■\\033[0m: i == j (diagonal)   \\033[93m■\\033[0m: token_type_ids\n+               Attention Matrix\n+\n+'▁Pl'     :  0 \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'ants'    :  1 ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁create' :  2 ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁energy' :  3 ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚ ⬚    |    \n+'▁through':  4 ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚ ⬚    |    \n+'▁a'      :  5 ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚ ⬚    |    \n+'▁process':  6 ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚ ⬚    |    \n+'▁known'  :  7 ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m ⬚    |    \n+'▁as'     :  8 ■ ■ ■ ■ ■ ■ ■ ■ \\033[92m■\\033[0m    |    \n+##########################################################################################################################################################################################################\n+\"\"\"  # noqa\n+\n+        self.assertEqual(_normalize(output), _normalize(expected_output))"
        }
    ],
    "stats": {
        "total": 150,
        "additions": 144,
        "deletions": 6
    }
}