{
    "author": "zucchini-nlp",
    "message": "ðŸ”´ [VLM] Add base model without head  (#37033)\n\n* i guessreverted all CdGen classes\n\n* style\n\n* llava onevision\n\n* fix copies\n\n* fix some tests\n\n* some more tests\n\n* dump\n\n* skip these\n\n* nevermind, i am dumb\n\n* revert fix not needed\n\n* fixup\n\n* fixup\n\n* another fixup\n\n* more fixup to make ci finally happy\n\n* fixup after rebasing\n\n* fix qwen tests\n\n* add internVL + typos here and there\n\n* image token index -> id\n\n* style\n\n* fix init weights\n\n* revert blip-2 not supported\n\n* address comments\n\n* fix copies\n\n* revert blip2 test file as well\n\n* as discussed internally, revert back CdGen models\n\n* fix some tests\n\n* fix more tests for compile\n\n* CI red\n\n* fix copies\n\n* enumerate explicitly allowed models\n\n* address comments\n\n* fix tests\n\n* fixup\n\n* style again\n\n* add tests for new model class\n\n* another fixup ( x _ x )\n\n* [fixup] unused attributes can be removed post-deprecation",
    "sha": "17742bd9c8852ab35986dcaa3e68415342ae7eef",
    "files": [
        {
            "sha": "89cd26db6493060c69a4fd8368b08033f9aacbda",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -102,6 +102,10 @@ response = processor.decode(output_ids, skip_special_tokens=True)\n \n [[autodoc]] AriaTextModel\n \n+## AriaModel\n+\n+[[autodoc]] AriaModel\n+\n ## AriaTextForCausalLM\n \n [[autodoc]] AriaTextForCausalLM"
        },
        {
            "sha": "f2a82089506d632160817e205207a459dc70ffc8",
            "filename": "docs/source/en/model_doc/aya_vision.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -237,6 +237,10 @@ for i, output in enumerate(batch_outputs):\n \n [[autodoc]] AyaVisionConfig\n \n+## AyaVisionModel\n+\n+[[autodoc]] AyaVisionModel\n+\n ## AyaVisionForConditionalGeneration\n \n [[autodoc]] AyaVisionForConditionalGeneration"
        },
        {
            "sha": "20b8a5e1cdbf90b6efc5a6a28f7bfad11ae60d17",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -174,6 +174,10 @@ for i, image in enumerate(images['pixel_values']):\n [[autodoc]] Emu3TextModel\n     - forward\n \n+## Emu3Model\n+\n+[[autodoc]] Emu3Model\n+\n ## Emu3ForCausalLM\n \n [[autodoc]] Emu3ForCausalLM"
        },
        {
            "sha": "60ae9efdf3f2c6a9c24768537f4ab29a8ccbe6c7",
            "filename": "docs/source/en/model_doc/fuyu.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ffuyu.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -103,6 +103,10 @@ The `LlamaTokenizer` is used as it is a standard wrapper around sentencepiece.\n \n [[autodoc]] FuyuConfig\n \n+## FuyuModel\n+\n+[[autodoc]] FuyuModel\n+\n ## FuyuForCausalLM\n \n [[autodoc]] FuyuForCausalLM"
        },
        {
            "sha": "8372fd9ed15a9c619665c832224d11e5d9b29ea2",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -254,6 +254,10 @@ visualizer(\"<img>What is shown in this image?\")\n [[autodoc]] Gemma3TextModel\n     - forward\n \n+## Gemma3Model\n+\n+[[autodoc]] Gemma3Model\n+\n ## Gemma3ForCausalLM\n \n [[autodoc]] Gemma3ForCausalLM"
        },
        {
            "sha": "c7a73659e88119f1101cce11e764242b3b26c4b2",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -277,6 +277,10 @@ alt=\"drawing\" width=\"600\"/>\n \n [[autodoc]] GotOcr2Processor\n \n+## GotOcr2Model\n+\n+[[autodoc]] GotOcr2Model\n+\n ## GotOcr2ForConditionalGeneration\n \n [[autodoc]] GotOcr2ForConditionalGeneration"
        },
        {
            "sha": "944e8888fcff6f24c0ce937629c90f0a4ad1077e",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -69,6 +69,10 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipQFormerModel\n     - forward\n \n+## InstructBlipModel\n+\n+[[autodoc]] InstructBlipModel\n+\n ## InstructBlipForConditionalGeneration\n \n [[autodoc]] InstructBlipForConditionalGeneration"
        },
        {
            "sha": "c021a4c7afa6b0044701f953ad2e7f17b43e8500",
            "filename": "docs/source/en/model_doc/instructblipvideo.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblipvideo.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -73,6 +73,10 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n [[autodoc]] InstructBlipVideoQFormerModel\n     - forward\n \n+## InstructBlipVideoModel\n+[[autodoc]] InstructBlipVideoModel\n+    - forward\n+\n ## InstructBlipVideoForConditionalGeneration\n \n [[autodoc]] InstructBlipVideoForConditionalGeneration"
        },
        {
            "sha": "8a19726e3e0e24af7474117f445c7e2bbddd618d",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -340,6 +340,11 @@ This example showcases how to handle a batch of chat conversations with interlea\n [[autodoc]] InternVLVisionModel\n     - forward\n \n+## InternVLModel\n+\n+[[autodoc]] InternVLModel\n+    - forward\n+\n ## InternVLForConditionalGeneration\n \n [[autodoc]] InternVLForConditionalGeneration"
        },
        {
            "sha": "dcf2cd2f3f680fd7c13c436fbfe5a342a481ca02",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -256,6 +256,10 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n \n [[autodoc]] LlavaProcessor\n \n+## LlavaModel\n+\n+[[autodoc]] LlavaModel\n+\n ## LlavaForConditionalGeneration\n \n [[autodoc]] LlavaForConditionalGeneration"
        },
        {
            "sha": "2af882b6118d25cce329ed08ec6eb162fb18e4cb",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -315,6 +315,10 @@ model = AutoModelForImageTextToText.from_pretrained(\n \n [[autodoc]] LlavaNextProcessor\n \n+## LlavaNextModel\n+\n+[[autodoc]] LlavaNextModel\n+\n ## LlavaNextForConditionalGeneration\n \n [[autodoc]] LlavaNextForConditionalGeneration"
        },
        {
            "sha": "d33062568467f0836156dedb313f6a27c1ea3364",
            "filename": "docs/source/en/model_doc/llava_next_video.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next_video.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -262,6 +262,10 @@ model = LlavaNextVideoForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaNextVideoImageProcessor\n \n+## LlavaNextVideoModel\n+\n+[[autodoc]] LlavaNextVideoModel\n+\n ## LlavaNextVideoForConditionalGeneration\n \n [[autodoc]] LlavaNextVideoForConditionalGeneration"
        },
        {
            "sha": "a00dd5a0e125aa68535240c85111cbcac8b27365",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -313,6 +313,10 @@ model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n \n [[autodoc]] LlavaOnevisionVideoProcessor\n \n+## LlavaOnevisionModel\n+\n+[[autodoc]] LlavaOnevisionModel\n+\n ## LlavaOnevisionForConditionalGeneration\n \n [[autodoc]] LlavaOnevisionForConditionalGeneration"
        },
        {
            "sha": "8eedb5de6bd9ac45d7fad409066fc595df4079e0",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -227,6 +227,9 @@ This example also how to use `BitsAndBytes` to load the model in 4bit quantizati\n \n [[autodoc]] Mistral3Config\n \n+## Mistral3Model\n+\n+[[autodoc]] Mistral3Model\n \n ## Mistral3ForConditionalGeneration\n "
        },
        {
            "sha": "cdd4da240af5e3e99b883f3d8ddb834055cec5e1",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -130,6 +130,10 @@ print(processor.decode(output[0], skip_special_tokens=True))\n [[autodoc]] MllamaTextModel\n     - forward\n \n+## MllamaModel\n+\n+[[autodoc]] MllamaModel\n+\n ## MllamaForCausalLM\n \n [[autodoc]] MllamaForCausalLM"
        },
        {
            "sha": "a0a0c1b714f5d7025867a3578b477a2bca2c7d7e",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -174,6 +174,10 @@ visualizer(\"<img> What is in this image?\")\n \n [[autodoc]] PaliGemmaProcessor\n \n+## PaliGemmaModel\n+\n+[[autodoc]] PaliGemmaModel\n+\n ## PaliGemmaForConditionalGeneration\n \n [[autodoc]] PaliGemmaForConditionalGeneration"
        },
        {
            "sha": "57b88d1b8daa4554acd00a33d7289293c78797c2",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -240,6 +240,10 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2_5_VLProcessor\n \n+## Qwen2_5_VLTextModel\n+\n+[[autodoc]] Qwen2_5_VLTextModel\n+    - forward\n \n ## Qwen2_5_VLModel\n "
        },
        {
            "sha": "7fef4e2fdbdc031213afc3d38deb0f852daad541",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -296,6 +296,11 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n \n [[autodoc]] Qwen2VLProcessor\n \n+## Qwen2VLTextModel\n+\n+[[autodoc]] Qwen2VLTextModel\n+    - forward\n+    \n ## Qwen2VLModel\n \n [[autodoc]] Qwen2VLModel"
        },
        {
            "sha": "ca1a06d4cdcca3c23bbf01d75743f1e014049197",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -215,6 +215,10 @@ model = VideoLlavaForConditionalGeneration.from_pretrained(\n \n [[autodoc]] VideoLlavaProcessor\n \n+## VideoLlavaModel\n+\n+[[autodoc]] VideoLlavaModel\n+\n ## VideoLlavaForConditionalGeneration\n \n [[autodoc]] VideoLlavaForConditionalGeneration"
        },
        {
            "sha": "8edf15402689a6ed2bff8f37c7ee108f715f76ad",
            "filename": "docs/source/en/model_doc/vipllava.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvipllava.md?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -101,6 +101,10 @@ A chat between a curious human and an artificial intelligence assistant. The ass\n \n [[autodoc]] VipLlavaConfig\n \n+## VipLlavaModel\n+\n+[[autodoc]] VipLlavaModel\n+\n ## VipLlavaForConditionalGeneration\n \n [[autodoc]] VipLlavaForConditionalGeneration"
        },
        {
            "sha": "344990c2c9f39007cf6f9b799b7c307bc91d6fde",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 1,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -216,6 +216,28 @@ def is_local_dist_rank_0():\n     \"kaiming_normal\": nn.init.kaiming_normal,\n }\n \n+# DO NOT MODIFY, KEPT FOR BC ONLY\n+VLMS = [\n+    \"aria\",\n+    \"aya_vision\",\n+    \"emu3\",\n+    \"fuyu\",\n+    \"got_ocr2\",\n+    \"gemma3\",\n+    \"internvl\",\n+    \"llava\",\n+    \"llava_next\",\n+    \"llava_next_video\",\n+    \"llava_onevision\",\n+    \"mistral3\",\n+    \"mllama\",\n+    \"paligemma\",\n+    \"qwen2_vl\",\n+    \"qwem2_5_vl\",\n+    \"video_llava\",\n+    \"vipllava\",\n+]\n+\n \n @contextmanager\n def no_init_weights():\n@@ -1778,6 +1800,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     main_input_name = \"input_ids\"\n     model_tags = None\n \n+    _checkpoint_conversion_mapping = {}  # used for BC support in VLMs, not meant to be used by new models\n+\n     _auto_class = None\n     _no_split_modules = None\n     _skip_keys_device_placement = None\n@@ -3484,6 +3508,21 @@ def save_pretrained(\n                         module_map[name + f\".{key}\"] = module\n             state_dict = model_to_save.state_dict()\n \n+        if any(allowed_name in self.__class__.__name__.lower() for allowed_name in VLMS):\n+            reverse_key_mapping = {v: k for k, v in self._checkpoint_conversion_mapping.items()}\n+\n+            original_state_dict = {}\n+            for key, value in state_dict.items():\n+                for pattern, replacement in reverse_key_mapping.items():\n+                    replacement = replacement.lstrip(\"^\")  # strip off un-needed chars and patterns\n+                    replacement = re.sub(r\"\\(.*?\\)\", \"\", pattern)\n+                    key, n_replace = re.subn(pattern, replacement, key)\n+                    # Early exit of the loop\n+                    if n_replace > 0:\n+                        break\n+                original_state_dict[key] = value\n+            state_dict = original_state_dict\n+\n         # Translate state_dict from smp to hf if saving with smp >= 1.10\n         if IS_SAGEMAKER_MP_POST_1_10:\n             for smp_to_hf, _ in smp.state.module_manager.translate_functions:\n@@ -4071,7 +4110,13 @@ def from_pretrained(\n         gguf_file = kwargs.pop(\"gguf_file\", None)\n         tp_plan = kwargs.pop(\"tp_plan\", None)\n         tp_size = kwargs.pop(\"tp_size\", None)\n-        key_mapping = kwargs.pop(\"key_mapping\", None)\n+\n+        # Load models with hardcoded key mapping on class for VLMs only,  to keep BC and standardize model\n+        if any(allowed_name in cls.__name__.lower() for allowed_name in VLMS):\n+            key_mapping = kwargs.pop(\"key_mapping\", cls._checkpoint_conversion_mapping)\n+        else:\n+            key_mapping = kwargs.pop(\"key_mapping\", None)\n+\n         # Not used anymore -- remove them from the kwargs\n         _ = kwargs.pop(\"resume_download\", None)\n         _ = kwargs.pop(\"trust_remote_code\", None)"
        },
        {
            "sha": "31e0980fdebf0f68061767c77f38cb89c0bc41f8",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 271,
            "deletions": 89,
            "changes": 360,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -42,7 +42,7 @@\n     replace_return_docstrings,\n )\n from ...utils.import_utils import is_torch_available\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_aria import AriaConfig, AriaTextConfig\n \n \n@@ -58,7 +58,9 @@\n \n \n logger = logging.get_logger(__name__)\n-_CONFIG_FOR_DOC = \"AriaTextConfig\"\n+\n+\n+_CONFIG_FOR_DOC = \"AriaConfig\"\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -659,7 +661,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n     \"\"\"\n \n-    config_class = AriaConfig\n+    config_class = AriaTextConfig\n     base_model_prefix = \"model\"\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n@@ -706,8 +708,8 @@ def _init_weights(self, module):\n     ARIA_TEXT_START_DOCSTRING,\n )\n class AriaPreTrainedModel(PreTrainedModel):\n-    config_class = AriaTextConfig\n-    base_model_prefix = \"model\"\n+    config_class = AriaConfig\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"AriaDecoderLayer\"]\n     _skip_keys_device_placement = [\"past_key_values\"]\n@@ -1097,6 +1099,9 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n+_CONFIG_FOR_TEXT_DOC = \"AriaTextConfig\"\n+\n+\n class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n     \"\"\"\n     Aria model for causal language modeling tasks.\n@@ -1112,7 +1117,6 @@ class AriaTextForCausalLM(AriaTextPreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n-    config_class = AriaTextConfig\n \n     def __init__(self, config: AriaTextConfig):\n         super().__init__(config)\n@@ -1141,9 +1145,8 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_TEXT_DOC)\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1255,7 +1258,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -1267,6 +1270,39 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n+@dataclass\n+class AriaModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Aria outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n ARIA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor`, *optional*):\n@@ -1320,30 +1356,131 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n \n \n @add_start_docstrings(\n-    \"\"\"Aria model for conditional generation tasks.\n-\n-    This model combines a vision tower, a multi-modal projector, and a language model\n-    to perform tasks that involve both image and text inputs.\"\"\",\n+    \"\"\"The Aria model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     ARIA_START_DOCSTRING,\n )\n-class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n-    config_class = AriaConfig\n-    _supports_flash_attn_2 = False\n-    _supports_flex_attn = False\n-    _supports_sdpa = False\n-    _tied_weights_keys = [\"language_model.lm_head.weight\"]\n+class AriaModel(AriaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n \n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n-\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n         self.multi_modal_projector = AriaProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def get_image_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_mask: torch.FloatTensor = None,\n+        vision_feature_layer: int = -1,\n+    ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            pixel_mask (`torch.FloatTensor]`, *optional*):\n+                The tensors corresponding to the input image mask.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n+        image_outputs = self.vision_tower(\n+            pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True\n+        )\n+        image_attn_mask = None\n+        if patch_attention_mask is not None:\n+            flattened_mask = patch_attention_mask.flatten(1)\n+            image_attn_mask = torch.logical_not(flattened_mask)\n+\n+        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n+        image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n+        return image_features\n+\n+    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, AriaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # 2. Merge text and images\n+        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                image_embeds = input_ids == self.config.image_token_id\n+                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                pixel_mask=pixel_mask,\n+                vision_feature_layer=self.config.vision_feature_layer,\n+            )\n+            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n+            n_image_features = n_images * n_features_per_image\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = AriaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values if use_cache else None,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n     def _create_patch_attention_mask(self, pixel_mask):\n         if pixel_mask is None:\n             return None\n@@ -1360,51 +1497,61 @@ def _create_patch_attention_mask(self, pixel_mask):\n         )\n         return (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n \n+\n+@add_start_docstrings(\n+    \"\"\"Aria model for conditional generation tasks.\n+    This model combines a vision tower, a multi-modal projector, and a language model\n+    to perform tasks that involve both image and text inputs.\"\"\",\n+    ARIA_START_DOCSTRING,\n+)\n+class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: AriaConfig):\n+        super().__init__(config)\n+        self.model = AriaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n     def get_input_embeddings(self):\n-        return self.language_model.get_input_embeddings()\n+        return self.model.get_input_embeddings()\n \n     def set_input_embeddings(self, value):\n-        self.language_model.set_input_embeddings(value)\n+        self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n \n     def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n+        self.lm_head = new_embeddings\n \n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n \n-    def get_image_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n-        vision_feature_layer: int = -1,\n-    ):\n-        patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n-        image_outputs = self.vision_tower(\n-            pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True\n-        )\n-        image_attn_mask = None\n-        if patch_attention_mask is not None:\n-            flattened_mask = patch_attention_mask.flatten(1)\n-            image_attn_mask = torch.logical_not(flattened_mask)\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n \n-        selected_image_feature = image_outputs.hidden_states[vision_feature_layer]\n-        image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n-        return image_features\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n \n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n+    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1413,10 +1560,11 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n-    ) -> AriaCausalLMOutputWithPast:\n+    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1482,49 +1630,27 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        # 2. Merge text and images\n-        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-            else:\n-                image_embeds = input_ids == self.config.image_token_id\n-                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                pixel_mask=pixel_mask,\n-                vision_feature_layer=self.config.vision_feature_layer,\n-            )\n-            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n-            n_image_features = n_images * n_features_per_image\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs: CausalLMOutputWithPast = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_mask=pixel_mask,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            logits_to_keep=logits_to_keep,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1552,7 +1678,7 @@ def prepare_inputs_for_generation(\n         logits_to_keep=None,\n         **kwargs,\n     ):\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1570,11 +1696,67 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n __all__ = [\n     \"AriaForConditionalGeneration\",\n     \"AriaPreTrainedModel\",\n     \"AriaTextPreTrainedModel\",\n     \"AriaTextModel\",\n+    \"AriaModel\",\n     \"AriaTextForCausalLM\",\n ]"
        },
        {
            "sha": "a42c7227772b9b6ac18eb04892335bca289baaf6",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 139,
            "deletions": 83,
            "changes": 222,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -18,7 +18,6 @@\n \n from ...activations import ACT2FN\n from ...configuration_utils import PretrainedConfig\n-from ...generation import GenerationMixin\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n from ...image_utils import (\n@@ -49,7 +48,7 @@\n     replace_return_docstrings,\n )\n from ...utils.import_utils import is_torch_available\n-from ..auto import CONFIG_MAPPING, AutoConfig, AutoModel, AutoModelForCausalLM, AutoTokenizer\n+from ..auto import CONFIG_MAPPING, AutoConfig, AutoTokenizer\n from ..llama.configuration_llama import LlamaConfig\n from ..llama.modeling_llama import (\n     LlamaDecoderLayer,\n@@ -59,7 +58,12 @@\n     LlamaPreTrainedModel,\n     LlamaRMSNorm,\n )\n-from ..llava.modeling_llava import LlavaCausalLMOutputWithPast\n+from ..llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n+)\n from ..llava_next.image_processing_llava_next import divide_to_patches\n \n \n@@ -70,6 +74,11 @@\n     from torch import nn\n \n \n+_CONFIG_FOR_DOC = \"AriaConfig\"\n+_CONFIG_FOR_TEXT_DOC = \"AriaTextConfig\"\n+ARIA_TEXT_INPUTS_DOCSTRING = None\n+\n+\n def sequential_experts_gemm(token_states, expert_weights, tokens_per_expert):\n     \"\"\"\n     Compute the matrix multiplication (GEMM) for each expert sequentially. This approach is computationally inefficient, especially when dealing with a large number of experts.\n@@ -1223,7 +1232,7 @@ class AriaTextPreTrainedModel(PreTrainedModel):\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n     \"\"\"\n \n-    config_class = AriaConfig\n+    config_class = AriaTextConfig\n     base_model_prefix = \"model\"\n     _no_split_modules = [\"AriaTextDecoderLayer\", \"AriaGroupedExpertsGemm\"]\n     supports_gradient_checkpointing = True\n@@ -1249,6 +1258,8 @@ def _init_weights(self, module):\n \n \n class AriaPreTrainedModel(LlamaPreTrainedModel):\n+    config_class = AriaConfig\n+    base_model_prefix = \"\"\n     _supports_static_cache = False  # MoE models don't work with torch.compile (dynamic slicing)\n     _supports_attention_backend = False\n \n@@ -1292,7 +1303,6 @@ class AriaTextForCausalLM(AriaTextPreTrainedModel, LlamaForCausalLM):\n     \"\"\"\n \n     _tied_weights_keys = [\"lm_head.weight\"]\n-    config_class = AriaTextConfig\n \n     def __init__(self, config: AriaTextConfig):\n         super().__init__(config)\n@@ -1303,11 +1313,20 @@ def __init__(self, config: AriaTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    @add_start_docstrings_to_model_forward(ARIA_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_TEXT_DOC)\n+    def forward(self, **super_kwargs):\n+        super().forward(self, **super_kwargs)\n+\n \n class AriaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n+class AriaModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n ARIA_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor`, *optional*):\n@@ -1360,30 +1379,10 @@ class AriaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n \"\"\"\n \n \n-@add_start_docstrings(\n-    \"\"\"Aria model for conditional generation tasks.\n-\n-    This model combines a vision tower, a multi-modal projector, and a language model\n-    to perform tasks that involve both image and text inputs.\"\"\",\n-    ARIA_START_DOCSTRING,\n-)\n-class AriaForConditionalGeneration(AriaPreTrainedModel, GenerationMixin):\n-    config_class = AriaConfig\n-    _supports_flash_attn_2 = False\n-    _supports_flex_attn = False\n-    _supports_sdpa = False\n-    _tied_weights_keys = [\"language_model.lm_head.weight\"]\n-\n+class AriaModel(LlavaModel):\n     def __init__(self, config: AriaConfig):\n         super().__init__(config)\n-\n-        self.vision_tower = AutoModel.from_config(config.vision_config)\n         self.multi_modal_projector = AriaProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-        self._use_flash_attention_2 = config.text_config._attn_implementation == \"flash_attention_2\"\n-        self.post_init()\n \n     def _create_patch_attention_mask(self, pixel_mask):\n         if pixel_mask is None:\n@@ -1401,30 +1400,27 @@ def _create_patch_attention_mask(self, pixel_mask):\n         )\n         return (patches_subgrid.sum(dim=(-1, -2)) > 0).bool()\n \n-    def get_input_embeddings(self):\n-        return self.language_model.get_input_embeddings()\n-\n-    def set_input_embeddings(self, value):\n-        self.language_model.set_input_embeddings(value)\n-\n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n-        pixel_mask: Optional[torch.FloatTensor] = None,\n+        pixel_mask: torch.FloatTensor = None,\n         vision_feature_layer: int = -1,\n     ):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n+               The tensors corresponding to the input images.\n+            pixel_mask (`torch.FloatTensor]`, *optional*):\n+                The tensors corresponding to the input image mask.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n         patch_attention_mask = self._create_patch_attention_mask(pixel_mask)\n         image_outputs = self.vision_tower(\n             pixel_values, patch_attention_mask=patch_attention_mask, output_hidden_states=True\n@@ -1438,14 +1434,94 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature, attn_mask=image_attn_mask)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, AriaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # 2. Merge text and images\n+        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                image_embeds = input_ids == self.config.image_token_id\n+                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                pixel_mask=pixel_mask,\n+                vision_feature_layer=self.config.vision_feature_layer,\n+            )\n+            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n+            n_image_features = n_images * n_features_per_image\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = AriaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values if use_cache else None,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Aria model for conditional generation tasks.\n+    This model combines a vision tower, a multi-modal projector, and a language model\n+    to perform tasks that involve both image and text inputs.\"\"\",\n+    ARIA_START_DOCSTRING,\n+)\n+class AriaForConditionalGeneration(LlavaForConditionalGeneration):\n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(ARIA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=AriaConfig)\n+    @replace_return_docstrings(output_type=AriaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_mask: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -1454,10 +1530,11 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         cache_position: Optional[torch.LongTensor] = None,\n         **loss_kwargs,\n-    ) -> AriaCausalLMOutputWithPast:\n+    ) -> Union[Tuple, AriaCausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1523,49 +1600,27 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        # 2. Merge text and images\n-        if pixel_values is not None and inputs_embeds.shape[1] != 1:\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-            else:\n-                image_embeds = input_ids == self.config.image_token_id\n-                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                pixel_mask=pixel_mask,\n-                vision_feature_layer=self.config.vision_feature_layer,\n-            )\n-            n_images, n_features_per_image = image_features.shape[0], image_features.shape[1]\n-            n_image_features = n_images * n_features_per_image\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs: CausalLMOutputWithPast = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_mask=pixel_mask,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            logits_to_keep=logits_to_keep,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n@@ -1593,7 +1648,7 @@ def prepare_inputs_for_generation(\n         logits_to_keep=None,\n         **kwargs,\n     ):\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1621,5 +1676,6 @@ def prepare_inputs_for_generation(\n     \"AriaPreTrainedModel\",\n     \"AriaTextPreTrainedModel\",\n     \"AriaTextModel\",\n+    \"AriaModel\",\n     \"AriaTextForCausalLM\",\n ]"
        },
        {
            "sha": "bd7dff185b2e91d64fc2a9aed9796f356202fe88",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 21,
            "deletions": 4,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -35,10 +35,11 @@\n         (\"albert\", \"AlbertModel\"),\n         (\"align\", \"AlignModel\"),\n         (\"altclip\", \"AltCLIPModel\"),\n-        (\"aria\", \"AriaForConditionalGeneration\"),\n+        (\"aria\", \"AriaModel\"),\n         (\"aria_text\", \"AriaTextModel\"),\n         (\"audio-spectrogram-transformer\", \"ASTModel\"),\n         (\"autoformer\", \"AutoformerModel\"),\n+        (\"aya_vision\", \"AyaVisionModel\"),\n         (\"bamba\", \"BambaModel\"),\n         (\"bark\", \"BarkModel\"),\n         (\"bart\", \"BartModel\"),\n@@ -108,6 +109,7 @@\n         (\"efficientformer\", \"EfficientFormerModel\"),\n         (\"efficientnet\", \"EfficientNetModel\"),\n         (\"electra\", \"ElectraModel\"),\n+        (\"emu3\", \"Emu3Model\"),\n         (\"encodec\", \"EncodecModel\"),\n         (\"ernie\", \"ErnieModel\"),\n         (\"ernie_m\", \"ErnieMModel\"),\n@@ -121,14 +123,16 @@\n         (\"focalnet\", \"FocalNetModel\"),\n         (\"fsmt\", \"FSMTModel\"),\n         (\"funnel\", (\"FunnelModel\", \"FunnelBaseModel\")),\n+        (\"fuyu\", \"FuyuModel\"),\n         (\"gemma\", \"GemmaModel\"),\n         (\"gemma2\", \"Gemma2Model\"),\n+        (\"gemma3\", \"Gemma3Model\"),\n         (\"gemma3_text\", \"Gemma3TextModel\"),\n         (\"git\", \"GitModel\"),\n         (\"glm\", \"GlmModel\"),\n         (\"glm4\", \"Glm4Model\"),\n         (\"glpn\", \"GLPNModel\"),\n-        (\"got_ocr2\", \"GotOcr2ForConditionalGeneration\"),\n+        (\"got_ocr2\", \"GotOcr2Model\"),\n         (\"gpt-sw3\", \"GPT2Model\"),\n         (\"gpt2\", \"GPT2Model\"),\n         (\"gpt_bigcode\", \"GPTBigCodeModel\"),\n@@ -156,6 +160,9 @@\n         (\"ijepa\", \"IJepaModel\"),\n         (\"imagegpt\", \"ImageGPTModel\"),\n         (\"informer\", \"InformerModel\"),\n+        (\"instructblip\", \"InstructBlipModel\"),\n+        (\"instructblipvideo\", \"InstructBlipVideoModel\"),\n+        (\"internvl\", \"InternVLModel\"),\n         (\"internvl_vision\", \"InternVLVisionModel\"),\n         (\"jamba\", \"JambaModel\"),\n         (\"janus\", \"JanusModel\"),\n@@ -170,6 +177,10 @@\n         (\"lilt\", \"LiltModel\"),\n         (\"llama\", \"LlamaModel\"),\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),\n+        (\"llava\", \"LlavaModel\"),\n+        (\"llava_next\", \"LlavaNextModel\"),\n+        (\"llava_next_video\", \"LlavaNextVideoModel\"),\n+        (\"llava_onevision\", \"LlavaOnevisionModel\"),\n         (\"longformer\", \"LongformerModel\"),\n         (\"longt5\", \"LongT5Model\"),\n         (\"luke\", \"LukeModel\"),\n@@ -189,8 +200,10 @@\n         (\"mgp-str\", \"MgpstrForSceneTextRecognition\"),\n         (\"mimi\", \"MimiModel\"),\n         (\"mistral\", \"MistralModel\"),\n+        (\"mistral3\", \"Mistral3Model\"),\n         (\"mixtral\", \"MixtralModel\"),\n         (\"mlcd\", \"MLCDVisionModel\"),\n+        (\"mllama\", \"MllamaModel\"),\n         (\"mobilebert\", \"MobileBertModel\"),\n         (\"mobilenet_v1\", \"MobileNetV1Model\"),\n         (\"mobilenet_v2\", \"MobileNetV2Model\"),\n@@ -221,6 +234,7 @@\n         (\"opt\", \"OPTModel\"),\n         (\"owlv2\", \"Owlv2Model\"),\n         (\"owlvit\", \"OwlViTModel\"),\n+        (\"paligemma\", \"PaliGemmaModel\"),\n         (\"patchtsmixer\", \"PatchTSMixerModel\"),\n         (\"patchtst\", \"PatchTSTModel\"),\n         (\"pegasus\", \"PegasusModel\"),\n@@ -240,11 +254,11 @@\n         (\"qdqbert\", \"QDQBertModel\"),\n         (\"qwen2\", \"Qwen2Model\"),\n         (\"qwen2_5_vl\", \"Qwen2_5_VLModel\"),\n-        (\"qwen2_5_vl_text\", \"Qwen2_5_VLModel\"),\n+        (\"qwen2_5_vl_text\", \"Qwen2_5_VLTextModel\"),\n         (\"qwen2_audio_encoder\", \"Qwen2AudioEncoder\"),\n         (\"qwen2_moe\", \"Qwen2MoeModel\"),\n         (\"qwen2_vl\", \"Qwen2VLModel\"),\n-        (\"qwen2_vl_text\", \"Qwen2VLModel\"),\n+        (\"qwen2_vl_text\", \"Qwen2VLTextModel\"),\n         (\"qwen3\", \"Qwen3Model\"),\n         (\"qwen3_moe\", \"Qwen3MoeModel\"),\n         (\"recurrent_gemma\", \"RecurrentGemmaModel\"),\n@@ -306,8 +320,10 @@\n         (\"unispeech-sat\", \"UniSpeechSatModel\"),\n         (\"univnet\", \"UnivNetModel\"),\n         (\"van\", \"VanModel\"),\n+        (\"video_llava\", \"VideoLlavaModel\"),\n         (\"videomae\", \"VideoMAEModel\"),\n         (\"vilt\", \"ViltModel\"),\n+        (\"vipllava\", \"VipLlavaModel\"),\n         (\"vision-text-dual-encoder\", \"VisionTextDualEncoderModel\"),\n         (\"visual_bert\", \"VisualBertModel\"),\n         (\"vit\", \"ViTModel\"),\n@@ -879,6 +895,7 @@\n         (\"llama4\", \"Llama4ForConditionalGeneration\"),\n         (\"llava\", \"LlavaForConditionalGeneration\"),\n         (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n+        (\"llava_next_video\", \"LlavaNextVideoForConditionalGeneration\"),\n         (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n         (\"mistral3\", \"Mistral3ForConditionalGeneration\"),\n         (\"mllama\", \"MllamaForConditionalGeneration\"),"
        },
        {
            "sha": "042ff0c05a7187dc033413a7c9d90df0798ddf01",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 249,
            "deletions": 96,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -27,15 +27,16 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_aya_vision import AyaVisionConfig\n \n \n@@ -115,9 +116,8 @@ def pixel_shuffle(self, image_features):  # B, S, D\n )\n class AyaVisionPreTrainedModel(PreTrainedModel):\n     config_class = AyaVisionConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"AyaVisionVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -169,7 +169,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -181,7 +181,40 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-AYA_VISION_INPUTS_DOCSTRING = \"\"\"\n+@dataclass\n+class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for AyaVision outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+AYA_VISION_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n             Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n@@ -193,8 +226,8 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n             [What are input IDs?](../glossary#input-ids)\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n             The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`AyaVisionProcessor`] uses\n-            [`GotOcr2ImageProcessor`] for processing images.\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`AyaVisionProcessor`] uses\n+            [`CLIPImageProcessor`] for processing images).\n         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -259,23 +292,18 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n \n \n @add_start_docstrings(\n-    \"\"\"The AyaVision model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The AyaVision model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     AYA_VISION_START_DOCSTRING,\n )\n-class AyaVisionForConditionalGeneration(AyaVisionPreTrainedModel, GenerationMixin):\n+class AyaVisionModel(AyaVisionPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: AyaVisionConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = AyaVisionMultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -284,18 +312,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -307,7 +323,7 @@ def get_image_features(\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n-            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n             vision_feature_layer (`Union[int, List[int]]`):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n@@ -342,6 +358,140 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, AyaVisionModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = AyaVisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The AyaVision model which consists of a vision backbone and a language model.\"\"\",\n+    AYA_VISION_START_DOCSTRING,\n+)\n+class AyaVisionForConditionalGeneration(AyaVisionPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: AyaVisionConfig):\n+        super().__init__(config)\n+        self.model = AyaVisionModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(AYA_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=AyaVisionCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -410,7 +560,6 @@ def forward(\n         >>> gen_tokens = model.generate(**inputs, max_new_tokens=300, do_sample=True, temperature=0.3)\n         >>> processor.tokenizer.decode(gen_tokens[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -425,81 +574,40 @@ def forward(\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return AyaVisionCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -515,7 +623,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -532,15 +640,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n+        return causal_mask\n \n \n-__all__ = [\"AyaVisionForConditionalGeneration\", \"AyaVisionPreTrainedModel\"]\n+__all__ = [\"AyaVisionForConditionalGeneration\", \"AyaVisionPreTrainedModel\", \"AyaVisionModel\"]"
        },
        {
            "sha": "5d7acecff8874a4c5e0a6fc95d4dd3b61a75adbf",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 6,
            "deletions": 101,
            "changes": 107,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -22,6 +22,7 @@\n from transformers.models.llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n+    LlavaModel,\n     LlavaPreTrainedModel,\n )\n \n@@ -88,21 +89,8 @@ def pixel_shuffle(self, image_features):  # B, S, D\n         return image_features\n \n \n-AYA_VISION_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`AyaVisionConfig`] or [`AyaVisionVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n+AYA_VISION_START_DOCSTRING = None\n+AYA_VISION_INPUTS_DOCSTRING = None\n \n \n @add_start_docstrings(\n@@ -133,98 +121,15 @@ class AyaVisionCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n-AYA_VISION_INPUTS_DOCSTRING = \"\"\"\n-    Args:\n-        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n-            it.\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`GotOcr2ImageProcessor.__call__`] for details. [`AyaVisionProcessor`] uses\n-            [`GotOcr2ImageProcessor`] for processing images.\n-        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n-\n-            - 1 for tokens that are **not masked**,\n-            - 0 for tokens that are **masked**.\n-\n-            [What are attention masks?](../glossary#attention-mask)\n-\n-            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n-            `past_key_values`).\n-\n-            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n-            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n-            information on the default strategy.\n-\n-            - 1 indicates the head is **not masked**,\n-            - 0 indicates the head is **masked**.\n-        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n-            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n-            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n-            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n-\n-            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n-            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n-            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n-        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n-            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n-            model's internal embedding lookup matrix.\n-        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n-            The index of the layer to select the vision feature. If multiple indices are provided,\n-            the vision feature of the corresponding indices will be concatenated to form the\n-            vision features.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Can be one of `\"default\"` or `\"full\"`.\n-        use_cache (`bool`, *optional*):\n-            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n-            `past_key_values`).\n-        output_attentions (`bool`, *optional*):\n-            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n-            tensors for more detail.\n-        output_hidden_states (`bool`, *optional*):\n-            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n-            more detail.\n-        return_dict (`bool`, *optional*):\n-            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n-        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n-            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n-            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n-            the complete sequence length.\n-\"\"\"\n+class AyaVisionModel(LlavaModel):\n+    pass\n \n \n @add_start_docstrings(\n     \"\"\"The AyaVision model which consists of a vision backbone and a language model.\"\"\",\n     AYA_VISION_START_DOCSTRING,\n )\n class AyaVisionForConditionalGeneration(LlavaForConditionalGeneration):\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        # update vocab size\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -312,4 +217,4 @@ def forward(\n         )\n \n \n-__all__ = [\"AyaVisionForConditionalGeneration\", \"AyaVisionPreTrainedModel\"]\n+__all__ = [\"AyaVisionForConditionalGeneration\", \"AyaVisionPreTrainedModel\", \"AyaVisionModel\"]"
        },
        {
            "sha": "c3a669d4c98bd10719b4d7f85f02fefc345672ec",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -175,8 +175,8 @@ def __init__(self, config: ColPaliConfig):\n         self.vocab_size = config.vlm_config.text_config.vocab_size\n \n         vlm = AutoModelForImageTextToText.from_config(config.vlm_config)\n-        if vlm.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"vlm.language_model.{k}\" for k in vlm.language_model._tied_weights_keys]\n+        if vlm._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"vlm.{k}\" for k in vlm._tied_weights_keys]\n         self.vlm = vlm\n \n         self.embedding_dim = self.config.embedding_dim\n@@ -246,33 +246,33 @@ def forward(\n         )\n \n     def get_input_embeddings(self):\n-        return self.vlm.language_model.get_input_embeddings()\n+        return self.vlm.get_input_embeddings()\n \n     def set_input_embeddings(self, value):\n-        self.vlm.language_model.set_input_embeddings(value)\n+        self.vlm.set_input_embeddings(value)\n \n     def get_output_embeddings(self):\n-        return self.vlm.language_model.get_output_embeddings()\n+        return self.vlm.get_output_embeddings()\n \n     def set_output_embeddings(self, new_embeddings):\n-        self.vlm.language_model.set_output_embeddings(new_embeddings)\n+        self.vlm.set_output_embeddings(new_embeddings)\n \n     def set_decoder(self, decoder):\n-        self.vlm.language_model.set_decoder(decoder)\n+        self.vlm.set_decoder(decoder)\n \n     def get_decoder(self):\n-        return self.vlm.language_model.get_decoder()\n+        return self.vlm.get_decoder()\n \n     def tie_weights(self):\n-        return self.vlm.language_model.tie_weights()\n+        return self.vlm.tie_weights()\n \n     def resize_token_embeddings(\n         self,\n         new_num_tokens: Optional[int] = None,\n         pad_to_multiple_of: Optional[int] = None,\n         mean_resizing: bool = True,\n     ) -> nn.Embedding:\n-        model_embeds = self.vlm.language_model.resize_token_embeddings(\n+        model_embeds = self.vlm.resize_token_embeddings(\n             new_num_tokens=new_num_tokens,\n             pad_to_multiple_of=pad_to_multiple_of,\n             mean_resizing=mean_resizing,"
        },
        {
            "sha": "45031a1a647e9628e797fcb99b4ead5c456d73cb",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 182,
            "deletions": 28,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -1778,13 +1778,16 @@ def forward(\n \"\"\"\n \n \n-class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compilable\n+class Emu3Model(Emu3PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"text_model.model\": \"text_model\"}\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)\n-        self.text_model = Emu3ForCausalLM._from_config(config.text_config)\n+        self.text_model = Emu3TextModel._from_config(config.text_config)\n+        if self.text_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"text_model.{k}\" for k in self.text_model._tied_weights_keys]\n+\n         self.vqmodel = Emu3VQVAE(config.vq_config)\n         self.vocabulary_mapping = Emu3ImageVocabularyMapping(config.vocabulary_map)\n \n@@ -1833,25 +1836,112 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None:\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n+            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        return outputs\n+\n+\n+class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    base_model_prefix = \"\"\n+    _checkpoint_conversion_mapping = {\n+        \"^text_model.model\": \"model.text_model\",\n+        \"^vqmodel\": \"model.vqmodel\",\n+        \"^text_model.lm_head\": \"lm_head\",\n+    }\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Emu3Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def text_model(self):\n+        return self.model.text_model\n+\n+    @property\n+    def vqmodel(self):\n+        return self.model.vqmodel\n+\n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> CausalLMOutputWithPast:\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1906,25 +1996,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n-            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        return self.text_model(\n+        outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1933,8 +2007,25 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1968,5 +2059,68 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"Emu3ForConditionalGeneration\", \"Emu3ForCausalLM\", \"Emu3TextModel\", \"Emu3PreTrainedModel\", \"Emu3VQVAE\"]\n+__all__ = [\n+    \"Emu3ForConditionalGeneration\",\n+    \"Emu3ForCausalLM\",\n+    \"Emu3TextModel\",\n+    \"Emu3PreTrainedModel\",\n+    \"Emu3VQVAE\",\n+    \"Emu3Model\",\n+]"
        },
        {
            "sha": "6075b8d737031eba48487c19e6b346b4b09cdfb1",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 175,
            "deletions": 27,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -1121,13 +1121,16 @@ def forward(**super_kwargs):\n         super().forward()\n \n \n-class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"text_model.lm_head.weight\"]\n-    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compilable\n+class Emu3Model(Emu3PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"text_model.model\": \"text_model\"}\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n \n     def __init__(self, config):\n         super().__init__(config)\n-        self.text_model = Emu3ForCausalLM._from_config(config.text_config)\n+        self.text_model = Emu3TextModel._from_config(config.text_config)\n+        if self.text_model._tied_weights_keys is not None:\n+            self._tied_weights_keys = [f\"text_model.{k}\" for k in self.text_model._tied_weights_keys]\n+\n         self.vqmodel = Emu3VQVAE(config.vq_config)\n         self.vocabulary_mapping = Emu3ImageVocabularyMapping(config.vocabulary_map)\n \n@@ -1176,25 +1179,112 @@ def decode_image_tokens(self, image_tokens: torch.LongTensor, height: int, width\n         image = self.vqmodel.decode(image_tokens)\n         return image\n \n+    @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\n+                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None:\n+            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n+            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n+            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+        )\n+\n+        return outputs\n+\n+\n+class Emu3ForConditionalGeneration(Emu3PreTrainedModel, GenerationMixin):\n+    base_model_prefix = \"\"\n+    _checkpoint_conversion_mapping = {\n+        \"^text_model.model\": \"model.text_model\",\n+        \"^vqmodel\": \"model.vqmodel\",\n+        \"^text_model.lm_head\": \"lm_head\",\n+    }\n+    _supports_static_cache = False  # `get_image_tokens()`, called when `pixel_values` is passed, is not compileable\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Emu3Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def text_model(self):\n+        return self.model.text_model\n+\n+    @property\n+    def vqmodel(self):\n+        return self.model.vqmodel\n+\n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(EMU3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> CausalLMOutputWithPast:\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -1249,25 +1339,9 @@ def forward(\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\n-                \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n-            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n-\n-        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n-        return self.text_model(\n+        outputs = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1276,8 +1350,25 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1311,11 +1402,68 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n __all__ = [\n     \"Emu3ForConditionalGeneration\",\n     \"Emu3ForCausalLM\",\n     \"Emu3TextModel\",\n     \"Emu3PreTrainedModel\",\n     \"Emu3VQVAE\",\n+    \"Emu3Model\",\n ]"
        },
        {
            "sha": "ec74b9be3d55e2fe0747c9a0fda263ed148ca49f",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 145,
            "deletions": 61,
            "changes": 206,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -21,9 +21,9 @@\n from torch import nn\n \n from ...generation import GenerationMixin\n-from ...modeling_outputs import CausalLMOutputWithPast\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...models.auto.modeling_auto import AutoModelForCausalLM\n+from ...models.auto.modeling_auto import AutoModel\n from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n from .configuration_fuyu import FuyuConfig\n \n@@ -143,18 +143,17 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"Fuyu Model with a language modeling head on top for causal language model conditioned on image patches and text.\",\n+    \"\"\"The Fuyu model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     FUYU_START_DOCSTRING,\n )\n-class FuyuForCausalLM(FuyuPreTrainedModel, GenerationMixin):\n+class FuyuModel(FuyuPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: FuyuConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.vision_embed_tokens = nn.Linear(\n             config.patch_size * config.patch_size * config.num_channels, config.hidden_size\n         )\n@@ -169,18 +168,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def gather_continuous_embeddings(\n         self,\n         word_embeddings: torch.Tensor,\n@@ -224,56 +211,21 @@ def gather_continuous_embeddings(\n         return output_embeddings\n \n     @add_start_docstrings_to_model_forward(FUYU_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        image_patches: Optional[\n-            torch.Tensor\n-        ] = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n-        image_patches_indices: Optional[torch.Tensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        image_patches: torch.Tensor = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n+        image_patches_indices: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n-        labels: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         **kwargs,\n-    ) -> Union[Tuple, CausalLMOutputWithPast]:\n-        r\"\"\"\n-        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n-\n-        Returns:\n-\n-        Examples:\n-\n-        ```python\n-        >>> from transformers import FuyuProcessor, FuyuForCausalLM\n-        >>> from PIL import Image\n-        >>> import requests\n-\n-        >>> processor = FuyuProcessor.from_pretrained(\"adept/fuyu-8b\")\n-        >>> model = FuyuForCausalLM.from_pretrained(\"adept/fuyu-8b\")\n-\n-        >>> url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-        >>> prompt = \"Generate a coco-style caption.\\n\"\n-\n-        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n-        >>> outputs = model(**inputs)\n-\n-        >>> generated_ids = model.generate(**inputs, max_new_tokens=7)\n-        >>> generation_text = processor.batch_decode(generated_ids[:, -7:], skip_special_tokens=True)\n-        >>> print(generation_text[0])\n-        A blue bus parked on the side of a road.\n-        ```\"\"\"\n-\n+    ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -327,14 +279,146 @@ def forward(\n             past_key_values=past_key_values,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            labels=labels,\n             use_cache=use_cache,\n             return_dict=return_dict,\n             **kwargs,\n         )\n \n         return outputs\n \n+\n+@add_start_docstrings(\n+    \"Fuyu Model with a language modeling head on top for causal language model conditioned on image patches and text.\",\n+    FUYU_START_DOCSTRING,\n+)\n+class FuyuForCausalLM(FuyuPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_embed_tokens\": \"model.vision_embed_tokens\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: FuyuConfig):\n+        super().__init__(config)\n+        self.model = FuyuModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model.set_decoder(decoder)\n+\n+    def get_decoder(self):\n+        return self.model.get_decoder()\n+\n+    @add_start_docstrings_to_model_forward(FUYU_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        image_patches: torch.Tensor = None,  # [batch_size, num_total_patches, patch_size_ x patch_size x num_channels ]\n+        image_patches_indices: torch.Tensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        logits_to_keep: Optional[int] = 0,\n+        **kwargs,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n+\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import FuyuProcessor, FuyuForCausalLM\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> processor = FuyuProcessor.from_pretrained(\"adept/fuyu-8b\")\n+        >>> model = FuyuForCausalLM.from_pretrained(\"adept/fuyu-8b\")\n+\n+        >>> url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+        >>> prompt = \"Generate a coco-style caption.\\n\"\n+\n+        >>> inputs = processor(images=image, text=prompt, return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+\n+        >>> generated_ids = model.generate(**inputs, max_new_tokens=7)\n+        >>> generation_text = processor.batch_decode(generated_ids[:, -7:], skip_special_tokens=True)\n+        >>> print(generation_text[0])\n+        A blue bus parked on the side of a road.\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            image_patches=image_patches,\n+            image_patches_indices=image_patches_indices,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            use_cache=use_cache,\n+            return_dict=return_dict,\n+            # don't pass kwargs because Persimmon-backbone doesn't accept FA2 kwargs yet, TODO: raushan\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(\n+                logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size, **kwargs\n+            )\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,\n@@ -373,4 +457,4 @@ def _reorder_cache(past_key_values, beam_idx):\n         return reordered_past\n \n \n-__all__ = [\"FuyuForCausalLM\", \"FuyuPreTrainedModel\"]\n+__all__ = [\"FuyuForCausalLM\", \"FuyuPreTrainedModel\", \"FuyuModel\"]"
        },
        {
            "sha": "74642ea6baa43b3522c430f2977980f54f9a28dc",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 264,
            "deletions": 89,
            "changes": 353,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -32,11 +32,12 @@\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n@@ -46,7 +47,7 @@\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n \n@@ -60,6 +61,39 @@\n _CONFIG_FOR_DOC = \"Gemma3Config\"\n \n \n+@dataclass\n+class Gemma3ModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Gemma3 outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class Gemma3CausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -88,7 +122,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n     \"\"\"\n \n@@ -480,7 +514,7 @@ def forward(\n )\n class Gemma3PreTrainedModel(PreTrainedModel):\n     config_class = Gemma3Config\n-    base_model_prefix = \"language_model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n@@ -1066,20 +1100,19 @@ def forward(self, vision_outputs: torch.Tensor):\n \n \n @add_start_docstrings(\n-    \"\"\"The GEMMA3 model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"Base Gemma3 model which consists of a vision backbone and a language model withou language modeling head.\"\"\",\n     GEMMA3_START_DOCSTRING,\n )\n-class Gemma3ForConditionalGeneration(Gemma3PreTrainedModel, GenerationMixin):\n+class Gemma3Model(Gemma3PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: Gemma3Config):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)\n         self.multi_modal_projector = Gemma3MultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n \n-        language_model = AutoModelForCausalLM.from_config(config=config.text_config)\n-\n-        if language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n+        language_model = AutoModel.from_config(config=config.text_config)\n         self.language_model = language_model\n \n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n@@ -1091,18 +1124,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def _update_causal_mask(\n         self,\n         attention_mask,\n@@ -1187,12 +1208,149 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return image_features\n \n     @can_return_tuple\n+    @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Gemma3ModelOutputWithPast]:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        is_training = token_type_ids is not None and labels is not None\n+\n+        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            llm_input_ids = input_ids.clone()\n+            llm_input_ids[special_image_mask] = 0\n+        else:\n+            llm_input_ids = input_ids\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # Merge text and images\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                raise ValueError(\n+                    f\"Number of images does not match number of special image tokens in the input text. \"\n+                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n+                    \"tokens from image embeddings.\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n+        )\n+        outputs = self.language_model(\n+            attention_mask=causal_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        return Gemma3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values if use_cache else None,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Gemma3 model which consists of a vision backbone and a language model.\"\"\",\n+    GEMMA3_START_DOCSTRING,\n+)\n+class Gemma3ForConditionalGeneration(Gemma3PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: Gemma3Config):\n+        super().__init__(config)\n+        self.model = Gemma3Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n@@ -1203,6 +1361,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[Tuple, Gemma3CausalLMOutputWithPast]:\n@@ -1260,80 +1419,34 @@ def forward(\n         ```\n         \"\"\"\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n-        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n-        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n-            special_image_mask = input_ids == self.config.image_token_id\n-            llm_input_ids = input_ids.clone()\n-            llm_input_ids[special_image_mask] = 0\n-        else:\n-            llm_input_ids = input_ids\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n-\n-        if cache_position is None:\n-            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n-            )\n-\n-        # Merge text and images\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        # mask out pad-token-ids in labels for BC\n-        if labels is not None and self.pad_token_id in labels:\n-            logger.warning_once(\n-                \"`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. \"\n-                \"You have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.\",\n-            )\n-            labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n-\n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n-        outputs: CausalLMOutputWithPast = self.language_model(\n-            attention_mask=causal_mask,\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            token_type_ids=token_type_ids,\n+            attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            labels=labels,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n@@ -1356,13 +1469,17 @@ def forward(\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n \n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1381,7 +1498,7 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1401,15 +1518,73 @@ def prepare_inputs_for_generation(\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self._update_causal_mask(\n+            causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n \n \n-__all__ = [\"Gemma3PreTrainedModel\", \"Gemma3TextModel\", \"Gemma3ForCausalLM\", \"Gemma3ForConditionalGeneration\"]\n+__all__ = [\n+    \"Gemma3PreTrainedModel\",\n+    \"Gemma3TextModel\",\n+    \"Gemma3ForCausalLM\",\n+    \"Gemma3ForConditionalGeneration\",\n+    \"Gemma3Model\",\n+]"
        },
        {
            "sha": "24206e6f8388bd4507120dbb2a0095ffad7626bb",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 137,
            "deletions": 106,
            "changes": 243,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -26,11 +26,12 @@\n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n from ...utils import (\n+    add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n     is_torchdynamo_compiling,\n@@ -50,7 +51,12 @@\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n-from ..paligemma.modeling_paligemma import PaliGemmaForConditionalGeneration\n+from ..paligemma.modeling_paligemma import (\n+    PaligemmaCausalLMOutputWithPast,\n+    PaliGemmaForConditionalGeneration,\n+    PaliGemmaModel,\n+    PaligemmaModelOutputWithPast,\n+)\n from ..siglip import SiglipVisionConfig\n \n \n@@ -302,43 +308,13 @@ def __init__(\n \n \n @dataclass\n-class Gemma3CausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for Gemma3 causal language model (or autoregressive) outputs.\n+class Gemma3ModelOutputWithPast(PaligemmaModelOutputWithPast):\n+    pass\n \n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size `(batch_size, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder after projecting last hidden state.\n-    \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[torch.FloatTensor] = None\n+@dataclass\n+class Gemma3CausalLMOutputWithPast(PaligemmaCausalLMOutputWithPast):\n+    pass\n \n \n class Gemma3TextScaledWordEmbedding(nn.Embedding):\n@@ -545,7 +521,7 @@ def forward(\n \n \n class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n-    base_model_prefix = \"language_model\"\n+    base_model_prefix = \"\"\n     _no_split_modules = [\n         \"Gemma3DecoderLayer\",\n         \"SiglipVisionEmbeddings\",\n@@ -755,10 +731,7 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n-class Gemma3ForConditionalGeneration(PaliGemmaForConditionalGeneration):\n-    def tie_weights(self):\n-        return self.language_model.tie_weights()\n-\n+class Gemma3Model(PaliGemmaModel):\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n@@ -843,12 +816,110 @@ def _update_causal_mask(\n         return causal_mask\n \n     @can_return_tuple\n+    @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Gemma3ModelOutputWithPast]:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        is_training = token_type_ids is not None and labels is not None\n+\n+        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n+        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n+            special_image_mask = input_ids == self.config.image_token_id\n+            llm_input_ids = input_ids.clone()\n+            llm_input_ids[special_image_mask] = 0\n+        else:\n+            llm_input_ids = input_ids\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        # Merge text and images\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                raise ValueError(\n+                    f\"Number of images does not match number of special image tokens in the input text. \"\n+                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n+                    \"tokens from image embeddings.\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        causal_mask = self._update_causal_mask(\n+            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n+        )\n+        outputs = self.language_model(\n+            attention_mask=causal_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        return Gemma3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values if use_cache else None,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Gemma3 model which consists of a vision backbone and a language model.\"\"\",\n+    GEMMA3_START_DOCSTRING,\n+)\n+class Gemma3ForConditionalGeneration(PaliGemmaForConditionalGeneration):\n     @add_start_docstrings_to_model_forward(GEMMA3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Gemma3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n@@ -859,6 +930,7 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n     ) -> Union[Tuple, Gemma3CausalLMOutputWithPast]:\n@@ -916,80 +988,34 @@ def forward(\n         ```\n         \"\"\"\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n-        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n-        if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n-            special_image_mask = input_ids == self.config.image_token_id\n-            llm_input_ids = input_ids.clone()\n-            llm_input_ids[special_image_mask] = 0\n-        else:\n-            llm_input_ids = input_ids\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(llm_input_ids)\n-\n-        if cache_position is None:\n-            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-            cache_position = torch.arange(\n-                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n-            )\n-\n-        # Merge text and images\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values)\n-\n-            if input_ids is None:\n-                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n-                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n-                )\n-            else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n-                raise ValueError(\n-                    f\"Number of images does not match number of special image tokens in the input text. \"\n-                    f\"Got {image_tokens_in_text} image tokens in the text but {image_features.shape[0] * image_features.shape[1]} \"\n-                    \"tokens from image embeddings.\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        # mask out pad-token-ids in labels for BC\n-        if labels is not None and self.pad_token_id in labels:\n-            logger.warning_once(\n-                \"`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. \"\n-                \"You have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.\",\n-            )\n-            labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n-\n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n-        outputs: CausalLMOutputWithPast = self.language_model(\n-            attention_mask=causal_mask,\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            token_type_ids=token_type_ids,\n+            attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            labels=labels,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n         loss = None\n         if labels is not None:\n             # Upcast to float if we need to compute the loss to avoid potential precision issues\n@@ -1012,13 +1038,17 @@ def forward(\n             flat_labels = shift_labels.view(-1).to(shift_logits.device)\n             loss = loss_fct(flat_logits, flat_labels)\n \n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n         return Gemma3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1037,7 +1067,7 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1057,7 +1087,7 @@ def prepare_inputs_for_generation(\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self._update_causal_mask(\n+            causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n@@ -1072,4 +1102,5 @@ def prepare_inputs_for_generation(\n     \"Gemma3TextModel\",\n     \"Gemma3ForCausalLM\",\n     \"Gemma3ForConditionalGeneration\",\n+    \"Gemma3Model\",\n ]"
        },
        {
            "sha": "15013677c23e56a88a4bfecb6da62da5a3019ba8",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 226,
            "deletions": 79,
            "changes": 305,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -28,19 +28,17 @@\n import torch.nn as nn\n import torch.nn.functional as F\n \n-from transformers.modeling_outputs import CausalLMOutputWithPast\n-\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     can_return_tuple,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_got_ocr2 import GotOcr2Config, GotOcr2VisionConfig\n \n \n@@ -545,7 +543,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -557,6 +555,39 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n+@dataclass\n+class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for GotOcr2 outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n GOT_OCR2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -575,14 +606,13 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n \n \n @add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare GotOcr2 Model outputting raw hidden-states without any specific head on top.\",\n     GOT_OCR2_START_DOCSTRING,\n )\n class GotOcr2PreTrainedModel(PreTrainedModel):\n     config_class = GotOcr2Config\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"GotOcr2VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -680,23 +710,18 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The GOT_OCR2 model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The GotOcr2 model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     GOT_OCR2_START_DOCSTRING,\n )\n-class GotOcr2ForConditionalGeneration(GotOcr2PreTrainedModel, GenerationMixin):\n+class GotOcr2Model(GotOcr2PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: GotOcr2Config):\n         super().__init__(config)\n         self.vision_tower = GotOcr2VisionEncoder(config.vision_config)\n \n         self.multi_modal_projector = GotOcr2MultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = config.pad_token_id\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -705,18 +730,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -732,13 +745,124 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, GotOcr2ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n+            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = GotOcr2ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The GOT_OCR2 model which consists of a vision backbone and a language model.\"\"\",\n+    GOT_OCR2_START_DOCSTRING,\n+)\n+class GotOcr2ForConditionalGeneration(GotOcr2PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: GotOcr2Config):\n+        super().__init__(config)\n+        self.model = GotOcr2Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -747,9 +871,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> GotOcr2CausalLMOutputWithPast:\n+    ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -794,75 +919,42 @@ def forward(\n         \"You should keep in mind what features from the module should be used, especially\n         when you're planning to sell a template.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs: CausalLMOutputWithPast = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -878,7 +970,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -895,5 +987,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"GotOcr2PreTrainedModel\", \"GotOcr2ForConditionalGeneration\"]\n+__all__ = [\"GotOcr2PreTrainedModel\", \"GotOcr2Model\", \"GotOcr2ForConditionalGeneration\"]"
        },
        {
            "sha": "f485146f2e0367cab01ab49304d32f592afb4c16",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 93,
            "deletions": 62,
            "changes": 155,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -14,16 +14,17 @@\n # limitations under the License.\n \n \n-from typing import List, Optional, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.nn as nn\n import torch.utils.checkpoint\n \n-from transformers.modeling_outputs import CausalLMOutputWithPast\n from transformers.models.llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n     LlavaPreTrainedModel,\n )\n from transformers.models.sam.modeling_sam import SamMLPBlock, SamVisionAttention, SamVisionEncoder, SamVisionLayer\n@@ -36,7 +37,7 @@\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import CONFIG_MAPPING, AutoConfig, AutoModelForCausalLM\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n if is_vision_available():\n@@ -278,6 +279,10 @@ class GotOcr2CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n+class GotOcr2ModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n     def _init_weights(self, module):\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n@@ -368,22 +373,11 @@ def _init_weights(self, module):\n \"\"\"\n \n \n-class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):\n+class GotOcr2Model(LlavaModel):\n     def __init__(self, config: GotOcr2Config):\n         super().__init__(config)\n         self.vision_tower = GotOcr2VisionEncoder(config.vision_config)\n \n-        self.multi_modal_projector = GotOcr2MultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = config.pad_token_id\n-\n-        self.post_init()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -399,13 +393,81 @@ def get_image_features(\n         image_outputs = self.vision_tower(pixel_values).last_hidden_state\n         return self.multi_modal_projector(image_outputs)\n \n+    @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, GotOcr2ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n+            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+            n_image_features = image_features.shape[0] * image_features.shape[1]\n+            if n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = GotOcr2ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class GotOcr2ForConditionalGeneration(LlavaForConditionalGeneration):\n     @can_return_tuple\n     @add_start_docstrings_to_model_forward(GOT_OCR2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GotOcr2CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -414,9 +476,10 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-    ) -> GotOcr2CausalLMOutputWithPast:\n+    ) -> Union[Tuple, GotOcr2CausalLMOutputWithPast]:\n         r\"\"\"\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n@@ -461,81 +524,49 @@ def forward(\n         \"You should keep in mind what features from the module should be used, especially\n         when you're planning to sell a template.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-            n_image_features = image_features.shape[0] * image_features.shape[1]\n-            if n_image_tokens != n_image_features:\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs: CausalLMOutputWithPast = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n         )\n \n-        logits = outputs.logits\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return GotOcr2CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n \n __all__ = [\n     \"GotOcr2VisionConfig\",\n     \"GotOcr2Config\",\n     \"GotOcr2PreTrainedModel\",\n+    \"GotOcr2Model\",\n     \"GotOcr2ForConditionalGeneration\",\n ]"
        },
        {
            "sha": "932c57ba0a667243ca0fa431ff20d7e16bffdf40",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -28,7 +28,7 @@\n import transformers.models.jamba.modeling_jamba as modeling_jamba\n from transformers.activations import ACT2FN\n \n-from ...cache_utils import Cache, StaticCache\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -1511,10 +1511,10 @@ def _update_causal_mask(\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n         # to infer the attention mask.\n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n+        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n \n         # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n+        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n             if AttentionMaskConverter._ignore_causal_mask_sdpa(\n                 attention_mask,\n                 inputs_embeds=input_tensor,\n@@ -1525,7 +1525,7 @@ def _update_causal_mask(\n \n         dtype = input_tensor.dtype\n         sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n+        if using_compilable_cache:\n             target_length = past_key_values.get_max_cache_shape()\n         else:\n             target_length = ("
        },
        {
            "sha": "6abed48c86a20f7efa2ca756499bca9cc9a175fe",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 158,
            "deletions": 2,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -41,7 +41,7 @@\n     replace_return_docstrings,\n     torch_int,\n )\n-from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n+from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n \n \n@@ -315,6 +315,9 @@ class InstructBlipPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _no_split_modules = [\n         \"InstructBlipQFormerEmbeddings\",\n@@ -339,7 +342,7 @@ def _init_weights(self, module):\n         elif isinstance(module, InstructBlipVisionEmbeddings):\n             nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n             nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-        elif isinstance(module, InstructBlipForConditionalGeneration):\n+        elif isinstance(module, (InstructBlipForConditionalGeneration, InstructBlipModel)):\n             module.query_tokens.data.zero_()\n \n \n@@ -1274,6 +1277,156 @@ def forward(\n         )\n \n \n+@add_start_docstrings(\n+    \"\"\"\n+    InstructBLIP base Model consisting of language model, qformer and vision encoder.\n+    \"\"\",\n+    INSTRUCTBLIP_START_DOCSTRING,\n+)\n+class InstructBlipModel(InstructBlipPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n+\n+    def __init__(self, config: InstructBlipConfig):\n+        super().__init__(config)\n+\n+        self.vision_model = InstructBlipVisionModel(config.vision_config)\n+        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n+        self.qformer = InstructBlipQFormerModel(config.qformer_config)\n+\n+        self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+\n+        if self.language_model._no_split_modules is not None:\n+            self._no_split_modules.extend(self.language_model._no_split_modules)\n+\n+        if self.language_model._keep_in_fp32_modules is not None:\n+            self._keep_in_fp32_modules.extend(self.language_model._keep_in_fp32_modules)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def _tie_weights(self):\n+        if not self.config.use_decoder_only_language_model:\n+            self.language_model.encoder.embed_tokens = self.language_model.shared\n+            self.language_model.decoder.embed_tokens = self.language_model.shared\n+\n+    def _preprocess_accelerate(self):\n+        r\"\"\"\n+        Some pre-processing hacks to make the model `accelerate` compatible. Check\n+        https://github.com/huggingface/transformers/pull/21707 for more details.\n+        \"\"\"\n+        hf_device_map = self.hf_device_map\n+\n+        if len(hf_device_map) > 1 and \"language_model\" not in hf_device_map and torch.cuda.device_count() > 1:\n+            # warn users about unexpected behavior when using multi-GPU + InstructBLIP + `accelerate`.\n+            logger.warning(\n+                \"The `language_model` is not in the `hf_device_map` dictionary and you are running your script\"\n+                \" in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\"\n+                \" Please pass a `device_map` that contains `language_model` to remove this warning.\"\n+                \" Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for\"\n+                \" more details on creating a `device_map` for large models.\",\n+            )\n+\n+        if hasattr(self.language_model, \"_hf_hook\"):\n+            self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n+\n+    @add_start_docstrings_to_model_forward(INSTRUCTBLIP_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.FloatTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n+    ) -> Union[Tuple, InstructBlipForConditionalGenerationModelOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # step 1: forward the images through the vision encoder,\n+        # to get image embeddings of shape (batch_size, seq_len, hidden_size)\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if attention_mask is None:\n+            attention_mask = torch.ones_like(input_ids)\n+\n+        special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+\n+        if self.config.use_decoder_only_language_model:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+\n+        if not return_dict:\n+            return (vision_outputs, query_outputs, outputs)\n+\n+        return InstructBlipForConditionalGenerationModelOutput(\n+            vision_outputs=vision_outputs,\n+            qformer_outputs=query_outputs,\n+            language_model_outputs=outputs,\n+        )\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     InstructBLIP Model for generating text given an image and an optional text prompt. The model consists of a vision\n@@ -1336,11 +1489,13 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n \n+    # Copied from transformers.models.instructblip.modeling_instructblip.InstructBlipModel._tie_weights\n     def _tie_weights(self):\n         if not self.config.use_decoder_only_language_model:\n             self.language_model.encoder.embed_tokens = self.language_model.shared\n             self.language_model.decoder.embed_tokens = self.language_model.shared\n \n+    # Copied from transformers.models.instructblip.modeling_instructblip.InstructBlipModel._preprocess_accelerate\n     def _preprocess_accelerate(self):\n         r\"\"\"\n         Some pre-processing hacks to make the model `accelerate` compatible. Check\n@@ -1645,6 +1800,7 @@ def generate(\n __all__ = [\n     \"InstructBlipQFormerModel\",\n     \"InstructBlipPreTrainedModel\",\n+    \"InstructBlipModel\",\n     \"InstructBlipForConditionalGeneration\",\n     \"InstructBlipVisionModel\",\n ]"
        },
        {
            "sha": "0ce752bed8bd90d34bc0081df70a756d82f8f69c",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 166,
            "deletions": 2,
            "changes": 168,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -45,7 +45,7 @@\n     replace_return_docstrings,\n     torch_int,\n )\n-from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n+from ..auto import AutoModel, AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblipvideo import (\n     InstructBlipVideoConfig,\n     InstructBlipVideoQFormerConfig,\n@@ -945,6 +945,9 @@ class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n     config_class = InstructBlipVideoConfig\n     base_model_prefix = \"blip\"\n     supports_gradient_checkpointing = True\n+    _supports_cache_class = True\n+    _supports_static_cache = True\n+    _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n \n     _no_split_modules = [\n         \"InstructBlipVideoQFormerEmbeddings\",\n@@ -969,7 +972,7 @@ def _init_weights(self, module):\n         elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n             nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n             nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-        elif isinstance(module, InstructBlipVideoForConditionalGeneration):\n+        elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n             module.query_tokens.data.zero_()\n \n \n@@ -1269,6 +1272,166 @@ def to_tuple(self) -> Tuple[Any]:\n         )\n \n \n+@add_start_docstrings(\n+    \"\"\"\n+    InstructBlipVideo base Model consisting of language model, qformer and vision encoder.\n+    \"\"\",\n+    INSTRUCTBLIPVIDEO_START_DOCSTRING,\n+)\n+class InstructBlipVideoModel(InstructBlipVideoPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    _keep_in_fp32_modules = [\"query_tokens\"]  # TODO @ArthurZucker I don't know why this is required for FP8\n+\n+    def __init__(self, config: InstructBlipVideoConfig):\n+        super().__init__(config)\n+\n+        self.vision_model = InstructBlipVideoVisionModel(config.vision_config)\n+        self.query_tokens = nn.Parameter(torch.zeros(1, config.num_query_tokens, config.qformer_config.hidden_size))\n+        self.qformer = InstructBlipVideoQFormerModel(config.qformer_config)\n+\n+        self.language_projection = nn.Linear(config.qformer_config.hidden_size, config.text_config.hidden_size)\n+        self.language_model = AutoModel.from_config(config.text_config)\n+\n+        if self.language_model._no_split_modules is not None:\n+            self._no_split_modules.extend(self.language_model._no_split_modules)\n+\n+        if self.language_model._keep_in_fp32_modules is not None:\n+            self._keep_in_fp32_modules.extend(self.language_model._keep_in_fp32_modules)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.language_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.language_model.set_input_embeddings(value)\n+\n+    def _tie_weights(self):\n+        if not self.config.use_decoder_only_language_model:\n+            self.language_model.encoder.embed_tokens = self.language_model.shared\n+            self.language_model.decoder.embed_tokens = self.language_model.shared\n+\n+    def _preprocess_accelerate(self):\n+        r\"\"\"\n+        Some pre-processing hacks to make the model `accelerate` compatible. Check\n+        https://github.com/huggingface/transformers/pull/21707 for more details.\n+        \"\"\"\n+        hf_device_map = self.hf_device_map\n+\n+        if len(hf_device_map) > 1 and \"language_model\" not in hf_device_map and torch.cuda.device_count() > 1:\n+            # warn users about unexpected behavior when using multi-GPU + InstructBlipVideo + `accelerate`.\n+            logger.warning(\n+                \"The `language_model` is not in the `hf_device_map` dictionary and you are running your script\"\n+                \" in a multi-GPU environment. this may lead to unexpected behavior when using `accelerate`.\"\n+                \" Please pass a `device_map` that contains `language_model` to remove this warning.\"\n+                \" Please refer to https://github.com/huggingface/blog/blob/main/accelerate-large-models.md for\"\n+                \" more details on creating a `device_map` for large models.\",\n+            )\n+\n+        if hasattr(self.language_model, \"_hf_hook\"):\n+            self.language_model._hf_hook.io_same_device = True  # For `generate` compatibility\n+\n+    @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.FloatTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n+    ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # step 1: forward the images through the vision encoder,\n+        # we process in a batched way, later unbatch it back (video has frames=4 always)\n+        batch_size, frames, channel, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+\n+        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+\n+        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n+        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if attention_mask is None:\n+            attention_mask = torch.ones_like(input_ids)\n+\n+        special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+\n+        if self.config.use_decoder_only_language_model:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+\n+        if not return_dict:\n+            return (vision_outputs, query_outputs, outputs)\n+\n+        return InstructBlipVideoForConditionalGenerationModelOutput(\n+            vision_outputs=vision_outputs,\n+            qformer_outputs=query_outputs,\n+            language_model_outputs=outputs,\n+        )\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     InstructBlipVideo Model for generating text given an image and an optional text prompt. The model consists of a vision\n@@ -1682,5 +1845,6 @@ def generate(\n     \"InstructBlipVideoVisionModel\",\n     \"InstructBlipVideoPreTrainedModel\",\n     \"InstructBlipVideoQFormerModel\",\n+    \"InstructBlipVideoModel\",\n     \"InstructBlipVideoForConditionalGeneration\",\n ]"
        },
        {
            "sha": "d28485545f7dcb81207b3c9397f19203bacaff96",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 107,
            "deletions": 1,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -27,14 +27,15 @@\n from transformers.models.instructblip.modeling_instructblip import (\n     InstructBlipForConditionalGeneration,\n     InstructBlipForConditionalGenerationModelOutput,\n+    InstructBlipModel,\n     InstructBlipPreTrainedModel,\n     InstructBlipQFormerModel,\n     InstructBlipVisionModel,\n )\n \n from ...configuration_utils import PretrainedConfig\n from ...models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n-from ...utils import logging\n+from ...utils import add_start_docstrings_to_model_forward, logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n@@ -191,6 +192,110 @@ class InstructBlipVideoForConditionalGenerationModelOutput(InstructBlipForCondit\n     pass\n \n \n+INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING = None\n+\n+\n+class InstructBlipVideoModel(InstructBlipModel):\n+    @add_start_docstrings_to_model_forward(INSTRUCTBLIPVIDEO_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        qformer_input_ids: torch.FloatTensor,\n+        qformer_attention_mask: Optional[torch.LongTensor] = None,\n+        input_ids: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        decoder_input_ids: Optional[torch.LongTensor] = None,\n+        decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+        use_cache: Optional[bool] = None,\n+    ) -> Union[Tuple, InstructBlipVideoForConditionalGenerationModelOutput]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # step 1: forward the images through the vision encoder,\n+        # we process in a batched way, later unbatch it back (video has frames=4 always)\n+        batch_size, frames, channel, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * frames, channel, height, width)\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+        image_embeds = vision_outputs[0]\n+\n+        # step 2: forward the query tokens through the QFormer, using the image embeddings for cross-attention\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        # difference with BLIP-2 here: we also feed the instruction prompt to the Q-Former\n+        query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n+        query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=image_embeds.device)\n+\n+        if qformer_attention_mask is None:\n+            qformer_attention_mask = torch.ones_like(qformer_input_ids)\n+\n+        qformer_input_ids = qformer_input_ids.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = qformer_attention_mask.repeat_interleave(frames, dim=0)\n+        qformer_attention_mask = torch.cat([query_attention_mask, qformer_attention_mask], dim=1)\n+        query_outputs = self.qformer(\n+            input_ids=qformer_input_ids,\n+            attention_mask=qformer_attention_mask,\n+            query_embeds=query_tokens,\n+            encoder_hidden_states=image_embeds,\n+            encoder_attention_mask=image_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+        query_output = query_outputs[0][:, : query_tokens.size(1), :]\n+\n+        # step 3: use the language model, conditioned on the query outputs and the prompt\n+        language_model_inputs = self.language_projection(query_output)\n+\n+        # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n+        language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n+        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if attention_mask is None:\n+            attention_mask = torch.ones_like(input_ids)\n+\n+        special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+\n+        if self.config.use_decoder_only_language_model:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            outputs = self.language_model(\n+                inputs_embeds=inputs_embeds,\n+                attention_mask=attention_mask,\n+                decoder_input_ids=decoder_input_ids,\n+                decoder_attention_mask=decoder_attention_mask,\n+                output_attentions=output_attentions,\n+                output_hidden_states=output_hidden_states,\n+                return_dict=return_dict,\n+                use_cache=use_cache,\n+            )\n+\n+        if not return_dict:\n+            return (vision_outputs, query_outputs, outputs)\n+\n+        return InstructBlipVideoForConditionalGenerationModelOutput(\n+            vision_outputs=vision_outputs,\n+            qformer_outputs=query_outputs,\n+            language_model_outputs=outputs,\n+        )\n+\n+\n class InstructBlipVideoForConditionalGeneration(InstructBlipForConditionalGeneration):\n     def forward(\n         self,\n@@ -508,5 +613,6 @@ def generate(\n     \"InstructBlipVideoVisionModel\",\n     \"InstructBlipVideoPreTrainedModel\",\n     \"InstructBlipVideoQFormerModel\",\n+    \"InstructBlipVideoModel\",\n     \"InstructBlipVideoForConditionalGeneration\",\n ]"
        },
        {
            "sha": "6c59d06d2eed6edea0fe4d211bc396d4999f7f81",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 291,
            "deletions": 128,
            "changes": 419,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -31,7 +31,7 @@\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, BaseModelOutputWithPooling\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -45,7 +45,7 @@\n     replace_return_docstrings,\n     torch_int,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n \n@@ -608,14 +608,13 @@ def forward(\n \n \n @add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare InternVL Model outputting raw hidden-states without any specific head on top.\",\n     INTERNVL_START_DOCSTRING,\n )\n class InternVLPreTrainedModel(PreTrainedModel):\n     config_class = InternVLConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"InternVLVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -654,15 +653,13 @@ def forward(self, image_features):\n \n \n @dataclass\n-class InternVLCausalLMOutputWithPast(ModelOutput):\n+class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n-    Base class for InternVL causal language model (or autoregressive) outputs.\n+    Base class for InternVL outputs, with hidden states and attentions.\n \n     Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n             `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n@@ -681,15 +678,10 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n@@ -771,23 +763,18 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n \n \n @add_start_docstrings(\n-    \"\"\"The INTERNVL model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The InternVL model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     INTERNVL_START_DOCSTRING,\n )\n-class InternVLForConditionalGeneration(InternVLPreTrainedModel, GenerationMixin):\n+class InternVLModel(InternVLPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: InternVLConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = InternVLMultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -796,18 +783,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -853,12 +828,221 @@ def get_image_features(\n \n         return vision_features\n \n+    @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, InternVLModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = InternVLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n+        \"\"\"Perform pixel shuffle downsampling on vision features.\n+\n+        Args:\n+            vision_features (`torch.Tensor`):\n+                Input tensor of shape (batch_size, width, height, channels).\n+            scale_factor (`float`, *optional*, defaults to `0.5`):\n+                Factor by which to downsample. Default is 0.5, which halves the dimensions.\n+\n+        Returns:\n+            vision_features (`torch.Tensor`):\n+                Downsampled tensor of shape (batch_size, height*scale_factor, width*scale_factor, channels/(scale_factor^2)).\n+        \"\"\"\n+        batch_size, width, height, channels = vision_features.size()\n+\n+        if height % scale_factor != 0 or width % scale_factor != 0:\n+            raise ValueError(\"Height and width must be divisible by scale_factor for proper downsampling.\")\n+\n+        # Reshape to allow downsampling\n+        vision_features = vision_features.view(\n+            batch_size, width, int(height * scale_factor), int(channels / scale_factor)\n+        )\n+        # Permute dimensions to align downsampled axis correctly\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        # Reshape to achieve final downsampled dimensions\n+        vision_features = vision_features.view(\n+            batch_size, int(height * scale_factor), int(width * scale_factor), int(channels / (scale_factor**2))\n+        )\n+\n+        # Swap height and width back for proper orientation\n+        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+\n+        return vision_features\n+\n+\n+@dataclass\n+class InternVLCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for InternVL causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The INTERNVL model which consists of a vision backbone and a language model.\"\"\",\n+    INTERNVL_START_DOCSTRING,\n+)\n+class InternVLForConditionalGeneration(InternVLPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: InternVLConfig):\n+        super().__init__(config)\n+        self.model = InternVLModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -872,7 +1056,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        image_sizes: torch.Tensor = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, InternVLCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -925,7 +1109,6 @@ def forward(\n         >>> print(processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True))\n         The images depict the Statue of Liberty and the Golden Gate Bridge.\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -940,81 +1123,40 @@ def forward(\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return InternVLCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1030,7 +1172,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1047,45 +1189,66 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n-        \"\"\"Perform pixel shuffle downsampling on vision features.\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n-            vision_features (`torch.Tensor`):\n-                Input tensor of shape (batch_size, width, height, channels).\n-            scale_factor (`float`, *optional*, defaults to `0.5`):\n-                Factor by which to downsample. Default is 0.5, which halves the dimensions.\n-\n-        Returns:\n-            vision_features (`torch.Tensor`):\n-                Downsampled tensor of shape (batch_size, height*scale_factor, width*scale_factor, channels/(scale_factor^2)).\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n         \"\"\"\n-        batch_size, width, height, channels = vision_features.size()\n-\n-        if height % scale_factor != 0 or width % scale_factor != 0:\n-            raise ValueError(\"Height and width must be divisible by scale_factor for proper downsampling.\")\n-\n-        # Reshape to allow downsampling\n-        vision_features = vision_features.view(\n-            batch_size, width, int(height * scale_factor), int(channels / scale_factor)\n-        )\n-        # Permute dimensions to align downsampled axis correctly\n-        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n-\n-        # Reshape to achieve final downsampled dimensions\n-        vision_features = vision_features.view(\n-            batch_size, int(height * scale_factor), int(width * scale_factor), int(channels / (scale_factor**2))\n-        )\n-\n-        # Swap height and width back for proper orientation\n-        vision_features = vision_features.permute(0, 2, 1, 3).contiguous()\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n \n-        return vision_features\n+        return causal_mask\n \n \n __all__ = [\n     \"InternVLVisionPreTrainedModel\",\n     \"InternVLVisionModel\",\n     \"InternVLPreTrainedModel\",\n+    \"InternVLModel\",\n     \"InternVLForConditionalGeneration\",\n ]"
        },
        {
            "sha": "bc3c3bfc6da3d3c6a5be73da53c72d4539c67b12",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 15,
            "deletions": 6,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -39,7 +39,12 @@\n from ..clip.modeling_clip import CLIPMLP\n from ..janus.modeling_janus import JanusVisionAttention\n from ..llama.modeling_llama import LlamaRMSNorm\n-from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration, LlavaPreTrainedModel\n+from ..llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaPreTrainedModel,\n+)\n from .configuration_internvl import InternVLConfig, InternVLVisionConfig\n \n \n@@ -573,11 +578,7 @@ def forward(self, image_features):\n         return hidden_states\n \n \n-class InternVLCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n-    pass\n-\n-\n-class InternVLForConditionalGeneration(LlavaForConditionalGeneration):\n+class InternVLModel(LlavaModel):\n     def pixel_shuffle(self, vision_features: torch.Tensor, scale_factor: float = 0.5):\n         \"\"\"Perform pixel shuffle downsampling on vision features.\n \n@@ -658,6 +659,13 @@ def get_image_features(\n \n         return vision_features\n \n+\n+class InternVLCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class InternVLForConditionalGeneration(LlavaForConditionalGeneration):\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(INTERNVL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=InternVLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(**super_kwargs):\n@@ -718,5 +726,6 @@ def forward(**super_kwargs):\n     \"InternVLVisionPreTrainedModel\",\n     \"InternVLVisionModel\",\n     \"InternVLPreTrainedModel\",\n+    \"InternVLModel\",\n     \"InternVLForConditionalGeneration\",\n ]"
        },
        {
            "sha": "3273d595a5a9ed4a08cdd5a3cce73170e063f6db",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 256,
            "deletions": 89,
            "changes": 345,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -23,16 +23,17 @@\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_llava import LlavaConfig\n \n \n@@ -44,6 +45,39 @@\n _CHECKPOINT_FOR_DOC = \"llava-hf/llava-1.5-7b-hf\"\n \n \n+@dataclass\n+class LlavaModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class LlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -72,7 +106,7 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -124,14 +158,13 @@ def forward(self, image_features):\n \n \n @add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare Llava Model outputting raw hidden-states without any specific head on top.\",\n     LLAVA_START_DOCSTRING,\n )\n class LlavaPreTrainedModel(PreTrainedModel):\n     config_class = LlavaConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -149,6 +182,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n LLAVA_INPUTS_DOCSTRING = r\"\"\"\n@@ -229,23 +265,18 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The LLAVA model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The Llava model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     LLAVA_START_DOCSTRING,\n )\n-class LlavaForConditionalGeneration(LlavaPreTrainedModel, GenerationMixin):\n+class LlavaModel(LlavaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: LlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = LlavaMultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -254,18 +285,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -277,7 +296,7 @@ def get_image_features(\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n \n         Args:\n-            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`):\n                The tensors corresponding to the input images.\n             vision_feature_layer (`Union[int, List[int]]`):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n@@ -312,12 +331,146 @@ def get_image_features(\n         image_features = self.multi_modal_projector(selected_image_feature)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_sizes=image_sizes,\n+            )\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The LLAVA model which consists of a vision backbone and a language model.\"\"\",\n+    LLAVA_START_DOCSTRING,\n+)\n+class LlavaForConditionalGeneration(LlavaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: LlavaConfig):\n+        super().__init__(config)\n+        self.model = LlavaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -331,7 +484,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        image_sizes: torch.Tensor = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, LlavaCausalLMOutputWithPast]:\n         r\"\"\"\n@@ -371,7 +524,6 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a busy city street with a stop sign prominently displayed\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -386,81 +538,40 @@ def forward(\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_sizes=image_sizes,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return LlavaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -476,7 +587,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -493,5 +604,61 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"LlavaForConditionalGeneration\", \"LlavaPreTrainedModel\"]\n+__all__ = [\"LlavaForConditionalGeneration\", \"LlavaPreTrainedModel\", \"LlavaModel\"]"
        },
        {
            "sha": "c17eb9622c8a04e231551c748e123af6935d75c1",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 256,
            "deletions": 105,
            "changes": 361,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -26,16 +26,17 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_llava_next import LlavaNextConfig\n \n \n@@ -151,6 +152,39 @@ def unpad_image(tensor, original_size):\n     return unpadded_tensor\n \n \n+@dataclass\n+class LlavaNextModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class LlavaNextCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -237,9 +271,9 @@ def forward(self, image_features):\n )\n class LlavaNextPreTrainedModel(PreTrainedModel):\n     config_class = LlavaNextConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlavaNextVisionAttention\"]\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -254,7 +288,7 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, LlavaNextForConditionalGeneration):\n+        elif isinstance(module, LlavaNextModel):\n             embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n             module.image_newline.data.normal_(mean=0.0, std=embed_std)\n \n@@ -340,10 +374,12 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The Llava-Next model which consists of a vision backbone and a language model without language modeling head.\"\"\",\n     LLAVA_NEXT_START_DOCSTRING,\n )\n-class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel, GenerationMixin):\n+class LlavaNextModel(LlavaNextPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: LlavaNextConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n@@ -353,48 +389,16 @@ def __init__(self, config: LlavaNextConfig):\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-        self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.post_init()\n \n-    @property\n-    def padding_side(self):\n-        return self._padding_side\n-\n-    @padding_side.setter\n-    def padding_side(self, padding_side: str):\n-        if padding_side not in [\"left\", \"right\"]:\n-            raise ValueError(f\"{padding_side} is not `left` or `right`.\")\n-        self._padding_side = padding_side\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_decoder\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_decoder\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -524,12 +528,152 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaNextModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None and pixel_values.size(0) > 0:\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+\n+            # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+                image_newline=self.image_newline,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaNextModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n+    LLAVA_NEXT_START_DOCSTRING,\n+)\n+class LlavaNextForConditionalGeneration(LlavaNextPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^image_newline\": \"model.image_newline\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: LlavaNextConfig):\n+        super().__init__(config)\n+        self.model = LlavaNextModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -597,45 +741,12 @@ def forward(\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None and pixel_values.size(0) > 0:\n-            image_features = self.get_image_features(\n-                pixel_values,\n-                image_sizes,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-            )\n-\n-            # NOTE we only support multimodal_patch_merge_type == \"spatial_unpad\"\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids,\n+            pixel_values=pixel_values,\n+            image_sizes=image_sizes,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n@@ -645,41 +756,25 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return LlavaNextCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -696,7 +791,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -714,5 +809,61 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"LlavaNextForConditionalGeneration\", \"LlavaNextPreTrainedModel\"]\n+__all__ = [\"LlavaNextForConditionalGeneration\", \"LlavaNextPreTrainedModel\", \"LlavaNextModel\"]"
        },
        {
            "sha": "6b6f14bc342305e54cdf1a19a4ab09e65859511e",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 352,
            "deletions": 187,
            "changes": 539,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -30,24 +30,65 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_llava_next_video import LlavaNextVideoConfig\n \n \n logger = logging.get_logger(__name__)\n \n+\n _CONFIG_FOR_DOC = \"LlavaNextVideoConfig\"\n \n \n+@dataclass\n+class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -167,6 +208,34 @@ def forward(self, image_features):\n \"\"\"\n \n \n+@add_start_docstrings(\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    LLAVA_NEXT_VIDEO_START_DOCSTRING,\n+)\n+class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n+    config_class = LlavaNextVideoConfig\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, LlavaNextVideoModel):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+\n+\n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n     \"\"\"\n     Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n@@ -355,38 +424,12 @@ def unpad_image(tensor, original_size):\n \n \n @add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    \"\"\"The Llava-Next model which consists of a vision backbone and a language model without language modeling head.\"\"\",\n     LLAVA_NEXT_VIDEO_START_DOCSTRING,\n )\n-class LlavaNextVideoPreTrainedModel(PreTrainedModel):\n-    config_class = LlavaNextVideoConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlavaNextVideoVisionAttention\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_cache_class = True\n-    _supports_flash_attn_2 = True\n-    _supports_sdpa = True\n-    _supports_quantized_cache = True\n-    _supports_static_cache = True\n-\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+class LlavaNextVideoModel(LlavaNextVideoPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n \n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, LlavaNextVideoForConditionalGeneration):\n-            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n-\n-\n-@add_start_docstrings(\n-    \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n-    LLAVA_NEXT_VIDEO_START_DOCSTRING,\n-)\n-class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel, GenerationMixin):\n     def __init__(\n         self,\n         config: LlavaNextVideoConfig,\n@@ -399,43 +442,17 @@ def __init__(\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-        self._padding_side = \"left\"  # set it to left by default, user can use setter to change padding_sides\n         self.vision_resampler = LlavaNextVideoPooler(config)\n         self.post_init()\n \n-    @property\n-    def padding_side(self):\n-        return self._padding_side\n-\n-    @padding_side.setter\n-    def padding_side(self, padding_side: str):\n-        if padding_side not in [\"left\", \"right\"]:\n-            raise ValueError(f\"{padding_side} is not `left` or `right`.\")\n-        self._padding_side = padding_side\n-\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, vision_feature_select_strategy, image_newline=None):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -564,13 +581,222 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaNextVideoModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self.vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        self.vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None and pixel_values.size(0) > 0:\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                self.vision_feature_select_strategy,\n+                image_newline=self.image_newline,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n+            video_features = [feature.flatten(0, 1) for feature in video_features]\n+            video_feature_lens = [feature.size(0) for feature in video_features]\n+            video_features = torch.cat(video_features, dim=0)\n+            video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n+\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_features = video_features.shape[0]\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaNextVideoModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input video.\n+            vision_feature_layer (`Union[int, List[int]]`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_videos, video_length, embed_dim)`).\n+        \"\"\"\n+        batch_size, frames, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n+        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_video_features = torch.cat(hs_pool, dim=-1)\n+\n+        if vision_feature_select_strategy == \"default\":\n+            selected_video_features = selected_video_features[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_video_features = selected_video_features\n+\n+        # Same as image features except that video has pooling layer\n+        video_features = self.vision_resampler(selected_video_features)\n+        video_features = self.multi_modal_projector(video_features)\n+        video_features = torch.split(video_features, frames, dim=0)\n+        return video_features\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n+    LLAVA_NEXT_VIDEO_START_DOCSTRING,\n+)\n+class LlavaNextVideoForConditionalGeneration(LlavaNextVideoPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^image_newline\": \"model.image_newline\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: LlavaNextVideoConfig):\n+        super().__init__(config)\n+        self.model = LlavaNextVideoModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -662,126 +888,56 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"USER: \\nWhat's the content of the image? ASSISTANT: The image shows a red stop sign on a pole, with a traditional Chinese archway (...)\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        self.vision_feature_layer = (\n+        vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n-        self.vision_feature_select_strategy = (\n+        vision_feature_select_strategy = (\n             vision_feature_select_strategy\n             if vision_feature_select_strategy is not None\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None and pixel_values.size(0) > 0:\n-            image_features = self.get_image_features(\n-                pixel_values,\n-                image_sizes,\n-                vision_feature_layer=self.vision_feature_layer,\n-                vision_feature_select_strategy=self.vision_feature_select_strategy,\n-            )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                self.vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n-            video_features = self.get_video_features(\n-                pixel_values_videos,\n-                vision_feature_layer=self.vision_feature_layer,\n-                vision_feature_select_strategy=self.vision_feature_select_strategy,\n-            )\n-            video_features = [feature.flatten(0, 1) for feature in video_features]\n-            video_feature_lens = [feature.size(0) for feature in video_features]\n-            video_features = torch.cat(video_features, dim=0)\n-            video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n-\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return LlavaNextVideoCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n-            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -799,7 +955,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- extra custom processing\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -818,51 +974,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    def get_video_features(\n-        self,\n-        pixel_values: torch.FloatTensor,\n-        vision_feature_layer: Union[int, List[int]],\n-        vision_feature_select_strategy: str,\n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n     ):\n         \"\"\"\n-        Obtains video last hidden states from the vision tower and apply multimodal projection.\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n \n         Args:\n-            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n-               The tensors corresponding to the input video.\n-            vision_feature_layer (`Union[int, List[int]]`):\n-                The index of the layer to select the vision feature. If multiple indices are provided,\n-                the vision feature of the corresponding indices will be concatenated to form the\n-                vision features.\n-            vision_feature_select_strategy (`str`):\n-                The feature selection strategy used to select the vision feature from the vision backbone.\n-                Can be one of `\"default\"` or `\"full\"`\n-        Returns:\n-            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n-            and are of shape `(num_videos, video_length, embed_dim)`).\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n         \"\"\"\n-        batch_size, frames, channels, height, width = pixel_values.shape\n-        pixel_values = pixel_values.reshape(batch_size * frames, channels, height, width)\n-        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n-\n-        # If we have one vision feature layer, return the corresponding hidden states,\n-        # otherwise, select the hidden states of each feature layer and concatenate them\n-        if isinstance(vision_feature_layer, int):\n-            selected_video_features = video_features.hidden_states[vision_feature_layer]\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n         else:\n-            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n-            selected_video_features = torch.cat(hs_pool, dim=-1)\n-\n-        if vision_feature_select_strategy == \"default\":\n-            selected_video_features = selected_video_features[:, 1:]\n-        elif vision_feature_select_strategy == \"full\":\n-            selected_video_features = selected_video_features\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n \n-        # Same as image features except that video has pooling layer\n-        video_features = self.vision_resampler(selected_video_features)\n-        video_features = self.multi_modal_projector(video_features)\n-        video_features = torch.split(video_features, frames, dim=0)\n-        return video_features\n+        return causal_mask\n \n \n-__all__ = [\"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoPreTrainedModel\"]\n+__all__ = [\"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoModel\", \"LlavaNextVideoPreTrainedModel\"]"
        },
        {
            "sha": "985a69a68ecbf49dc2a3164272c635c2915013af",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 246,
            "deletions": 106,
            "changes": 352,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -24,22 +24,29 @@\n from transformers.models.llava_next.modeling_llava_next import (\n     LlavaNextCausalLMOutputWithPast,\n     LlavaNextForConditionalGeneration,\n+    LlavaNextModel,\n+    LlavaNextModelOutputWithPast,\n     LlavaNextMultiModalProjector,\n-    LlavaNextPreTrainedModel,\n     image_size_to_num_patches,\n )\n \n from ...configuration_utils import PretrainedConfig\n from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n+    replace_return_docstrings,\n )\n from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n \n \n+_CONFIG_FOR_DOC = \"LlavaNextVideoConfig\"\n+\n+\n class LlavaNextVideoConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`LlavaNextVideoForConditionalGeneration`]. It is used to instantiate an\n@@ -182,6 +189,17 @@ def __init__(\n         super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n \n \n+@dataclass\n+class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):\n+    \"\"\"\n+    video_hidden_states (`torch.FloatTensor`, *optional*):\n+        A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+        video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class LlavaNextVideoCausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n     \"\"\"\n@@ -231,20 +249,7 @@ class LlavaNextVideoMultiModalProjector(LlavaNextMultiModalProjector):\n     pass\n \n \n-class LlavaNextVideoPreTrainedModel(LlavaNextPreTrainedModel):\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, LlavaNextVideoForConditionalGeneration):\n-            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n-\n-\n-class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n+class LlavaNextVideoModel(LlavaNextModel):\n     def __init__(self, config: LlavaNextVideoConfig, **super_kwargs):\n         super().__init__(config, **super_kwargs)\n         self.vision_resampler = LlavaNextVideoPooler(config)\n@@ -358,9 +363,209 @@ def get_video_features(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaNextVideoModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        self.vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        self.vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None and pixel_values.size(0) > 0:\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                self.vision_feature_select_strategy,\n+                image_newline=self.image_newline,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=self.vision_feature_layer,\n+                vision_feature_select_strategy=self.vision_feature_select_strategy,\n+            )\n+            video_features = [feature.flatten(0, 1) for feature in video_features]\n+            video_feature_lens = [feature.size(0) for feature in video_features]\n+            video_features = torch.cat(video_features, dim=0)\n+            video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n+\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_features = video_features.shape[0]\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaNextVideoModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n+            The tensors corresponding to the input images. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`LlavaNextVideoImageProcessor.__call__`] for details. [`LlavaProcessor`] uses\n+            [`LlavaNextVideoImageProcessor`] for processing images.\n+        image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):\n+            The sizes of the images in the batch, being (height, width) for each image.\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            `past_key_values`).\n+\n+            If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n+            and modify to your needs. See diagram 1 in [the paper](https://arxiv.org/abs/1910.13461) for more\n+            information on the default strategy.\n+\n+            - 1 indicates the head is **not masked**,\n+            - 0 indicates the head is **masked**.\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of shape\n+            `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks and in the cross-attention\n+            blocks) that can be used (see `past_key_values` input) to speed up sequential decoding.\n+\n+            If `past_key_values` are used, the user can optionally input only the last `decoder_input_ids` (those that\n+            don't have their past key value states given to this model) of shape `(batch_size, 1)` instead of all\n+            `decoder_input_ids` of shape `(batch_size, sequence_length)`.\n+        inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+            Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n+            is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n+            model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n+            If `\"full\"`, the full vision features are used.\n+        use_cache (`bool`, *optional*):\n+            If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n+            `past_key_values`).\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n+\"\"\"\n+\n+\n+class LlavaNextVideoForConditionalGeneration(LlavaNextForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(LLAVA_NEXT_VIDEO_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=LlavaNextVideoCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -452,126 +657,56 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"USER: \\nWhat's the content of the image? ASSISTANT: The image shows a red stop sign on a pole, with a traditional Chinese archway (...)\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        self.vision_feature_layer = (\n+        vision_feature_layer = (\n             vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n         )\n-        self.vision_feature_select_strategy = (\n+        vision_feature_select_strategy = (\n             vision_feature_select_strategy\n             if vision_feature_select_strategy is not None\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None and pixel_values.size(0) > 0:\n-            image_features = self.get_image_features(\n-                pixel_values,\n-                image_sizes,\n-                vision_feature_layer=self.vision_feature_layer,\n-                vision_feature_select_strategy=self.vision_feature_select_strategy,\n-            )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                self.vision_feature_select_strategy,\n-                image_newline=self.image_newline,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        if pixel_values_videos is not None and pixel_values_videos.size(0) > 0:\n-            video_features = self.get_video_features(\n-                pixel_values_videos,\n-                vision_feature_layer=self.vision_feature_layer,\n-                vision_feature_select_strategy=self.vision_feature_select_strategy,\n-            )\n-            video_features = [feature.flatten(0, 1) for feature in video_features]\n-            video_feature_lens = [feature.size(0) for feature in video_features]\n-            video_features = torch.cat(video_features, dim=0)\n-            video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n-\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return LlavaNextVideoCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n-            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -589,7 +724,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- extra custom processing\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -609,4 +744,9 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n \n-__all__ = [\"LlavaNextVideoConfig\", \"LlavaNextVideoForConditionalGeneration\", \"LlavaNextVideoPreTrainedModel\"]\n+__all__ = [\n+    \"LlavaNextVideoConfig\",\n+    \"LlavaNextVideoForConditionalGeneration\",\n+    \"LlavaNextVideoModel\",\n+    \"LlavaNextVideoPreTrainedModel\",  # noqa: F822\n+]"
        },
        {
            "sha": "df49004eb2496e137831fa10dba30071e617abc2",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -4,9 +4,25 @@\n #             the file from the modular. If any change should be done, please apply the change to the\n #                          modular_llava_onevision.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n \n from typing import List, Optional, Union\n \n+import torch\n+\n from ...image_processing_utils import BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_processing_utils_fast import (\n     BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n@@ -28,11 +44,9 @@\n     make_flat_list_of_images,\n )\n from ...processing_utils import Unpack\n-from ...utils import TensorType, add_start_docstrings, is_torch_available, is_torchvision_v2_available\n+from ...utils import TensorType, add_start_docstrings, is_torchvision_v2_available\n \n \n-if is_torch_available():\n-    import torch\n if is_torchvision_v2_available():\n     from torchvision.transforms.v2 import functional as F\n else:"
        },
        {
            "sha": "5ef23387e9fe5766292901842d3cc3e3a286bcf4",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 467,
            "deletions": 264,
            "changes": 731,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/llava_onevision/modular_llava_onevision.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_llava_onevision.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,37 +18,191 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch Llava-Onevision model.\"\"\"\n \n import math\n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n \n import numpy as np\n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_llava_onevision import LlavaOnevisionConfig\n \n \n logger = logging.get_logger(__name__)\n \n-_CONFIG_FOR_DOC = \"LlavaNextConfig\"\n+\n+@dataclass\n+class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n+class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for LlavaOnevision causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+class LlavaOnevisionPooler(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        mode = config.spatial_pool_mode\n+        stride = config.spatial_pool_stride\n+        out_channels = getattr(config, \"spatial_pool_out_channels\", config.vision_config.hidden_size)\n+        self.image_size = (config.vision_config.image_size // config.vision_config.patch_size) ** 2\n+\n+        if mode == \"average\":\n+            self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride)\n+        elif mode == \"max\":\n+            self.pool = nn.MaxPool2d(kernel_size=stride, stride=stride)\n+        elif mode == \"conv\":\n+            self.pool = nn.Conv2d(\n+                in_channels=config.vision_config.hidden_size,\n+                out_channels=out_channels,\n+                kernel_size=stride,\n+                stride=stride,\n+            )\n+        else:\n+            raise ValueError(f\"Unknown pooling mode: {mode}. Has to be one of [`average`, `max`, `conv`]\")\n+\n+    def forward(self, image_features):\n+        ori_width = int(math.sqrt(image_features.shape[1] * self.image_size // self.image_size))\n+        ori_height = int(ori_width * self.image_size // self.image_size)\n+\n+        batch_size, _, dim = image_features.shape\n+        image_features_spatial = image_features.view(batch_size, ori_height, ori_height, dim).permute(0, 3, 1, 2)\n+        image_features_spatial_pool = self.pool(image_features_spatial)\n+\n+        return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n+\n+\n+class LlavaOnevisionMultiModalProjector(nn.Module):\n+    def __init__(self, config: LlavaOnevisionConfig):\n+        super().__init__()\n+        # We have hidden_size * the number of vision feature layers\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n+        self.linear_1 = nn.Linear(\n+            config.vision_config.hidden_size * num_feature_layers,\n+            config.text_config.hidden_size,\n+            bias=config.multimodal_projector_bias,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(\n+            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n+        )\n+\n+    def forward(self, image_features):\n+        hidden_states = self.linear_1(image_features)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+LLAVA_ONEVISION_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`LlavaOnevisionConfig`] or [`LlavaOnevisionVisionConfig`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n \n \n-# Copied from transformers.models.llava_next.modeling_llava_next.get_anyres_image_grid_shape\n def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n     \"\"\"\n     Calculate the shape of the image patch grid after the preprocessing for images of any resolution.\n@@ -74,7 +234,6 @@ def get_anyres_image_grid_shape(image_size, grid_pinpoints, patch_size):\n     return height // patch_size, width // patch_size\n \n \n-# Copied from transformers.models.llava_next.modeling_llava_next.image_size_to_num_patches\n def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n     \"\"\"\n     Calculate the number of patches after the preprocessing for images of any resolution.\n@@ -112,7 +271,6 @@ def image_size_to_num_patches(image_size, grid_pinpoints, patch_size: int):\n     return num_patches\n \n \n-# Copied from transformers.models.llava_next.modeling_llava_next.unpad_image\n def unpad_image(tensor, original_size):\n     \"\"\"\n     Unpads a PyTorch tensor of a padded and resized image.\n@@ -152,121 +310,6 @@ def unpad_image(tensor, original_size):\n     return unpadded_tensor\n \n \n-@dataclass\n-# Copied from transformers.models.llava_next_video.modeling_llava_next_video.LlavaNextVideoCausalLMOutputWithPast with LlavaNextVideo->LlavaOnevision\n-class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for LlavaOnevision causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size * num_patches, num_images, sequence_length, hidden_size)`.\n-            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-\n-        video_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n-            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    image_hidden_states: Optional[torch.FloatTensor] = None\n-    video_hidden_states: Optional[torch.FloatTensor] = None\n-\n-\n-# Copied from transformers.models.llava.modeling_llava.LlavaMultiModalProjector with Llava->LlavaOnevision\n-class LlavaOnevisionMultiModalProjector(nn.Module):\n-    def __init__(self, config: LlavaOnevisionConfig):\n-        super().__init__()\n-        # We have hidden_size * the number of vision feature layers\n-        num_feature_layers = 1 if isinstance(config.vision_feature_layer, int) else len(config.vision_feature_layer)\n-        self.linear_1 = nn.Linear(\n-            config.vision_config.hidden_size * num_feature_layers,\n-            config.text_config.hidden_size,\n-            bias=config.multimodal_projector_bias,\n-        )\n-        self.act = ACT2FN[config.projector_hidden_act]\n-        self.linear_2 = nn.Linear(\n-            config.text_config.hidden_size, config.text_config.hidden_size, bias=config.multimodal_projector_bias\n-        )\n-\n-    def forward(self, image_features):\n-        hidden_states = self.linear_1(image_features)\n-        hidden_states = self.act(hidden_states)\n-        hidden_states = self.linear_2(hidden_states)\n-        return hidden_states\n-\n-\n-LLAVA_ONEVISION_START_DOCSTRING = r\"\"\"\n-    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n-    etc.)\n-\n-    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n-    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n-    and behavior.\n-\n-    Parameters:\n-        config ([`LlavaNextConfig`] or [`LlavaNextVisionConfig`]):\n-            Model configuration class with all the parameters of the model. Initializing with a config file does not\n-            load the weights associated with the model, only the configuration. Check out the\n-            [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n-\"\"\"\n-\n-\n-@add_start_docstrings(\n-    \"The bare LLaVA-Onevision Model outputting raw hidden-states without any specific head on top.\",\n-    LLAVA_ONEVISION_START_DOCSTRING,\n-)\n-class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n-    config_class = LlavaOnevisionConfig\n-    base_model_prefix = \"model\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlavaOnevisionVisionAttention\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-    _supports_flash_attn_2 = True\n-    _supports_cache_class = True\n-    _supports_static_cache = True\n-    _supports_quantized_cache = True\n-    _supports_sdpa = True\n-\n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextPreTrainedModel._init_weights with LlavaNext->LlavaOnevision\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, LlavaOnevisionForConditionalGeneration):\n-            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n-\n-\n LLAVA_ONEVISION_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -279,16 +322,10 @@ def _init_weights(self, module):\n             [What are input IDs?](../glossary#input-ids)\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n             The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`LlavaNextImageProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextImageProcessor`] for processing images.\n+            [`AutoImageProcessor`]. See [`LlavaOnevisionImageProcessor.__call__`] for details. [`LlavaProcessor`] uses\n+            [`LlavaOnevisionImageProcessor`] for processing images.\n         image_sizes (`torch.LongTensor` of shape `(batch_size, 2)`, *optional*):\n             The sizes of the images in the batch, being (height, width) for each image.\n-        pixel_values_videos (`torch.FloatTensor` of shape `(batch_size, frames, num_channels, image_size, image_size)):\n-            The tensors corresponding to the input videos. Pixel values can be obtained using\n-            [`LlavaNextVideoProcessor`]. See [`LlavaNextVideoProcessor.__call__`] for details. [`LlavaProcessor`] uses\n-            [`LlavaNextVideoProcessor`] for processing videos.\n-        image_sizes_videos (`torch.LongTensor` of shape `(batch_size, frames, 2)`, *optional*):\n-            The sizes of the videos in the batch, being (height, width) for each frame in the video.\n         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n \n@@ -335,8 +372,6 @@ def _init_weights(self, module):\n             The feature selection strategy used to select the vision feature from the vision backbone.\n             Can be one of `\"default\"` or `\"full\"`. If `\"default\"`, the CLS token is removed from the vision features.\n             If `\"full\"`, the full vision features are used.\n-        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n-            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n         use_cache (`bool`, *optional*):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n             `past_key_values`).\n@@ -356,11 +391,41 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The LLaVA-Onevision model which consists of a vision backbone and a language model.\"\"\",\n+    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n     LLAVA_ONEVISION_START_DOCSTRING,\n )\n-class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):\n-    def __init__(self, config: LlavaOnevisionConfig):\n+class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n+    config_class = LlavaOnevisionConfig\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n+    _supports_cache_class = True\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n+\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n+\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, LlavaOnevisionModel):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Llava-Next model which consists of a vision backbone and a language model without language modeling head.\"\"\",\n+    LLAVA_ONEVISION_START_DOCSTRING,\n+)\n+class LlavaOnevisionModel(LlavaOnevisionPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n+    def __init__(self, config):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n@@ -369,36 +434,16 @@ def __init__(self, config: LlavaOnevisionConfig):\n         self.image_newline = nn.Parameter(torch.randn(config.text_config.hidden_size, dtype=self.dtype) * embed_std)\n \n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n+        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.set_decoder\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    # Copied from transformers.models.llava_next.modeling_llava_next.LlavaNextForConditionalGeneration.get_decoder\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n         \"\"\"\n         Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n@@ -465,20 +510,6 @@ def pack_image_features(self, image_features, image_sizes, image_newline=None, v\n         feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n         return image_features, feature_lens\n \n-    def apply_pooling(self, image_features):\n-        height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n-        batch_frames, seq_len, dim = image_features.shape\n-        image_features = image_features.view(batch_frames, height, width, -1)\n-        image_features = image_features.permute(0, 3, 1, 2).contiguous()\n-\n-        height, width = image_features.shape[2:]\n-        scaled_shape = [math.ceil(height / 2), math.ceil(width / 2)]\n-        image_features = nn.functional.interpolate(image_features, size=scaled_shape, mode=\"bilinear\")\n-\n-        image_features = image_features.permute(0, 2, 3, 1)\n-        image_features = image_features.view(batch_frames, -1, dim)\n-        return image_features\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -494,7 +525,7 @@ def get_image_features(\n                The tensors corresponding to the input images.\n             image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n                 Actual image size of each images (H, W).\n-            vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            vision_feature_layer (`Union[int, List[int]]`):\n                 The index of the layer to select the vision feature. If multiple indices are provided,\n                 the vision feature of the corresponding indices will be concatenated to form the\n                 vision features.\n@@ -539,6 +570,131 @@ def get_image_features(\n         image_features = torch.split(image_features, image_num_patches, dim=0)\n         return image_features\n \n+    @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes_videos: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaOnevisionModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # Images are processed with Anyres\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                image_newline=self.image_newline,\n+                vision_aspect_ratio=vision_aspect_ratio,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        # Video are simply embedded and further pooled to decrease seq len\n+        if pixel_values_videos is not None:\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            image_newline = (\n+                self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n+            )\n+            video_features = torch.cat((video_features, image_newline), dim=1)\n+            video_features = video_features.flatten(0, 1)\n+\n+            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n+                n_video_features = video_features.shape[0]\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaOnevisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+        )\n+\n+        return output if return_dict else output.to_tuple()\n+\n     def get_video_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -585,13 +741,74 @@ def get_video_features(\n \n         return video_features\n \n+    def apply_pooling(self, image_features):\n+        height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        batch_frames, seq_len, dim = image_features.shape\n+        image_features = image_features.view(batch_frames, height, width, -1)\n+        image_features = image_features.permute(0, 3, 1, 2).contiguous()\n+\n+        height, width = image_features.shape[2:]\n+        scaled_shape = [math.ceil(height / 2), math.ceil(width / 2)]\n+        image_features = nn.functional.interpolate(image_features, size=scaled_shape, mode=\"bilinear\")\n+\n+        image_features = image_features.permute(0, 2, 3, 1)\n+        image_features = image_features.view(batch_frames, -1, dim)\n+        return image_features\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The LLAVA-NeXT model which consists of a vision backbone and a language model.\"\"\",\n+    LLAVA_ONEVISION_START_DOCSTRING,\n+)\n+class LlavaOnevisionForConditionalGeneration(LlavaOnevisionPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^image_newline\": \"model.image_newline\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: LlavaOnevisionConfig):\n+        super().__init__(config)\n+        self.model = LlavaOnevisionModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n-        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n@@ -673,114 +890,45 @@ def forward(\n             vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        # Images are processed with Anyres\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values,\n-                image_sizes,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-            )\n-            image_features, feature_lens = self.pack_image_features(\n-                image_features,\n-                image_sizes,\n-                image_newline=self.image_newline,\n-                vision_aspect_ratio=vision_aspect_ratio,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        # Video are simply embedded and further pooled to decrease seq len\n-        if pixel_values_videos is not None:\n-            video_features = self.get_video_features(\n-                pixel_values_videos,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-            )\n-            image_newline = (\n-                self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n-            )\n-            video_features = torch.cat((video_features, image_newline), dim=1)\n-            video_features = video_features.flatten(0, 1)\n-\n-            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n-                n_video_features = video_features.shape[0]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_sizes=image_sizes,\n+            image_sizes_videos=image_sizes_videos,\n+            vision_aspect_ratio=vision_aspect_ratio,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return LlavaOnevisionCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n-            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -799,7 +947,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -819,5 +967,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"LlavaOnevisionForConditionalGeneration\", \"LlavaOnevisionPreTrainedModel\"]\n+__all__ = [\"LlavaOnevisionModel\", \"LlavaOnevisionForConditionalGeneration\", \"LlavaOnevisionPreTrainedModel\"]"
        },
        {
            "sha": "bc692c10a645d5b72ebd4c8042f00a7e9ba51418",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 476,
            "deletions": 2,
            "changes": 478,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -1,16 +1,48 @@\n+# coding=utf-8\n+# Copyright 2024 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+import math\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n from transformers.models.llava_next.image_processing_llava_next_fast import LlavaNextImageProcessorFast\n+from transformers.models.llava_next_video.modeling_llava_next_video import (\n+    LlavaNextVideoCausalLMOutputWithPast,\n+    LlavaNextVideoForConditionalGeneration,\n+    LlavaNextVideoModel,\n+    LlavaNextVideoModelOutputWithPast,\n+    LlavaNextVideoPreTrainedModel,\n+    get_anyres_image_grid_shape,\n+    unpad_image,\n+)\n \n from ...image_processing_utils_fast import BASE_IMAGE_PROCESSOR_FAST_DOCSTRING\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n     PILImageResampling,\n )\n-from ...utils import add_start_docstrings, logging\n+from ...utils import add_start_docstrings, can_return_tuple, is_torchdynamo_compiling, logging\n \n \n logger = logging.get_logger(__name__)\n \n+LLAVA_ONEVISION_INPUTS_DOCSTRING = None\n+\n \n @add_start_docstrings(\n     \"Constructs a fast ConvNeXT image processor. Based on [`SiglipImageProcessor`] with incorporation of processing each video frame.\",\n@@ -42,4 +74,446 @@ class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):\n     model_input_names = [\"pixel_values_videos\"]\n \n \n-__all__ = [\"LlavaOnevisionImageProcessorFast\"]\n+class LlavaOnevisionModelOutputWithPast(LlavaNextVideoModelOutputWithPast):\n+    pass\n+\n+\n+class LlavaOnevisionCausalLMOutputWithPast(LlavaNextVideoCausalLMOutputWithPast):\n+    pass\n+\n+\n+class LlavaOnevisionPreTrainedModel(LlavaNextVideoPreTrainedModel):\n+    pass\n+\n+\n+class LlavaOnevisionModel(LlavaNextVideoModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        del self.vision_resampler\n+\n+    def pack_image_features(self, image_features, image_sizes, image_newline=None, vision_aspect_ratio=\"anyres_max_9\"):\n+        \"\"\"\n+        Reshape, unpad and then pack each image_feature into a single image_features tensor containing all visual vectors.\n+\n+        Args:\n+            image_features (`List[torch.Tensor]` of length num_images, each of shape `(num_patches, image_length, embed_dim)`)\n+                List of image feature tensor, each contains all the visual feature of all patches.\n+            image_sizes (`torch.Tensor` of shape `(num_images, 2)`)\n+                Actual image size of each images (H, W).\n+            image_newline (`torch.Tensor` of shape `(embed_dim)`)\n+                New line embedding vector.\n+            vision_aspect_ratio (`str`, *optional*, \"anyres_max_9\"):\n+                Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        Returns:\n+            image_features (`torch.Tensor` of shape `(all_feat_len, embed_dim)`)\n+            feature_lens (`List[int]`)\n+                token length of each image in image_features\n+        \"\"\"\n+        new_image_features = []\n+        feature_lens = []\n+        for image_idx, image_feature in enumerate(image_features):\n+            if image_feature.shape[0] > 1:\n+                base_image_feature = image_feature[0]\n+                image_feature = image_feature[1:]\n+                height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+                if height * width != base_image_feature.shape[0]:\n+                    raise ValueError(\"The number of patches is not consistent with the image size.\")\n+                num_patch_height, num_patch_width = get_anyres_image_grid_shape(\n+                    image_sizes[image_idx],\n+                    self.config.image_grid_pinpoints,\n+                    self.config.vision_config.image_size,\n+                )\n+                image_feature = image_feature.view(num_patch_height, num_patch_width, height, width, -1)\n+                image_feature = image_feature.permute(4, 0, 2, 1, 3).contiguous()\n+                image_feature = image_feature.flatten(1, 2).flatten(2, 3)\n+                image_feature = unpad_image(image_feature, image_sizes[image_idx])\n+                max_num_patches = int(vision_aspect_ratio.strip(\"anyres_max_\"))\n+                channels, curr_height, curr_width = image_feature.shape\n+                ratio = math.sqrt(curr_height * curr_width / (max_num_patches * height**2))\n+                if ratio > 1.1:\n+                    image_feature = image_feature[None]\n+                    image_feature = nn.functional.interpolate(\n+                        image_feature, [int(curr_height // ratio), int(curr_width // ratio)], mode=\"bilinear\"\n+                    )[0]\n+                if image_newline is not None:\n+                    image_feature = torch.cat(\n+                        (\n+                            image_feature,\n+                            image_newline[:, None, None]\n+                            .expand(*image_feature.shape[:-1], 1)\n+                            .to(image_feature.device, image_feature.dtype),\n+                        ),\n+                        dim=-1,\n+                    )\n+                image_feature = image_feature.flatten(1, 2).transpose(0, 1)\n+                image_feature = torch.cat((base_image_feature, image_feature), dim=0)\n+            else:\n+                image_feature = image_feature[0]\n+                if image_newline is not None:\n+                    image_feature = torch.cat((image_feature, image_newline[None].to(image_feature)), dim=0)\n+            new_image_features.append(image_feature)\n+            feature_lens.append(image_feature.size(0))\n+        image_features = torch.cat(new_image_features, dim=0)\n+        feature_lens = torch.tensor(feature_lens, dtype=torch.long, device=image_features.device)\n+        return image_features, feature_lens\n+\n+    def apply_pooling(self, image_features):\n+        height = width = self.config.vision_config.image_size // self.config.vision_config.patch_size\n+        batch_frames, seq_len, dim = image_features.shape\n+        image_features = image_features.view(batch_frames, height, width, -1)\n+        image_features = image_features.permute(0, 3, 1, 2).contiguous()\n+\n+        height, width = image_features.shape[2:]\n+        scaled_shape = [math.ceil(height / 2), math.ceil(width / 2)]\n+        image_features = nn.functional.interpolate(image_features, size=scaled_shape, mode=\"bilinear\")\n+\n+        image_features = image_features.permute(0, 2, 3, 1)\n+        image_features = image_features.view(batch_frames, -1, dim)\n+        return image_features\n+\n+    def get_video_features(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        vision_feature_layer: Union[int, List[int]],\n+        vision_feature_select_strategy: str,\n+    ):\n+        \"\"\"\n+        Obtains video last hidden states from the vision tower, apply multimodal projection and pooling.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, num_frames, channels, height, width)`)\n+               The tensors corresponding to the input video.\n+            vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+                The index of the layer to select the vision feature. If multiple indices are provided,\n+                the vision feature of the corresponding indices will be concatenated to form the\n+                vision features.\n+            vision_feature_select_strategy (`str`):\n+                The feature selection strategy used to select the vision feature from the vision backbone.\n+                Can be one of `\"default\"` or `\"full\"`\n+        Returns:\n+            video_features (List[`torch.Tensor`]): List of video feature tensor, each contains all the visual feature of all patches\n+            and are of shape `(num_videos, video_length, embed_dim)`).\n+        \"\"\"\n+        batch_size, frames, channels, height, width = pixel_values.shape\n+        pixel_values = pixel_values.view(batch_size * frames, channels, height, width)\n+        video_features = self.vision_tower(pixel_values, output_hidden_states=True)\n+\n+        # If we have one vision feature layer, return the corresponding hidden states,\n+        # otherwise, select the hidden states of each feature layer and concatenate them\n+        if isinstance(vision_feature_layer, int):\n+            selected_video_feature = video_features.hidden_states[vision_feature_layer]\n+        else:\n+            hs_pool = [video_features.hidden_states[layer_idx] for layer_idx in vision_feature_layer]\n+            selected_video_feature = torch.cat(hs_pool, dim=-1)\n+\n+        if vision_feature_select_strategy == \"default\":\n+            selected_video_feature = selected_video_feature[:, 1:]\n+        elif vision_feature_select_strategy == \"full\":\n+            selected_video_feature = selected_video_feature\n+        video_features = self.multi_modal_projector(selected_video_feature)\n+\n+        video_features = self.apply_pooling(video_features)\n+        video_features = video_features.reshape(batch_size, frames * video_features.shape[1], -1)\n+\n+        return video_features\n+\n+    @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes_videos: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaOnevisionModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n+                \"and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        # Images are processed with Anyres\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values,\n+                image_sizes,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            image_features, feature_lens = self.pack_image_features(\n+                image_features,\n+                image_sizes,\n+                image_newline=self.image_newline,\n+                vision_aspect_ratio=vision_aspect_ratio,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        # Video are simply embedded and further pooled to decrease seq len\n+        if pixel_values_videos is not None:\n+            video_features = self.get_video_features(\n+                pixel_values_videos,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            image_newline = (\n+                self.image_newline[None, None, :].repeat(video_features.shape[0], 1, 1).to(video_features.device)\n+            )\n+            video_features = torch.cat((video_features, image_newline), dim=1)\n+            video_features = video_features.flatten(0, 1)\n+\n+            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n+                n_video_features = video_features.shape[0]\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_video_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = LlavaOnevisionModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+        )\n+\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class LlavaOnevisionForConditionalGeneration(LlavaNextVideoForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings(LLAVA_ONEVISION_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        image_sizes: Optional[torch.LongTensor] = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        image_sizes_videos: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        vision_aspect_ratio: Optional[str] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, LlavaOnevisionCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+            [`~LlavaOnevisionCausalLMOutputWithPast`] (if `return_dict=True`) or a `tuple`.\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> import torch\n+        >>> from transformers import LlavaOnevisionProcessor, LlavaOnevisionForConditionalGeneration\n+\n+        >>> model = LlavaOnevisionForConditionalGeneration.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\", torch_dtype=\"float16\", device_map=\"cuda:0\")\n+        >>> processor = LlavaOnevisionProcessor.from_pretrained(\"llava-hf/llava-onevision-qwen2-7b-ov-hf\")\n+\n+        >>> conversation = [\n+        ...     {\n+        ...       \"role\": \"user\",\n+        ...       \"content\": [\n+        ...           {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ...           {\"type\": \"image\"},\n+        ...         ],\n+        ...     },\n+        ... ]\n+        >>> prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)\n+\n+        >>> image_file = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> raw_image = Image.open(requests.get(image_file, stream=True).raw)\n+        >>> inputs = processor(text=prompt, images=raw_image, return_tensors='pt').to(0, torch.float16)\n+\n+        >>> output = model.generate(**inputs, max_new_tokens=20, do_sample=False)\n+        >>> processor.batch_decode(output, skip_special_tokens=True)[0]\n+        \"user\\n\\nWhat is shown in this image?\\nassistant\\ncat\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+        vision_aspect_ratio = (\n+            vision_aspect_ratio if vision_aspect_ratio is not None else self.config.vision_aspect_ratio\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_sizes=image_sizes,\n+            image_sizes_videos=image_sizes_videos,\n+            vision_aspect_ratio=vision_aspect_ratio,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **lm_kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return LlavaOnevisionCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n+        )\n+\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        inputs_embeds=None,\n+        pixel_values=None,\n+        image_sizes=None,\n+        pixel_values_videos=None,\n+        image_sizes_videos=None,\n+        attention_mask=None,\n+        cache_position=None,\n+        logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n+\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n+        )\n+\n+        if cache_position[0] == 0:\n+            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n+            # Otherwise we need pixel values to be passed to model\n+            model_inputs[\"pixel_values\"] = pixel_values\n+            model_inputs[\"image_sizes\"] = image_sizes\n+            model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n+            model_inputs[\"image_sizes_videos\"] = image_sizes_videos\n+\n+        return model_inputs\n+\n+\n+__all__ = [\n+    \"LlavaOnevisionImageProcessorFast\",\n+    \"LlavaOnevisionModel\",\n+    \"LlavaOnevisionForConditionalGeneration\",\n+    \"LlavaOnevisionPreTrainedModel\",\n+]"
        },
        {
            "sha": "7078631552f652b1e7c5965ba4dc1d90ee75a903",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 246,
            "deletions": 92,
            "changes": 338,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -28,19 +28,20 @@\n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_mistral3 import Mistral3Config\n \n \n-_CONFIG_FOR_DOC = \"Mistral3Config\"\n+_CONFIG_FOR_DOC = \"Mistra3Config\"\n \n \n @use_kernel_forward_from_hub(\"RMSNorm\")\n@@ -156,7 +157,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -168,6 +169,39 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n     image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n+@dataclass\n+class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for Mistral3 outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n MISTRAL3_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -186,14 +220,13 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n \n \n @add_start_docstrings(\n-    \"The bare LLaMA Model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare Mistral3 Model outputting raw hidden-states without any specific head on top.\",\n     MISTRAL3_START_DOCSTRING,\n )\n class Mistral3PreTrainedModel(PreTrainedModel):\n     config_class = Mistral3Config\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"Mistral3VisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -202,12 +235,18 @@ class Mistral3PreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n+        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, Mistral3RMSNorm):\n             module.weight.data.fill_(1.0)\n \n@@ -290,23 +329,18 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The MISTRAL3 model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The Mistral3 model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     MISTRAL3_START_DOCSTRING,\n )\n-class Mistral3ForConditionalGeneration(Mistral3PreTrainedModel, GenerationMixin):\n+class Mistral3Model(Mistral3PreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: Mistral3Config):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = Mistral3MultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -315,18 +349,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -364,24 +386,147 @@ def get_image_features(\n         return image_features\n \n     @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Mistral3ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                image_sizes=image_sizes,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = Mistral3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The MISTRAL3 model which consists of a vision backbone and a language model.\"\"\",\n+    MISTRAL3_START_DOCSTRING,\n+)\n+class Mistral3ForConditionalGeneration(Mistral3PreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: Mistral3Config):\n+        super().__init__(config)\n+        self.model = Mistral3Model(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        image_sizes: torch.Tensor = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -421,90 +566,44 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is the image?The image depicts two cats lying on a pink blanket.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        vision_feature_layer = (\n-            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n-        )\n-\n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n-                image_sizes=image_sizes,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return Mistral3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -520,7 +619,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -537,5 +636,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"Mistral3PreTrainedModel\", \"Mistral3ForConditionalGeneration\"]\n+__all__ = [\"Mistral3Model\", \"Mistral3PreTrainedModel\", \"Mistral3ForConditionalGeneration\"]"
        },
        {
            "sha": "5ef6663bde012b309aa93550107e316a9ab108b5",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 124,
            "deletions": 63,
            "changes": 187,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -19,14 +19,29 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...utils import is_torchdynamo_compiling, logging\n-from ..llava.modeling_llava import LlavaCausalLMOutputWithPast, LlavaForConditionalGeneration, LlavaPreTrainedModel\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from ..llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n+    LlavaPreTrainedModel,\n+)\n from ..mistral.modeling_mistral import MistralRMSNorm\n from .configuration_mistral3 import Mistral3Config\n \n \n logger = logging.get_logger(__name__)\n \n+MISTRAL3_INPUTS_DOCSTRING = None\n+_CONFIG_FOR_DOC = \"Mistra3Config\"\n+\n \n class Mistral3RMSNorm(MistralRMSNorm):\n     pass\n@@ -100,19 +115,29 @@ class Mistral3CausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n     pass\n \n \n+class Mistral3ModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n class Mistral3PreTrainedModel(LlavaPreTrainedModel):\n     def _init_weights(self, module):\n+        # important: this ported version of Mistral3 isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/Mistral3/tree/main/mistral3 should serve for that purpose\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n         elif isinstance(module, Mistral3RMSNorm):\n             module.weight.data.fill_(1.0)\n \n \n-class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n+class Mistral3Model(LlavaModel):\n     def get_image_features(\n         self,\n         pixel_values: torch.FloatTensor,\n@@ -151,21 +176,102 @@ def get_image_features(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        image_sizes: torch.Tensor = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, Mistral3ModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values,\n+                vision_feature_layer=vision_feature_layer,\n+                image_sizes=image_sizes,\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = Mistral3ModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class Mistral3ForConditionalGeneration(LlavaForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(MISTRAL3_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Mistral3CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n-        image_sizes: Optional[torch.Tensor] = None,\n+        image_sizes: torch.Tensor = None,\n         **lm_kwargs,\n     ) -> Union[Tuple, Mistral3CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -205,94 +311,49 @@ def forward(\n         >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n         \"What is the image?The image depicts two cats lying on a pink blanket.\"\n         ```\"\"\"\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        vision_feature_layer = (\n-            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n-        )\n-\n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values,\n-                vision_feature_layer=vision_feature_layer,\n-                image_sizes=image_sizes,\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n+            image_sizes=image_sizes,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return Mistral3CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n \n __all__ = [\n+    \"Mistral3Model\",\n     \"Mistral3PreTrainedModel\",  # noqa\n     \"Mistral3ForConditionalGeneration\",\n ]"
        },
        {
            "sha": "79278c9892ec517d157e0a92b8807f556c9cc1e0",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 167,
            "deletions": 118,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -32,6 +32,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -1968,10 +1969,11 @@ def forward(\n \n \n @add_start_docstrings(\n-    \"\"\"The Mllama model which consists of a vision encoder and a language model.\"\"\",\n+    \"\"\"The Mllama model which consists of a vision encoder and a language model without language modeling head.\"\"\",\n     MLLAMA_START_DOCSTRING,\n )\n-class MllamaForConditionalGeneration(MllamaPreTrainedModel, GenerationMixin):\n+class MllamaModel(MllamaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n     _supports_quantized_cache = False  # quant cache not supported in encoder-decoder setting\n \n     def __init__(self, config: MllamaConfig):\n@@ -1983,10 +1985,7 @@ def __init__(self, config: MllamaConfig):\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n \n         self.vision_model = MllamaVisionModel._from_config(config.vision_config)\n-        self.language_model = MllamaForCausalLM._from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = MllamaTextModel._from_config(config.text_config)\n         self.multi_modal_projector = nn.Linear(\n             config.vision_config.vision_output_dim,\n             config.text_config.hidden_size,\n@@ -2000,18 +1999,139 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n+    @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        aspect_ratio_mask: Optional[torch.Tensor] = None,\n+        aspect_ratio_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        cross_attention_mask: Optional[torch.Tensor] = None,\n+        cross_attention_states: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if pixel_values is not None and cross_attention_states is not None:\n+            raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n+\n+        if pixel_values is not None:\n+            if aspect_ratio_ids is None:\n+                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n+            # get vision tokens from vision model\n+            vision_outputs = self.vision_model(\n+                pixel_values=pixel_values,\n+                aspect_ratio_ids=aspect_ratio_ids,\n+                aspect_ratio_mask=aspect_ratio_mask,\n+                output_hidden_states=output_hidden_states,\n+                output_attentions=output_attentions,\n+                return_dict=return_dict,\n+            )\n+            cross_attention_states = vision_outputs[0]\n+            cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n+                -1, cross_attention_states.shape[-2], self.hidden_size\n+            )\n+\n+        if cross_attention_mask is not None:\n+            cross_attention_mask, full_text_row_masked_out_mask = _prepare_cross_attention_mask(\n+                cross_attention_mask,\n+                num_vision_tokens=self.vision_model.num_patches,\n+                dtype=self.dtype,\n+            )\n+        else:\n+            full_text_row_masked_out_mask = None\n+\n+        if cross_attention_mask is not None and cache_position is not None:\n+            cross_attention_mask = cross_attention_mask[:, :, cache_position]\n+            full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n+\n+        outputs = self.language_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            cross_attention_states=cross_attention_states,\n+            cross_attention_mask=cross_attention_mask,\n+            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            inputs_embeds=inputs_embeds,\n+            output_hidden_states=output_hidden_states,\n+            output_attentions=output_attentions,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = BaseModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The Mllama model which consists of a vision encoder and a language model.\"\"\",\n+    MLLAMA_START_DOCSTRING,\n+)\n+class MllamaForConditionalGeneration(MllamaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_model\": \"model.vision_model\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _supports_quantized_cache = False  # quant cache not supported in encoder-decoder setting\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: MllamaConfig):\n+        super().__init__(config)\n+        self.model = MllamaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n     def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n+        return self.lm_head\n \n     def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n+        self.lm_head = new_embeddings\n \n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n \n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n+    @property\n+    def vision_model(self):\n+        return self.model.vision_model\n \n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(MLLAMA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=\"MllamaConfig\")\n     def forward(\n@@ -2084,78 +2204,36 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if pixel_values is not None and cross_attention_states is not None:\n-            raise ValueError(\"`pixel_values` and `cross_attention_states` cannot be provided simultaneously\")\n-\n-        if pixel_values is not None:\n-            if aspect_ratio_ids is None:\n-                raise ValueError(\"`aspect_ratio_ids` must be provided if `pixel_values` is provided\")\n-            # get vision tokens from vision model\n-            vision_outputs = self.vision_model(\n-                pixel_values=pixel_values,\n-                aspect_ratio_ids=aspect_ratio_ids,\n-                aspect_ratio_mask=aspect_ratio_mask,\n-                output_hidden_states=output_hidden_states,\n-                output_attentions=output_attentions,\n-                return_dict=return_dict,\n-            )\n-            cross_attention_states = vision_outputs[0]\n-            cross_attention_states = self.multi_modal_projector(cross_attention_states).reshape(\n-                -1, cross_attention_states.shape[-2], self.hidden_size\n-            )\n-\n-        if cross_attention_mask is not None:\n-            cross_attention_mask, full_text_row_masked_out_mask = _prepare_cross_attention_mask(\n-                cross_attention_mask,\n-                num_vision_tokens=self.vision_model.num_patches,\n-                dtype=self.dtype,\n-            )\n-        else:\n-            full_text_row_masked_out_mask = None\n-\n-        if cross_attention_mask is not None and cache_position is not None:\n-            cross_attention_mask = cross_attention_mask[:, :, cache_position]\n-            full_text_row_masked_out_mask = full_text_row_masked_out_mask[:, :, cache_position]\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n             input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            aspect_ratio_mask=aspect_ratio_mask,\n+            aspect_ratio_ids=aspect_ratio_ids,\n+            cross_attention_mask=cross_attention_mask,\n+            cross_attention_states=cross_attention_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n-            cross_attention_states=cross_attention_states,\n-            cross_attention_mask=cross_attention_mask,\n-            full_text_row_masked_out_mask=full_text_row_masked_out_mask,\n             past_key_values=past_key_values,\n-            use_cache=use_cache,\n             inputs_embeds=inputs_embeds,\n-            output_hidden_states=output_hidden_states,\n+            use_cache=use_cache,\n             output_attentions=output_attentions,\n-            return_dict=return_dict,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n-            **loss_kwargs,\n         )\n \n-        # Temporary fix to calculate the loss in main class, as the model's vocab size may be resized\n-        loss = None\n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n+        loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.config.get_text_config().vocab_size, **loss_kwargs)\n-\n-        if not return_dict:\n-            return (loss,) + outputs if loss is not None else outputs\n+            loss = self.loss_function(logits, labels, self.config.text_config.vocab_size, **loss_kwargs)\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n-            logits=outputs.logits,\n+            logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n@@ -2179,58 +2257,28 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        if past_key_values is not None:\n-            if (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        # TODO: we have no attention_mask so this won't work, check if we really won't need attention mask and find another way\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"cross_attention_mask\": cross_attention_mask,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            inputs_embeds=inputs_embeds,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            pixel_values=pixel_values,\n+            aspect_ratio_ids=aspect_ratio_ids,\n+            aspect_ratio_mask=aspect_ratio_mask,\n+            cross_attention_mask=cross_attention_mask,\n+            cache_position=cache_position,\n+            logits_to_keep=logits_to_keep,\n+            **kwargs,\n         )\n \n         # If we're in pre-fill or cacheless decoding step, then we need pixel_values and aspect ratios\n         # to compute image hidden states, otherwise they are cached within each cross attn layer\n-        if cache_position[0] == 0:\n-            model_inputs[\"pixel_values\"] = pixel_values\n-            model_inputs[\"aspect_ratio_ids\"] = aspect_ratio_ids\n-            model_inputs[\"aspect_ratio_mask\"] = aspect_ratio_mask\n+        if cache_position[0] != 0:\n+            model_inputs[\"pixel_values\"] = None\n+            model_inputs[\"aspect_ratio_ids\"] = None\n+            model_inputs[\"aspect_ratio_mask\"] = None\n \n         return model_inputs\n \n@@ -2257,4 +2305,5 @@ def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_\n     \"MllamaTextModel\",\n     \"MllamaVisionModel\",\n     \"MllamaPreTrainedModel\",\n+    \"MllamaModel\",\n ]"
        },
        {
            "sha": "f32ad303bf1276268187f7c9dba44e99ab5ecdc9",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,8 +13,6 @@\n # limitations under the License.\n \"\"\"PaliGemmamodel configuration\"\"\"\n \n-import warnings\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING, AutoConfig\n@@ -39,8 +37,6 @@ class PaliGemmaConfig(PretrainedConfig):\n             Custom vision config or dict\n         text_config (`Union[AutoConfig, dict]`, *optional*):\n             The config object of the text backbone. Can be any of `LlamaConfig` or `MistralConfig`.\n-        ignore_index (`int`, *optional*, defaults to -100):\n-            The ignore index for the loss function.\n         image_token_index (`int`, *optional*, defaults to 256000):\n             The image token index to encode the image prompt.\n         vocab_size (`int`, *optional*, defaults to 257152):\n@@ -83,16 +79,13 @@ def __init__(\n         self,\n         vision_config=None,\n         text_config=None,\n-        ignore_index=-100,\n         image_token_index=256000,\n         vocab_size=257152,\n         projection_dim=2048,\n         hidden_size=2048,\n         **kwargs,\n     ):\n-        self._ignore_index = ignore_index\n         self.image_token_index = image_token_index\n-        self._vocab_size = vocab_size\n         self.projection_dim = projection_dim\n         self.hidden_size = hidden_size\n         self.vision_config = vision_config\n@@ -133,22 +126,5 @@ def __init__(\n         self.vision_config.projection_dim = projection_dim\n         super().__init__(**kwargs)\n \n-    @property\n-    def ignore_index(self):\n-        warnings.warn(\n-            \"The `ignore_index` attribute is deprecated and will be removed in v4.47.\",\n-            FutureWarning,\n-        )\n-        return self._ignore_index\n-\n-    @ignore_index.setter\n-    def ignore_index(self, value):\n-        self._ignore_index = value\n-\n-    def to_dict(self):\n-        output = super().to_dict()\n-        output.pop(\"_ignore_index\", None)\n-        return output\n-\n \n __all__ = [\"PaliGemmaConfig\"]"
        },
        {
            "sha": "9561f0f062858f1554808e31d6db11402622dbd3",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 245,
            "deletions": 169,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -23,17 +23,18 @@\n \n from ...cache_utils import Cache, HybridCache, StaticCache\n from ...generation import GenerationMixin\n-from ...modeling_outputs import CausalLMOutputWithPast\n+from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_paligemma import PaliGemmaConfig\n \n \n@@ -42,78 +43,43 @@\n _CONFIG_FOR_DOC = \"PaliGemmaConfig\"\n \n \n-# Adapted from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-# But Paligemma has no causal mask on prefix\n-def _prepare_4d_causal_attention_mask_with_cache_position(\n-    attention_mask: torch.Tensor,\n-    sequence_length: int,\n-    target_length: int,\n-    dtype: torch.dtype,\n-    min_dtype: float,\n-    cache_position: torch.Tensor,\n-    batch_size: int,\n-    is_training: bool = False,\n-    token_type_ids: Optional[torch.Tensor] = None,\n-    **kwargs,\n-):\n+@dataclass\n+class PaligemmaModelOutputWithPast(BaseModelOutputWithPast):\n     \"\"\"\n-    Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-    `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+    Base class for Paligemma outputs, with hidden states and attentions.\n \n     Args:\n-        attention_mask (`torch.Tensor`):\n-            A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-        sequence_length (`int`):\n-            The sequence length being processed.\n-        target_length (`int`):\n-            The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-        dtype (`torch.dtype`):\n-            The dtype to use for the 4D attention mask.\n-        min_dtype (`float`):\n-            The minimum value representable with the dtype `dtype`.\n-        cache_position (`torch.Tensor`):\n-            Indices depicting the position of the input sequence tokens in the sequence.\n-        batch_size (`torch.Tensor`):\n-            Batch size.\n-        is_training (`bool`):\n-            Whether the model is in training mode or in inference. The condition is checked by presence/absence of `token_type_ids/labels`\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n-    if attention_mask is not None and attention_mask.dim() == 4:\n-        # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-        causal_mask = attention_mask\n-    else:\n-        causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-        )\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            if is_training:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            else:\n-                causal_mask[:, :sequence_length] = 0.0\n \n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-            # we are training thus we need to create a full mask on the image + prefix but causal on suffix\n-            if is_training:\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n-                )\n-    return causal_mask\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n-    Base class for PaliGemmacausal language model (or autoregressive) outputs.\n+    Base class for PaliGemma causal language model (or autoregressive) outputs.\n \n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n@@ -184,7 +150,7 @@ def forward(self, image_features):\n )\n class PaliGemmaPreTrainedModel(PreTrainedModel):\n     config_class = PaliGemmaConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"PaliGemmaMultiModalProjector\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -276,57 +242,40 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The PALIGEMMA model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"Base Paligemma model which consists of a vision backbone and a language model withou language modeling head.\"\"\",\n     PALIGEMMA_START_DOCSTRING,\n )\n-class PaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel, GenerationMixin):\n+class PaliGemmaModel(PaliGemmaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: PaliGemmaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config=config.vision_config)\n         self.multi_modal_projector = PaliGemmaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n \n-        language_model = AutoModelForCausalLM.from_config(config=config.text_config)\n-\n-        if language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in language_model._tied_weights_keys]\n+        language_model = AutoModel.from_config(config=config.text_config)\n         self.language_model = language_model\n \n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_input_embeddings with Llava->PaliGemma\n+    # Copied from transformers.models.llava.modeling_llava.LlavaModel.get_input_embeddings with Llava->PaliGemma\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_input_embeddings with Llava->PaliGemma\n+    # Copied from transformers.models.llava.modeling_llava.LlavaModel.set_input_embeddings with Llava->PaliGemma\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_output_embeddings with Llava->PaliGemma\n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_output_embeddings with Llava->PaliGemma\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_decoder with Llava->PaliGemma\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_decoder with Llava->PaliGemma\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def _update_causal_mask(\n         self,\n         attention_mask,\n         token_type_ids=None,\n         past_key_values=None,\n         cache_position=None,\n         input_tensor=None,\n-        is_training: Optional[bool] = None,\n+        is_training: bool = None,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n@@ -404,11 +353,10 @@ def get_image_features(self, pixel_values: torch.FloatTensor):\n         return image_features\n \n     @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n@@ -420,46 +368,8 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        logits_to_keep: Union[int, torch.Tensor] = 0,\n         **lm_kwargs,\n-    ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n-\n-            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n-                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n-                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n-                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n-                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n-                This is useful when using packed tensor format (single dimension for batch and sequence length).\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n-\n-        >>> model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-mix-224\")\n-        >>> processor = AutoProcessor.from_pretrained(\"google/paligemma2-3b-mix-224\")\n-\n-        >>> prompt = \"Where is the cat standing?\"\n-        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(**inputs,)\n-        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"Where is the cat standing?\\nsnow\"\n-        ```\"\"\"\n-\n+    ) -> Union[Tuple, PaligemmaModelOutputWithPast]:\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n@@ -471,7 +381,7 @@ def forward(\n \n         is_training = token_type_ids is not None and labels is not None\n \n-        # Replace image id with PAD if the image token if OOV, to avoid index-errors\n+        # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n             llm_input_ids = input_ids.clone()\n@@ -513,18 +423,10 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        # mask out pad-token-ids in labels for BC\n-        if labels is not None and self.pad_token_id in labels:\n-            logger.warning_once(\n-                \"`labels` contains `pad_token_id` which will be masked with `config.ignore_index`. \"\n-                \"You have to mask out `pad_token_id` when preparing `labels`, this behavior will be removed in v.4.46.\",\n-            )\n-            labels = torch.where(input_ids == self.pad_token_id, self.config.ignore_index, labels)\n-\n         causal_mask = self._update_causal_mask(\n             attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n         )\n-        outputs: CausalLMOutputWithPast = self.language_model(\n+        outputs = self.language_model(\n             attention_mask=causal_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n@@ -534,42 +436,160 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        output = PaligemmaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"Base Paligemma model which consists of a vision backbone and a language model withou language modeling head.\"\"\",\n+    PALIGEMMA_START_DOCSTRING,\n+)\n+class PaliGemmaForConditionalGeneration(PaliGemmaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: PaliGemmaConfig):\n+        super().__init__(config)\n+        self.model = PaliGemmaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(PALIGEMMA_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=PaliGemmaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[List[torch.FloatTensor], Cache]] = None,\n+        token_type_ids: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, PaliGemmaCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, PaliGemmaForConditionalGeneration\n+\n+        >>> model = PaliGemmaForConditionalGeneration.from_pretrained(\"google/paligemma2-3b-mix-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/paligemma2-3b-mix-224\")\n+\n+        >>> prompt = \"Where is the cat standing?\"\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, text=prompt,  return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs,)\n+        >>> processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Where is the cat standing?\\nsnow\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            token_type_ids=token_type_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            labels=labels,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            shift_logits = logits[..., :-1, :]\n-            shift_labels = labels[..., 1:]\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -shift_logits.shape[1] :].to(logits.device)\n-                shift_logits = shift_logits[shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = shift_labels[shift_attention_mask.to(shift_labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = shift_logits.contiguous()\n-                shift_labels = shift_labels.contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-\n-            flat_logits = shift_logits.view(-1, self.config.text_config.vocab_size)\n-            flat_labels = shift_labels.view(-1).to(shift_logits.device)\n-            loss = loss_fct(flat_logits, flat_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n-        output = PaliGemmaCausalLMOutputWithPast(\n+        return PaliGemmaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n-        return output if return_dict else output.to_tuple()\n \n     def prepare_inputs_for_generation(\n         self,\n@@ -587,7 +607,7 @@ def prepare_inputs_for_generation(\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -610,12 +630,68 @@ def prepare_inputs_for_generation(\n         is_training = token_type_ids is not None and labels is not None\n         if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n             input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self._update_causal_mask(\n+            causal_mask = self.model._update_causal_mask(\n                 attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n             )\n             model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"PaliGemmaForConditionalGeneration\", \"PaliGemmaPreTrainedModel\"]\n+__all__ = [\"PaliGemmaForConditionalGeneration\", \"PaliGemmaPreTrainedModel\", \"PaliGemmaModel\"]"
        },
        {
            "sha": "b1b875abcd1ed532da7411148106a43b3915388c",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -2056,7 +2056,7 @@ def _update_causal_mask(\n                 if is_padding_right:\n                     raise ValueError(\n                         \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen25OmniThinkerText. Make sure to \"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen25OmniThinker. Make sure to \"\n                         \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n                     )\n             if attention_mask is not None and 0.0 in attention_mask:\n@@ -2156,7 +2156,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n-            config (`Qwen25OmniThinkerTextConfig`):\n+            config (`Qwen25OmniThinkerConfig`):\n                 The model's configuration class\n             past_key_values (`Cache`):\n                 The cache class that is being used currently to generate\n@@ -2772,7 +2772,7 @@ def _update_causal_mask(\n                 if is_padding_right:\n                     raise ValueError(\n                         \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen25OmniTalker. Make sure to \"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2_5Omni. Make sure to \"\n                         \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n                     )\n             if attention_mask is not None and 0.0 in attention_mask:\n@@ -2872,7 +2872,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n                 Indices depicting the position of the input sequence tokens in the sequence.\n             batch_size (`torch.Tensor`):\n                 Batch size.\n-            config (`Qwen25OmniTalkerConfig`):\n+            config (`Qwen2_5OmniConfig`):\n                 The model's configuration class\n             past_key_values (`Cache`):\n                 The cache class that is being used currently to generate"
        },
        {
            "sha": "5a51e787577111be445261ee82c8cf7da9dce569",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -33,8 +33,8 @@\n     Qwen2_5_VisionTransformerPretrainedModel,\n     Qwen2_5_VLAttention,\n     Qwen2_5_VLMLP,\n-    Qwen2_5_VLModel,\n     Qwen2_5_VLPreTrainedModel,\n+    Qwen2_5_VLTextModel,\n     Qwen2_5_VLVisionBlock,\n     Qwen2RMSNorm,\n )\n@@ -2165,7 +2165,7 @@ class Qwen2MLP(Qwen2_5_VLMLP):\n     \"The bare Qwen2.5OmniThinker Model outputting raw hidden-states without any specific head on top.\",\n     QWEN2_5OMNI_START_DOCSTRING.format(config_class=\"Qwen2_5OmniTextConfig\"),\n )\n-class Qwen2_5OmniThinkerTextModel(Qwen2_5_VLModel):\n+class Qwen2_5OmniThinkerTextModel(Qwen2_5_VLTextModel):\n     config_class = Qwen2_5OmniTextConfig\n     _no_split_modules = [\"Qwen2_5OmniDecoderLayer\"]\n \n@@ -2591,7 +2591,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n     \"The bare Qwen2.5OmniTalker Model outputting raw hidden-states without any specific head on top.\",\n     QWEN2_5OMNI_START_DOCSTRING.format(config_class=\"Qwen2_5OmniTalkerConfig\"),\n )\n-class Qwen2_5OmniTalkerModel(Qwen2_5_VLModel):\n+class Qwen2_5OmniTalkerModel(Qwen2_5_VLTextModel):\n     config_class = Qwen2_5OmniTalkerConfig\n     _no_split_modules = [\"Qwen2_5OmniTalkerDecoderLayer\"]\n "
        },
        {
            "sha": "f87260c69b7e8bf5b92a87039b82d5e1904ee66b",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 281,
            "deletions": 121,
            "changes": 402,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -31,7 +31,6 @@\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -44,6 +43,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n@@ -565,6 +565,42 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         return hidden_states\n \n \n+@dataclass\n+class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n class Qwen2_5_VLRotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2_5_VLTextConfig, device=None):\n         super().__init__()\n@@ -1076,7 +1112,7 @@ def forward(\n     \"The bare Qwen2_5_VL Model outputting raw hidden-states without any specific head on top.\",\n     Qwen2_5_VL_START_DOCSTRING,\n )\n-class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n+class Qwen2_5_VLTextModel(Qwen2_5_VLPreTrainedModel):\n     config_class = Qwen2_5_VLTextConfig\n \n     def __init__(self, config: Qwen2_5_VLTextConfig):\n@@ -1374,45 +1410,6 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-@dataclass\n-class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n-    \"\"\"\n-    Base class for Qwen2_5_VL causal language model (or autoregressive) outputs.\n-\n-    Args:\n-        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n-            Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n-            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n-            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n-            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n-\n-            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n-            `past_key_values` input) to speed up sequential decoding.\n-        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n-            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n-            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n-\n-            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n-        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n-            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-            sequence_length)`.\n-\n-            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n-            heads.\n-        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n-            The rope index difference between sequence length and multimodal rope.\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[List[torch.FloatTensor]] = None\n-    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n-    attentions: Optional[Tuple[torch.FloatTensor]] = None\n-    rope_deltas: Optional[torch.LongTensor] = None\n-\n-\n QWEN2_5_VL_INPUTS_DOCSTRING = r\"\"\"\n     Args:\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n@@ -1489,41 +1486,25 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n \"\"\"\n \n \n-class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+class Qwen2_5_VLModel(Qwen2_5_VLPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n     config_class = Qwen2_5_VLConfig\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n     def __init__(self, config):\n         super().__init__(config)\n         self.visual = Qwen2_5_VisionTransformerPretrainedModel._from_config(config.vision_config)\n-\n-        text_config = config.get_text_config()\n-        self.model = Qwen2_5_VLModel._from_config(text_config)\n-        self.vocab_size = text_config.vocab_size\n-        self.lm_head = nn.Linear(text_config.hidden_size, text_config.vocab_size, bias=False)\n+        self.language_model = Qwen2_5_VLTextModel._from_config(config.text_config)\n         self.rope_deltas = None  # cache rope_deltas here\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     def get_input_embeddings(self):\n-        return self.model.embed_tokens\n+        return self.language_model.get_input_embeddings()\n \n     def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n+        self.language_model.set_input_embeddings(value)\n \n     def get_rope_index(\n         self,\n@@ -1708,15 +1689,13 @@ def get_rope_index(\n             return position_ids, mrope_position_deltas\n \n     @add_start_docstrings_to_model_forward(QWEN2_5_VL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Qwen2_5_VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1728,54 +1707,15 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n-\n-        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n-        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n-\n-        >>> messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n-\n+    ) -> Union[Tuple, Qwen2_5_VLModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.model.embed_tokens(input_ids)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.visual.dtype)\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n@@ -1846,7 +1786,7 @@ def forward(\n                 position_ids = position_ids.add(delta)\n                 position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n \n-        outputs = self.model(\n+        outputs = self.language_model(\n             input_ids=None,\n             position_ids=position_ids,\n             attention_mask=attention_mask,\n@@ -1855,6 +1795,182 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = Qwen2_5_VLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@dataclass\n+class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Qwen2_5_VL causal language model (or autoregressive) outputs.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Language modeling loss (for next-token prediction).\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+            Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n+class Qwen2_5_VLForConditionalGeneration(Qwen2_5_VLPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^visual\": \"model.visual\",\n+        r\"^model(?!\\.(language_model|visual))\": \"model.language_model\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen2_5_VLModel(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def visual(self):\n+        return self.model.visual\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(QWEN2_5_VL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Qwen2_5_VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        rope_deltas: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        second_per_grid_ts: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n+\n+        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            second_per_grid_ts=second_per_grid_ts,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n@@ -1864,18 +1980,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1887,7 +1992,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            rope_deltas=self.rope_deltas,\n+            rope_deltas=outputs.rope_deltas,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -2055,5 +2160,60 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"Qwen2_5_VLForConditionalGeneration\", \"Qwen2_5_VLModel\", \"Qwen2_5_VLPreTrainedModel\"]\n+__all__ = [\"Qwen2_5_VLForConditionalGeneration\", \"Qwen2_5_VLModel\", \"Qwen2_5_VLPreTrainedModel\", \"Qwen2_5_VLTextModel\"]"
        },
        {
            "sha": "2d220aa2a90db3d42651c7e56032fe5849d752e4",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 123,
            "deletions": 65,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -26,7 +26,6 @@\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.utils.checkpoint\n-from torch.nn import CrossEntropyLoss\n \n from transformers.models.qwen2_vl.configuration_qwen2_vl import Qwen2VLConfig, Qwen2VLTextConfig\n from transformers.models.qwen2_vl.modeling_qwen2_vl import (\n@@ -36,6 +35,7 @@\n     Qwen2VLCausalLMOutputWithPast,\n     Qwen2VLForConditionalGeneration,\n     Qwen2VLModel,\n+    Qwen2VLModelOutputWithPast,\n     Qwen2VLPreTrainedModel,\n     VisionAttention,\n     VisionRotaryEmbedding,\n@@ -50,7 +50,12 @@\n from ...modeling_flash_attention_utils import is_flash_attn_available\n from ...processing_utils import ProcessingKwargs, Unpack, VideosKwargs\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    logging,\n+    replace_return_docstrings,\n+)\n \n \n if is_flash_attn_available():\n@@ -59,6 +64,8 @@\n \n logger = logging.get_logger(__name__)\n \n+_CONFIG_FOR_DOC = \"Qwen2_5_VLConfig\"\n+\n \n def apply_rotary_pos_emb_flashatt(\n     q: torch.Tensor, k: torch.Tensor, cos: torch.Tensor, sin: torch.Tensor\n@@ -406,16 +413,12 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         return hidden_states\n \n \n-class Qwen2_5_VLModel(Qwen2VLModel):\n-    pass\n-\n-\n @dataclass\n-class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast):\n+class Qwen2_5_VLModelOutputWithPast(Qwen2VLModelOutputWithPast):\n     pass\n \n \n-class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):\n+class Qwen2_5_VLModel(Qwen2VLModel):\n     config_class = Qwen2_5_VLConfig\n     _no_split_modules = [\"Qwen2_5_VLDecoderLayer\", \"Qwen2_5_VLVisionBlock\"]\n \n@@ -607,12 +610,11 @@ def get_rope_index(\n \n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -624,54 +626,15 @@ def forward(\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         second_per_grid_ts: Optional[torch.Tensor] = None,\n-    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n-\n-        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n-        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n-\n-        >>> messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n-\n+    ) -> Union[Tuple, Qwen2_5_VLModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.model.embed_tokens(input_ids)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.visual.dtype)\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n@@ -742,7 +705,7 @@ def forward(\n                 position_ids = position_ids.add(delta)\n                 position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n \n-        outputs = self.model(\n+        outputs = self.language_model(\n             input_ids=None,\n             position_ids=position_ids,\n             attention_mask=attention_mask,\n@@ -751,6 +714,111 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = Qwen2_5_VLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@dataclass\n+class Qwen2_5_VLCausalLMOutputWithPast(Qwen2VLCausalLMOutputWithPast):\n+    pass\n+\n+\n+QWEN2_5_VL_INPUTS_DOCSTRING = None\n+\n+\n+class Qwen2_5_VLForConditionalGeneration(Qwen2VLForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(QWEN2_5_VL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Qwen2_5_VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        rope_deltas: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        second_per_grid_ts: Optional[torch.Tensor] = None,\n+    ) -> Union[Tuple, Qwen2_5_VLCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Qwen2_5_VLForConditionalGeneration\n+\n+        >>> model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-VL-7B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            second_per_grid_ts=second_per_grid_ts,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n@@ -760,18 +828,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -783,7 +840,7 @@ def forward(\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            rope_deltas=self.rope_deltas,\n+            rope_deltas=outputs.rope_deltas,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -985,4 +1042,5 @@ def __call__(\n     \"Qwen2_5_VLModel\",\n     \"Qwen2_5_VLPreTrainedModel\",\n     \"Qwen2_5_VLProcessor\",\n+    \"Qwen2_5_VLTextModel\",  # noqa: F822\n ]"
        },
        {
            "sha": "03700451ce8f070db265d2e0ca6eee27bb93b6b2",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -799,27 +799,21 @@ def padding_side(self, padding_side: str):\n             raise ValueError(f\"{padding_side} is not `left` or `right`.\")\n         self._padding_side = padding_side\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.language_model.get_input_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.language_model.get_output_embeddings()\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.language_model.set_output_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.set_decoder\n     def set_decoder(self, decoder):\n         self.language_model.set_decoder(decoder)\n \n-    # Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration.get_decoder\n     def get_decoder(self):\n         return self.language_model.get_decoder()\n "
        },
        {
            "sha": "2e46c8927a74a0809ef0ba82d47e958cc63151dc",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 251,
            "deletions": 91,
            "changes": 342,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -27,7 +27,7 @@\n import torch.nn as nn\n import torch.nn.functional as F\n import torch.utils.checkpoint\n-from torch.nn import CrossEntropyLoss, LayerNorm\n+from torch.nn import LayerNorm\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n@@ -40,7 +40,9 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n@@ -61,6 +63,42 @@\n _CONFIG_FOR_DOC = \"Qwen2VLConfig\"\n \n \n+@dataclass\n+class Qwen2VLModelOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for Llava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        rope_deltas (`torch.LongTensor` of shape `(batch_size, )`, *optional*):\n+            The rope index difference between sequence length and multimodal rope.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    rope_deltas: Optional[torch.LongTensor] = None\n+\n+\n @dataclass\n class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -1027,7 +1065,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n     \"The bare Qwen2VL Model outputting raw hidden-states without any specific head on top.\",\n     QWEN2VL_START_DOCSTRING,\n )\n-class Qwen2VLModel(Qwen2VLPreTrainedModel):\n+class Qwen2VLTextModel(Qwen2VLPreTrainedModel):\n     config_class = Qwen2VLTextConfig\n \n     def __init__(self, config: Qwen2VLTextConfig):\n@@ -1403,39 +1441,23 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n \"\"\"\n \n \n-class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n-    _tied_weights_keys = [\"lm_head.weight\"]\n+class Qwen2VLModel(Qwen2VLPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"^model\": \"language_model\"}\n \n-    def __init__(self, config):\n+    def __init__(self, config: Qwen2VLConfig):\n         super().__init__(config)\n         self.visual = Qwen2VisionTransformerPretrainedModel._from_config(config.vision_config)\n-\n-        text_config = config.get_text_config()\n-        self.model = Qwen2VLModel._from_config(text_config)\n-        self.vocab_size = text_config.vocab_size\n-        self.lm_head = nn.Linear(text_config.hidden_size, text_config.vocab_size, bias=False)\n+        self.language_model = Qwen2VLTextModel._from_config(config.text_config)\n         self.rope_deltas = None  # cache rope_deltas here\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n     def get_input_embeddings(self):\n-        return self.model.embed_tokens\n+        return self.language_model.get_input_embeddings()\n \n     def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def set_decoder(self, decoder):\n-        self.model = decoder\n-\n-    def get_decoder(self):\n-        return self.model\n+        self.language_model.set_input_embeddings(value)\n \n     def get_rope_index(\n         self,\n@@ -1587,15 +1609,13 @@ def get_rope_index(\n             return position_ids, mrope_position_deltas\n \n     @add_start_docstrings_to_model_forward(QWEN2_VL_INPUTS_DOCSTRING)\n-    @replace_return_docstrings(output_type=Qwen2VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n+        input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n-        labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1606,60 +1626,21 @@ def forward(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n-        r\"\"\"\n-            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n-\n-        Returns:\n-\n-        Example:\n-\n-        ```python\n-        >>> from PIL import Image\n-        >>> import requests\n-        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n-\n-        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n-        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n-\n-        >>> messages = [\n-            {\n-                \"role\": \"user\",\n-                \"content\": [\n-                    {\"type\": \"image\"},\n-                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n-                ],\n-            },\n-        ]\n-        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-\n-        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n-\n-        >>> # Generate\n-        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n-        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n-        ```\"\"\"\n-\n+    ) -> Union[Tuple, Qwen2VLModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if inputs_embeds is None:\n-            inputs_embeds = self.model.embed_tokens(input_ids)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n             if pixel_values is not None:\n                 pixel_values = pixel_values.type(self.visual.get_dtype())\n                 image_embeds = self.visual(pixel_values, grid_thw=image_grid_thw)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_embeds.shape[0]\n-                if n_image_tokens != n_image_features:\n+                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                     raise ValueError(\n                         f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                     )\n@@ -1675,9 +1656,9 @@ def forward(\n             if pixel_values_videos is not None:\n                 pixel_values_videos = pixel_values_videos.type(self.visual.get_dtype())\n                 video_embeds = self.visual(pixel_values_videos, grid_thw=video_grid_thw)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_embeds.shape[0]\n-                if n_video_tokens != n_video_features:\n+                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                     raise ValueError(\n                         f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                     )\n@@ -1690,6 +1671,9 @@ def forward(\n                 video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n+            if attention_mask is not None:\n+                attention_mask = attention_mask.to(inputs_embeds.device)\n+\n         # if we get 4D attention mask we cannot calculate rope deltas anymore. TODO @raushan fixme\n         if position_ids is None and (attention_mask is None or attention_mask.ndim == 2):\n             # calculate RoPE index once per generation in the pre-fill stage only\n@@ -1714,7 +1698,7 @@ def forward(\n                 position_ids = position_ids.add(delta)\n                 position_ids = position_ids.unsqueeze(0).expand(3, -1, -1)\n \n-        outputs = self.model(\n+        outputs = self.language_model(\n             input_ids=None,\n             position_ids=position_ids,\n             attention_mask=attention_mask,\n@@ -1723,6 +1707,141 @@ def forward(\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+        )\n+\n+        output = Qwen2VLModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            rope_deltas=self.rope_deltas,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class Qwen2VLForConditionalGeneration(Qwen2VLPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^visual\": \"model.visual\",\n+        r\"^model(?!\\.(language_model|visual))\": \"model.language_model\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Qwen2VLModel(config)\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self):\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    def set_decoder(self, decoder):\n+        self.model = decoder\n+\n+    def get_decoder(self):\n+        return self.model\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def visual(self):\n+        return self.model.visual\n+\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(QWEN2_VL_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Qwen2VLCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        image_grid_thw: Optional[torch.LongTensor] = None,\n+        video_grid_thw: Optional[torch.LongTensor] = None,\n+        rope_deltas: Optional[torch.LongTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+    ) -> Union[Tuple, Qwen2VLCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Qwen2VLForConditionalGeneration\n+\n+        >>> model = Qwen2VLForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n+        >>> processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\")\n+\n+        >>> messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+                ],\n+            },\n+        ]\n+        >>> url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+        >>> inputs = processor(text=[text], images=[image], vision_infos=[vision_infos])\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"The image shows a street scene with a red stop sign in the foreground. In the background, there is a large red gate with Chinese characters ...\"\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            pixel_values_videos=pixel_values_videos,\n+            image_grid_thw=image_grid_thw,\n+            video_grid_thw=video_grid_thw,\n+            position_ids=position_ids,\n+            attention_mask=attention_mask,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n         )\n@@ -1732,30 +1851,15 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            # Upcast to float if we need to compute the loss to avoid potential precision issues\n-            logits = logits.float()\n-            # Shift so that tokens < n predict n\n-            shift_logits = logits[..., :-1, :].contiguous()\n-            shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = CrossEntropyLoss()\n-            shift_logits = shift_logits.view(-1, self.config.vocab_size)\n-            shift_labels = shift_labels.view(-1)\n-            # Enable model parallelism\n-            shift_labels = shift_labels.to(shift_logits.device)\n-            loss = loss_fct(shift_logits, shift_labels)\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size)\n \n         return Qwen2VLCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            rope_deltas=self.rope_deltas,\n+            rope_deltas=outputs.rope_deltas,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -1921,5 +2025,61 @@ def _expand_dict_for_generation(dict_to_expand):\n \n         return input_ids, model_kwargs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"Qwen2VLForConditionalGeneration\", \"Qwen2VLModel\", \"Qwen2VLPreTrainedModel\"]\n+__all__ = [\"Qwen2VLForConditionalGeneration\", \"Qwen2VLModel\", \"Qwen2VLPreTrainedModel\", \"Qwen2VLTextModel\"]"
        },
        {
            "sha": "d40dd4db8877beeacbbec50becb13cb705945a61",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 276,
            "deletions": 95,
            "changes": 371,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -28,11 +28,12 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_video_llava import VideoLlavaConfig\n \n \n@@ -41,6 +42,47 @@\n _CONFIG_FOR_DOC = \"VideoLlavaConfig\"\n \n \n+@dataclass\n+class VideoLlavaModelOutputWithPast(ModelOutput):\n+    \"\"\"\n+    Base class for VideoLlava base model outputs.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+        video_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor`  of size `(batch_size * num_frames, num_videos, sequence_length, hidden_size)`.\n+            video_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    last_hidden_state: torch.FloatTensor = None\n+    past_key_values: Optional[List[torch.FloatTensor]] = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor]] = None\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+    video_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n @dataclass\n class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n@@ -130,7 +172,7 @@ def forward(self, image_features):\n )\n class VideoLlavaPreTrainedModel(PreTrainedModel):\n     config_class = VideoLlavaConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VideoLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n@@ -242,21 +284,20 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The VideoLlava model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The VideoLlava model which consists of a vision backbone and a language model without language modeling head.\"\"\",\n     VIDEO_LLAVA_START_DOCSTRING,\n )\n-class VideoLlavaForConditionalGeneration(VideoLlavaPreTrainedModel, GenerationMixin):\n+class VideoLlavaModel(VideoLlavaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: VideoLlavaConfig):\n         super().__init__(config)\n         self.video_tower = AutoModel.from_config(config.vision_config)\n         self.image_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = VideoLlavaMultiModalProjector(config)\n         self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n         self.post_init()\n \n@@ -266,18 +307,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n     def get_image_features(\n         self,\n         pixel_values_images: torch.FloatTensor,\n@@ -358,13 +387,165 @@ def get_video_features(\n \n         return video_features, num_frames\n \n+    @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values_images: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layer: Optional[Union[int, List[int]]] = None,\n+        vision_feature_select_strategy: Optional[str] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, VideoLlavaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layer = (\n+            vision_feature_layer if vision_feature_layer is not None else self.config.vision_feature_layer\n+        )\n+        vision_feature_select_strategy = (\n+            vision_feature_select_strategy\n+            if vision_feature_select_strategy is not None\n+            else self.config.vision_feature_select_strategy\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if (pixel_values_images is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both `pixel_values_images`/`pixel_values_videos` and `inputs_embeds` at the same \"\n+                \"time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values_images is not None:\n+            image_features = self.get_image_features(\n+                pixel_values_images,\n+                vision_feature_layer=vision_feature_layer,\n+                vision_feature_select_strategy=vision_feature_select_strategy,\n+            )\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        if pixel_values_videos is not None:\n+            video_features, num_frames = self.get_video_features(\n+                pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n+            )\n+\n+            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n+                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n+                n_video_features = video_features.shape[0] * video_features.shape[1]\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = VideoLlavaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values_images is not None else None,\n+            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VideoLlava model which consists of a vision backbone and a language model.\"\"\",\n+    VIDEO_LLAVA_START_DOCSTRING,\n+)\n+class VideoLlavaForConditionalGeneration(VideoLlavaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^image_tower\": \"model.image_tower\",\n+        \"^video_tower\": \"model.video_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: VideoLlavaConfig):\n+        super().__init__(config)\n+        self.model = VideoLlavaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def video_tower(self):\n+        return self.model.video_tower\n+\n+    @property\n+    def image_tower(self):\n+        return self.model.image_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(VIDEO_LLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VideoLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values_images: Optional[torch.FloatTensor] = None,\n-        pixel_values_videos: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values_images: torch.FloatTensor = None,\n+        pixel_values_videos: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -475,97 +656,41 @@ def forward(\n             else self.config.vision_feature_select_strategy\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if (pixel_values_images is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values_images`/`pixel_values_videos` and `inputs_embeds` at the same \"\n-                \"time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values_images is not None:\n-            image_features = self.get_image_features(\n-                pixel_values_images,\n-                vision_feature_layer=vision_feature_layer,\n-                vision_feature_select_strategy=vision_feature_select_strategy,\n-            )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        if pixel_values_videos is not None:\n-            video_features, num_frames = self.get_video_features(\n-                pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n-            )\n-\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n-                n_video_features = video_features.shape[0] * video_features.shape[1]\n-                raise ValueError(\n-                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                )\n-            video_features = video_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, video_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values_images=pixel_values_images,\n+            pixel_values_videos=pixel_values_videos,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            vision_feature_layer=vision_feature_layer,\n+            vision_feature_select_strategy=vision_feature_select_strategy,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                # we use the input attention mask to shift the logits and labels, because it is 2D.\n-                # we also crop attn mask in case it is longer, which happens in PrefixTuning with peft\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return VideoLlavaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values_images is not None else None,\n-            video_hidden_states=video_features if pixel_values_videos is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n+            video_hidden_states=outputs.video_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -582,7 +707,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -600,5 +725,61 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"VideoLlavaPreTrainedModel\", \"VideoLlavaForConditionalGeneration\"]\n+__all__ = [\"VideoLlavaPreTrainedModel\", \"VideoLlavaModel\", \"VideoLlavaForConditionalGeneration\"]"
        },
        {
            "sha": "4916959332065795232480e1d44967d717b69718",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 252,
            "deletions": 91,
            "changes": 343,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/vipllava/modular_vipllava.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_vipllava.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2023 the HuggingFace Inc. team. All rights reserved.\n #\n@@ -12,37 +18,65 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\"\"\"PyTorch VipLlava model.\"\"\"\n \n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...generation import GenerationMixin\n-from ...modeling_outputs import ModelOutput\n+from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n     is_torchdynamo_compiling,\n-    logging,\n     replace_return_docstrings,\n )\n-from ..auto import AutoModel, AutoModelForCausalLM\n+from ..auto import AutoModel\n from .configuration_vipllava import VipLlavaConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"VipLlavaConfig\"\n \n \n @dataclass\n-# Copied from transformers.models.llava.modeling_llava.LlavaCausalLMOutputWithPast with Llava->VipLlava\n+class VipLlavaModelOutputWithPast(BaseModelOutputWithPast):\n+    \"\"\"\n+    Base class for VipLlava outputs, with hidden states and attentions.\n+\n+    Args:\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n+            `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n+\n+            Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n+            `past_key_values` input) to speed up sequential decoding.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        image_hidden_states (`torch.FloatTensor`, *optional*):\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n+            image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n+    \"\"\"\n+\n+    image_hidden_states: Optional[torch.FloatTensor] = None\n+\n+\n+@dataclass\n class VipLlavaCausalLMOutputWithPast(ModelOutput):\n     \"\"\"\n     Base class for VipLlava causal language model (or autoregressive) outputs.\n@@ -70,7 +104,7 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n             Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n             heads.\n         image_hidden_states (`torch.FloatTensor`, *optional*):\n-            A `torch.FloatTensor` of size (batch_size, num_images, sequence_length, hidden_size)`.\n+            A `torch.FloatTensor` of size `(batch_size, num_images, sequence_length, hidden_size)`.\n             image_hidden_states of the model produced by the vision encoder and after projecting the last hidden state.\n     \"\"\"\n \n@@ -129,9 +163,8 @@ def forward(self, hidden_states):\n )\n class VipLlavaPreTrainedModel(PreTrainedModel):\n     config_class = VipLlavaConfig\n-    base_model_prefix = \"model\"\n+    base_model_prefix = \"\"\n     supports_gradient_checkpointing = True\n-    _no_split_modules = [\"VipLlavaVisionAttention\"]\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_cache_class = True\n     _supports_flash_attn_2 = True\n@@ -140,15 +173,18 @@ class VipLlavaPreTrainedModel(PreTrainedModel):\n     _supports_static_cache = True\n \n     def _init_weights(self, module):\n+        # important: this ported version of VipLlava isn't meant for training from scratch - only\n+        # inference and fine-tuning - so the proper init weights code has been removed - the original codebase\n+        # https://github.com/haotian-liu/VipLlava/tree/main/vipllava should serve for that purpose\n         std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n         if isinstance(module, nn.Linear):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n \n \n VIPLLAVA_INPUTS_DOCSTRING = r\"\"\"\n@@ -163,7 +199,7 @@ def _init_weights(self, module):\n             [What are input IDs?](../glossary#input-ids)\n         pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, image_size, image_size)):\n             The tensors corresponding to the input images. Pixel values can be obtained using\n-            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`LlavaProcessor`] uses\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details ([]`VipLlavaProcessor`] uses\n             [`CLIPImageProcessor`] for processing images).\n         attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n@@ -203,6 +239,13 @@ def _init_weights(self, module):\n             Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation. This\n             is useful if you want more control over how to convert `input_ids` indices into associated vectors than the\n             model's internal embedding lookup matrix.\n+        vision_feature_layer (`Union[int, List[int]], *optional*, defaults to -2`):\n+            The index of the layer to select the vision feature. If multiple indices are provided,\n+            the vision feature of the corresponding indices will be concatenated to form the\n+            vision features.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Can be one of `\"default\"` or `\"full\"`.\n         use_cache (`bool`, *optional*):\n             If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding (see\n             `past_key_values`).\n@@ -222,24 +265,18 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"\"\"The VIPLLAVA model which consists of a vision backbone and a language model.\"\"\",\n+    \"\"\"The VipLlava model which consists of a vision backbone and a language model, without a language modeling head.\"\"\",\n     VIPLLAVA_START_DOCSTRING,\n )\n-# Copied from transformers.models.llava.modeling_llava.LlavaForConditionalGeneration with LLAVA->VIPLLAVA,Llava->VipLlava\n-class VipLlavaForConditionalGeneration(VipLlavaPreTrainedModel, GenerationMixin):\n+class VipLlavaModel(VipLlavaPreTrainedModel):\n+    _checkpoint_conversion_mapping = {\"language_model.model\": \"language_model\"}\n+\n     def __init__(self, config: VipLlavaConfig):\n         super().__init__(config)\n         self.vision_tower = AutoModel.from_config(config.vision_config)\n \n         self.multi_modal_projector = VipLlavaMultiModalProjector(config)\n-        self.vocab_size = config.text_config.vocab_size\n-        self.language_model = AutoModelForCausalLM.from_config(config.text_config)\n-\n-        if self.language_model._tied_weights_keys is not None:\n-            self._tied_weights_keys = [f\"language_model.{k}\" for k in self.language_model._tied_weights_keys]\n-\n-        self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n-\n+        self.language_model = AutoModel.from_config(config.text_config)\n         self.post_init()\n \n     def get_input_embeddings(self):\n@@ -248,19 +285,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.language_model.get_output_embeddings()\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.language_model.set_output_embeddings(new_embeddings)\n-\n-    def set_decoder(self, decoder):\n-        self.language_model.set_decoder(decoder)\n-\n-    def get_decoder(self):\n-        return self.language_model.get_decoder()\n-\n-    # Ignore copy\n     def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: Union[int, List[int]]):\n         \"\"\"\n         Obtains image last hidden states from the vision tower and apply multimodal projection.\n@@ -287,13 +311,133 @@ def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_lay\n         image_features = self.multi_modal_projector(image_features)\n         return image_features\n \n+    @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layers: Optional[Union[int, List[int]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, VipLlavaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layers = (\n+            vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = VipLlavaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The VIPLLAVA model which consists of a vision backbone and a language model.\"\"\",\n+    VIPLLAVA_START_DOCSTRING,\n+)\n+class VipLlavaForConditionalGeneration(VipLlavaPreTrainedModel, GenerationMixin):\n+    _checkpoint_conversion_mapping = {\n+        \"^language_model.model\": \"model.language_model\",\n+        \"^vision_tower\": \"model.vision_tower\",\n+        \"^multi_modal_projector\": \"model.multi_modal_projector\",\n+        \"^language_model.lm_head\": \"lm_head\",\n+    }\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+\n+    def __init__(self, config: VipLlavaConfig):\n+        super().__init__(config)\n+        self.model = VipLlavaModel(config)\n+        self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return self.model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.model.set_input_embeddings(value)\n+\n+    def get_output_embeddings(self) -> nn.Module:\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        self.lm_head = new_embeddings\n+\n+    # Make modules available throught conditional class for BC\n+    @property\n+    def language_model(self):\n+        return self.model.language_model\n+\n+    @property\n+    def vision_tower(self):\n+        return self.model.vision_tower\n+\n+    @property\n+    def multi_modal_projector(self):\n+        return self.model.multi_modal_projector\n+\n+    @can_return_tuple\n     @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=VipLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n-    # Ignore copy\n     def forward(\n         self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_values: Optional[List[torch.FloatTensor]] = None,\n@@ -358,76 +502,38 @@ def forward(\n             vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n         )\n \n-        if (input_ids is None) ^ (inputs_embeds is not None):\n-            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n-\n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n-        if pixel_values is not None:\n-            image_features = self.get_image_features(\n-                pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n-            )\n-\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_features.shape[0] * image_features.shape[1]\n-                raise ValueError(\n-                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                )\n-            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n-\n-        outputs = self.language_model(\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n+            vision_feature_layers=vision_feature_layers,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n+            return_dict=True,\n             cache_position=cache_position,\n-            logits_to_keep=logits_to_keep,\n             **lm_kwargs,\n         )\n \n-        logits = outputs[0]\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n \n         loss = None\n         if labels is not None:\n-            # Shift so that tokens < n predict n\n-            if attention_mask is not None:\n-                shift_attention_mask = attention_mask[:, -(logits.shape[1] - 1) :].to(logits.device)\n-                shift_logits = logits[..., :-1, :][shift_attention_mask.to(logits.device) != 0].contiguous()\n-                shift_labels = labels[..., 1:][shift_attention_mask.to(labels.device) != 0].contiguous()\n-            else:\n-                shift_logits = logits[..., :-1, :].contiguous()\n-                shift_labels = labels[..., 1:].contiguous()\n-            # Flatten the tokens\n-            loss_fct = nn.CrossEntropyLoss()\n-            loss = loss_fct(\n-                shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1).to(shift_logits.device)\n-            )\n-\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return (loss,) + output if loss is not None else output\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n \n         return VipLlavaCausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n             past_key_values=outputs.past_key_values,\n             hidden_states=outputs.hidden_states,\n             attentions=outputs.attentions,\n-            image_hidden_states=image_features if pixel_values is not None else None,\n+            image_hidden_states=outputs.image_hidden_states,\n         )\n \n     def prepare_inputs_for_generation(\n@@ -443,7 +549,7 @@ def prepare_inputs_for_generation(\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        model_inputs = self.language_model.prepare_inputs_for_generation(\n+        model_inputs = super().prepare_inputs_for_generation(\n             input_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -460,5 +566,60 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n+    @staticmethod\n+    def _prepare_4d_causal_attention_mask_with_cache_position(\n+        attention_mask: torch.Tensor,\n+        sequence_length: int,\n+        target_length: int,\n+        dtype: torch.dtype,\n+        cache_position: torch.Tensor,\n+        batch_size: int,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n+        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n+\n+        Args:\n+            attention_mask (`torch.Tensor`):\n+                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n+                `(batch_size, 1, query_length, key_value_length)`.\n+            sequence_length (`int`):\n+                The sequence length being processed.\n+            target_length (`int`):\n+                The target length: when generating with static cache, the mask should be as long as the static cache,\n+                to account for the 0 padding, the part of the cache that is not filled yet.\n+            dtype (`torch.dtype`):\n+                The dtype to use for the 4D attention mask.\n+            cache_position (`torch.Tensor`):\n+                Indices depicting the position of the input sequence tokens in the sequence.\n+            batch_size (`torch.Tensor`):\n+                Batch size.\n+        \"\"\"\n+        if attention_mask is not None and attention_mask.dim() == 4:\n+            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n+            causal_mask = attention_mask\n+        else:\n+            min_dtype = torch.finfo(dtype).min\n+            causal_mask = torch.full(\n+                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+            )\n+            if sequence_length != 1:\n+                causal_mask = torch.triu(causal_mask, diagonal=1)\n+            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n+            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n+            if attention_mask is not None:\n+                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n+                mask_length = attention_mask.shape[-1]\n+                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n+                    causal_mask.device\n+                )\n+                padding_mask = padding_mask == 0\n+                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n+                    padding_mask, min_dtype\n+                )\n+\n+        return causal_mask\n+\n \n-__all__ = [\"VipLlavaForConditionalGeneration\", \"VipLlavaPreTrainedModel\"]\n+__all__ = [\"VipLlavaModel\", \"VipLlavaForConditionalGeneration\", \"VipLlavaPreTrainedModel\"]"
        },
        {
            "sha": "93fdecffec915a1a3c36c8f9ecf3b175b4108a86",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "added",
            "additions": 294,
            "deletions": 0,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -0,0 +1,294 @@\n+# coding=utf-8\n+# Copyright 2023 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+from torch import nn\n+\n+from transformers.models.llava.modeling_llava import (\n+    LlavaCausalLMOutputWithPast,\n+    LlavaForConditionalGeneration,\n+    LlavaModel,\n+    LlavaModelOutputWithPast,\n+    LlavaPreTrainedModel,\n+)\n+\n+from ...activations import ACT2FN\n+from ...utils import (\n+    add_start_docstrings_to_model_forward,\n+    can_return_tuple,\n+    is_torchdynamo_compiling,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_vipllava import VipLlavaConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+VIPLLAVA_INPUTS_DOCSTRING = None\n+_CONFIG_FOR_DOC = \"VipLlavaConfig\"\n+\n+\n+class VipLlavaModelOutputWithPast(LlavaModelOutputWithPast):\n+    pass\n+\n+\n+class VipLlavaCausalLMOutputWithPast(LlavaCausalLMOutputWithPast):\n+    pass\n+\n+\n+class VipLlavaMultiModalProjector(nn.Module):\n+    def __init__(self, config: VipLlavaConfig):\n+        super().__init__()\n+        num_feature_layers = 1 if isinstance(config.vision_feature_layers, int) else len(config.vision_feature_layers)\n+        self.projector_layernorm = nn.LayerNorm(\n+            num_feature_layers * config.vision_config.hidden_size, eps=config.projector_layernorm_eps\n+        )\n+\n+        self.linear_1 = nn.Linear(\n+            num_feature_layers * config.vision_config.hidden_size,\n+            config.text_config.hidden_size,\n+            bias=True,\n+        )\n+        self.act = ACT2FN[config.projector_hidden_act]\n+        self.linear_2 = nn.Linear(config.text_config.hidden_size, config.text_config.hidden_size, bias=True)\n+\n+    def forward(self, hidden_states):\n+        hidden_states = self.projector_layernorm(hidden_states)\n+        hidden_states = self.linear_1(hidden_states)\n+        hidden_states = self.act(hidden_states)\n+        hidden_states = self.linear_2(hidden_states)\n+        return hidden_states\n+\n+\n+class VipLlavaPreTrainedModel(LlavaPreTrainedModel):\n+    pass\n+\n+\n+class VipLlavaModel(LlavaModel):\n+    def get_image_features(self, pixel_values: torch.FloatTensor, vision_feature_layers: Union[int, List[int]]):\n+        \"\"\"\n+        Obtains image last hidden states from the vision tower and apply multimodal projection.\n+\n+        Args:\n+            pixel_values (`torch.FloatTensor]` of shape `(batch_size, channels, height, width)`)\n+               The tensors corresponding to the input images.\n+            vision_feature_layers (`Union[int, List[int]]`):\n+                The vision feature layer, or the list of indexes of the layers to select\n+                the vision feature.\n+        Returns:\n+            image_features (`torch.Tensor`): Image feature tensor of shape `(num_images, image_length, embed_dim)`).\n+        \"\"\"\n+        image_outputs = self.vision_tower(pixel_values, output_hidden_states=True)\n+\n+        # If multiple feature layers are provided (which is usually the case)\n+        # then the image features are concatenated after the CLS is removed.\n+        if isinstance(vision_feature_layers, int):\n+            image_features = image_outputs.hidden_states[vision_feature_layers][:, 1:]\n+        else:\n+            # Usually, we select the features from index 1: the layers -2, -5, -8, -11 and 6\n+            image_features = [image_outputs.hidden_states[index][:, 1:] for index in vision_feature_layers]\n+            image_features = torch.cat(image_features, dim=-1)\n+        image_features = self.multi_modal_projector(image_features)\n+        return image_features\n+\n+    @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layers: Optional[Union[int, List[int]]] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, VipLlavaModelOutputWithPast]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layers = (\n+            vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n+        )\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if pixel_values is not None and inputs_embeds is not None:\n+            raise ValueError(\n+                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n+            )\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n+        if pixel_values is not None:\n+            image_features = self.get_image_features(\n+                pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n+            )\n+\n+            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n+                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                n_image_features = image_features.shape[0] * image_features.shape[1]\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n+\n+        outputs = self.language_model(\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        output = VipLlavaModelOutputWithPast(\n+            last_hidden_state=outputs.last_hidden_state,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=image_features if pixel_values is not None else None,\n+        )\n+        return output if return_dict else output.to_tuple()\n+\n+\n+class VipLlavaForConditionalGeneration(LlavaForConditionalGeneration):\n+    @can_return_tuple\n+    @add_start_docstrings_to_model_forward(VIPLLAVA_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=VipLlavaCausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n+    # Ignore copy\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        pixel_values: torch.FloatTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        vision_feature_layers: Optional[Union[int, List[int]]] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **lm_kwargs,\n+    ) -> Union[Tuple, VipLlavaCausalLMOutputWithPast]:\n+        r\"\"\"\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            logits_to_keep (`int` or `torch.Tensor`, *optional*):\n+                If an `int`, compute logits for the last `logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+                If a `torch.Tensor`, must be 1D corresponding to the indices to keep in the sequence length dimension.\n+                This is useful when using packed tensor format (single dimension for batch and sequence length).\n+\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, VipLlavaForConditionalGeneration\n+\n+        >>> model = VipLlavaForConditionalGeneration.from_pretrained(\"llava-hf/vip-llava-7b-hf\", device_map=\"auto\", torch_dtype=torch.float16)\n+        >>> processor = AutoProcessor.from_pretrained(\"llava-hf/vip-llava-7b-hf\")\n+\n+        >>> prompt = \"A chat between a curious human and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the human's questions.###Human: <image>\\n{}###Assistant:\"\n+        >>> question = \"Can you please describe this image?\"\n+        >>> prompt = prompt.format(question)\n+        >>> url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/diffusers/compel-neg.png\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(text=text, images=image, return_tensors=\"pt\").to(0, torch.float16)\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(**inputs, max_new_tokens=20)\n+        >>> processor.decode(generate_ids[0][len(inputs[\"input_ids\"][0]):], skip_special_tokens=True)\n+        The image features a brown and white cat sitting on a green surface, with a red ball in its\n+        ```\"\"\"\n+\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+        vision_feature_layers = (\n+            vision_feature_layers if vision_feature_layers is not None else self.config.vision_feature_layers\n+        )\n+\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            pixel_values=pixel_values,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            vision_feature_layers=vision_feature_layers,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=True,\n+            cache_position=cache_position,\n+            **lm_kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.text_config.vocab_size)\n+\n+        return VipLlavaCausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            image_hidden_states=outputs.image_hidden_states,\n+        )\n+\n+\n+__all__ = [\"VipLlavaModel\", \"VipLlavaForConditionalGeneration\", \"VipLlavaPreTrainedModel\"]"
        },
        {
            "sha": "3baa74a8bdc90cf5d85908019c437aab48b45f5a",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 14,
            "deletions": 1,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -21,6 +21,7 @@\n from transformers import (\n     AriaConfig,\n     AriaForConditionalGeneration,\n+    AriaModel,\n     AriaTextConfig,\n     AutoProcessor,\n     AutoTokenizer,\n@@ -175,7 +176,7 @@ class AriaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterMi\n     Model tester for `AriaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (AriaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (AriaModel, AriaForConditionalGeneration) if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True\n@@ -281,6 +282,18 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n+    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Aria uses nn.MHA which is not compatible with offloading\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n \n @require_torch\n class AriaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "f4a464ed6212d20ac6ac5f69695cbc506350b6f3",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -46,6 +46,7 @@\n \n     from transformers import (\n         AyaVisionForConditionalGeneration,\n+        AyaVisionModel,\n     )\n \n \n@@ -158,7 +159,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class AyaVisionModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (AyaVisionForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            AyaVisionModel,\n+            AyaVisionForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     all_generative_model_classes = (AyaVisionForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "b6d93ef73f2c48c3b804e404780050296719fb79",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -46,6 +46,7 @@\n     from transformers import (\n         Emu3ForCausalLM,\n         Emu3ForConditionalGeneration,\n+        Emu3Model,\n         Emu3Processor,\n         Emu3TextModel,\n     )\n@@ -310,7 +311,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class Emu3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (Emu3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            Emu3Model,\n+            Emu3ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {}\n     test_headmasking = False\n     test_pruning = False\n@@ -395,6 +403,10 @@ def test_initialization(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n+    @unittest.skip(\"Emu3 doesn't support Flex attn yet!\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n \n @require_torch\n class Emu3IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "1be8d9fcca8e06a8fb1a1ee5f41a4783c28ac4d6",
            "filename": "tests/models/fuyu/test_modeling_fuyu.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_modeling_fuyu.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -38,7 +38,7 @@\n \n \n if is_torch_available():\n-    from transformers import FuyuForCausalLM\n+    from transformers import FuyuForCausalLM, FuyuModel\n \n \n class FuyuModelTester:\n@@ -145,7 +145,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class FuyuModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (FuyuForCausalLM,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            FuyuModel,\n+            FuyuForCausalLM,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = (\n         {\"text-generation\": FuyuForCausalLM, \"image-text-to-text\": FuyuForCausalLM} if is_torch_available() else {}\n     )"
        },
        {
            "sha": "efd6f4095cdca79972769c33b69658d9b7643003",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 4,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -50,6 +50,7 @@\n     from transformers import (\n         Gemma3ForCausalLM,\n         Gemma3ForConditionalGeneration,\n+        Gemma3Model,\n         Gemma3Processor,\n         Gemma3TextModel,\n     )\n@@ -148,9 +149,9 @@ def __init__(\n         self,\n         parent,\n         mm_tokens_per_image=2,\n-        image_token_index=1,\n-        boi_token_index=2,\n-        eoi_token_index=3,\n+        image_token_index=4,\n+        boi_token_index=5,\n+        eoi_token_index=6,\n         seq_length=25,\n         is_training=True,\n         vision_config={\n@@ -242,7 +243,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class Gemma3Vision2TextModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n-    all_model_classes = (Gemma3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            Gemma3Model,\n+            Gemma3ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     all_generative_model_classes = (Gemma3ForConditionalGeneration,) if is_torch_available() else ()\n     test_headmasking = False\n     test_pruning = False"
        },
        {
            "sha": "e4706b6db0ca11ec3248a308a062cf12ed9ac485",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -34,6 +34,7 @@\n \n     from transformers import (\n         GotOcr2ForConditionalGeneration,\n+        GotOcr2Model,\n     )\n \n \n@@ -140,7 +141,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class GotOcr2ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (GotOcr2ForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            GotOcr2Model,\n+            GotOcr2ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = (\n         {\n             \"image-to-text\": GotOcr2ForConditionalGeneration,\n@@ -228,6 +236,10 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_past_key_values_format(self):\n         pass\n \n+    @unittest.skip(reason=\"Vision backbone doesn't support FLEX yet!\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n \n @require_torch\n class GotOcr2IntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "b154a09c2bc01d23508da49da0eff250e4aa6ce5",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -54,7 +54,7 @@\n     import torch\n     from torch import nn\n \n-    from transformers import InstructBlipForConditionalGeneration, InstructBlipVisionModel\n+    from transformers import InstructBlipForConditionalGeneration, InstructBlipModel, InstructBlipVisionModel\n \n \n if is_vision_available():\n@@ -460,14 +460,20 @@ def prepare_config_and_inputs_for_common(self):\n             \"attention_mask\": attention_mask,\n             \"qformer_input_ids\": qformer_input_ids,\n             \"qformer_attention_mask\": qformer_attention_mask,\n-            \"labels\": input_ids,\n         }\n         return config, inputs_dict\n \n \n @require_torch\n class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n-    all_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            InstructBlipModel,\n+            InstructBlipForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     fx_compatible = False\n     test_head_masking = False"
        },
        {
            "sha": "9bd617b4666ac36e26287a0a1a91fa3117ecacf8",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -54,7 +54,11 @@\n     import torch\n     from torch import nn\n \n-    from transformers import InstructBlipVideoForConditionalGeneration, InstructBlipVideoVisionModel\n+    from transformers import (\n+        InstructBlipVideoForConditionalGeneration,\n+        InstructBlipVideoModel,\n+        InstructBlipVideoVisionModel,\n+    )\n \n \n class InstructBlipVideoVisionModelTester:\n@@ -477,7 +481,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"attention_mask\": attention_mask,\n             \"qformer_input_ids\": qformer_input_ids,\n             \"qformer_attention_mask\": qformer_attention_mask,\n-            \"labels\": input_ids,\n         }\n         return config, inputs_dict\n \n@@ -486,7 +489,9 @@ def prepare_config_and_inputs_for_common(self):\n class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     ModelTesterMixin, GenerationTesterMixin, unittest.TestCase\n ):\n-    all_model_classes = (InstructBlipVideoForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel) if is_torch_available() else ()\n+    )\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False"
        },
        {
            "sha": "bb85f0cba415c8c088b9b151ea57a1d329ace010",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -47,9 +47,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import (\n-        InternVLForConditionalGeneration,\n-    )\n+    from transformers import InternVLForConditionalGeneration, InternVLModel\n \n \n if is_vision_available():\n@@ -191,7 +189,7 @@ def create_and_check_model_fp16_autocast_forward(self, config, input_ids, pixel_\n \n @require_torch\n class InternVLModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (InternVLForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (InternVLForConditionalGeneration, InternVLModel) if is_torch_available() else ()\n     all_generative_model_classes = (InternVLForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {"
        },
        {
            "sha": "5d60f6248b1df7e239f871fae41d5baf287e788c",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 18,
            "deletions": 7,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava model.\"\"\"\n \n+import copy\n import unittest\n \n import requests\n@@ -23,6 +24,7 @@\n     AutoTokenizer,\n     LlavaConfig,\n     LlavaForConditionalGeneration,\n+    LlavaModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -166,7 +168,14 @@ class LlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTesterM\n     Model tester for `LlavaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (LlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            LlavaModel,\n+            LlavaForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = (\n         {\"image-to-text\": LlavaForConditionalGeneration, \"image-text-to-text\": LlavaForConditionalGeneration}\n         if is_torch_available()\n@@ -238,16 +247,17 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -281,7 +291,8 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n     @unittest.skip("
        },
        {
            "sha": "c8789f0ba3872f992157485928d68918ea0dba86",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava-NeXT model.\"\"\"\n \n+import copy\n import unittest\n \n import requests\n@@ -23,6 +24,7 @@\n     AutoProcessor,\n     LlavaNextConfig,\n     LlavaNextForConditionalGeneration,\n+    LlavaNextModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -181,7 +183,14 @@ class LlavaNextForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     Model tester for `LlavaNextForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (LlavaNextForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            LlavaNextModel,\n+            LlavaNextForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": LlavaNextForConditionalGeneration} if is_torch_available() else {}\n     test_pruning = False\n     test_head_masking = False\n@@ -265,18 +274,19 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n-            input_dict[\"image_sizes\"] = input_dict[\"image_sizes\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"image_sizes\"] = curr_input_dict[\"image_sizes\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n-            image_sizes = input_dict[\"image_sizes\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n+            image_sizes = curr_input_dict[\"image_sizes\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -324,7 +334,8 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n     @unittest.skip("
        },
        {
            "sha": "e68a1e4362e6097e72e2907b7db71c3463320c89",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 20,
            "deletions": 9,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Llava-NeXT-Video model.\"\"\"\n \n+import copy\n import unittest\n \n import numpy as np\n@@ -23,6 +24,7 @@\n     AutoProcessor,\n     LlavaNextVideoConfig,\n     LlavaNextVideoForConditionalGeneration,\n+    LlavaNextVideoModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -196,7 +198,14 @@ class LlavaNextVideoForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     Model tester for `LlavaNextVideoForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (LlavaNextVideoForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            LlavaNextVideoModel,\n+            LlavaNextVideoForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     test_pruning = False\n     test_head_masking = False\n     _is_composite = True\n@@ -281,18 +290,19 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n-            input_dict[\"image_sizes\"] = input_dict[\"image_sizes\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"image_sizes\"] = curr_input_dict[\"image_sizes\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n-            image_sizes = input_dict[\"image_sizes\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n+            image_sizes = curr_input_dict[\"image_sizes\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -340,7 +350,8 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n     @unittest.skip("
        },
        {
            "sha": "ba95c330dbdb22b9b8790a445e88047646f71044",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -24,6 +24,7 @@\n     AutoProcessor,\n     LlavaOnevisionConfig,\n     LlavaOnevisionForConditionalGeneration,\n+    LlavaOnevisionModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -182,7 +183,14 @@ class LlavaOnevisionForConditionalGenerationModelTest(ModelTesterMixin, Generati\n     Model tester for `LlavaOnevisionForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (LlavaOnevisionForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            LlavaOnevisionModel,\n+            LlavaOnevisionForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = (\n         {\"image-text-to-text\": LlavaOnevisionForConditionalGeneration} if is_torch_available() else {}\n     )\n@@ -296,7 +304,8 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n     @unittest.skip("
        },
        {
            "sha": "7c8f60fdc0a5f7ef5a91eaf5c0b4445e766d3105",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -42,6 +42,7 @@\n \n     from transformers import (\n         Mistral3ForConditionalGeneration,\n+        Mistral3Model,\n     )\n \n \n@@ -162,7 +163,14 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class Mistral3ModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (Mistral3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            Mistral3Model,\n+            Mistral3ForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     all_generative_model_classes = (Mistral3ForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = (\n         {\n@@ -278,6 +286,10 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n+    @unittest.skip(\"Pixtral does not support attention interfaces.\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n \n @slow\n @require_torch_gpu"
        },
        {
            "sha": "e67e0455e1f7cdf519dbe2b89feef52da896338b",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 34,
            "deletions": 15,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -25,6 +25,7 @@\n     MllamaConfig,\n     MllamaForCausalLM,\n     MllamaForConditionalGeneration,\n+    MllamaModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -262,7 +263,14 @@ class MllamaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTester\n     Model tester for `MllamaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (MllamaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            MllamaModel,\n+            MllamaForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": MllamaForConditionalGeneration} if is_torch_available() else ()\n     test_pruning = False\n     test_head_masking = False\n@@ -325,19 +333,18 @@ def test_resize_embeddings_results_in_successful_loss(self):\n         # resizing embeddings should result in successful loss computation\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n \n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model_vocab_size = config.get_text_config().vocab_size\n-            inputs = self._prepare_for_class(inputs, model_class, return_labels=True)\n-            # Resize embeddings and call forward\n-            model.resize_token_embeddings(model_vocab_size + 10)\n-            output = model(\n-                input_ids=inputs[\"input_ids\"],\n-                attention_mask=inputs[\"attention_mask\"],\n-                labels=inputs[\"labels\"],\n-                return_dict=True,\n-            )\n-            self.assertTrue(\"loss\" in output)\n+        model = MllamaForConditionalGeneration(config).to(torch_device)\n+        model_vocab_size = config.get_text_config().vocab_size\n+        inputs = self._prepare_for_class(inputs, MllamaForConditionalGeneration, return_labels=True)\n+        # Resize embeddings and call forward\n+        model.resize_token_embeddings(model_vocab_size + 10)\n+        output = model(\n+            input_ids=inputs[\"input_ids\"],\n+            attention_mask=inputs[\"attention_mask\"],\n+            labels=inputs[\"labels\"],\n+            return_dict=True,\n+        )\n+        self.assertTrue(\"loss\" in output)\n \n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n@@ -409,6 +416,18 @@ def test_contrastive_generate_low_memory(self, assistant_type):\n     def test_assisted_decoding_with_num_logits_to_keep(self):\n         pass\n \n+    @unittest.skip(reason=\"Mllama uses self.weights dirrectly causing device mismatch when offloading`\")\n+    def test_cpu_offload(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Mllama uses self.weights dirrectly causing device mismatch when offloading`\")\n+    def test_disk_offload_bin(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Mllama uses self.weights dirrectly causing device mismatch when offloading`\")\n+    def test_disk_offload_safetensors(self):\n+        pass\n+\n     @pytest.mark.generate\n     # overridden because mllama is not an encoder-decoder model, but has encoder-decoder-like cache\n     def test_past_key_values_format(self):\n@@ -501,7 +520,7 @@ def test_generate_text_only_with_cache(self):\n         \"\"\"\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n-        for model_class in self.all_model_classes:\n+        for model_class in self.all_generative_model_classes:\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()"
        },
        {
            "sha": "a4d323baa311f3020a7e98e4e534bcd0157bb3ba",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 16,
            "deletions": 6,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,13 +13,15 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch PaliGemma model.\"\"\"\n \n+import copy\n import unittest\n \n import requests\n \n from transformers import (\n     PaliGemmaConfig,\n     PaliGemmaForConditionalGeneration,\n+    PaliGemmaModel,\n     PaliGemmaProcessor,\n     is_torch_available,\n     is_vision_available,\n@@ -177,7 +179,14 @@ class PaliGemmaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTes\n     Model tester for `PaliGemmaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (PaliGemmaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            PaliGemmaModel,\n+            PaliGemmaForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": PaliGemmaForConditionalGeneration}\n     fx_compatible = False\n     test_pruning = False\n@@ -242,16 +251,17 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error"
        },
        {
            "sha": "bc62c527e2965be5dda19ddf56208cc3ce0e7b09",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch PaliGemma model.\"\"\"\n \n+import copy\n import unittest\n \n import pytest\n@@ -239,16 +240,17 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error"
        },
        {
            "sha": "5f06e1c84be8c91c1e75773857744d68a69d8974",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 40,
            "deletions": 13,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen2.5-VL model.\"\"\"\n \n+import copy\n import gc\n import tempfile\n import unittest\n@@ -23,6 +24,7 @@\n     AutoProcessor,\n     Qwen2_5_VLConfig,\n     Qwen2_5_VLForConditionalGeneration,\n+    Qwen2_5_VLModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -180,17 +182,11 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n         input_ids[:, self.num_image_tokens] = self.image_token_id\n         input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n-        labels = torch.zeros(\n-            (self.batch_size, self.seq_length),\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"labels\": labels,\n         }\n         return config, inputs_dict\n \n@@ -201,7 +197,14 @@ class Qwen2_5_VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.Test\n     Model tester for `Qwen2_5_VLForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (Qwen2_5_VLForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            Qwen2_5_VLModel,\n+            Qwen2_5_VLForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     test_pruning = False\n     test_head_masking = False\n \n@@ -236,19 +239,20 @@ def test_mismatching_num_image_tokens(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n             _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)\n \n             # remove one image but leave the image token in text\n             patch_size = config.vision_config.patch_size\n             one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-one_img_length:, ...]\n-            input_dict[\"image_grid_thw\"] = input_dict[\"image_grid_thw\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            curr_input_dict[\"image_grid_thw\"] = curr_input_dict[\"image_grid_thw\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:one_img_length]\n-            image_grid_thw = input_dict[\"image_grid_thw\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = curr_input_dict[\"image_grid_thw\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -375,6 +379,29 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n+    # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n+    # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n+    # TODO: @raushan\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n \n @require_torch\n class Qwen2_5_VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "57e112790cc030469dff932c83a3f98c0298c8aa",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 44,
            "deletions": 16,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen2-VL model.\"\"\"\n \n+import copy\n import gc\n import unittest\n \n@@ -22,6 +23,7 @@\n     AutoProcessor,\n     Qwen2VLConfig,\n     Qwen2VLForConditionalGeneration,\n+    Qwen2VLModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -169,17 +171,12 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids[input_ids == self.vision_start_token_id] = self.pad_token_id\n         input_ids[:, self.num_image_tokens] = self.image_token_id\n         input_ids[:, self.num_image_tokens - 1] = self.vision_start_token_id\n-        labels = torch.zeros(\n-            (self.batch_size, self.seq_length),\n-            dtype=torch.long,\n-            device=torch_device,\n-        )\n+\n         inputs_dict = {\n             \"pixel_values\": pixel_values,\n             \"image_grid_thw\": torch.tensor([[1, 1, 1]] * self.batch_size),\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"labels\": labels,\n         }\n         return config, inputs_dict\n \n@@ -190,7 +187,14 @@ class Qwen2VLModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCas\n     Model tester for `Qwen2VLForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (Qwen2VLForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            Qwen2VLModel,\n+            Qwen2VLForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": Qwen2VLForConditionalGeneration}\n     test_pruning = False\n     test_head_masking = False\n@@ -226,20 +230,21 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)\n+            _ = model(**curr_input_dict)  # successfull forward with no modifications\n \n             # remove one image but leave the image token in text\n             patch_size = config.vision_config.patch_size\n             one_img_length = (self.model_tester.image_size**2) // (patch_size**2)\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-one_img_length:, ...]\n-            input_dict[\"image_grid_thw\"] = input_dict[\"image_grid_thw\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-one_img_length:, ...]\n+            curr_input_dict[\"image_grid_thw\"] = curr_input_dict[\"image_grid_thw\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:one_img_length]\n-            image_grid_thw = input_dict[\"image_grid_thw\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:one_img_length]\n+            image_grid_thw = curr_input_dict[\"image_grid_thw\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -262,11 +267,11 @@ def test_forward_with_rope_deltas_cached(self):\n             model = model_class(config).to(torch_device)\n \n             # Generate and make sure rope_deltas are not `None`\n-            self.assertTrue(model.rope_deltas is None)\n+            self.assertTrue(model.model.rope_deltas is None)\n             generation_output = model.generate(\n                 **input_dict, max_new_tokens=4, return_dict_in_generate=True, output_logits=True\n             )\n-            self.assertTrue(model.rope_deltas is not None)\n+            self.assertTrue(model.model.rope_deltas is not None)\n \n             # Now if we try to do forward pass, we should get new rope logits, because cache is not passed\n             forward_output = model(**input_dict)\n@@ -320,6 +325,29 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     def test_save_load_fast_init_from_base(self):\n         pass\n \n+    # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n+    # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n+    # TODO: @raushan\n+    def test_inputs_embeds_matches_input_ids(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+\n+            with torch.no_grad():\n+                out_ids = model(input_ids=input_ids, **inputs)[0]\n+                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n+            torch.testing.assert_close(out_embeds, out_ids)\n+\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "92ad5a193bf5137b52c0458c134536d49a357eac",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 32,
            "deletions": 18,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch VideoLlava model.\"\"\"\n \n+import copy\n import unittest\n \n import numpy as np\n@@ -23,6 +24,7 @@\n from transformers import (\n     VideoLlavaConfig,\n     VideoLlavaForConditionalGeneration,\n+    VideoLlavaModel,\n     VideoLlavaProcessor,\n     is_torch_available,\n     is_vision_available,\n@@ -190,7 +192,14 @@ class VideoLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTe\n     Model tester for `VideoLlavaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (VideoLlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            VideoLlavaModel,\n+            VideoLlavaForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n@@ -235,46 +244,49 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n     def test_mixed_input(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n+            curr_inputs = copy.deepcopy(inputs)\n             model = model_class(config).to(torch_device).eval()\n             # test that the forward does not fail\n             with torch.no_grad():\n-                _ = model(**inputs)\n+                _ = model(**curr_inputs)\n \n             # if we remove some images from inputs leaving only one\n             # image number mismatch error should raise\n-            inputs[\"pixel_values_images\"] = inputs[\"pixel_values_images\"][:1]\n+            curr_inputs[\"pixel_values_images\"] = curr_inputs[\"pixel_values_images\"][:1]\n             with self.assertRaises(ValueError):\n-                _ = model(**inputs)\n+                _ = model(**curr_inputs)\n \n     def test_video_only_input(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n+            curr_inputs = copy.deepcopy(inputs)\n             model = model_class(config).to(torch_device).eval()\n             # replace image token id with dummy id\n             # Error will be raised as num-image-tokens and num-of-image-embeds mismatch\n-            inputs[\"input_ids\"][:, : self.model_tester.num_image_tokens] = 2\n+            curr_inputs[\"input_ids\"][:, : self.model_tester.num_image_tokens] = 2\n             with self.assertRaises(ValueError):\n-                _ = model(**inputs)\n+                _ = model(**curr_inputs)\n \n-            inputs[\"pixel_values_images\"] = None\n-            _ = model(**inputs)\n+            curr_inputs[\"pixel_values_images\"] = None\n+            _ = model(**curr_inputs)\n \n     def test_image_only_input(self):\n         config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n+            curr_inputs = copy.deepcopy(inputs)\n             model = model_class(config).to(torch_device).eval()\n             # set dummy id, which is not video token id\n             # Error will be raised as num-video-tokens and num-of-video-embeds mismatch\n-            inputs[\"input_ids\"][\n+            curr_inputs[\"input_ids\"][\n                 :,\n                 self.model_tester.num_image_tokens : self.model_tester.num_image_tokens\n                 + self.model_tester.num_video_tokens,\n             ] = 2\n             with self.assertRaises(ValueError):\n-                _ = model(**inputs)\n+                _ = model(**curr_inputs)\n \n-            inputs[\"pixel_values_videos\"] = None\n-            _ = model(**inputs)\n+            curr_inputs[\"pixel_values_videos\"] = None\n+            _ = model(**curr_inputs)\n \n     def test_batching_equivalence(self):\n         def recursive_check(batched_object, single_row_object, model_name, key):\n@@ -386,16 +398,17 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)\n+            _ = model(**curr_input_dict)  # successfull forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values_images\"] = input_dict[\"pixel_values_images\"][-1:, ...]\n+            curr_input_dict[\"pixel_values_images\"] = curr_input_dict[\"pixel_values_images\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values_images\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values_images\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -429,7 +442,8 @@ def test_vision_feature_layers(self, vision_feature_layer):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n "
        },
        {
            "sha": "79b57e12ffa930f0a0e35cb58770d2e8a8e60d82",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 22,
            "deletions": 7,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -13,6 +13,7 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch VipLlava model.\"\"\"\n \n+import copy\n import unittest\n \n import requests\n@@ -22,6 +23,7 @@\n     AutoProcessor,\n     VipLlavaConfig,\n     VipLlavaForConditionalGeneration,\n+    VipLlavaModel,\n     is_torch_available,\n     is_vision_available,\n )\n@@ -165,7 +167,14 @@ class VipLlavaForConditionalGenerationModelTest(ModelTesterMixin, GenerationTest\n     Model tester for `VipLlavaForConditionalGeneration`.\n     \"\"\"\n \n-    all_model_classes = (VipLlavaForConditionalGeneration,) if is_torch_available() else ()\n+    all_model_classes = (\n+        (\n+            VipLlavaModel,\n+            VipLlavaForConditionalGeneration,\n+        )\n+        if is_torch_available()\n+        else ()\n+    )\n     pipeline_model_mapping = {\"image-text-to-text\": VipLlavaForConditionalGeneration} if is_torch_available() else {}\n     fx_compatible = False\n     test_pruning = False\n@@ -236,16 +245,17 @@ def test_mismatching_num_image_tokens(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n             model = model_class(config).to(torch_device)\n-            _ = model(**input_dict)  # successful forward with no modifications\n+            curr_input_dict = copy.deepcopy(input_dict)  # in=place modifications further\n+            _ = model(**curr_input_dict)  # successful forward with no modifications\n \n             # remove one image but leave the image token in text\n-            input_dict[\"pixel_values\"] = input_dict[\"pixel_values\"][-1:, ...]\n+            curr_input_dict[\"pixel_values\"] = curr_input_dict[\"pixel_values\"][-1:, ...]\n             with self.assertRaises(ValueError):\n-                _ = model(**input_dict)\n+                _ = model(**curr_input_dict)\n \n             # simulate multi-image case by concatenating inputs where each has exactly one image/image-token\n-            input_ids = input_dict[\"input_ids\"][:1]\n-            pixel_values = input_dict[\"pixel_values\"][:1]\n+            input_ids = curr_input_dict[\"input_ids\"][:1]\n+            pixel_values = curr_input_dict[\"pixel_values\"][:1]\n             input_ids = torch.cat([input_ids, input_ids], dim=0)\n \n             # one image and two image tokens raise an error\n@@ -284,7 +294,8 @@ def test_vision_feature_layers(self, vision_feature_layers):\n             model = model_class(config).to(torch_device)\n             # We should have the right number of input features,\n             # and should be able to run a forward pass without exploding\n-            assert model.multi_modal_projector.linear_1.in_features == expected_features\n+            base_model = getattr(model, \"model\", model)\n+            assert base_model.multi_modal_projector.linear_1.in_features == expected_features\n             model(**input_dict)\n \n     @unittest.skip(\n@@ -311,6 +322,10 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n         pass\n \n+    @unittest.skip(\"LLaVA vision backbones doesn't support flex attention yet\")\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n \n @require_torch\n class VipLlavaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "b8a4fda96d33b505efe66d5757304de5260012c2",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -3494,8 +3494,8 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 vision_model_name = [name for name in vision_model_names if hasattr(model_sdpa, name)][0]\n                 language_model_name = [name for name in language_model_names if hasattr(model_sdpa, name)][0]\n \n-                vision_model_sdpa = getattr(model, vision_model_name)\n-                language_model_sdpa = getattr(model, language_model_name)\n+                vision_model_sdpa = getattr(model_sdpa, vision_model_name)\n+                language_model_sdpa = getattr(model_sdpa, language_model_name)\n                 text_attn = \"sdpa\" if language_model_sdpa._supports_sdpa else \"eager\"\n                 vision_attn = \"sdpa\" if vision_model_sdpa._supports_sdpa else \"eager\"\n \n@@ -4489,7 +4489,8 @@ def recursively_check(eager_outputs, exported_outputs):\n     @require_torch_gpu\n     def test_flex_attention_with_grads(self):\n         for model_class in self.all_model_classes:\n-            if not model_class._supports_flex_attn:\n+            # TODO: raushan, fix for composite models after making VLMs support new attn API\n+            if not model_class._supports_flex_attn or self._is_composite:\n                 self.skipTest(reason=\"This model does not support flex attention\")\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             config._attn_implementation = \"flex_attention\""
        },
        {
            "sha": "cea9ed693d5bfe8605e76a5568b3707620ca9d49",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -378,8 +378,8 @@ def check_attribute_being_used(config_class, attributes, default_value, source_s\n         \"rope_theta\",\n         \"partial_rotary_factor\",\n         \"pretraining_tp\",\n-        \"boi_token_index\",\n-        \"eoi_token_index\",\n+        \"boi_token_id\",\n+        \"eoi_token_id\",\n     ]\n     attributes_used_in_generation = [\"encoder_no_repeat_ngram_size\"]\n "
        },
        {
            "sha": "9eb5ccdf06a826efa200e705234ed96e0c6c6fd4",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/17742bd9c8852ab35986dcaa3e68415342ae7eef/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/17742bd9c8852ab35986dcaa3e68415342ae7eef/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=17742bd9c8852ab35986dcaa3e68415342ae7eef",
            "patch": "@@ -156,6 +156,8 @@\n         \"Llama4VisionModel\",  # Building part of bigger (tested) model. # TODO: add tests\n         \"Emu3VQVAE\",  # Building part of bigger (tested) model\n         \"Emu3TextModel\",  # Building part of bigger (tested) model\n+        \"Qwen2VLTextModel\",  # Building part of bigger (tested) model\n+        \"Qwen2_5_VLTextModel\",  # Building part of bigger (tested) model\n         \"InternVLVisionModel\",  # Building part of bigger (tested) model\n         \"JanusVisionModel\",  # Building part of bigger (tested) model\n         \"TimesFmModel\",  # Building part of bigger (tested) model"
        }
    ],
    "stats": {
        "total": 10306,
        "additions": 7496,
        "deletions": 2810
    }
}