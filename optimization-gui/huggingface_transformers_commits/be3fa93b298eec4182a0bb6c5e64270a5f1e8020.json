{
    "author": "zucchini-nlp",
    "message": "Subconfig is a class attribute (#41308)\n\n* delete\n\n* fix this test\n\n* fix copies\n\n* oke, more tests to fix\n\n* fix last tests on DPT\n\n* deleted accidentally",
    "sha": "be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
    "files": [
        {
            "sha": "d00289ebe8c5682bbd39c695a2dcbd0b7a185d15",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -880,7 +880,6 @@ def to_diff_dict(self) -> dict[str, Any]:\n                 isinstance(getattr(self, key, None), PreTrainedConfig)\n                 and key in class_config_dict\n                 and isinstance(class_config_dict[key], dict)\n-                or key in self.sub_configs\n             ):\n                 # For nested configs we need to clean the diff recursively\n                 diff = recursive_diff_dict(value, default_config_dict, config_obj=getattr(self, key, None))"
        },
        {
            "sha": "e1962f765fb14060d12750be83d3bdc44424d9d0",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 33,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -1219,13 +1219,13 @@ def _get_dtype(\n                 dtype = getattr(torch, dtype)\n                 config.dtype = dtype\n                 for sub_config_key in config.sub_configs:\n-                    sub_config = getattr(config, sub_config_key)\n-                    sub_config.dtype = dtype\n+                    if (sub_config := getattr(config, sub_config_key)) is not None:\n+                        sub_config.dtype = dtype\n         elif isinstance(dtype, torch.dtype):\n             config.dtype = dtype\n             for sub_config_key in config.sub_configs:\n-                sub_config = getattr(config, sub_config_key)\n-                sub_config.dtype = dtype\n+                if (sub_config := getattr(config, sub_config_key)) is not None:\n+                    sub_config.dtype = dtype\n         elif isinstance(dtype, dict):\n             for key, curr_dtype in dtype.items():\n                 if hasattr(config, key):\n@@ -1250,8 +1250,8 @@ def _get_dtype(\n         default_dtype = torch.get_default_dtype()\n         config.dtype = default_dtype\n         for key in config.sub_configs:\n-            value = getattr(config, key)\n-            value.dtype = default_dtype\n+            if (sub_config := getattr(config, key)) is not None:\n+                sub_config.dtype = default_dtype\n \n     return config, dtype, dtype_orig\n \n@@ -2700,34 +2700,34 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n \n         # We need this as some old and badly designed models use subconfigs without declaring the corresponding modules as PreTrainedModel\n         for subconfig_key in self.config.sub_configs:\n-            subconfig = getattr(self.config, subconfig_key)\n-            sub_implementation = (\n-                requested_implementation\n-                if not isinstance(attn_implementation, dict)\n-                else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n-            )\n-            # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n-            if (\n-                not hasattr(subconfig, \"_attn_was_changed\")\n-                # If it's already the same, then no need to enter here and raise warnings\n-                and sub_implementation != subconfig._attn_implementation\n-            ):\n-                if sub_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n-                    raise ValueError(\n-                        f'Specified `attn_implementation=\"{sub_implementation}\"` is not supported for {subconfig_key}. '\n-                        'The only possible arguments are \"eager\" (manual attention implementation)'\n-                        f\"or one of the following: {list(ALL_ATTENTION_FUNCTIONS.valid_keys())}\"\n-                    )\n-                subconfig._attn_implementation_internal = sub_implementation\n-                logger.warning(\n-                    f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{sub_implementation}` \"\n-                    \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n-                    \"You may encounter undefined behavior.\"\n+            if (subconfig := getattr(self.config, subconfig_key)) is not None:\n+                sub_implementation = (\n+                    requested_implementation\n+                    if not isinstance(attn_implementation, dict)\n+                    else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n                 )\n-            # Unset the attribute in this case, to avoid issues in the future\n-            else:\n-                if hasattr(subconfig, \"_attn_was_changed\"):\n-                    del subconfig._attn_was_changed\n+                # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n+                if (\n+                    not hasattr(subconfig, \"_attn_was_changed\")\n+                    # If it's already the same, then no need to enter here and raise warnings\n+                    and sub_implementation != subconfig._attn_implementation\n+                ):\n+                    if sub_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n+                        raise ValueError(\n+                            f'Specified `attn_implementation=\"{sub_implementation}\"` is not supported for {subconfig_key}. '\n+                            'The only possible arguments are \"eager\" (manual attention implementation)'\n+                            f\"or one of the following: {list(ALL_ATTENTION_FUNCTIONS.valid_keys())}\"\n+                        )\n+                    subconfig._attn_implementation_internal = sub_implementation\n+                    logger.warning(\n+                        f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{sub_implementation}` \"\n+                        \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n+                        \"You may encounter undefined behavior.\"\n+                    )\n+                # Unset the attribute in this case, to avoid issues in the future\n+                else:\n+                    if hasattr(subconfig, \"_attn_was_changed\"):\n+                        del subconfig._attn_was_changed\n \n     def enable_input_require_grads(self):\n         \"\"\""
        },
        {
            "sha": "36693772df7c156bf3070c4fb3e62cf2625684c7",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -23,7 +23,7 @@\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -135,6 +135,7 @@ class ConditionalDetrConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"conditional_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -245,22 +246,6 @@ def __init__(\n         self.focal_alpha = focal_alpha\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n class ConditionalDetrOnnxConfig(OnnxConfig):\n     torch_onnx_minimum_version = version.parse(\"1.11\")"
        },
        {
            "sha": "9a7464042dee0b3f68c10fbe393b9d665cfbf6c8",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -21,7 +21,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -194,6 +194,7 @@ class DFineConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"d_fine\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     layer_types = [\"basic\", \"bottleneck\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -396,22 +397,6 @@ def __init__(\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "93505d8deaaf0c3985d48678ddf83f5206c5c5df",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_transforms import corners_to_center_format\n from ...utils import is_torchdynamo_compiling, logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n from ..rt_detr.modeling_rt_detr import (\n     RTDetrConvNormLayer,\n     RTDetrDecoder,\n@@ -213,6 +213,7 @@ class DFineConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"d_fine\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     layer_types = [\"basic\", \"bottleneck\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -415,22 +416,6 @@ def __init__(\n             )\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "364128485c301d434d1bd0341242925448e45f59",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -136,6 +136,7 @@ class DabDetrConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"dab-detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\n         \"num_attention_heads\": \"encoder_attention_heads\",\n@@ -256,13 +257,5 @@ def __init__(\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"DabDetrConfig\"]"
        },
        {
            "sha": "93cee9c53969e3452f935afb74a81ab6c244b3c3",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -144,6 +144,7 @@ class DeformableDetrConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"deformable_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n         \"num_attention_heads\": \"encoder_attention_heads\",\n@@ -270,21 +271,5 @@ def __init__(\n         self.disable_custom_kernels = disable_custom_kernels\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"DeformableDetrConfig\"]"
        },
        {
            "sha": "9e263bb6406a41578656eff7ca63683d1e9239d5",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 24,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -14,12 +14,10 @@\n # limitations under the License.\n \"\"\"DepthAnything model configuration\"\"\"\n \n-import copy\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -89,6 +87,7 @@ class DepthAnythingConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"depth_anything\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -151,26 +150,5 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-\n-        if output[\"backbone_config\"] is not None:\n-            output[\"backbone_config\"] = self.backbone_config.to_dict()\n-\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output\n-\n \n __all__ = [\"DepthAnythingConfig\"]"
        },
        {
            "sha": "7c69f06318a1a6cdb0be6f450cb1173e85601951",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -23,7 +23,7 @@\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -133,6 +133,7 @@ class DetrConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -244,22 +245,6 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DetrConfig`] (or a derived class) from a pre-trained backbone model configuration."
        },
        {
            "sha": "99277ab8736828ca1b79cc5bed33cf7f0ea256d9",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 24,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -14,12 +14,10 @@\n # limitations under the License.\n \"\"\"DPT model configuration\"\"\"\n \n-import copy\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n from ..bit import BitConfig\n \n \n@@ -140,6 +138,7 @@ class DPTConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"dpt\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -275,26 +274,5 @@ def __init__(\n         self.pooler_output_size = pooler_output_size if pooler_output_size else hidden_size\n         self.pooler_act = pooler_act\n \n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-\n-        if output[\"backbone_config\"] is not None:\n-            output[\"backbone_config\"] = self.backbone_config.to_dict()\n-\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"DPTConfig\"]"
        },
        {
            "sha": "7a8577e80cd764901098e7a90308cfd4850b4424",
            "filename": "src/transformers/models/esm/configuration_esm.py",
            "status": "modified",
            "additions": 155,
            "deletions": 155,
            "changes": 310,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fconfiguration_esm.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -23,7 +23,159 @@\n \n logger = logging.get_logger(__name__)\n \n-# TODO Update this\n+\n+@dataclass\n+class StructureModuleConfig:\n+    \"\"\"\n+    Args:\n+        sequence_dim:\n+            Single representation channel dimension\n+        pairwise_dim:\n+            Pair representation channel dimension\n+        ipa_dim:\n+            IPA hidden channel dimension\n+        resnet_dim:\n+            Angle resnet (Alg. 23 lines 11-14) hidden channel dimension\n+        num_heads_ipa:\n+            Number of IPA heads\n+        num_qk_points:\n+            Number of query/key points to generate during IPA\n+        num_v_points:\n+            Number of value points to generate during IPA\n+        dropout_rate:\n+            Dropout rate used throughout the layer\n+        num_blocks:\n+            Number of structure module blocks\n+        num_transition_layers:\n+            Number of layers in the single representation transition (Alg. 23 lines 8-9)\n+        num_resnet_blocks:\n+            Number of blocks in the angle resnet\n+        num_angles:\n+            Number of angles to generate in the angle resnet\n+        trans_scale_factor:\n+            Scale of single representation transition hidden dimension\n+        epsilon:\n+            Small number used in angle resnet normalization\n+        inf:\n+            Large number used for attention masking\n+    \"\"\"\n+\n+    sequence_dim: int = 384\n+    pairwise_dim: int = 128\n+    ipa_dim: int = 16\n+    resnet_dim: int = 128\n+    num_heads_ipa: int = 12\n+    num_qk_points: int = 4\n+    num_v_points: int = 8\n+    dropout_rate: float = 0.1\n+    num_blocks: int = 8\n+    num_transition_layers: int = 1\n+    num_resnet_blocks: int = 2\n+    num_angles: int = 7\n+    trans_scale_factor: int = 10\n+    epsilon: float = 1e-8\n+    inf: float = 1e5\n+\n+    def to_dict(self):\n+        return asdict(self)\n+\n+\n+@dataclass\n+class TrunkConfig:\n+    num_blocks: int = 48\n+    sequence_state_dim: int = 1024\n+    pairwise_state_dim: int = 128\n+    sequence_head_width: int = 32\n+    pairwise_head_width: int = 32\n+    position_bins: int = 32\n+    dropout: float = 0\n+    layer_drop: float = 0\n+    cpu_grad_checkpoint: bool = False\n+    max_recycles: int = 4\n+    chunk_size: Optional[int] = 128\n+    structure_module: \"StructureModuleConfig\" = None\n+\n+    def __post_init__(self):\n+        if self.structure_module is None:\n+            self.structure_module = StructureModuleConfig()\n+        elif isinstance(self.structure_module, dict):\n+            self.structure_module = StructureModuleConfig(**self.structure_module)\n+\n+        if self.max_recycles <= 0:\n+            raise ValueError(f\"`max_recycles` should be positive, got {self.max_recycles}.\")\n+        if self.sequence_state_dim % self.sequence_state_dim != 0:\n+            raise ValueError(\n+                \"`sequence_state_dim` should be a round multiple of `sequence_state_dim`, got\"\n+                f\" {self.sequence_state_dim} and {self.sequence_state_dim}.\"\n+            )\n+        if self.pairwise_state_dim % self.pairwise_state_dim != 0:\n+            raise ValueError(\n+                \"`pairwise_state_dim` should be a round multiple of `pairwise_state_dim`, got\"\n+                f\" {self.pairwise_state_dim} and {self.pairwise_state_dim}.\"\n+            )\n+\n+        sequence_num_heads = self.sequence_state_dim // self.sequence_head_width\n+        pairwise_num_heads = self.pairwise_state_dim // self.pairwise_head_width\n+\n+        if self.sequence_state_dim != sequence_num_heads * self.sequence_head_width:\n+            raise ValueError(\n+                \"`sequence_state_dim` should be equal to `sequence_num_heads * sequence_head_width, got\"\n+                f\" {self.sequence_state_dim} != {sequence_num_heads} * {self.sequence_head_width}.\"\n+            )\n+        if self.pairwise_state_dim != pairwise_num_heads * self.pairwise_head_width:\n+            raise ValueError(\n+                \"`pairwise_state_dim` should be equal to `pairwise_num_heads * pairwise_head_width, got\"\n+                f\" {self.pairwise_state_dim} != {pairwise_num_heads} * {self.pairwise_head_width}.\"\n+            )\n+        if self.pairwise_state_dim % 2 != 0:\n+            raise ValueError(f\"`pairwise_state_dim` should be even, got {self.pairwise_state_dim}.\")\n+\n+        if self.dropout >= 0.4:\n+            raise ValueError(f\"`dropout` should not be greater than 0.4, got {self.dropout}.\")\n+\n+    def to_dict(self):\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+\n+        Returns:\n+            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n+        \"\"\"\n+        output = asdict(self)\n+        output[\"structure_module\"] = self.structure_module.to_dict()\n+        return output\n+\n+\n+@dataclass\n+class EsmFoldConfig:\n+    esm_type: Optional[str] = None\n+    fp16_esm: bool = True\n+    use_esm_attn_map: bool = False\n+    esm_ablate_pairwise: bool = False\n+    esm_ablate_sequence: bool = False\n+    esm_input_dropout: float = 0\n+\n+    embed_aa: bool = True\n+    bypass_lm: bool = False\n+\n+    lddt_head_hid_dim: int = 128\n+    trunk: \"TrunkConfig\" = None\n+\n+    def __post_init__(self):\n+        if self.trunk is None:\n+            self.trunk = TrunkConfig()\n+        elif isinstance(self.trunk, dict):\n+            self.trunk = TrunkConfig(**self.trunk)\n+\n+    def to_dict(self):\n+        \"\"\"\n+        Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`].\n+\n+        Returns:\n+            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n+        \"\"\"\n+        output = asdict(self)\n+        output[\"trunk\"] = self.trunk.to_dict()\n+        return output\n \n \n class EsmConfig(PreTrainedConfig):\n@@ -94,6 +246,7 @@ class EsmConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"esm\"\n+    sub_configs = {\"esmfold_config\": EsmFoldConfig}\n \n     def __init__(\n         self,\n@@ -153,6 +306,7 @@ def __init__(\n         if self.esmfold_config is not None and getattr(self.esmfold_config, \"use_esm_attn_map\", False):\n             raise ValueError(\"The HuggingFace port of ESMFold does not support use_esm_attn_map at this time!\")\n \n+    # TODO: update ESM to inherit from PreTrainedConfig\n     def to_dict(self):\n         \"\"\"\n         Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n@@ -166,160 +320,6 @@ def to_dict(self):\n         return output\n \n \n-@dataclass\n-class EsmFoldConfig:\n-    esm_type: Optional[str] = None\n-    fp16_esm: bool = True\n-    use_esm_attn_map: bool = False\n-    esm_ablate_pairwise: bool = False\n-    esm_ablate_sequence: bool = False\n-    esm_input_dropout: float = 0\n-\n-    embed_aa: bool = True\n-    bypass_lm: bool = False\n-\n-    lddt_head_hid_dim: int = 128\n-    trunk: \"TrunkConfig\" = None\n-\n-    def __post_init__(self):\n-        if self.trunk is None:\n-            self.trunk = TrunkConfig()\n-        elif isinstance(self.trunk, dict):\n-            self.trunk = TrunkConfig(**self.trunk)\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n-\n-        Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = asdict(self)\n-        output[\"trunk\"] = self.trunk.to_dict()\n-        return output\n-\n-\n-@dataclass\n-class TrunkConfig:\n-    num_blocks: int = 48\n-    sequence_state_dim: int = 1024\n-    pairwise_state_dim: int = 128\n-    sequence_head_width: int = 32\n-    pairwise_head_width: int = 32\n-    position_bins: int = 32\n-    dropout: float = 0\n-    layer_drop: float = 0\n-    cpu_grad_checkpoint: bool = False\n-    max_recycles: int = 4\n-    chunk_size: Optional[int] = 128\n-    structure_module: \"StructureModuleConfig\" = None\n-\n-    def __post_init__(self):\n-        if self.structure_module is None:\n-            self.structure_module = StructureModuleConfig()\n-        elif isinstance(self.structure_module, dict):\n-            self.structure_module = StructureModuleConfig(**self.structure_module)\n-\n-        if self.max_recycles <= 0:\n-            raise ValueError(f\"`max_recycles` should be positive, got {self.max_recycles}.\")\n-        if self.sequence_state_dim % self.sequence_state_dim != 0:\n-            raise ValueError(\n-                \"`sequence_state_dim` should be a round multiple of `sequence_state_dim`, got\"\n-                f\" {self.sequence_state_dim} and {self.sequence_state_dim}.\"\n-            )\n-        if self.pairwise_state_dim % self.pairwise_state_dim != 0:\n-            raise ValueError(\n-                \"`pairwise_state_dim` should be a round multiple of `pairwise_state_dim`, got\"\n-                f\" {self.pairwise_state_dim} and {self.pairwise_state_dim}.\"\n-            )\n-\n-        sequence_num_heads = self.sequence_state_dim // self.sequence_head_width\n-        pairwise_num_heads = self.pairwise_state_dim // self.pairwise_head_width\n-\n-        if self.sequence_state_dim != sequence_num_heads * self.sequence_head_width:\n-            raise ValueError(\n-                \"`sequence_state_dim` should be equal to `sequence_num_heads * sequence_head_width, got\"\n-                f\" {self.sequence_state_dim} != {sequence_num_heads} * {self.sequence_head_width}.\"\n-            )\n-        if self.pairwise_state_dim != pairwise_num_heads * self.pairwise_head_width:\n-            raise ValueError(\n-                \"`pairwise_state_dim` should be equal to `pairwise_num_heads * pairwise_head_width, got\"\n-                f\" {self.pairwise_state_dim} != {pairwise_num_heads} * {self.pairwise_head_width}.\"\n-            )\n-        if self.pairwise_state_dim % 2 != 0:\n-            raise ValueError(f\"`pairwise_state_dim` should be even, got {self.pairwise_state_dim}.\")\n-\n-        if self.dropout >= 0.4:\n-            raise ValueError(f\"`dropout` should not be greater than 0.4, got {self.dropout}.\")\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n-\n-        Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = asdict(self)\n-        output[\"structure_module\"] = self.structure_module.to_dict()\n-        return output\n-\n-\n-@dataclass\n-class StructureModuleConfig:\n-    \"\"\"\n-    Args:\n-        sequence_dim:\n-            Single representation channel dimension\n-        pairwise_dim:\n-            Pair representation channel dimension\n-        ipa_dim:\n-            IPA hidden channel dimension\n-        resnet_dim:\n-            Angle resnet (Alg. 23 lines 11-14) hidden channel dimension\n-        num_heads_ipa:\n-            Number of IPA heads\n-        num_qk_points:\n-            Number of query/key points to generate during IPA\n-        num_v_points:\n-            Number of value points to generate during IPA\n-        dropout_rate:\n-            Dropout rate used throughout the layer\n-        num_blocks:\n-            Number of structure module blocks\n-        num_transition_layers:\n-            Number of layers in the single representation transition (Alg. 23 lines 8-9)\n-        num_resnet_blocks:\n-            Number of blocks in the angle resnet\n-        num_angles:\n-            Number of angles to generate in the angle resnet\n-        trans_scale_factor:\n-            Scale of single representation transition hidden dimension\n-        epsilon:\n-            Small number used in angle resnet normalization\n-        inf:\n-            Large number used for attention masking\n-    \"\"\"\n-\n-    sequence_dim: int = 384\n-    pairwise_dim: int = 128\n-    ipa_dim: int = 16\n-    resnet_dim: int = 128\n-    num_heads_ipa: int = 12\n-    num_qk_points: int = 4\n-    num_v_points: int = 8\n-    dropout_rate: float = 0.1\n-    num_blocks: int = 8\n-    num_transition_layers: int = 1\n-    num_resnet_blocks: int = 2\n-    num_angles: int = 7\n-    trans_scale_factor: int = 10\n-    epsilon: float = 1e-8\n-    inf: float = 1e5\n-\n-    def to_dict(self):\n-        return asdict(self)\n-\n-\n def get_default_vocab_list():\n     return (\n         \"<cls>\","
        },
        {
            "sha": "5e8ed02ba972c891ce0e586814833e0653cc22fb",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -146,6 +146,7 @@ class GroundingDinoConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"grounding-dino\"\n+    sub_configs = {\"backbone_config\": AutoConfig, \"text_config\": AutoConfig}\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n         \"num_attention_heads\": \"encoder_attention_heads\",\n@@ -286,24 +287,5 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        sub_configs = {}\n-        backbone_config = getattr(self, \"backbone_config\", None)\n-        text_config = getattr(self, \"text_config\", None)\n-        if isinstance(backbone_config, PreTrainedConfig):\n-            sub_configs[\"backbone_config\"] = type(backbone_config)\n-        if isinstance(text_config, PreTrainedConfig):\n-            sub_configs[\"text_config\"] = type(self.text_config)\n-        return sub_configs\n-\n \n __all__ = [\"GroundingDinoConfig\"]"
        },
        {
            "sha": "22f2f7034aa7544e46818eab81c1adbebacc3ae3",
            "filename": "src/transformers/models/mask2former/configuration_mask2former.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -19,7 +19,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -128,6 +128,7 @@ class Mask2FormerConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"mask2former\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     backbones_supported = [\"swin\"]\n     attribute_map = {\"hidden_size\": \"hidden_dim\"}\n \n@@ -236,14 +237,6 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`Mask2FormerConfig`] (or a derived class) from a pre-trained backbone model configuration."
        },
        {
            "sha": "6d16780818c194860cc979625bcc554251e6bd50",
            "filename": "src/transformers/models/maskformer/configuration_maskformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 10,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -19,7 +19,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n from ..detr import DetrConfig\n from ..swin import SwinConfig\n \n@@ -103,6 +103,7 @@ class MaskFormerConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"maskformer\"\n+    sub_configs = {\"backbone_config\": AutoConfig, \"decoder_config\": AutoConfig}\n     attribute_map = {\"hidden_size\": \"mask_feature_size\"}\n     backbones_supported = [\"resnet\", \"swin\"]\n     decoders_supported = [\"detr\"]\n@@ -200,15 +201,6 @@ def __init__(\n         self.backbone_kwargs = backbone_kwargs\n         super().__init__(**kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        sub_configs = {}\n-        if self.backbone_config is not None and self.backbone_config != {}:\n-            sub_configs[\"backbone_config\"] = type(self.backbone_config)\n-        if self.decoder_config is not None and self.decoder_config != {}:\n-            sub_configs[\"decoder_config\"] = type(self.decoder_config)\n-        return sub_configs\n-\n     @classmethod\n     def from_backbone_and_decoder_configs(\n         cls, backbone_config: PreTrainedConfig, decoder_config: PreTrainedConfig, **kwargs"
        },
        {
            "sha": "8ee2e1ce3c13bd6691e9cc8155d5faac0d6a6cd7",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -22,7 +22,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -146,6 +146,7 @@ class MMGroundingDinoConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"mm-grounding-dino\"\n+    sub_configs = {\"backbone_config\": AutoConfig, \"text_config\": AutoConfig}\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n         \"num_attention_heads\": \"encoder_attention_heads\",\n@@ -280,24 +281,5 @@ def __init__(\n         self.init_std = init_std\n         self.layer_norm_eps = layer_norm_eps\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        sub_configs = {}\n-        backbone_config = getattr(self, \"backbone_config\", None)\n-        text_config = getattr(self, \"text_config\", None)\n-        if isinstance(backbone_config, PreTrainedConfig):\n-            sub_configs[\"backbone_config\"] = type(backbone_config)\n-        if isinstance(text_config, PreTrainedConfig):\n-            sub_configs[\"text_config\"] = type(self.text_config)\n-        return sub_configs\n-\n \n __all__ = [\"MMGroundingDinoConfig\"]"
        },
        {
            "sha": "90122c30f9de30f742049fff74bce4f40ff601a1",
            "filename": "src/transformers/models/omdet_turbo/configuration_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 12,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -145,6 +145,7 @@ class OmDetTurboConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"omdet-turbo\"\n+    sub_configs = {\"backbone_config\": AutoConfig, \"text_config\": AutoConfig}\n     attribute_map = {\n         \"encoder_hidden_dim\": \"d_model\",\n         \"num_attention_heads\": \"encoder_attention_heads\",\n@@ -289,16 +290,5 @@ def __init__(\n \n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        sub_configs = {}\n-        backbone_config = getattr(self, \"backbone_config\", None)\n-        text_config = getattr(self, \"text_config\", None)\n-        if isinstance(backbone_config, PreTrainedConfig):\n-            sub_configs[\"backbone_config\"] = type(backbone_config)\n-        if isinstance(text_config, PreTrainedConfig):\n-            sub_configs[\"text_config\"] = type(text_config)\n-        return sub_configs\n-\n \n __all__ = [\"OmDetTurboConfig\"]"
        },
        {
            "sha": "29c8d5dabb4982f06a8b3ff1e3e42e5f1e2551d7",
            "filename": "src/transformers/models/oneformer/configuration_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -19,7 +19,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -146,6 +146,7 @@ class OneFormerConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"oneformer\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     attribute_map = {\"hidden_size\": \"hidden_dim\"}\n \n     def __init__(\n@@ -273,13 +274,5 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"OneFormerConfig\"]"
        },
        {
            "sha": "19f1e3c50dc04627e11db21fee349c60e813e497",
            "filename": "src/transformers/models/pegasus/configuration_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fconfiguration_pegasus.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -152,13 +152,5 @@ def __init__(\n             **kwargs,\n         )\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n \n __all__ = [\"PegasusConfig\"]"
        },
        {
            "sha": "c36f86442dbe41f10f7efbf3a0501b0feb3fc45a",
            "filename": "src/transformers/models/pegasus_x/configuration_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fconfiguration_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fconfiguration_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fconfiguration_pegasus_x.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -165,13 +165,5 @@ def __init__(\n             **kwargs,\n         )\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n \n __all__ = [\"PegasusXConfig\"]"
        },
        {
            "sha": "22983bcccd1f7fd0dbf3086ff430bb31121c82cb",
            "filename": "src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 24,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,12 +17,10 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import copy\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -92,6 +90,7 @@ class PromptDepthAnythingConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"prompt_depth_anything\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -154,26 +153,5 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-\n-        if output[\"backbone_config\"] is not None:\n-            output[\"backbone_config\"] = self.backbone_config.to_dict()\n-\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output\n-\n \n __all__ = [\"PromptDepthAnythingConfig\"]"
        },
        {
            "sha": "f176390fd7b547930500d52b1aa81e657e62c359",
            "filename": "src/transformers/models/rt_detr/configuration_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n from .configuration_rt_detr_resnet import RTDetrResNetConfig\n \n \n@@ -175,6 +175,7 @@ class RTDetrConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"rt_detr\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     layer_types = [\"basic\", \"bottleneck\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -335,22 +336,6 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "a711f6a4e6fe03a869400d718cb755d69db18e6c",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -22,7 +22,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -185,6 +185,7 @@ class RTDetrV2Config(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"rt_detr_v2\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     layer_types = [\"basic\", \"bottleneck\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -358,14 +359,6 @@ def __init__(\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "b96b8b494d64a3151e3ff4e78a25288eadac3dce",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -25,7 +25,7 @@\n from ...utils.backbone_utils import (\n     verify_backbone_config_arguments,\n )\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n from ..rt_detr.modeling_rt_detr import (\n     RTDetrDecoder,\n     RTDetrDecoderLayer,\n@@ -196,6 +196,7 @@ class RTDetrV2Config(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"rt_detr_v2\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     layer_types = [\"basic\", \"bottleneck\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -369,14 +370,6 @@ def __init__(\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "448229b3ae008e3841032dee1ee4094a67586a54",
            "filename": "src/transformers/models/superglue/configuration_superglue.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -15,7 +15,7 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n if TYPE_CHECKING:\n@@ -68,6 +68,7 @@ class SuperGlueConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"superglue\"\n+    sub_configs = {\"keypoint_detector_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -114,9 +115,5 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n-    @property\n-    def sub_configs(self):\n-        return {\"keypoint_detector_config\": type(self.keypoint_detector_config)}\n-\n \n __all__ = [\"SuperGlueConfig\"]"
        },
        {
            "sha": "8f8963c54a574c63e76a157dc6dc4e83fda6a4e7",
            "filename": "src/transformers/models/table_transformer/configuration_table_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -23,7 +23,7 @@\n from ...onnx import OnnxConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -133,6 +133,7 @@ class TableTransformerConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"table-transformer\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n     attribute_map = {\n         \"hidden_size\": \"d_model\",\n@@ -245,22 +246,6 @@ def __init__(\n         self.eos_coefficient = eos_coefficient\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n-    @property\n-    def num_attention_heads(self) -> int:\n-        return self.encoder_attention_heads\n-\n-    @property\n-    def hidden_size(self) -> int:\n-        return self.d_model\n-\n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n # Copied from transformers.models.detr.configuration_detr.DetrOnnxConfig\n class TableTransformerOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "7d4081b59c8beeccc47b832442b3d6c65c91857b",
            "filename": "src/transformers/models/tvp/configuration_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 24,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -14,12 +14,10 @@\n # limitations under the License.\n \"\"\"TVP model configuration\"\"\"\n \n-import copy\n-\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto import CONFIG_MAPPING\n+from ..auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -99,6 +97,7 @@ class TvpConfig(PreTrainedConfig):\n     \"\"\"\n \n     model_type = \"tvp\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -172,14 +171,6 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`TvpConfig`] (or a derived class) from a pre-trained backbone model configuration.\n@@ -192,18 +183,5 @@ def from_backbone_config(cls, backbone_config: PreTrainedConfig, **kwargs):\n         \"\"\"\n         return cls(backbone_config=backbone_config, **kwargs)\n \n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`].\n-\n-        Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-        if output[\"backbone_config\"] is not None:\n-            output[\"backbone_config\"] = self.backbone_config.to_dict()\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output\n-\n \n __all__ = [\"TvpConfig\"]"
        },
        {
            "sha": "27b1c38bc52f0a253c4a609ce337f8bf2bc12ff3",
            "filename": "src/transformers/models/upernet/configuration_upernet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -17,7 +17,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -83,6 +83,7 @@ class UperNetConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"upernet\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -136,13 +137,5 @@ def __init__(\n         self.auxiliary_concat_input = auxiliary_concat_input\n         self.loss_ignore_index = loss_ignore_index\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"UperNetConfig\"]"
        },
        {
            "sha": "1fdeb03af759090777d18753ed9fc70f57a02fd0",
            "filename": "src/transformers/models/vitmatte/configuration_vitmatte.py",
            "status": "modified",
            "additions": 2,
            "deletions": 20,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -14,13 +14,12 @@\n # limitations under the License.\n \"\"\"VitMatte model configuration\"\"\"\n \n-import copy\n from typing import Optional\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -78,6 +77,7 @@ class VitMatteConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vitmatte\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -122,23 +122,5 @@ def __init__(\n         self.convstream_hidden_sizes = convstream_hidden_sizes\n         self.fusion_hidden_sizes = fusion_hidden_sizes\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n-    def to_dict(self):\n-        \"\"\"\n-        Serializes this instance to a Python dictionary. Override the default [`~PreTrainedConfig.to_dict`]. Returns:\n-            `dict[str, any]`: Dictionary of all the attributes that make up this configuration instance,\n-        \"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-        output[\"backbone_config\"] = self.backbone_config.to_dict()\n-        output[\"model_type\"] = self.__class__.model_type\n-        return output\n-\n \n __all__ = [\"VitMatteConfig\"]"
        },
        {
            "sha": "e9ae2813f9d8c239f2bad5b7dde68630065ea146",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -19,7 +19,7 @@\n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n from ...utils.backbone_utils import verify_backbone_config_arguments\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -74,6 +74,7 @@ class VitPoseConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"vitpose\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -122,13 +123,5 @@ def __init__(\n         self.scale_factor = scale_factor\n         self.use_simple_decoder = use_simple_decoder\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"VitPoseConfig\"]"
        },
        {
            "sha": "9f82523d5caa1a52683994894deb2467c8268923",
            "filename": "src/transformers/models/zoedepth/configuration_zoedepth.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -16,7 +16,7 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n-from ..auto.configuration_auto import CONFIG_MAPPING\n+from ..auto.configuration_auto import CONFIG_MAPPING, AutoConfig\n \n \n logger = logging.get_logger(__name__)\n@@ -133,6 +133,7 @@ class ZoeDepthConfig(PreTrainedConfig):\n     ```\"\"\"\n \n     model_type = \"zoedepth\"\n+    sub_configs = {\"backbone_config\": AutoConfig}\n \n     def __init__(\n         self,\n@@ -233,13 +234,5 @@ def __init__(\n         self.patch_transformer_intermediate_size = patch_transformer_intermediate_size\n         self.patch_transformer_num_attention_heads = patch_transformer_num_attention_heads\n \n-    @property\n-    def sub_configs(self):\n-        return (\n-            {\"backbone_config\": type(self.backbone_config)}\n-            if getattr(self, \"backbone_config\", None) is not None\n-            else {}\n-        )\n-\n \n __all__ = [\"ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP\", \"ZoeDepthConfig\"]"
        },
        {
            "sha": "90c00f6f8c636172a832f69ea8605443a34afdb7",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 23,
            "deletions": 22,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -131,28 +131,29 @@ def create_and_test_config_from_and_save_pretrained_composite(self):\n             # Iterate over all sub_configs if there are any and load them with their own classes\n             sub_configs = general_config_loaded.sub_configs\n             for sub_config_key, sub_class in sub_configs.items():\n-                if sub_class.__name__ == \"AutoConfig\":\n-                    sub_class = sub_class.for_model(**general_config_dict[sub_config_key]).__class__\n-                    sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n-                else:\n-                    sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n-\n-                # Pop `transformers_version`, it never exists when a config is part of a general composite config\n-                # Verify that loading with subconfig class results in same dict as if we loaded with general composite config class\n-                sub_config_loaded_dict = sub_config_loaded.to_dict()\n-                sub_config_loaded_dict.pop(\"transformers_version\", None)\n-                general_config_dict[sub_config_key].pop(\"transformers_version\", None)\n-                self.parent.assertEqual(sub_config_loaded_dict, general_config_dict[sub_config_key])\n-\n-                # Verify that the loaded config type is same as in the general config\n-                type_from_general_config = type(getattr(general_config_loaded, sub_config_key))\n-                self.parent.assertTrue(isinstance(sub_config_loaded, type_from_general_config))\n-\n-                # Now save only the sub-config and load it back to make sure the whole load-save-load pipeline works\n-                with tempfile.TemporaryDirectory() as tmpdirname2:\n-                    sub_config_loaded.save_pretrained(tmpdirname2)\n-                    sub_config_loaded_2 = sub_class.from_pretrained(tmpdirname2)\n-                    self.parent.assertEqual(sub_config_loaded.to_dict(), sub_config_loaded_2.to_dict())\n+                if general_config_dict[sub_config_key] is not None:\n+                    if sub_class.__name__ == \"AutoConfig\":\n+                        sub_class = sub_class.for_model(**general_config_dict[sub_config_key]).__class__\n+                        sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n+                    else:\n+                        sub_config_loaded = sub_class.from_pretrained(tmpdirname)\n+\n+                    # Pop `transformers_version`, it never exists when a config is part of a general composite config\n+                    # Verify that loading with subconfig class results in same dict as if we loaded with general composite config class\n+                    sub_config_loaded_dict = sub_config_loaded.to_dict()\n+                    sub_config_loaded_dict.pop(\"transformers_version\", None)\n+                    general_config_dict[sub_config_key].pop(\"transformers_version\", None)\n+                    self.parent.assertEqual(sub_config_loaded_dict, general_config_dict[sub_config_key])\n+\n+                    # Verify that the loaded config type is same as in the general config\n+                    type_from_general_config = type(getattr(general_config_loaded, sub_config_key))\n+                    self.parent.assertTrue(isinstance(sub_config_loaded, type_from_general_config))\n+\n+                    # Now save only the sub-config and load it back to make sure the whole load-save-load pipeline works\n+                    with tempfile.TemporaryDirectory() as tmpdirname2:\n+                        sub_config_loaded.save_pretrained(tmpdirname2)\n+                        sub_config_loaded_2 = sub_class.from_pretrained(tmpdirname2)\n+                        self.parent.assertEqual(sub_config_loaded.to_dict(), sub_config_loaded_2.to_dict())\n \n     def create_and_test_config_from_pretrained_custom_kwargs(self):\n         \"\"\""
        },
        {
            "sha": "3c27829e51660ea225536a52b1d508c3c26bb97e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 30,
            "deletions": 15,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/be3fa93b298eec4182a0bb6c5e64270a5f1e8020/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=be3fa93b298eec4182a0bb6c5e64270a5f1e8020",
            "patch": "@@ -1257,7 +1257,8 @@ def test_attention_outputs(self):\n             del inputs_dict[\"output_attentions\"]\n             config.output_attentions = True\n             for k in config.sub_configs:\n-                getattr(config, k).output_attentions = True\n+                if getattr(config, k) is not None:\n+                    getattr(config, k).output_attentions = True\n \n             model = model_class(config)\n             model.to(torch_device)\n@@ -1736,20 +1737,23 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n             del inputs_dict[\"output_hidden_states\"]\n             config.output_hidden_states = True\n             for k in config.sub_configs:\n-                getattr(config, k).output_hidden_states = True\n+                if getattr(config, k) is not None:\n+                    getattr(config, k).output_hidden_states = True\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n     def test_retain_grad_hidden_states_attentions(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for k in config.sub_configs:\n-            getattr(config, k).output_hidden_states = True\n+            if getattr(config, k) is not None:\n+                getattr(config, k).output_hidden_states = True\n \n         config.output_hidden_states = True\n         config.output_attentions = self.has_attentions\n \n         for k in config.sub_configs:\n-            getattr(config, k).output_attentions = self.has_attentions\n+            if getattr(config, k) is not None:\n+                getattr(config, k).output_attentions = self.has_attentions\n \n         # force eager attention to support output attentions\n         if self.has_attentions:\n@@ -3188,13 +3192,15 @@ def test_attn_implementation_composite_models(self):\n             # we just need to test if passing 'attn_implementation' as a dict fails or not\n             attn_implementation_per_subconfig = {\"\": \"eager\"}\n             for key in config.sub_configs:\n-                attn_implementation_per_subconfig[key] = \"eager\"\n+                if getattr(config, key) is not None:\n+                    attn_implementation_per_subconfig[key] = \"eager\"\n \n             config._attn_implementation = attn_implementation_per_subconfig\n             model = model_class(config)\n             for key in config.sub_configs:\n-                sub_config = getattr(model.config, key)\n-                self.assertTrue(sub_config._attn_implementation == \"eager\")\n+                if getattr(config, key) is not None:\n+                    sub_config = getattr(model.config, key)\n+                    self.assertTrue(sub_config._attn_implementation == \"eager\")\n \n             for name, submodule in model.named_modules():\n                 class_name = submodule.__class__.__name__\n@@ -3934,8 +3940,9 @@ def update_config_headdim(config, requested_dim):\n         # Update config values\n         update_config_headdim(config, requested_dim)\n         for key in config.sub_configs:\n-            sub_config = getattr(config, key)\n-            update_config_headdim(sub_config, requested_dim)\n+            if getattr(config, key) is not None:\n+                sub_config = getattr(config, key)\n+                update_config_headdim(sub_config, requested_dim)\n \n         return config\n \n@@ -4119,7 +4126,10 @@ def test_internal_model_config_and_subconfig_are_same(self):\n                     for subconfig_key in subconfig_keys:\n                         # Get the subconfig from the model config\n                         subconfig_from_model_config = getattr(model.config, subconfig_key)\n-                        if subconfig_from_model_config.__class__ == subconfig_from_model_internal.__class__:\n+                        if (\n+                            subconfig_from_model_config is not None\n+                            and subconfig_from_model_config.__class__ == subconfig_from_model_internal.__class__\n+                        ):\n                             # Since some composite models have different submodels parameterized by 2 of the same config\n                             # class instances, we need to check against a list of matching classes, and check that at least\n                             # 1 is the exact object (instead of checking immediately for similar object)\n@@ -4150,7 +4160,8 @@ def test_can_set_attention_dynamically(self):\n             # sanity check to make sure everything is correctly eager\n             self.assertTrue(model.config._attn_implementation == \"eager\")\n             for subconfig_key in model.config.sub_configs:\n-                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+                if getattr(config, subconfig_key) is not None:\n+                    self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n \n             if not all(\n                 submodule._can_set_attn_implementation()\n@@ -4170,7 +4181,8 @@ def test_can_set_attention_dynamically(self):\n             # Check everything was correctly changed\n             self.assertTrue(model.config._attn_implementation == \"sdpa\")\n             for subconfig_key in model.config.sub_configs:\n-                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n+                if getattr(config, subconfig_key) is not None:\n+                    self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n \n             # Check we cannot set it to random values, and it raises an error\n             with self.assertRaisesRegex(ValueError, 'Specified `attn_implementation=\"foo\"` is not supported'):\n@@ -4179,7 +4191,8 @@ def test_can_set_attention_dynamically(self):\n             # Should still be sdpa everywhere\n             self.assertTrue(model.config._attn_implementation == \"sdpa\")\n             for subconfig_key in model.config.sub_configs:\n-                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n+                if getattr(config, subconfig_key) is not None:\n+                    self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n \n     def test_can_set_attention_dynamically_composite_model(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4198,7 +4211,8 @@ def test_can_set_attention_dynamically_composite_model(self):\n             # sanity check to make sure everything is correctly eager\n             self.assertTrue(model.config._attn_implementation == \"eager\")\n             for subconfig_key in model.config.sub_configs:\n-                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+                if getattr(config, subconfig_key) is not None:\n+                    self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n \n             if not all(\n                 submodule._can_set_attn_implementation()\n@@ -4213,7 +4227,8 @@ def test_can_set_attention_dynamically_composite_model(self):\n             # Check only top-most was correctly changed\n             self.assertTrue(model.config._attn_implementation == \"sdpa\")\n             for subconfig_key in model.config.sub_configs:\n-                self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n+                if getattr(config, subconfig_key) is not None:\n+                    self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"eager\")\n \n     @require_torch\n     def test_bc_torch_dtype(self):"
        }
    ],
    "stats": {
        "total": 907,
        "additions": 291,
        "deletions": 616
    }
}