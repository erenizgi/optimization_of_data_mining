{
    "author": "bvantuan",
    "message": "Fix missing initialization of `FastSpeech2Conformer` (#39689)\n\n* fix missing initialization of FastSpeech2Conformer\n\n* switch order and reactivate tests\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5",
    "files": [
        {
            "sha": "2b038a93396de6711918c53cbfdc1797134a7da6",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5",
            "patch": "@@ -966,14 +966,18 @@ class FastSpeech2ConformerPreTrainedModel(PreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (nn.LayerNorm)):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n+        if isinstance(module, nn.Linear):\n+            nn.init.normal_(module.weight, std=1.0 / math.sqrt(module.weight.size(1)))\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, nn.Conv1d):\n             nn.init.kaiming_normal_(module.weight)\n             if module.bias is not None:\n                 key = math.sqrt(module.groups / (module.in_channels * module.kernel_size[0]))\n                 nn.init.uniform_(module.bias, a=-key, b=key)\n+        elif isinstance(module, (nn.LayerNorm, nn.BatchNorm1d)):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_()\n             if module.padding_idx is not None:"
        },
        {
            "sha": "4565414a367467a4bca75aef281b3dfb5340d60a",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=6a61e16626d26ea209e94a15b9ab3bfff7ba9bc5",
            "patch": "@@ -28,7 +28,6 @@\n     Expectations,\n     require_g2p_en,\n     require_torch,\n-    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -123,7 +122,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@require_torch_accelerator\n @require_torch\n class FastSpeech2ConformerModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerModel,) if is_torch_available() else ()\n@@ -561,7 +559,6 @@ def prepare_config_and_inputs_for_common(self):\n         return config, inputs_dict\n \n \n-@require_torch_accelerator\n @require_torch\n class FastSpeech2ConformerWithHifiGanTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (FastSpeech2ConformerWithHifiGan,) if is_torch_available() else ()"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 7,
        "deletions": 6
    }
}