{
    "author": "lukepayyapilli",
    "message": "ðŸš¨ Fix ConvNeXt image processor default interpolation to BICUBIC (#42934)\n\n* Fix ConvNeXt image processor default interpolation to BICUBIC\n\nThe original ConvNeXt implementation uses BICUBIC interpolation for image\npreprocessing, but the HuggingFace image processor defaulted to BILINEAR.\nThis change aligns the default with the original implementation.\n\nChanges:\n- Update default resample from BILINEAR to BICUBIC in ConvNextImageProcessor\n- Update conversion script to explicitly use BICUBIC\n- Add test to verify default interpolation is BICUBIC\n\nReference: https://github.com/facebookresearch/ConvNeXt/blob/main/datasets.py\n\nFixes part of #28180\n\n* Fix import ordering for ruff linter\n\n* Fix ConvNextImageProcessorFast default interpolation to BICUBIC\n\n* nit + fix center_crop\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>",
    "sha": "60634caac9202f42b747bbfb8fef64bb4c813bae",
    "files": [
        {
            "sha": "4f46adf202b5bba3db3305200f262ecbd0c424fb",
            "filename": "src/transformers/models/convnext/image_processing_convnext.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/60634caac9202f42b747bbfb8fef64bb4c813bae/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60634caac9202f42b747bbfb8fef64bb4c813bae/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext.py?ref=60634caac9202f42b747bbfb8fef64bb4c813bae",
            "patch": "@@ -78,7 +78,7 @@ class ConvNextImageProcessor(BaseImageProcessor):\n         crop_pct (`float` *optional*, defaults to 224 / 256):\n             Percentage of the image to crop. Only has an effect if `do_resize` is `True` and size < 384. Can be\n             overridden by `crop_pct` in the `preprocess` method.\n-        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n             Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n         do_rescale (`bool`, *optional*, defaults to `True`):\n             Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n@@ -105,7 +105,7 @@ def __init__(\n         do_resize: bool = True,\n         size: Optional[dict[str, int]] = None,\n         crop_pct: Optional[float] = None,\n-        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_normalize: bool = True,"
        },
        {
            "sha": "d58e4edfea97771c67c68d02938d999d83a3e023",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 9,
            "deletions": 12,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/60634caac9202f42b747bbfb8fef64bb4c813bae/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60634caac9202f42b747bbfb8fef64bb4c813bae/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=60634caac9202f42b747bbfb8fef64bb4c813bae",
            "patch": "@@ -20,18 +20,15 @@\n from torchvision.transforms.v2 import functional as F\n \n from ...image_processing_utils import BatchFeature\n-from ...image_processing_utils_fast import (\n-    BaseImageProcessorFast,\n-    group_images_by_shape,\n-    reorder_images,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, group_images_by_shape, reorder_images\n from ...image_transforms import get_resize_output_image_size\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n+    SizeDict,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -43,7 +40,7 @@\n \n @auto_docstring\n class ConvNextImageProcessorFast(BaseImageProcessorFast):\n-    resample = PILImageResampling.BILINEAR\n+    resample = PILImageResampling.BICUBIC\n     image_mean = IMAGENET_STANDARD_MEAN\n     image_std = IMAGENET_STANDARD_STD\n     size = {\"shortest_edge\": 384}\n@@ -98,23 +95,23 @@ def resize(\n             resize_size = get_resize_output_image_size(\n                 image, size=resize_shortest_edge, default_to_square=False, input_data_format=ChannelDimension.FIRST\n             )\n-            image = F.resize(\n+            image = super().resize(\n                 image,\n-                resize_size,\n+                SizeDict(height=resize_size[0], width=resize_size[1]),\n                 interpolation=interpolation,\n                 **kwargs,\n             )\n             # then crop to (shortest_edge, shortest_edge)\n-            return F.center_crop(\n+            return self.center_crop(\n                 image,\n-                (shortest_edge, shortest_edge),\n+                SizeDict(height=shortest_edge, width=shortest_edge),\n                 **kwargs,\n             )\n         else:\n             # warping (no cropping) when evaluated at 384 or larger\n-            return F.resize(\n+            return super().resize(\n                 image,\n-                (shortest_edge, shortest_edge),\n+                SizeDict(height=shortest_edge, width=shortest_edge),\n                 interpolation=interpolation,\n                 **kwargs,\n             )"
        },
        {
            "sha": "2aa1a40c628072c9ca86b90acc9b09623dc514a4",
            "filename": "tests/models/convnext/test_image_processing_convnext.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/60634caac9202f42b747bbfb8fef64bb4c813bae/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/60634caac9202f42b747bbfb8fef64bb4c813bae/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py?ref=60634caac9202f42b747bbfb8fef64bb4c813bae",
            "patch": "@@ -114,9 +114,3 @@ def test_image_processor_from_dict_with_kwargs(self):\n \n             image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n             self.assertEqual(image_processor.size, {\"shortest_edge\": 42})\n-\n-    @unittest.skip(\n-        \"Skipping as ConvNextImageProcessor uses center_crop and center_crop functions are not equivalent for fast and slow processors\"\n-    )\n-    def test_slow_fast_equivalence_batched(self):\n-        pass"
        }
    ],
    "stats": {
        "total": 31,
        "additions": 11,
        "deletions": 20
    }
}