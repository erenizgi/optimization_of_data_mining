{
    "author": "yao-matrix",
    "message": "enable mllama cases on xpu (#37644)\n\n* enable mllama testing on xpu\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* more mllama cases enabling\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* make cases pass on A100\n\nSigned-off-by: N <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nSigned-off-by: N <matrix.yao@intel.com>",
    "sha": "9167461a7db39a1a7c793bb46c5aed9e14e8492e",
    "files": [
        {
            "sha": "41ad23d7d75215c468642ae3f451cea0cb7ac731",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 69,
            "deletions": 16,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/9167461a7db39a1a7c793bb46c5aed9e14e8492e/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9167461a7db39a1a7c793bb46c5aed9e14e8492e/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=9167461a7db39a1a7c793bb46c5aed9e14e8492e",
            "patch": "@@ -31,11 +31,12 @@\n from transformers.cache_utils import Cache\n from transformers.models.mllama.configuration_mllama import MllamaTextConfig\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     require_bitsandbytes,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_accelerator,\n     slow,\n     torch_device,\n )\n@@ -524,7 +525,7 @@ def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @require_read_token\n     def test_11b_model_integration_generate(self):\n@@ -537,9 +538,18 @@ def test_11b_model_integration_generate(self):\n \n         inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(torch_device)\n \n+        input_ids = inputs[\"input_ids\"]\n+\n         # Check inputs ids\n-        expected_input_ids = torch.tensor([[128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device)  # fmt: skip\n-        self.assertTrue(torch.equal(inputs[\"input_ids\"], expected_input_ids))\n+        expected_input_ids_all = Expectations(\n+            {\n+                (\"xpu\", 3): torch.tensor([[128000, 128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n+                (\"cuda\", 7): torch.tensor([[128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n+                (\"cuda\", 8): torch.tensor([[128000, 128256, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342, 369, 420, 832]], device=torch_device),\n+            }\n+        )  # fmt: skip\n+        expected_input_ids = expected_input_ids_all.get_expectation()\n+        self.assertTrue(torch.equal(input_ids, expected_input_ids))\n \n         # Load model in 4 bit\n         quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n@@ -551,7 +561,14 @@ def test_11b_model_integration_generate(self):\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\"  # fmt: skip\n+        expected_outputs = Expectations(\n+                {\n+                    (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\",\n+                    (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                }\n+            )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n \n         self.assertEqual(\n             decoded_output,\n@@ -560,18 +577,26 @@ def test_11b_model_integration_generate(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @require_read_token\n     def test_11b_model_integration_generate_text_only(self):\n         # Prepare inputs\n         processor = AutoProcessor.from_pretrained(self.base_model_checkpoint)\n         prompt = \"If I had to write a haiku\"\n         inputs = processor(text=prompt, return_tensors=\"pt\").to(torch_device)\n+        input_ids = inputs[\"input_ids\"].cpu().squeeze().tolist()\n \n         # Check inputs ids\n-        expected_input_ids = [128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342]\n-        self.assertEqual(inputs[\"input_ids\"].cpu().squeeze().tolist(), expected_input_ids)\n+        expected_input_ids_all = Expectations(\n+            {\n+                (\"xpu\", 3): [128000, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n+                (\"cuda\", 7): [128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n+                (\"cuda\", 8): [128000, 128000, 2746, 358, 1047, 311, 3350, 264, 6520, 39342],\n+            }\n+        )\n+        expected_input_ids = expected_input_ids_all.get_expectation()\n+        self.assertEqual(input_ids, expected_input_ids)\n \n         # Load model in 4 bit\n         quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n@@ -583,16 +608,22 @@ def test_11b_model_integration_generate_text_only(self):\n         output = model.generate(**inputs, do_sample=False, max_new_tokens=25)\n \n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = \"If I had to write a haiku about my life, I think it would be something like:\\n\\\"Life is a messy stream\\nTwists and turns, ups\"  # fmt: skip\n-\n+        expected_outputs = Expectations(\n+                {\n+                    (\"xpu\", 3): \"If I had to write a haiku about my life, I would write:\\nLife is a messy tapestry\\n Threads of joy and sorrow\\nWeft of memories\",\n+                    (\"cuda\", 7): \"If I had to write a haiku about my life, I think it would be something like:\\n\\\"Life is a messy stream\\nTwists and turns, ups\",\n+                    (\"cuda\", 8): \"If I had to write a haiku about my life, I would write:\\nLife is a messy stream\\nRipples of joy and pain\\nFlowing, ever\",\n+                }\n+            )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n         self.assertEqual(\n             decoded_output,\n             expected_output,\n             f\"Decoded output: {decoded_output}\\nExpected output: {expected_output}\",\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @require_read_token\n     def test_11b_model_integration_forward(self):\n@@ -616,7 +647,15 @@ def test_11b_model_integration_forward(self):\n             output = model(**inputs)\n \n         actual_logits = output.logits[0, -1, :5].cpu()\n-        expected_logits = torch.tensor([8.3594, 7.7148, 4.7266, 0.7803, 3.1504])\n+        expected_logits_all = Expectations(\n+            {\n+                (\"xpu\", 3): torch.tensor([9.1562, 8.9141, 5.0664, 1.6855, 3.2324]),\n+                (\"cuda\", 7): torch.tensor([8.3594, 7.7148, 4.7266, 0.7803, 3.1504]),\n+                (\"cuda\", 8): torch.tensor([9.0703, 8.8750, 5.0781, 1.6279, 3.2207]),\n+            }\n+        )\n+\n+        expected_logits = expected_logits_all.get_expectation()\n         self.assertTrue(\n             torch.allclose(actual_logits, expected_logits, atol=0.1),\n             f\"Actual logits: {actual_logits}\"\n@@ -625,7 +664,7 @@ def test_11b_model_integration_forward(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @require_read_token\n     def test_11b_model_integration_batched_generate(self):\n@@ -653,7 +692,14 @@ def test_11b_model_integration_batched_generate(self):\n \n         # Check first output\n         decoded_output = processor.decode(output[0], skip_special_tokens=True)\n-        expected_output = \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\"  # fmt: skip\n+        expected_outputs = Expectations(\n+                {\n+                    (\"xpu\", 3): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                    (\"cuda\", 7): \"If I had to write a haiku for this one, it would be:.\\\\nI'm not a poet.\\\\nBut I'm a photographer.\\\\nAnd I'm a\",\n+                    (\"cuda\", 8): \"If I had to write a haiku for this one, it would be:.\\\\nA dock on a lake.\\\\nA mountain in the distance.\\\\nA long exposure.\",\n+                 }\n+            )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n \n         self.assertEqual(\n             decoded_output,\n@@ -663,7 +709,14 @@ def test_11b_model_integration_batched_generate(self):\n \n         # Check second output\n         decoded_output = processor.decode(output[1], skip_special_tokens=True)\n-        expected_output = \"This image shows is a photograph of a stop sign in front of a Chinese archway. The stop sign is red with white letters and is\"  # fmt: skip\n+        expected_outputs = Expectations(\n+                {\n+                    (\"xpu\", 3): \"This image shows\\nI'm not able to provide information on the person in this image. I can give you an idea of what's happening\",\n+                    (\"cuda\", 7): \"This image shows is a photograph of a stop sign in front of a Chinese archway. The stop sign is red with white letters and is\",\n+                    (\"cuda\", 8): \"This image shows\\nI'm not able to provide information on the person in this image. I can give you an idea of what's happening\",\n+                }\n+            )  # fmt: skip\n+        expected_output = expected_outputs.get_expectation()\n \n         self.assertEqual(\n             decoded_output,\n@@ -672,7 +725,7 @@ def test_11b_model_integration_batched_generate(self):\n         )\n \n     @slow\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @require_bitsandbytes\n     @require_read_token\n     def test_11b_model_integration_multi_image_generate(self):"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 69,
        "deletions": 16
    }
}