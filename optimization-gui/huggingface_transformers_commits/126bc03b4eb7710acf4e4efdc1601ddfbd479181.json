{
    "author": "ydshieh",
    "message": "Allow to be able to run `torch.compile` tests with `fullgraph=True` (#40164)\n\n* fix\n\n* address comment\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "126bc03b4eb7710acf4e4efdc1601ddfbd479181",
    "files": [
        {
            "sha": "fe12045f87a277d350b638031846191c06a5de4b",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/126bc03b4eb7710acf4e4efdc1601ddfbd479181/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/126bc03b4eb7710acf4e4efdc1601ddfbd479181/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=126bc03b4eb7710acf4e4efdc1601ddfbd479181",
            "patch": "@@ -23,7 +23,7 @@\n import _pytest\n import pytest\n \n-from transformers.testing_utils import HfDoctestModule, HfDocTestParser, is_torch_available\n+from transformers.testing_utils import HfDoctestModule, HfDocTestParser, is_torch_available, patch_torch_compile_force_graph\n \n \n NOT_DEVICE_TESTS = {\n@@ -136,3 +136,7 @@ def check_output(self, want, got, optionflags):\n     # The flag below controls whether to allow TF32 on cuDNN. This flag defaults to True.\n     # We set it to `False` for CI. See https://github.com/pytorch/pytorch/issues/157274#issuecomment-3090791615\n     torch.backends.cudnn.allow_tf32 = False\n+\n+    # patch `torch.compile`: if `TORCH_COMPILE_FORCE_FULLGRAPH=1` (or values considered as true, e.g. yes, y, etc.),\n+    # the patched version will always run with `fullgraph=True`.\n+    patch_torch_compile_force_graph()"
        },
        {
            "sha": "c5333ba00dcdfae96bffca1dbd55141bb47aadc9",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/126bc03b4eb7710acf4e4efdc1601ddfbd479181/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/126bc03b4eb7710acf4e4efdc1601ddfbd479181/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=126bc03b4eb7710acf4e4efdc1601ddfbd479181",
            "patch": "@@ -3473,6 +3473,32 @@ def __repr__(self):\n         return f\"{self.data}\"\n \n \n+def patch_torch_compile_force_graph():\n+    \"\"\"\n+    Patch `torch.compile` to always use `fullgraph=True`.\n+\n+    This is useful when some `torch.compile` tests are running with `fullgraph=False` and we want to be able to run\n+    them with `fullgraph=True` in some occasion (without introducing new tests) to make sure there is no graph break.\n+\n+    After PR #40137, `CompileConfig.fullgraph` is `False` by default, this patch is necessary.\n+    \"\"\"\n+\n+    force_fullgraph = os.environ.get(\"TORCH_COMPILE_FORCE_FULLGRAPH\", \"\")\n+    force_fullgraph = force_fullgraph.lower() in (\"yes\", \"true\", \"on\", \"t\", \"y\", \"1\")\n+\n+    if force_fullgraph:\n+        import torch\n+\n+        orig_method = torch.compile\n+\n+        def patched(*args, **kwargs):\n+            # In `torch_compile`, all arguments except `model` is keyword only argument.\n+            kwargs[\"fullgraph\"] = True\n+            return orig_method(*args, **kwargs)\n+\n+        torch.compile = patched\n+\n+\n def torchrun(script: str, nproc_per_node: int, is_torchrun: bool = True, env: Optional[dict] = None):\n     \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n     with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:"
        }
    ],
    "stats": {
        "total": 32,
        "additions": 31,
        "deletions": 1
    }
}