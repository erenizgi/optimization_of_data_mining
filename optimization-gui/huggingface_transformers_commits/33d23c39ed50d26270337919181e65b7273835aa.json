{
    "author": "Aguedoom",
    "message": "Update BioGPT model card (#38214)\n\n* Update BioGPT model card\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/biogpt.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* correction for CPU fallback\n\n* added quantization code and method\n\n* fixed transformers-cli call\n\n---------\n\nCo-authored-by: Aguedo <aguedo@fakeemail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "33d23c39ed50d26270337919181e65b7273835aa",
    "files": [
        {
            "sha": "0b6eb877647e1fe1efcbbfa7743d019dfdccb256",
            "filename": "docs/source/en/model_doc/biogpt.md",
            "status": "modified",
            "additions": 98,
            "deletions": 56,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/33d23c39ed50d26270337919181e65b7273835aa/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/33d23c39ed50d26270337919181e65b7273835aa/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbiogpt.md?ref=33d23c39ed50d26270337919181e65b7273835aa",
            "patch": "@@ -14,79 +14,121 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# BioGPT\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+            <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+            <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The BioGPT model was proposed in [BioGPT: generative pre-trained transformer for biomedical text generation and mining](https://academic.oup.com/bib/advance-article/doi/10.1093/bib/bbac409/6713511?guestAccessKey=a66d9b5d-4f83-4017-bb52-405815c907b9) by Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon and Tie-Yan Liu. BioGPT is a domain-specific generative pre-trained Transformer language model for biomedical text generation and mining. BioGPT follows the Transformer language model backbone, and is pre-trained on 15M PubMed abstracts from scratch.\n-\n-The abstract from the paper is the following:\n-\n-*Pre-trained language models have attracted increasing attention in the biomedical domain, inspired by their great success in the general natural language domain. Among the two main branches of pre-trained language models in the general language domain, i.e. BERT (and its variants) and GPT (and its variants), the first one has been extensively studied in the biomedical domain, such as BioBERT and PubMedBERT. While they have achieved great success on a variety of discriminative downstream biomedical tasks, the lack of generation ability constrains their application scope. In this paper, we propose BioGPT, a domain-specific generative Transformer language model pre-trained on large-scale biomedical literature. We evaluate BioGPT on six biomedical natural language processing tasks and demonstrate that our model outperforms previous models on most tasks. Especially, we get 44.98%, 38.42% and 40.76% F1 score on BC5CDR, KD-DTI and DDI end-to-end relation extraction tasks, respectively, and 78.2% accuracy on PubMedQA, creating a new record. Our case study on text generation further demonstrates the advantage of BioGPT on biomedical literature to generate fluent descriptions for biomedical terms.*\n+# BioGPT\n \n-This model was contributed by [kamalkraj](https://huggingface.co/kamalkraj). The original code can be found [here](https://github.com/microsoft/BioGPT).\n+[BioGPT](https://huggingface.co/papers/2210.10341) is a generative Transformer model based on [GPT-2](./gpt2) and pretrained on 15 million PubMed abstracts. It is designed for biomedical language tasks.\n \n-## Usage tips\n+You can find all the original BioGPT checkpoints under the [Microsoft](https://huggingface.co/microsoft?search_models=biogpt) organization.\n \n-- BioGPT is a model with absolute position embeddings so it's usually advised to pad the inputs on the right rather than the left.\n-- BioGPT was trained with a causal language modeling (CLM) objective and is therefore powerful at predicting the next token in a sequence. Leveraging this feature allows BioGPT to generate syntactically coherent text as it can be observed in the run_generation.py example script.\n-- The model can take the `past_key_values` (for PyTorch) as input, which is the previously computed key/value attention pairs. Using this (past_key_values or past) value prevents the model from re-computing pre-computed values in the context of text generation. For PyTorch, see past_key_values argument of the BioGptForCausalLM.forward() method for more information on its usage.\n-- The `head_mask` argument is ignored when using all attention implementation other than \"eager\". If you have a `head_mask` and want it to have effect, load the model with `XXXModel.from_pretrained(model_id, attn_implementation=\"eager\")`  \n+> [!TIP]\n+> Click on the BioGPT models in the right sidebar for more examples of how to apply BioGPT to different language tasks.\n \n-### Using Scaled Dot Product Attention (SDPA)\n+The example below demonstrates how to generate biomedical text with [`Pipeline`], [`AutoModel`], and also from the command line.\n \n-PyTorch includes a native scaled dot-product attention (SDPA) operator as part of `torch.nn.functional`. This function\n-encompasses several implementations that can be applied depending on the inputs and the hardware in use. See the\n-[official documentation](https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html)\n-or the [GPU Inference](https://huggingface.co/docs/transformers/main/en/perf_infer_gpu_one#pytorch-scaled-dot-product-attention)\n-page for more information.\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n-`attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n+```py\n+import torch\n+from transformers import pipeline\n \n+generator = pipeline(\n+    task=\"text-generation\",\n+    model=\"microsoft/biogpt\",\n+    torch_dtype=torch.float16,\n+    device=0,\n+)\n+result = generator(\"Ibuprofen is best used for\", truncation=True, max_length=50, do_sample=True)[0][\"generated_text\"]\n+print(result)\n ```\n-from transformers import BioGptForCausalLM\n-model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", attn_implementation=\"sdpa\", torch_dtype=torch.float16)\n-```\n-\n-On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a CausalLM head,\n-we saw the following speedups during training.\n \n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"microsoft/biogpt\",\n+    torch_dtype=torch.float16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+\n+input_text = \"Ibuprofen is best used for\"\n+inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+\n+with torch.no_grad():\n+    generated_ids = model.generate(**inputs, max_length=50)\n+    \n+output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+print(output)\n+```\n \n-| num_training_steps | batch_size | seq_len | is cuda | Time per batch (eager - s) | Time per batch (sdpa - s) | Speedup (%) | Eager peak mem (MB) | sdpa peak mem (MB) | Mem saving (%) |\n-|--------------------|------------|---------|---------|----------------------------|---------------------------|-------------|---------------------|--------------------|----------------|\n-| 100                | 1          | 128     | False   | 0.038                      | 0.031                     | 21.301      | 1601.862            | 1601.497           | 0.023          |\n-| 100                | 1          | 256     | False   | 0.039                      | 0.034                     | 15.084      | 1624.944            | 1625.296           | -0.022         |\n-| 100                | 2          | 128     | False   | 0.039                      | 0.033                     | 16.820      | 1624.567            | 1625.296           | -0.045         |\n-| 100                | 2          | 256     | False   | 0.065                      | 0.059                     | 10.255      | 1672.164            | 1672.164           | 0.000          |\n-| 100                | 4          | 128     | False   | 0.062                      | 0.058                     | 6.998       | 1671.435            | 1672.164           | -0.044         |\n-| 100                | 4          | 256     | False   | 0.113                      | 0.100                     | 13.316      | 2350.179            | 1848.435           | 27.144         |\n-| 100                | 8          | 128     | False   | 0.107                      | 0.098                     | 9.883       | 2098.521            | 1848.435           | 13.530         |\n-| 100                | 8          | 256     | False   | 0.222                      | 0.196                     | 13.413      | 3989.980            | 2986.492           | 33.601         |\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n \n-On a local benchmark (NVIDIA GeForce RTX 2060-8GB, PyTorch 2.3.1, OS Ubuntu 20.04) with `float16` and `microsoft/biogpt` model with a simple AutoModel head,\n-we saw the following speedups during inference.\n+```bash\n+echo -e \"Ibuprofen is best used for\" | transformers-cli run --task text-generation --model microsoft/biogpt --device 0\n+```\n \n-| num_batches | batch_size | seq_len | is cuda | is half | use mask | Per token latency eager (ms) | Per token latency SDPA (ms) | Speedup (%) | Mem eager (MB) | Mem BT (MB) | Mem saved (%) |\n-|-------------|------------|---------|---------|---------|----------|------------------------------|-----------------------------|-------------|----------------|--------------|---------------|\n-| 50          | 1          | 64      | True    | True    | True     | 0.115                        | 0.098                       | 17.392      | 716.998        | 716.998      | 0.000         |\n-| 50          | 1          | 128     | True    | True    | True     | 0.115                        | 0.093                       | 24.640      | 730.916        | 730.916      | 0.000         |\n-| 50          | 2          | 64      | True    | True    | True     | 0.114                        | 0.096                       | 19.204      | 730.900        | 730.900      | 0.000         |\n-| 50          | 2          | 128     | True    | True    | True     | 0.117                        | 0.095                       | 23.529      | 759.262        | 759.262      | 0.000         |\n-| 50          | 4          | 64      | True    | True    | True     | 0.113                        | 0.096                       | 18.325      | 759.229        | 759.229      | 0.000         |\n-| 50          | 4          | 128     | True    | True    | True     | 0.186                        | 0.178                       | 4.289       | 816.478        | 816.478      | 0.000         |\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bit precision.\n+\n+```py\n+import torch\n+from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n+\n+bnb_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.bfloat16,\n+    bnb_4bit_use_double_quant=True\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"microsoft/BioGPT-Large\")\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"microsoft/BioGPT-Large\", \n+    quantization_config=bnb_config,\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+\n+input_text = \"Ibuprofen is best used for\"\n+inputs = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n+with torch.no_grad():\n+    generated_ids = model.generate(**inputs, max_length=50)    \n+output = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+print(output)\n+```\n \n+## Notes\n \n-## Resources\n+- Pad inputs on the right because BioGPT uses absolute position embeddings.\n+- BioGPT can reuse previously computed key-value attention pairs. Access this feature with the [past_key_values](https://huggingface.co/docs/transformers/main/en/model_doc/biogpt#transformers.BioGptModel.forward.past_key_values) parameter in [`BioGPTModel.forward`].\n+- The `head_mask` argument is ignored when using an attention implementation other than \"eager\". If you want to use `head_mask`, make sure `attn_implementation=\"eager\"`).\n \n-- [Causal language modeling task guide](../tasks/language_modeling)\n+   ```py\n+   from transformers import AutoModelForCausalLM\n+   \n+   model = AutoModelForCausalLM.from_pretrained(\n+      \"microsoft/biogpt\",\n+      attn_implementation=\"eager\"\n+   )\n \n ## BioGptConfig\n "
        }
    ],
    "stats": {
        "total": 154,
        "additions": 98,
        "deletions": 56
    }
}