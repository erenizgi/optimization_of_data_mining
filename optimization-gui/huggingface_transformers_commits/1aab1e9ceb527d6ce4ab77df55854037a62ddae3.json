{
    "author": "stevhliu",
    "message": "[docs] WeightConverter (#42636)\n\n* weight converter draft\n\n* fix\n\n* feedback\n\n* update",
    "sha": "1aab1e9ceb527d6ce4ab77df55854037a62ddae3",
    "files": [
        {
            "sha": "9e6cb4a829f7785277d70f7dae17f8bd4c741db0",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1aab1e9ceb527d6ce4ab77df55854037a62ddae3",
            "patch": "@@ -9,6 +9,8 @@\n - isExpanded: false\n   sections:\n   - sections:\n+    - local: weightconverter\n+      title: Dynamic weight loading\n     - local: models\n       title: Loading models\n     - local: custom_models"
        },
        {
            "sha": "f7e7229c8fafa8c036b2a0e7b1fb5bbed6c6b99f",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=1aab1e9ceb527d6ce4ab77df55854037a62ddae3",
            "patch": "@@ -20,6 +20,24 @@ This page lists all the custom layers used by the library, as well as the utilit\n \n Most of those are only useful if you are studying the code of the models in the library.\n \n+## WeightConverter\n+\n+[[autodoc]] WeightConverter\n+\n+### Conversion operations\n+\n+[[autodoc]] ConversionOps\n+\n+[[autodoc]] Chunk\n+\n+[[autodoc]] Concatenate\n+\n+[[autodoc]] MergeModulelist\n+\n+[[autodoc]] SplitModulelist\n+\n+[[autodoc]] PermuteForRope\n+\n ## Layers\n \n [[autodoc]] GradientCheckpointingLayer"
        },
        {
            "sha": "d6bad3e3441d9f5ba6c39a555a806e188b60e5b6",
            "filename": "docs/source/en/weightconverter.md",
            "status": "added",
            "additions": 119,
            "deletions": 0,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2Fweightconverter.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/docs%2Fsource%2Fen%2Fweightconverter.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fweightconverter.md?ref=1aab1e9ceb527d6ce4ab77df55854037a62ddae3",
            "patch": "@@ -0,0 +1,119 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# Dynamic weight loading\n+\n+Checkpoints are often serialized in a format that does not match what a model expects at runtime. Quantization and parallelism frequently require reshaping, splitting, or merging tensors into the expected model format instead of loading weights as-is.\n+\n+Dynamic weight loading addresses this by applying scheduled, reversible operations to checkpoint tensors as they are loaded. Transformers makes this available through [`WeightConverter`], which maps one or more source keys to target keys by running a list of composable conversion operations. This approach adapts to new weight layouts, and supports loading quantized mixture-of-experts (MoEs) or enabling tensor parallelism and MoEs.\n+\n+This guide demonstrates how to use the [`WeightConverter`] to convert tensors. Your [`WeightConverter`] should be added inside [_build_checkpoint_conversion_mapping()](https://github.com/huggingface/transformers/blob/4c9fde2a2a3aece0bcf1be93f696e88297da9397/src/transformers/conversion_mapping.py#L34) in the [conversion_mapping.py](https://github.com/huggingface/transformers/blob/main/src/transformers/conversion_mapping.py) file.\n+\n+## Conversion operations\n+\n+The [`WeightConverter`] class has several operations that are executed when [`~PreTrainedModel.from_pretrained`] is called for transforming checkpoint source tensors into model target tensors.\n+\n+Operations are fully reversible. Saving reverses the conversions and returns the original checkpoint so you can easily work across different frameworks.\n+\n+### Chunk\n+\n+The [`Chunk`] operation is used to split a tensor. For example, if a model expects Q, K, and V as three separate tensors instead of a single tensor.\n+\n+```py\n+WeightConverter(\n+    \"self_attn.qkv_proj\",\n+    [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],\n+    operations=[Chunk(dim=0)],\n+)\n+```\n+\n+### Concatenate\n+\n+The [`Concatenate`] operation allows you to fuse separate tensors into a single tensor. For example, if a model expects Q, K, and V as a single tensor instead of separate tensors.\n+\n+```py\n+WeightConverter(\n+    [\"self_attn.q_proj\", \"self_attn.k_proj\", \"self_attn.v_proj\"],\n+    \"self_attn.qkv_proj\",\n+    operations=[Concatenate(dim=0)],\n+)\n+```\n+\n+### MergeModulelist\n+\n+[`MergeModulelist`] merges a list of tensors into a single tensor. For example, you can compose [`MergeModulelist`] with [`Concatenate`] to stack the experts in a MoE and pack them into one tensor.\n+\n+```py\n+WeightConverter(\n+    [\"block_sparse_moe.experts.*.w1.weight\", \"block_sparse_moe.experts.*.w3.weight\",],\n+    \"mlp.experts.gate_up_proj\",\n+    operations=[\n+        MergeModulelist(dim=0),\n+        Concatenate(dim=1),\n+    ],\n+)\n+```\n+\n+### SplitModulelist\n+\n+[`SplitModulelist`] splits a tensor back into a list of tensors. For example, you can split a stack of experts back into individual experts.\n+\n+```py\n+WeightConverter(\n+    \"mlp.experts.down_proj\",\n+    \"block_sparse_moe.experts.*.w2.weight\",\n+    operations=[SplitModulelist(dim=0)],\n+)\n+```\n+\n+### PermuteForRope\n+\n+[`PermuteForRope`] converts weights from the interleaved format to use the sin/cos format. For example, you can compose [`Chunk`] with [`PermuteForRope`] to split a fused QKV tensor and apply the sin/cos RoPE permutation to Q and K.\n+\n+```py\n+WeightConverter(\n+    [\"model.layers.*.self_attn.qkv_proj.weight\"],\n+    [\"model.layers.*.self_attn.q_proj.weight\", \"model.layers.*.self_attn.k_proj.weight\", \"model.layers.*.self_attn.v_proj.weight\",],\n+    operations=[\n+        Chunk(dim=0),\n+        PermuteForRope(),\n+    ],\n+)\n+```\n+\n+## Fast and efficient model loading\n+\n+Loading a model is faster and uses less memory because the loader knows which tensors are required for operations and schedules their materialization lazily.\n+\n+The loader scans the checkpoint *once* to discover pattern matches and collect tensors. It stores them as `Future` objects and submits them to a thread pool for asynchronous loading without blocking the GIL. A parameter starts loading as soon as a thread becomes available to it.\n+\n+If your system runs other heavy processes, multiple threads may slow down loading instead of accelerating it. In this case, set the environment variable `HF_DEACTIVATE_ASYNC_LOAD=1` to load weights sequentially.\n+\n+> [!NOTE]\n+> The default is 4 threads for asynchronous parameter loading. This provides the best trade-off across loading scenarios and hardware. The work is mostly I/O bound, but depending on accelerator hardware and the `dtype` required at loading, it can become CPU/GPU-bound if the `dtype` differs from the serialized one (this requires an additional copy operation).\n+\n+When converting a weight, the converter waits for all required tensors to materialize if they haven't loaded yet. For example, the [`MergeModulelist`] operation requires all weights in `ModuleList` to be loaded before merging.\n+\n+Concatenating tensors requires a temporary copy, so operations like [`MergeModulelist`] and [`Concatenate`] need 2x the memory of the underlying tensors during conversion. Once merged, only the resulting tensor stays in memory. The theoretical worst-case memory peak is the model size plus the tensors required for the largest [`MergeModulelist`] or [`Concatenate`] operation.\n+\n+This worst case only occurs when all other parameters have loaded before the demanding conversion runs. Two scenarios trigger this.\n+\n+1. All parameters loaded asynchronously before entering the demanding conversion (the thread pool was faster than the conversion queue).\n+2. The demanding conversion is the last one.\n+\n+For example, a MoE model using [`MergeModulelist`] for experts on each layer, the theoretical worst-case memory peak is model size plus experts on one layer.\n+\n+These worst-case scenarios are uncommon. The actual memory peak tends to stay close to the model size."
        },
        {
            "sha": "e4dbf7dddc80e71d73eb3ce18369d1c87910b6ed",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1aab1e9ceb527d6ce4ab77df55854037a62ddae3/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=1aab1e9ceb527d6ce4ab77df55854037a62ddae3",
            "patch": "@@ -441,6 +441,15 @@\n         \"convert_and_export_with_cache\",\n     ]\n \n+    _import_structure[\"core_model_loading\"] = [\n+        \"Chunk\",\n+        \"Concatenate\",\n+        \"ConversionOps\",\n+        \"MergeModulelist\",\n+        \"PermuteForRope\",\n+        \"SplitModulelist\",\n+        \"WeightConverter\",\n+    ]\n     _import_structure[\"modeling_flash_attention_utils\"] = []\n     _import_structure[\"modeling_layers\"] = [\"GradientCheckpointingLayer\"]\n     _import_structure[\"modeling_outputs\"] = []\n@@ -494,6 +503,13 @@\n     from .configuration_utils import PretrainedConfig as PretrainedConfig\n     from .convert_slow_tokenizer import SLOW_TO_FAST_CONVERTERS as SLOW_TO_FAST_CONVERTERS\n     from .convert_slow_tokenizer import convert_slow_tokenizer as convert_slow_tokenizer\n+    from .core_model_loading import Chunk as Chunk\n+    from .core_model_loading import Concatenate as Concatenate\n+    from .core_model_loading import ConversionOps as ConversionOps\n+    from .core_model_loading import MergeModulelist as MergeModulelist\n+    from .core_model_loading import PermuteForRope as PermuteForRope\n+    from .core_model_loading import SplitModulelist as SplitModulelist\n+    from .core_model_loading import WeightConverter as WeightConverter\n \n     # Data\n     from .data import DataProcessor as DataProcessor"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 155,
        "deletions": 0
    }
}