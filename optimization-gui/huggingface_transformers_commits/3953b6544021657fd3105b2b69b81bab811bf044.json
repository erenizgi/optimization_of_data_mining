{
    "author": "hmellor",
    "message": "Reinstate early CUDA init fix (#41617)\n\n* Reinstate early CUDA init fix\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* Delay import further\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n---------\n\nSigned-off-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
    "sha": "3953b6544021657fd3105b2b69b81bab811bf044",
    "files": [
        {
            "sha": "863050f1e72bf5c4919dcfa485b6b8458d4ed908",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/3953b6544021657fd3105b2b69b81bab811bf044/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3953b6544021657fd3105b2b69b81bab811bf044/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=3953b6544021657fd3105b2b69b81bab811bf044",
            "patch": "@@ -30,7 +30,6 @@\n from ...configuration_utils import PretrainedConfig\n from ...generation.configuration_utils import GenerationConfig\n from ...generation.logits_process import LogitsProcessor\n-from ...integrations.hub_kernels import load_and_register_attn_kernel\n from ...utils.logging import logging\n from ...utils.metrics import ContinuousBatchProcessorMetrics, attach_tracer, traced\n from .cache import PagedAttentionCache\n@@ -743,7 +742,9 @@ def __init__(\n             from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n \n             if attn_implementation not in ALL_ATTENTION_FUNCTIONS._global_mapping:  # when its a kernel\n+                # load_and_register_attn_kernel is imported here to avoid CUDA init\n                 from ...integrations.flash_paged import paged_attention_forward\n+                from ...integrations.hub_kernels import load_and_register_attn_kernel\n \n                 load_and_register_attn_kernel(attn_implementation, paged_attention_forward)\n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}