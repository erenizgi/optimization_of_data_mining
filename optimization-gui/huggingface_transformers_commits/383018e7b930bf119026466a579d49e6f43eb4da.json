{
    "author": "ydshieh",
    "message": "Remove doc files of other langs for deleted models (#42276)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "383018e7b930bf119026466a579d49e6f43eb4da",
    "files": [
        {
            "sha": "e2d4eefd154850c6c52406cfa8bb48779e603204",
            "filename": "docs/source/ja/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/383018e7b930bf119026466a579d49e6f43eb4da/docs%2Fsource%2Fja%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/383018e7b930bf119026466a579d49e6f43eb4da/docs%2Fsource%2Fja%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2F_toctree.yml?ref=383018e7b930bf119026466a579d49e6f43eb4da",
            "patch": "@@ -252,8 +252,6 @@\n         title: Blenderbot Small\n       - local: model_doc/bloom\n         title: BLOOM\n-      - local: model_doc/bort\n-        title: BORT\n       - local: model_doc/byt5\n         title: ByT5\n       - local: model_doc/camembert\n@@ -297,8 +295,6 @@\n         title: Deformable DETR\n       - local: model_doc/deit\n         title: DeiT\n-      - local: model_doc/deta\n-        title: DETA\n       - local: model_doc/detr\n         title: DETR\n       - local: model_doc/dinat"
        },
        {
            "sha": "185187219e74f6e036e50c20297b37750230b74d",
            "filename": "docs/source/ja/model_doc/bort.md",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fbort.md?ref=95d75eb7b16edda3437aec315fadd32b40684f8a",
            "patch": "@@ -1,55 +0,0 @@\n-<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# BORT\n-\n-<Tip warning={true}>\n-\n-このモデルはメンテナンス モードのみであり、コードを変更する新しい PR は受け付けられません。\n-\n-このモデルの実行中に問題が発生した場合は、このモデルをサポートしていた最後のバージョン (v4.30.0) を再インストールしてください。\n-これを行うには、コマンド `pip install -U Transformers==4.30.0` を実行します。\n-\n-</Tip>\n-\n-## Overview\n-\n-BORT モデルは、[Optimal Subarchitecture Extraction for BERT](https://huggingface.co/papers/2010.10499) で提案されました。\n-Adrian de Wynter and Daniel J. Perry.これは、BERT のアーキテクチャ パラメータの最適なサブセットです。\n-著者は「ボルト」と呼んでいます。\n-\n-論文の要約は次のとおりです。\n-\n-*Devlin らから BERT アーキテクチャのアーキテクチャ パラメータの最適なサブセットを抽出します。 (2018)\n-ニューラル アーキテクチャ検索のアルゴリズムにおける最近の画期的な技術を適用します。この最適なサブセットを次のように呼びます。\n-\"Bort\" は明らかに小さく、有効 (つまり、埋め込み層を考慮しない) サイズは 5.5% です。\n-オリジナルの BERT 大規模アーキテクチャ、およびネット サイズの 16%。 Bort は 288 GPU 時間で事前トレーニングすることもできます。\n-最高パフォーマンスの BERT パラメトリック アーキテクチャ バリアントである RoBERTa-large の事前トレーニングに必要な時間の 1.2%\n-(Liu et al., 2019)、同じマシンで BERT-large をトレーニングするのに必要な GPU 時間の世界記録の約 33%\n-ハードウェア。また、CPU 上で 7.9 倍高速であるだけでなく、他の圧縮バージョンよりもパフォーマンスが優れています。\n-アーキテクチャ、および一部の非圧縮バリアント: 0.3% ～ 31% のパフォーマンス向上が得られます。\n-BERT-large に関して、複数の公開自然言語理解 (NLU) ベンチマークにおける絶対的な評価。*\n-\n-このモデルは [stefan-it](https://huggingface.co/stefan-it) によって提供されました。元のコードは[ここ](https://github.com/alexa/bort/)にあります。\n-\n-## Usage tips\n-\n-- BORT のモデル アーキテクチャは BERT に基づいています。詳細については、[BERT のドキュメント ページ](bert) を参照してください。\n-  モデルの API リファレンスと使用例。\n-- BORT は BERT トークナイザーの代わりに RoBERTa トークナイザーを使用します。トークナイザーの API リファレンスと使用例については、[RoBERTa のドキュメント ページ](roberta) を参照してください。\n-- BORT には、 [Agora](https://adewynter.github.io/notes/bort_algorithms_and_applications.html#fine-tuning-with-algebraic-topology) と呼ばれる特定の微調整アルゴリズムが必要です。\n-  残念ながらまだオープンソース化されていません。誰かが実装しようとすると、コミュニティにとって非常に役立ちます。\n-  BORT の微調整を機能させるためのアルゴリズム。\n\\ No newline at end of file"
        },
        {
            "sha": "7c8a5687ba5344ff028f393b16add08c9c70d9a8",
            "filename": "docs/source/ja/model_doc/deta.md",
            "status": "removed",
            "additions": 0,
            "deletions": 64,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fdeta.md?ref=95d75eb7b16edda3437aec315fadd32b40684f8a",
            "patch": "@@ -1,64 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# DETA\n-\n-## Overview\n-\n-DETA モデルは、[NMS Strikes Back](https://huggingface.co/papers/2212.06137) で Jeffrey Ouyang-Zhang、Jang Hyun Cho、Xingyi Zhou、Philipp Krähenbühl によって提案されました。\n-DETA (Detection Transformers with Assignment の略) は、1 対 1 の 2 部ハンガリアン マッチング損失を置き換えることにより、[Deformable DETR](deformable_detr) を改善します。\n-非最大抑制 (NMS) を備えた従来の検出器で使用される 1 対多のラベル割り当てを使用します。これにより、最大 2.5 mAP の大幅な増加が得られます。\n-\n-論文の要約は次のとおりです。\n-\n-*Detection Transformer (DETR) は、トレーニング中に 1 対 1 の 2 部マッチングを使用してクエリを一意のオブジェクトに直接変換し、エンドツーエンドのオブジェクト検出を可能にします。最近、これらのモデルは、紛れもない優雅さで COCO の従来の検出器を上回りました。ただし、モデル アーキテクチャやトレーニング スケジュールなど、さまざまな設計において従来の検出器とは異なるため、1 対 1 マッチングの有効性は完全には理解されていません。この研究では、DETR での 1 対 1 のハンガリー語マッチングと、非最大監視 (NMS) を備えた従来の検出器での 1 対多のラベル割り当てとの間の厳密な比較を行います。驚くべきことに、NMS を使用した 1 対多の割り当ては、同じ設定の下で標準的な 1 対 1 のマッチングよりも一貫して優れており、最大 2.5 mAP という大幅な向上が見られます。従来の IoU ベースのラベル割り当てを使用して Deformable-DETR をトレーニングする当社の検出器は、ResNet50 バックボーンを使用して 12 エポック (1x スケジュール) 以内に 50.2 COCO mAP を達成し、この設定で既存のすべての従来の検出器またはトランスベースの検出器を上回りました。複数のデータセット、スケジュール、アーキテクチャに関して、私たちは一貫して、パフォーマンスの高い検出トランスフォーマーには二部マッチングが不要であることを示しています。さらに、検出トランスの成功は、表現力豊かなトランス アーキテクチャによるものであると考えています。*\n-\n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/deta_architecture.jpg\"\n-alt=\"drawing\" width=\"600\"/>\n-\n-<small> DETA の概要。 <a href=\"https://huggingface.co/papers/2212.06137\">元の論文</a>から抜粋。 </small>\n-\n-このモデルは、[nielsr](https://huggingface.co/nielsr) によって提供されました。\n-元のコードは [ここ](https://github.com/jozhang97/DETA) にあります。\n-\n-## Resources\n-\n-DETA の使用を開始するのに役立つ公式 Hugging Face およびコミュニティ (🌎 で示されている) リソースのリスト。\n-\n-- DETA のデモ ノートブックは [こちら](https://github.com/NielsRogge/Transformers-Tutorials/tree/master/DETA) にあります。\n-- 参照: [オブジェクト検出タスク ガイド](../tasks/object_detection)\n-\n-ここに含めるリソースの送信に興味がある場合は、お気軽にプル リクエストを開いてください。審査させていただきます。リソースは、既存のリソースを複製するのではなく、何か新しいものを示すことが理想的です。\n-\n-## DetaConfig\n-\n-[[autodoc]] DetaConfig\n-\n-## DetaImageProcessor\n-\n-[[autodoc]] DetaImageProcessor\n-    - preprocess\n-    - post_process_object_detection\n-\n-## DetaModel\n-\n-[[autodoc]] DetaModel\n-    - forward\n-\n-## DetaForObjectDetection\n-\n-[[autodoc]] DetaForObjectDetection\n-    - forward"
        },
        {
            "sha": "f957f48d37e01cdaadaef3b13cd2e5c401aa0d99",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/383018e7b930bf119026466a579d49e6f43eb4da/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/383018e7b930bf119026466a579d49e6f43eb4da/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=383018e7b930bf119026466a579d49e6f43eb4da",
            "patch": "@@ -1193,8 +1193,6 @@\n     - sections:\n       - local: in_translation\n         title: Decision Transformer\n-      - local: model_doc/trajectory_transformer\n-        title: Trajectory Transformer\n       title: 강화학습 모델\n     - sections:\n       - local: model_doc/autoformer\n@@ -1210,10 +1208,6 @@\n       - local: in_translation\n         title: TimesFM\n       title: 시게열 모델\n-    - sections:\n-      - local: model_doc/graphormer\n-        title: Graphormer\n-      title: 그래프 모델\n     title: 모델\n   - sections:\n     - local: internal/modeling_utils"
        },
        {
            "sha": "9e1a893fc5f6388362eac9fecbd105bc271917a6",
            "filename": "docs/source/ko/model_doc/graphormer.md",
            "status": "removed",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fko%2Fmodel_doc%2Fgraphormer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fko%2Fmodel_doc%2Fgraphormer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgraphormer.md?ref=95d75eb7b16edda3437aec315fadd32b40684f8a",
            "patch": "@@ -1,52 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team and Microsoft. All rights reserved.\n-\n-Licensed under the MIT License; you may not use this file except in compliance with\n-the License.\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Graphormer[[graphormer]]\n-\n-<Tip warning={true}>\n-\n-이 모델은 유지 보수 모드로만 운영되며, 코드를 변경하는 새로운 PR(Pull Request)은 받지 않습니다.\n-이 모델을 실행하는 데 문제가 발생한다면, 이 모델을 지원하는 마지막 버전인 v4.40.2를 다시 설치해 주세요. 다음 명령어를 실행하여 재설치할 수 있습니다: `pip install -U transformers==4.40.2`.\n-\n-</Tip>\n-\n-## 개요[[overview]]\n-\n-Graphormer 모델은 Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, Tie-Yan Liu가 제안한 [트랜스포머가 그래프 표현에 있어서 정말 약할까?](https://huggingface.co/papers/2106.05234) 라는 논문에서 소개되었습니다. Graphormer는 그래프 트랜스포머 모델입니다. 텍스트 시퀀스 대신 그래프에서 계산을 수행할 수 있도록 수정되었으며, 전처리와 병합 과정에서 임베딩과 관심 특성을 생성한 후 수정된 어텐션을 사용합니다.\n-\n-해당 논문의 초록입니다:\n-\n-*트랜스포머 아키텍처는 자연어 처리와 컴퓨터 비전 등 많은 분야에서 지배적인 선택을 받고 있는 아키텍처 입니다. 그러나 그래프 수준 예측 리더보드 상에서는 주류 GNN 변형모델들에 비해 경쟁력 있는 성능을 달성하지 못했습니다. 따라서 트랜스포머가 그래프 표현 학습에서 어떻게 잘 수행될 수 있을지는 여전히 미스터리였습니다. 본 논문에서는 Graphormer를 제시함으로써 이 미스터리를 해결합니다. Graphormer는 표준 트랜스포머 아키텍처를 기반으로 구축되었으며, 특히 최근의 OpenGraphBenchmark Large-Scale Challenge(OGB-LSC)의 광범위한 그래프 표현 학습 작업에서 탁월한 결과를 얻을 수 있었습니다. 그래프에서 트랜스포머를 활용하는데 핵심은 그래프의 구조적 정보를 모델에 효과적으로 인코딩하는 것입니다. 이를 위해 우리는 Graphormer가 그래프 구조 데이터를 더 잘 모델링할 수 있도록 돕는 몇 가지 간단하면서도 효과적인 구조적 인코딩 방법을 제안합니다. 또한, 우리는 Graphormer의 표현을 수학적으로 특성화하고, 그래프의 구조적 정보를 인코딩하는 우리의 방식으로 많은 인기 있는 GNN 변형모델들이 Graphormer의 특수한 경우로 포함될 수 있음을 보여줍니다.*\n-\n-이 모델은 [clefourrier](https://huggingface.co/clefourrier)가 기여했습니다. 원본 코드는 [이곳](https://github.com/microsoft/Graphormer)에서 확인할 수 있습니다.\n-\n-## 사용 팁[[usage-tips]]\n-\n-이 모델은 큰 그래프(100개 이상의 노드개수/엣지개수)에서는 메모리 사용량이 폭발적으로 증가하므로 잘 작동하지 않습니다. 대안으로 배치 크기를 줄이거나, RAM을 늘리거나 또는 algos_graphormer.pyx 파일의 `UNREACHABLE_NODE_DISTANCE` 매개변수를 줄이는 방법도 있지만, 700개 이상의 노드개수/엣지개수를 처리하기에는 여전히 어려울 것입니다.\n-\n-이 모델은 토크나이저를 사용하지 않고, 대신 훈련 중에 특별한 콜레이터(collator)를 사용합니다.\n-\n-## GraphormerConfig[[transformers.GraphormerConfig]]\n-\n-[[autodoc]] GraphormerConfig\n-\n-## GraphormerModel[[transformers.GraphormerModel]]\n-\n-[[autodoc]] GraphormerModel\n-    - forward\n-\n-## GraphormerForGraphClassification[[transformers.GraphormerForGraphClassification]]\n-\n-[[autodoc]] GraphormerForGraphClassification\n-    - forward"
        },
        {
            "sha": "9f72a6f71e6ddd98c108af235f94ef64829bed64",
            "filename": "docs/source/ko/model_doc/trajectory_transformer.md",
            "status": "removed",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fko%2Fmodel_doc%2Ftrajectory_transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/95d75eb7b16edda3437aec315fadd32b40684f8a/docs%2Fsource%2Fko%2Fmodel_doc%2Ftrajectory_transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Ftrajectory_transformer.md?ref=95d75eb7b16edda3437aec315fadd32b40684f8a",
            "patch": "@@ -1,49 +0,0 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-⚠️ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# 궤적 트랜스포머[[trajectory-transformer]]\n-\n-<Tip warning={true}>\n-\n-\n-이 모델은 유지 보수 모드로만 운영되며, 코드를 변경하는 새로운 PR(Pull Request)은 받지 않습니다.\n-이 모델을 실행하는 데 문제가 발생한다면, 이 모델을 지원하는 마지막 버전인 v4.30.0를 다시 설치해 주세요. 다음 명령어를 실행하여 재설치할 수 있습니다: `pip install -U transformers==4.30.0`.\n-\n-</Tip>\n-\n-## 개요[[overview]]\n-\n-Trajectory Transformer 모델은 Michael Janner, Qiyang Li, Sergey Levine이 제안한 [하나의 커다란 시퀀스 모델링 문제로서의 오프라인 강화학습](https://huggingface.co/papers/2106.02039)라는 논문에서 소개되었습니다.\n-\n-해당 논문의 초록입니다:\n-\n-*강화학습(RL)은 일반적으로 마르코프 속성을 활용하여 시간에 따라 문제를 인수분해하면서 정적 정책이나 단일 단계 모델을 추정하는 데 중점을 둡니다. 하지만 우리는 RL을 높은 보상 시퀀스로 이어지는 행동 시퀀스를 생성하는 것을 목표로 하는 일반적인 시퀀스 모델링 문제로 볼 수도 있습니다. 이러한 관점에서, 자연어 처리와 같은 다른 도메인에서 잘 작동하는 고용량 시퀀스 예측 모델이 RL 문제에도 효과적인 해결책을 제공할 수 있는지 고려해 볼 만합니다. 이를 위해 우리는 RL을 시퀀스 모델링의 도구로 어떻게 다룰 수 있는지 탐구하며, 트랜스포머 아키텍처를 사용하여 궤적에 대한 분포를 모델링하고 빔 서치를 계획 알고리즘으로 재활용합니다. RL을 시퀀스 모델링 문제로 프레임화하면 다양한 설계 결정이 단순화되어, 오프라인 RL 알고리즘에서 흔히 볼 수 있는 많은 구성 요소를 제거할 수 있습니다. 우리는 이 접근 방식의 유연성을 장기 동역학 예측, 모방 학습, 목표 조건부 RL, 오프라인 RL에 걸쳐 입증합니다. 더 나아가, 이 접근 방식을 기존의 모델 프리 알고리즘과 결합하여 희소 보상, 장기 과제에서 최신 계획기(planner)를 얻을 수 있음을 보여줍니다.*\n-\n-이 모델은 [CarlCochet](https://huggingface.co/CarlCochet)에 의해 기여되었습니다.\n-원본 코드는 [이곳](https://github.com/jannerm/trajectory-transformer)에서 확인할 수 있습니다.\n-\n-## 사용 팁[[usage-tips]]\n-\n-이 트랜스포머는 심층 강화학습에 사용됩니다. 사용하려면 이전의 모든 타임스텝에서의 행동, 상태, 보상으로부터 시퀀스를 생성해야 합니다. 이 모델은 이 모든 요소를 함께 하나의 큰 시퀀스(궤적)로 취급합니다.\n-\n-## TrajectoryTransformerConfig[[transformers.TrajectoryTransformerConfig]]\n-\n-[[autodoc]] TrajectoryTransformerConfig\n-\n-## TrajectoryTransformerModel[[transformers.TrajectoryTransformerModel]]\n-\n-[[autodoc]] TrajectoryTransformerModel\n-    - forward"
        }
    ],
    "stats": {
        "total": 230,
        "additions": 0,
        "deletions": 230
    }
}