{
    "author": "MekkCyber",
    "message": "[Quantization] Fixing some tests (#42763)\n\n* fix\n\n* skip",
    "sha": "8f978e5b4ed99efa8603249765bead48baa4d8de",
    "files": [
        {
            "sha": "85ecbefb27a2ed72935ed51d135878b23c8a305e",
            "filename": "src/transformers/quantizers/quantizer_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f978e5b4ed99efa8603249765bead48baa4d8de/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f978e5b4ed99efa8603249765bead48baa4d8de/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bitnet.py?ref=8f978e5b4ed99efa8603249765bead48baa4d8de",
            "patch": "@@ -81,7 +81,6 @@ def _process_model_before_weight_loading(\n             model,\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n-            pre_quantized=self.pre_quantized,\n         )\n \n     def adjust_max_memory(self, max_memory: dict[str, int | str]) -> dict[str, int | str]:"
        },
        {
            "sha": "878e4ecc01a99f538adf42c4a0152a4707c52b79",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f978e5b4ed99efa8603249765bead48baa4d8de/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f978e5b4ed99efa8603249765bead48baa4d8de/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=8f978e5b4ed99efa8603249765bead48baa4d8de",
            "patch": "@@ -722,13 +722,13 @@ def test_serialization(self, quant_type=\"nf4\", double_quant=True):\n                         self.assertTrue(v0 == v1)\n \n         # comparing forward() outputs\n-        encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = tokenizer(self.input_text, return_tensors=\"pt\", return_token_type_ids=False).to(torch_device)\n         out_0 = model_0(**encoded_input)\n         out_1 = model_1(**encoded_input)\n         torch.testing.assert_close(out_0[\"logits\"], out_1[\"logits\"], rtol=0.05, atol=0.05)\n \n         # comparing generate() outputs\n-        encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n+        encoded_input = tokenizer(self.input_text, return_tensors=\"pt\", return_token_type_ids=False).to(torch_device)\n         output_sequences_0 = model_0.generate(**encoded_input, max_new_tokens=10)\n         output_sequences_1 = model_1.generate(**encoded_input, max_new_tokens=10)\n "
        },
        {
            "sha": "e04d96d7c394e7b265de15466fa20e59a8dafca6",
            "filename": "tests/quantization/torchao_integration/test_torchao.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8f978e5b4ed99efa8603249765bead48baa4d8de/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8f978e5b4ed99efa8603249765bead48baa4d8de/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ftorchao_integration%2Ftest_torchao.py?ref=8f978e5b4ed99efa8603249765bead48baa4d8de",
            "patch": "@@ -677,7 +677,7 @@ def test_autoquant(self):\n         self.assertIn(tokenizer.decode(output[0], skip_special_tokens=True), EXPECTED_OUTPUTS)\n \n \n-@require_torchao_version_greater_or_equal(\"0.11.0\")\n+@require_torchao_version_greater_or_equal(\"0.15.0\")\n @slow\n class TorchAoSerializationTest(unittest.TestCase):\n     input_text = \"What are we having for dinner?\""
        }
    ],
    "stats": {
        "total": 7,
        "additions": 3,
        "deletions": 4
    }
}