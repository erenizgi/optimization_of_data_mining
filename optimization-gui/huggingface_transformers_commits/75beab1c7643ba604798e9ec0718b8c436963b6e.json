{
    "author": "YangKai0616",
    "message": "Fixed paged|FA2 kernel loading logic and UT. (#42547)\n\n* Fixed UT and kernel loading logic.\n\n* Revision based on comments\n\n* Simplify code\n\n* make style\n\n* simplify CB part\n\n* retrigger ci\n\n---------\n\nCo-authored-by: vasqu <antonprogamer@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "75beab1c7643ba604798e9ec0718b8c436963b6e",
    "files": [
        {
            "sha": "5c0951ef8164ef35ce16bfb491fc7e9e823b453f",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/75beab1c7643ba604798e9ec0718b8c436963b6e/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75beab1c7643ba604798e9ec0718b8c436963b6e/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=75beab1c7643ba604798e9ec0718b8c436963b6e",
            "patch": "@@ -763,15 +763,9 @@ def __init__(\n             num_kv_padding_intervals: (optional) Number of intervals used to pad the keys/values dimension\n             allow_prefix_sharing: (optional) Whether to allow prefix sharing if the model has only full attention layers\n         \"\"\"\n+        # Reloade paged version if necessary\n         if \"paged|\" not in model.config._attn_implementation:\n-            attn_implementation = f\"paged|{model.config._attn_implementation}\"\n-            model.config._attn_implementation = attn_implementation\n-\n-            # lazy loading flash attention including kernel variations\n-            if \"flash\" in attn_implementation:\n-                from ...modeling_flash_attention_utils import lazy_import_paged_flash_attention\n-\n-                lazy_import_paged_flash_attention(attn_implementation)\n+            model.set_attn_implementation(f\"paged|{model.config._attn_implementation}\")\n \n         self.model = model.eval()\n         generation_config = model.generation_config if generation_config is None else generation_config"
        },
        {
            "sha": "86baefb1792fde606b00be0304e0e784078df094",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/75beab1c7643ba604798e9ec0718b8c436963b6e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75beab1c7643ba604798e9ec0718b8c436963b6e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=75beab1c7643ba604798e9ec0718b8c436963b6e",
            "patch": "@@ -85,7 +85,7 @@\n     verify_tp_plan,\n )\n from .loss.loss_utils import LOSS_MAPPING\n-from .modeling_flash_attention_utils import lazy_import_flash_attention\n+from .modeling_flash_attention_utils import lazy_import_flash_attention, lazy_import_paged_flash_attention\n from .pytorch_utils import id_tensor_storage\n from .quantizers import HfQuantizer\n from .quantizers.auto import get_hf_quantizer\n@@ -1763,9 +1763,12 @@ def _check_and_adjust_attn_implementation(\n         \"\"\"\n         applicable_attn_implementation = attn_implementation\n \n+        is_paged = attn_implementation is not None and attn_implementation.startswith(\"paged|\")\n+\n         # If FA not installed, do not fail but use kernels instead\n         requested_original_flash_attn = attn_implementation is not None and (\n-            attn_implementation == \"flash_attention_2\" or attn_implementation == \"flash_attention_3\"\n+            attn_implementation.removeprefix(\"paged|\") == \"flash_attention_2\"\n+            or attn_implementation.removeprefix(\"paged|\") == \"flash_attention_3\"\n         )\n         if (\n             requested_original_flash_attn\n@@ -1783,10 +1786,16 @@ def _check_and_adjust_attn_implementation(\n             else:\n                 applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n \n+            if is_paged:\n+                applicable_attn_implementation = f\"paged|{applicable_attn_implementation}\"\n+\n         if is_kernel(applicable_attn_implementation):\n             try:\n                 # preload flash attention here to allow compile with fullgraph\n-                lazy_import_flash_attention(applicable_attn_implementation)\n+                if is_paged:\n+                    lazy_import_paged_flash_attention(applicable_attn_implementation)\n+                else:\n+                    lazy_import_flash_attention(applicable_attn_implementation)\n \n                 # log that we used kernel fallback if successful\n                 if requested_original_flash_attn:"
        },
        {
            "sha": "07691894203f1ca0104f0ded741c7fef8df69cec",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/75beab1c7643ba604798e9ec0718b8c436963b6e/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/75beab1c7643ba604798e9ec0718b8c436963b6e/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=75beab1c7643ba604798e9ec0718b8c436963b6e",
            "patch": "@@ -22,6 +22,7 @@\n from transformers.generation.continuous_batching.continuous_api import build_attention_mask\n from transformers.testing_utils import (\n     Expectations,\n+    require_deterministic_for_xpu,\n     require_kernels,\n     require_read_token,\n     require_torch_accelerator,\n@@ -137,6 +138,7 @@ def test_attention_mask(\n                 f\"Actual mask:\\n{str_mask}\"\n             )\n \n+    @require_deterministic_for_xpu\n     def _continuous_batching_parity(\n         self, model_id: str, attn_implementation: str, expected_outputs: dict[str, str]\n     ) -> None:"
        }
    ],
    "stats": {
        "total": 27,
        "additions": 16,
        "deletions": 11
    }
}