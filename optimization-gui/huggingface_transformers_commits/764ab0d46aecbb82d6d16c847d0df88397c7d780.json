{
    "author": "cyyever",
    "message": "Merge tensor operations with device transfer operations (#37097)\n\n* Merge operations with to\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Use dtype\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "764ab0d46aecbb82d6d16c847d0df88397c7d780",
    "files": [
        {
            "sha": "3938deb4826025834967ec7782b47242c5d77899",
            "filename": "src/transformers/generation/beam_search.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Fbeam_search.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fbeam_search.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -724,7 +724,7 @@ def step_sentence_constraint(\n             advance_state.reset(pre_seq.tolist())\n \n             if not advance_state.completed:\n-                advance_tokens = torch.LongTensor(advance_state.advance()).to(device)\n+                advance_tokens = torch.tensor(advance_state.advance(), dtype=torch.long, device=device)\n                 for advance_token in advance_tokens:\n                     # since adding each `advance_token` leads to a different hypothesis, create new state instance.\n                     new_state = advance_state.copy(stateful=True)\n@@ -775,14 +775,14 @@ def step_sentence_constraint(\n                     track_new[\"new_states\"].append(advance_state)\n \n         if len(track_new[\"new_indices\"]) > 0:\n-            new_indices = torch.tensor(track_new[\"new_indices\"]).to(device)\n+            new_indices = torch.tensor(track_new[\"new_indices\"], device=device)\n             new_tokens = torch.stack(track_new[\"new_tokens\"]).to(device)\n             new_scores = torch.stack(track_new[\"new_scores\"]).to(device)\n \n             all_states = topk_contraint_states + track_new[\"new_states\"]\n             all_tokens = torch.cat((sent_beam_tokens, new_tokens), -1)\n             all_scores = torch.cat((sent_beam_scores, new_scores), -1)\n-            all_banks = torch.tensor([one.get_bank() for one in all_states]).to(device)\n+            all_banks = torch.tensor([one.get_bank() for one in all_states], device=device)\n \n             zipped = all_banks * 100 + all_scores\n             indices = zipped.sort(descending=True).indices"
        },
        {
            "sha": "fe57f532e68750af1e38bb0387dabf594b3d3b82",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -719,7 +719,9 @@ def get_target_logits(self, assistant_logits: torch.FloatTensor) -> torch.FloatT\n         \"\"\"\n \n         target_shape: tuple[int, ...] = (*assistant_logits.shape[:-1], self.target_vocab_size)\n-        target_logits: torch.FloatTensor = torch.full(target_shape, self.FILTER_VALUE).to(self._assistant_model_device)\n+        target_logits: torch.FloatTensor = torch.full(\n+            target_shape, self.FILTER_VALUE, device=self._assistant_model_device\n+        )\n         # Mask for valid indices\n         assistant_indices_mask = self._assistant_to_target_input_ids != self.SUPPRESS_TOKEN_ID\n         # Exclude invalid indices"
        },
        {
            "sha": "16c04478f08a64bbd1b65af5e720f1470940833b",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1157,7 +1157,7 @@ def _prepare_bias_variables(self, scores: torch.FloatTensor):\n \n         # Precompute the bias tensors to be applied. Sequences of length 1 are kept separately, as they can be applied\n         # with simpler logic.\n-        self.length_1_bias = torch.zeros((vocabulary_size,), dtype=torch.float).to(scores.device)\n+        self.length_1_bias = torch.zeros((vocabulary_size,), dtype=torch.float, device=scores.device)\n         for sequence_ids, bias in self.sequence_bias.items():\n             if len(sequence_ids) == 1:\n                 self.length_1_bias[sequence_ids[-1]] = bias"
        },
        {
            "sha": "232cceeedf6d1642cd4d345811d17ca65f93d4d1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -2599,7 +2599,7 @@ def _has_unfinished_sequences(self, this_peer_finished: bool, synced_gpus: bool,\n         if synced_gpus:\n             # Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\n             # The following logic allows an early break if all peers finished generating their sequence\n-            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0).to(device)\n+            this_peer_finished_flag = torch.tensor(0.0 if this_peer_finished else 1.0, device=device)\n             # send 0.0 if we finished, 1.0 otherwise\n             dist.all_reduce(this_peer_finished_flag, op=dist.ReduceOp.SUM)\n             # did all peers finish? the reduced sum will be 0.0 then"
        },
        {
            "sha": "8cf2c3bd3ea629bede1df054fbfda115370567c2",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -64,7 +64,7 @@ def _compute_default_rope_parameters(\n     attention_factor = 1.0  # Unused in this type of RoPE\n \n     # Compute the inverse frequencies\n-    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n+    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n     return inv_freq, attention_factor\n \n \n@@ -156,7 +156,7 @@ def _compute_dynamic_ntk_parameters(\n \n     # Compute the inverse frequencies\n     base = base * ((factor * seq_len / max_position_embeddings) - (factor - 1)) ** (dim / (dim - 2))\n-    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n+    inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / dim))\n     return inv_freq, attention_factor\n \n \n@@ -241,14 +241,14 @@ def linear_ramp_factor(min, max, dim):\n \n     # Note on variable naming: \"interpolation\" comes from the original technique, where we interpolate the position IDs\n     # to expand the possible context length. In other words, interpolation = apply scaling factor.\n-    pos_freqs = base ** (torch.arange(0, dim, 2).float().to(device) / dim)\n+    pos_freqs = base ** (torch.arange(0, dim, 2).to(device=device, dtype=torch.float) / dim)\n     inv_freq_extrapolation = 1.0 / pos_freqs\n     inv_freq_interpolation = 1.0 / (factor * pos_freqs)\n \n     low, high = find_correction_range(beta_fast, beta_slow, dim, base, original_max_position_embeddings)\n \n     # Get n-dimensional rotational scaling corrected for extrapolation\n-    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).float().to(device)\n+    inv_freq_extrapolation_factor = 1 - linear_ramp_factor(low, high, dim // 2).to(device=device, dtype=torch.float)\n     inv_freq = (\n         inv_freq_interpolation * (1 - inv_freq_extrapolation_factor)\n         + inv_freq_extrapolation * inv_freq_extrapolation_factor"
        },
        {
            "sha": "35c1730f1e9067bb2ac2e3ec8a38ed3ff124f248",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -783,7 +783,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "5af06951c647c8621fc9baf11c1dac63ec4f663d",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -173,7 +173,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "e5b7c0a49a3d00c2146c3832c393feb81f9e2ab4",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -897,7 +897,7 @@ def generate(\n         # pass input_ids in order to stay consistent with the transformers generate method even though it is not used\n         # (except to get the input seq_len - that's why we keep the first 257 tokens)\n         semantic_output = super().generate(\n-            torch.ones((batch_size, max_input_semantic_length + 1), dtype=torch.int).to(self.device),\n+            torch.ones((batch_size, max_input_semantic_length + 1), dtype=torch.int, device=self.device),\n             input_embeds=input_embeds,\n             logits_processor=[suppress_tokens_logits_processor, early_stopping_logits_processor],\n             generation_config=semantic_generation_config,\n@@ -989,8 +989,8 @@ def preprocess_histories(\n \n         else:\n             # shape: (batch_size, 0)\n-            x_semantic_history = torch.tensor([[]] * batch_size, dtype=torch.int).to(self.device)\n-            x_coarse_history = torch.tensor([[]] * batch_size, dtype=torch.int).to(self.device)\n+            x_semantic_history = torch.tensor([[]] * batch_size, dtype=torch.int, device=self.device)\n+            x_coarse_history = torch.tensor([[]] * batch_size, dtype=torch.int, device=self.device)\n \n         return x_semantic_history, x_coarse_history\n \n@@ -1097,7 +1097,7 @@ def generate(\n             input_coarse = torch.hstack(\n                 [\n                     input_coarse,\n-                    torch.tensor([[coarse_generation_config.coarse_infer_token]] * batch_size).to(self.device),\n+                    torch.tensor([[coarse_generation_config.coarse_infer_token]] * batch_size, device=self.device),\n                     x_coarse[:, -max_coarse_history:],\n                 ]\n             )"
        },
        {
            "sha": "dd9b57973c5c3323364ad483b5b514d35a393a32",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1198,7 +1198,7 @@ def generate(\n \n         image_embeds = vision_outputs[0]\n \n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n \n         if isinstance(input_ids, list):\n             input_ids = torch.LongTensor(input_ids)\n@@ -1424,7 +1424,7 @@ def generate(\n \n         image_embeds = vision_outputs[0]\n \n-        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long).to(image_embeds.device)\n+        image_attention_mask = torch.ones(image_embeds.size()[:-1], dtype=torch.long, device=image_embeds.device)\n \n         if isinstance(input_ids, list):\n             input_ids = torch.LongTensor(input_ids)\n@@ -1439,7 +1439,9 @@ def generate(\n \n         question_embeds = question_outputs[0]\n \n-        question_attention_mask = torch.ones(question_embeds.size()[:-1], dtype=torch.long).to(question_embeds.device)\n+        question_attention_mask = torch.ones(\n+            question_embeds.size()[:-1], dtype=torch.long, device=question_embeds.device\n+        )\n \n         bos_ids = torch.full(\n             (question_embeds.size(0), 1), fill_value=self.decoder_start_token_id, device=question_embeds.device"
        },
        {
            "sha": "a02868359efbd410417644b6dfb872402ed57a61",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -2498,7 +2498,7 @@ def forward(\n \n         if use_image_text_matching_head:\n             query_tokens = self.query_tokens.expand(image_embeds.shape[0], -1, -1)\n-            query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long).to(query_tokens.device)\n+            query_attention_mask = torch.ones(query_tokens.size()[:-1], dtype=torch.long, device=query_tokens.device)\n             attention_mask = torch.cat([query_attention_mask, attention_mask], dim=1)\n \n             query_embeds = self.embeddings("
        },
        {
            "sha": "ee278631f278ae2c753cdd816a3a1c53addfebf2",
            "filename": "src/transformers/models/bros/modeling_bros.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fmodeling_bros.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1158,7 +1158,7 @@ def forward(\n         subsequent_token_logits = subsequent_token_logits.masked_fill(\n             invalid_token_mask[:, None, :], torch.finfo(subsequent_token_logits.dtype).min\n         )\n-        self_token_mask = torch.eye(max_seq_length, max_seq_length + 1).to(device).bool()\n+        self_token_mask = torch.eye(max_seq_length, max_seq_length + 1).to(device=device, dtype=torch.bool)\n         subsequent_token_logits = subsequent_token_logits.masked_fill(\n             self_token_mask[None, :, :], torch.finfo(subsequent_token_logits.dtype).min\n         )\n@@ -1287,13 +1287,13 @@ def forward(\n             batch_size, max_seq_length = attention_mask.shape\n             device = attention_mask.device\n \n-            self_token_mask = torch.eye(max_seq_length, max_seq_length + 1).to(device).bool()\n+            self_token_mask = torch.eye(max_seq_length, max_seq_length + 1).to(device=device, dtype=torch.bool)\n \n             mask = bbox_first_token_mask.view(-1)\n             bbox_first_token_mask = torch.cat(\n                 [\n                     ~bbox_first_token_mask,\n-                    torch.zeros([batch_size, 1], dtype=torch.bool).to(device),\n+                    torch.zeros([batch_size, 1], dtype=torch.bool, device=device),\n                 ],\n                 axis=1,\n             )"
        },
        {
            "sha": "7ccc660aac804b7e88c07e7f66b39b57f0786827",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -95,7 +95,10 @@ def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None, s\n         self.dim = dim\n         self.max_position_embeddings = max_position_embeddings\n         self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        inv_freq = 1.0 / (\n+            self.base\n+            ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n+        )\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         # For BC we register cos and sin cached\n         self.max_seq_len_cached = max_position_embeddings\n@@ -138,7 +141,8 @@ def forward(self, x, position_ids):\n                 (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n             ) ** (self.dim / (self.dim - 2))\n             inv_freq = 1.0 / (\n-                base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(x.device) / self.dim)\n+                base\n+                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=x.device, dtype=torch.float) / self.dim)\n             )\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: this may break with compilation\n "
        },
        {
            "sha": "e9a4bbf8ae51c216448b313e9746ba2547fded1e",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -300,7 +300,7 @@ def disentangled_attention_bias(self, query_layer, key_layer, relative_pos, rel_\n             raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {relative_pos.dim()}\")\n \n         att_span = self.pos_ebd_size\n-        relative_pos = relative_pos.long().to(query_layer.device)\n+        relative_pos = relative_pos.to(device=query_layer.device, dtype=torch.long)\n \n         rel_embeddings = rel_embeddings[0 : att_span * 2, :].unsqueeze(0)\n         if self.share_att_key:"
        },
        {
            "sha": "faf24729d642336c40b2f606001b55ecc07c6344",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -233,7 +233,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             mask_value = torch.finfo(attn_weights.dtype).min\n             # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n             # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n+            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n             attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n \n         if attention_mask is not None:"
        },
        {
            "sha": "5a09e857796a2c5db4cd6097dc59c3bccd592168",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -113,7 +113,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "27c9aaa37190bc5b0e88c956525ee62264d444ab",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -237,7 +237,7 @@ def prepare_coco_panoptic_annotation(\n     new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n \n     if \"segments_info\" in target:\n-        masks = read_image(annotation_path).permute(1, 2, 0).to(torch.int32).to(image.device)\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(dtype=torch.int32, device=image.device)\n         masks = rgb_to_id(masks)\n \n         ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)"
        },
        {
            "sha": "98bc7fb70aaf5ab9ee0f60dbc59f8fb81a2188f0",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -73,7 +73,10 @@ def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         self.dim = dim\n         self.max_position_embeddings = max_position_embeddings\n         self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        inv_freq = 1.0 / (\n+            self.base\n+            ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n+        )\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n         # Build here to make `torch.jit.trace` work.\n@@ -135,7 +138,10 @@ def _set_cos_sin_cache(self, seq_len, device, dtype):\n             base = self.base * (\n                 (self.scaling_factor * seq_len / self.max_position_embeddings) - (self.scaling_factor - 1)\n             ) ** (self.dim / (self.dim - 2))\n-            inv_freq = 1.0 / (base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+            inv_freq = 1.0 / (\n+                base\n+                ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n+            )\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n         t = torch.arange(self.max_seq_len_cached, device=device, dtype=torch.int64).type_as(self.inv_freq)"
        },
        {
            "sha": "16bef79e5993cf71b3c1d3a7688da0781f199382",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -254,7 +254,7 @@ def prepare_coco_panoptic_annotation(\n     new_target[\"orig_size\"] = torch.as_tensor([image_height, image_width], dtype=torch.int64, device=image.device)\n \n     if \"segments_info\" in target:\n-        masks = read_image(annotation_path).permute(1, 2, 0).to(torch.int32).to(image.device)\n+        masks = read_image(annotation_path).permute(1, 2, 0).to(dtype=torch.int32, device=image.device)\n         masks = rgb_to_id(masks)\n \n         ids = torch.as_tensor([segment_info[\"id\"] for segment_info in target[\"segments_info\"]], device=image.device)"
        },
        {
            "sha": "b25c19384e441d9d1b429508cc303115d664f8d2",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -675,7 +675,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "e013e86632d7d20fae8e7fe18d5065bf861803d5",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1256,7 +1256,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "50638858de3ab6c32386aa5b4f10ad58c62ca13a",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -158,7 +158,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "31df29e6a086c78733ff403ec162785bf48aaa80",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -149,7 +149,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "556849d0bc4a7c549d6dbd1147b72477e2b029f4",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -396,7 +396,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "1078a01d9379ac6bde3c78b44647212d3ec6c914",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -191,7 +191,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "7d77f3f2f1514698576fcfb5099355d4c3c8ad68",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -313,7 +313,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "1af13669259642da4dfee9f168a5fceb579ffed3",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -243,7 +243,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             mask_value = torch.finfo(attn_weights.dtype).min\n             # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n             # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n+            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n             attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n \n         if attention_mask is not None:"
        },
        {
            "sha": "9cc18c0ea95b5dca27682c7038adaec3473ad0cd",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -219,7 +219,7 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n         mask_value = torch.finfo(attn_weights.dtype).min\n         # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n         # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-        mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n+        mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n         attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n \n         if attention_mask is not None:  # no matter the length, we just slice it"
        },
        {
            "sha": "220e0b6e72ecd9741b9e715d0e1c853648769cb0",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -337,7 +337,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "26ee5e392ab9c98ed0aad2146e3abe240807a0b8",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -281,7 +281,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "74bb0d054fb1025c5f4eaa34fbff127de74c94bd",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -367,7 +367,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "39441473cb141c636e249a20ef64185604876c6a",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -211,7 +211,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "29d1b598f476ee619c51f66cbf258e95f119445b",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -801,7 +801,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "6b786f656ccc831248d0a1b800127c84931576b5",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -132,7 +132,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "949702a5af97da779cb6dab842b0029d274417dc",
            "filename": "src/transformers/models/ibert/quant_modules.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fibert%2Fquant_modules.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -651,7 +651,7 @@ def forward(ctx, x, k, percentile_mode, scale):\n         Returns:\n             `torch.Tensor`: Symmetric-quantized value of *input*.\n         \"\"\"\n-        zero_point = torch.tensor(0.0).to(scale.device)\n+        zero_point = torch.tensor(0.0, device=scale.device)\n \n         n = 2 ** (k - 1) - 1\n         new_quant_x = linear_quantize(x, scale, zero_point, inplace=False)"
        },
        {
            "sha": "9c21213b0b676fef115fbaa4bf8da3b0e9dfb607",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -415,7 +415,10 @@ def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n         self.dim = dim\n         self.max_position_embeddings = max_position_embeddings\n         self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float().to(device) / self.dim))\n+        inv_freq = 1.0 / (\n+            self.base\n+            ** (torch.arange(0, self.dim, 2, dtype=torch.int64).to(device=device, dtype=torch.float) / self.dim)\n+        )\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n \n         # Build here to make `torch.jit.trace` work."
        },
        {
            "sha": "f75e24852b88818a7cb21538fa565509fcab3771",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -247,7 +247,7 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n             mask_value = torch.finfo(attn_weights.dtype).min\n             # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n             # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n+            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n             attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n \n         if attention_mask is not None:\n@@ -297,7 +297,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             mask_value = torch.finfo(attn_weights.dtype).min\n             # Need to be a tensor, otherwise we get error: `RuntimeError: expected scalar type float but found double`.\n             # Need to be on the same device, otherwise `RuntimeError: ..., x and y to be on the same device`\n-            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype).to(attn_weights.device)\n+            mask_value = torch.tensor(mask_value, dtype=attn_weights.dtype, device=attn_weights.device)\n             attn_weights = torch.where(causal_mask, attn_weights, mask_value)\n \n         if attention_mask is not None:"
        },
        {
            "sha": "ae2bb44bf9617d76cb0e8efd3999a26120566e6a",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -443,7 +443,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "d64dd9b7b656f698c723dd2398abe2364462b925",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -138,7 +138,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "b76cebc1880ccb59ba186f4903f21a0b9c14ec58",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -412,7 +412,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "b22e92f6f6d95ed27b23469a5fdcba8ef6b9cd1f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -321,7 +321,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "973fff5f17c44e413d0d5d7e1c883dd2a3b13988",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -443,7 +443,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "4f087ec3826bdbc5c3f65c86c1c7193e0d14425f",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -295,7 +295,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "040358ad461eab07fc759ef45d777b930cd5b854",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -366,7 +366,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "fcd4aea9315c534dcbc5e60fb3dc31d5e84609db",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -356,7 +356,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "d9cd4df990a33abc4942605f00ac9d9511fe3882",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -143,7 +143,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "749b729b12984ad72ae2af242d65e700d56394ce",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -332,7 +332,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "35f0376f2d9873e1a6e8a8b32f598058d548545a",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -333,7 +333,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "9589e4dd7a095bf695eae8a9ba66a3ff2c372b4b",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -206,7 +206,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "e95e4a522bdf4a88bd770be5e8de89755ccb3da9",
            "filename": "src/transformers/models/omdet_turbo/modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fmodeling_omdet_turbo.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1315,7 +1315,7 @@ def _get_encoder_input(self, vision_features):\n \n         # [batch_size, height*width, channels]\n         new_vision_features = torch.cat(new_vision_features, 1)\n-        new_vision_shapes = torch.tensor(new_vision_shapes_list, dtype=torch.int64).to(vision_features[0].device)\n+        new_vision_shapes = torch.tensor(new_vision_shapes_list, dtype=torch.int64, device=vision_features[0].device)\n         level_start_index = torch.cat((new_vision_shapes.new_zeros((1,)), new_vision_shapes.prod(1).cumsum(0)[:-1]))\n \n         return new_vision_features, new_vision_shapes, new_vision_shapes_list, level_start_index\n@@ -1330,7 +1330,9 @@ def _get_decoder_input(\n         )\n         predicted_class_features = self.encoder_vision_features(\n             torch.where(\n-                valid_mask, vision_features, torch.tensor(0.0, dtype=vision_features.dtype).to(vision_features.device)\n+                valid_mask,\n+                vision_features,\n+                torch.tensor(0.0, dtype=vision_features.dtype, device=vision_features.device),\n             )\n         )\n "
        },
        {
            "sha": "5c9c62cc46bc666ce65cdee2018368e9a435a92c",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -113,7 +113,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "539be9216d48cb6147fee60bab21c728317a0ede",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -328,7 +328,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "5d5464dcae3dcb6b83c31799dc1d59db15ce2aa6",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -334,7 +334,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "5e5be0a8f1840bd52b261fd873dc59f1ebcb183a",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -216,7 +216,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "0559ec789b3c5467caad70e9083e9266fe2e1934",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -361,7 +361,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "aa1418bd1cafa36974175221f27e509faba18885",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -456,7 +456,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "b4ebc88b0e18c3c70f4953c022c3cea088b3edba",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 23,
            "deletions": 20,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -2873,7 +2873,7 @@ def generate(\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n             else:\n                 raise ValueError(\n                     \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n@@ -3144,7 +3144,7 @@ def generate(\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n             else:\n                 raise ValueError(\n                     \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n@@ -3420,7 +3420,7 @@ def generate(\n \n         # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n         text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -3441,7 +3441,8 @@ def generate(\n             idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n             idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n             idx_most_probable_sequences_per_batch = (\n-                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n+                idx_most_probable_sequences_per_batch\n+                + torch.arange(batch_size, device=self.device) * num_return_sequences\n             )\n             sequences = sequences[idx_most_probable_sequences_per_batch]\n \n@@ -3462,8 +3463,8 @@ def generate(\n         # Compute t2u decoder_input_ids\n         t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n         t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n-        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n-            self.device\n+        t2u_decoder_input_ids = torch.tensor(\n+            [[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size, device=self.device\n         )\n         kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n         # second generation\n@@ -3480,9 +3481,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n+        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n \n@@ -3748,7 +3749,7 @@ def generate(\n         text_decoder_input_ids = kwargs_text.get(\"decoder_input_ids\")\n         # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n         text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -3779,7 +3780,8 @@ def generate(\n             idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n             idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n             idx_most_probable_sequences_per_batch = (\n-                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n+                idx_most_probable_sequences_per_batch\n+                + torch.arange(batch_size, device=self.device) * num_return_sequences\n             )\n             sequences = sequences[idx_most_probable_sequences_per_batch]\n \n@@ -3800,8 +3802,8 @@ def generate(\n         # Compute t2u decoder_input_ids\n         t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n         t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n-        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n-            self.device\n+        t2u_decoder_input_ids = torch.tensor(\n+            [[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size, device=self.device\n         )\n         kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n \n@@ -3819,9 +3821,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n+        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n \n@@ -4171,7 +4173,7 @@ def generate(\n         if tgt_lang is not None:\n             # tgt_lang gets priority over decoder input ids\n             text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-            text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+            text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -4221,7 +4223,8 @@ def generate(\n             idx_most_probable_sequences_per_batch = text_generation_output.sequences_scores.view(batch_size, -1)\n             idx_most_probable_sequences_per_batch = idx_most_probable_sequences_per_batch.argmax(-1)\n             idx_most_probable_sequences_per_batch = (\n-                idx_most_probable_sequences_per_batch + torch.arange(batch_size).to(self.device) * num_return_sequences\n+                idx_most_probable_sequences_per_batch\n+                + torch.arange(batch_size, device=self.device) * num_return_sequences\n             )\n             sequences = sequences[idx_most_probable_sequences_per_batch]\n \n@@ -4242,8 +4245,8 @@ def generate(\n         # Compute t2u decoder_input_ids\n         t2u_decoder_input_ids = kwargs_speech.get(\"decoder_input_ids\")\n         t2u_tgt_lang_id = self.generation_config.t2u_lang_code_to_id.get(tgt_lang)\n-        t2u_decoder_input_ids = torch.tensor([[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size).to(\n-            self.device\n+        t2u_decoder_input_ids = torch.tensor(\n+            [[self.config.t2u_eos_token_id, t2u_tgt_lang_id]] * batch_size, device=self.device\n         )\n         kwargs_speech[\"decoder_input_ids\"] = t2u_decoder_input_ids\n \n@@ -4261,9 +4264,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids)).to(self.device)\n+        spkr_id = torch.tensor([[spkr_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(input_ids=unit_ids, spkr_id=spkr_id, lang_id=vocoder_tgt_lang_id)\n "
        },
        {
            "sha": "7a36633db48dc5c99f9fb7903b5df1cf696836fd",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 11,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -3153,7 +3153,7 @@ def generate(\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n             else:\n                 raise ValueError(\n                     \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n@@ -3434,7 +3434,7 @@ def generate(\n                     )\n                 # tgt_lang gets priority over decoder input ids\n                 text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+                text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n             else:\n                 raise ValueError(\n                     \"\"\"This model generation config doesn't have a `text_decoder_lang_to_code_id` key which maps\n@@ -3720,7 +3720,7 @@ def generate(\n \n         # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n         text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -3810,9 +3810,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids)).to(self.device)\n+        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(\n             input_ids=unit_ids, speaker_id=speaker_id, lang_id=vocoder_tgt_lang_id\n@@ -4090,7 +4090,7 @@ def generate(\n         text_decoder_input_ids = kwargs_text.get(\"decoder_input_ids\")\n         # overwrite text_decoder_input_ids if tgt_lang is passed. The latter gets priority over decoder_input_ids.\n         text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+        text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -4190,9 +4190,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids)).to(self.device)\n+        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(\n             input_ids=unit_ids, speaker_id=speaker_id, lang_id=vocoder_tgt_lang_id\n@@ -4559,7 +4559,7 @@ def generate(\n         if tgt_lang is not None:\n             # tgt_lang gets priority over decoder input ids\n             text_tgt_lang_id = self.generation_config.text_decoder_lang_to_code_id.get(tgt_lang)\n-            text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size).to(self.device)\n+            text_decoder_input_ids = torch.tensor([[text_tgt_lang_id]] * batch_size, device=self.device)\n \n         kwargs_text[\"decoder_input_ids\"] = text_decoder_input_ids\n \n@@ -4679,9 +4679,9 @@ def generate(\n         )\n \n         vocoder_tgt_lang_id = self.generation_config.vocoder_lang_code_to_id.get(tgt_lang)\n-        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids)).to(self.device)\n+        vocoder_tgt_lang_id = torch.tensor([[vocoder_tgt_lang_id]] * len(unit_ids), device=self.device)\n \n-        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids)).to(self.device)\n+        speaker_id = torch.tensor([[speaker_id]] * len(unit_ids), device=self.device)\n \n         waveform, waveform_lengths = self.vocoder(\n             input_ids=unit_ids, speaker_id=speaker_id, lang_id=vocoder_tgt_lang_id"
        },
        {
            "sha": "26c7c1f47acd77f3874c8ccd7fb88a9a6064f877",
            "filename": "src/transformers/models/seggpt/image_processing_seggpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fimage_processing_seggpt.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -586,7 +586,7 @@ def post_process_semantic_segmentation(\n         palette_tensor = None\n         palette = self.get_palette(num_labels) if num_labels is not None else None\n         if palette is not None:\n-            palette_tensor = torch.tensor(palette).float().to(masks.device)\n+            palette_tensor = torch.tensor(palette).to(device=masks.device, dtype=torch.float)\n             _, num_channels, _, _ = masks.shape\n             palette_tensor = palette_tensor.view(1, 1, num_labels + 1, num_channels)\n "
        },
        {
            "sha": "9f49a46a05fc442d81d7b1d7d1ad4ff9e760d85e",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -820,7 +820,7 @@ def disentangled_attention_bias(self, query_layer, key_layer, relative_pos, rel_\n             raise ValueError(f\"Relative position ids must be of dim 2 or 3 or 4. {relative_pos.dim()}\")\n \n         att_span = self.pos_ebd_size\n-        relative_pos = relative_pos.long().to(query_layer.device)\n+        relative_pos = relative_pos.to(device=query_layer.device, dtype=torch.long)\n \n         rel_embeddings = rel_embeddings[0 : att_span * 2, :].unsqueeze(0)\n         if self.share_att_key:"
        },
        {
            "sha": "d85e52924aa9e1ed14e84c8418c47fa850155399",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -431,7 +431,7 @@ def __init__(self, dim, max_length=1000):\n \n     def forward(self, hidden_states):\n         seq_len = hidden_states.shape[1]\n-        pos_seq = torch.arange(0, seq_len).long().to(hidden_states.device)\n+        pos_seq = torch.arange(0, seq_len).to(device=hidden_states.device, dtype=torch.long)\n         pos_seq = pos_seq[:, None] - pos_seq[None, :]\n \n         pos_seq[pos_seq < -self.max_length] = -self.max_length"
        },
        {
            "sha": "8d2b3f96ec925d090318881c24b29be3f23756c1",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -118,7 +118,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "6fbb652c99c4aacdc0f4fd97aba8d103f9e5d199",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -325,7 +325,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "95b097013e5c46111480b5ff2cc8c8dc57907330",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -1243,8 +1243,8 @@ def forward(\n         if table_mask is None:\n             table_mask = torch.where(row_ids > 0, torch.ones_like(row_ids), torch.zeros_like(row_ids))\n         # torch.FloatTensor[batch_size, seq_length]\n-        input_mask_float = attention_mask.float().to(device)\n-        table_mask_float = table_mask.float().to(device)\n+        input_mask_float = attention_mask.to(device=device, dtype=torch.float)\n+        table_mask_float = table_mask.to(device=device, dtype=torch.float)\n         # Mask for cells that exist in the table (i.e. that are not padding).\n         cell_mask, _ = reduce_mean(input_mask_float, cell_index)\n "
        },
        {
            "sha": "2c60cc2276dd1284e84cfb3a6f3175e45bf002f3",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -270,7 +270,9 @@ def forward(self, x, position_ids):\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float().to(x.device) @ position_ids_expanded.float()).transpose(1, 2)\n+            freqs = (\n+                inv_freq_expanded.to(device=x.device, dtype=torch.float) @ position_ids_expanded.float()\n+            ).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()"
        },
        {
            "sha": "5b238e610ebfb66d59b220fe3b5f49200033fc76",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -2454,7 +2454,7 @@ def _inner_training_loop(\n         self.state.init_training_references(self, max_steps, num_train_epochs, trial)\n \n         # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n-        tr_loss = torch.tensor(0.0).to(args.device)\n+        tr_loss = torch.tensor(0.0, device=args.device)\n         # _total_loss_scalar is updated everytime .item() has to be called on tr_loss and stores the sum of all losses\n         self._total_loss_scalar = 0.0\n         self._globalstep_last_logged = self.state.global_step"
        },
        {
            "sha": "30474daea6e435b6567b99d745f12cc8053f6e83",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -225,7 +225,7 @@ def distributed_broadcast_scalars(\n     device: Optional[torch.device] = torch.device(\"cuda\"),\n ) -> torch.Tensor:\n     try:\n-        tensorized_scalar = torch.tensor(scalars).to(device)\n+        tensorized_scalar = torch.tensor(scalars, device=device)\n         output_tensors = [tensorized_scalar.clone() for _ in range(dist.get_world_size())]\n         dist.all_gather(output_tensors, tensorized_scalar)\n         concat = torch.cat(output_tensors, dim=0)"
        },
        {
            "sha": "5a618f901c2d9619728b810171021ee119af6659",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/764ab0d46aecbb82d6d16c847d0df88397c7d780/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=764ab0d46aecbb82d6d16c847d0df88397c7d780",
            "patch": "@@ -596,7 +596,7 @@ def is_torch_bf16_available_on_device(device):\n         return True\n \n     try:\n-        x = torch.zeros(2, 2, dtype=torch.bfloat16).to(device)\n+        x = torch.zeros(2, 2, dtype=torch.bfloat16, device=device)\n         _ = x @ x\n     except:  # noqa: E722\n         # TODO: more precise exception matching, if possible."
        }
    ],
    "stats": {
        "total": 322,
        "additions": 209,
        "deletions": 113
    }
}