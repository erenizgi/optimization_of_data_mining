{
    "author": "Cyrilvallez",
    "message": "[modular] Classes can now be defined and referenced in arbitrary order (without bringing unwanted dependencies) (#40507)\n\n* remove future class from dependency graph\n\n* convert all",
    "sha": "98289c5546cf167fb10178053b36719f9732fc03",
    "files": [
        {
            "sha": "5cc2f5e221d10c1150b1200cf24880ad87201ad4",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 679,
            "deletions": 683,
            "changes": 1362,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -436,331 +436,553 @@ def forward(\n         return outputs\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for outputs of the RT-DETR encoder-decoder model.\n+@auto_docstring\n+class DFinePreTrainedModel(PreTrainedModel):\n+    config: DFineConfig\n+    base_model_prefix = \"d_fine\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [r\"DFineHybridEncoder\", r\"DFineDecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        # initialize linear layer bias value according to a given probability value.\n+        if isinstance(module, (DFineForObjectDetection, DFineDecoder)):\n+            if module.class_embed is not None:\n+                for layer in module.class_embed:\n+                    prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+                    bias = float(-math.log((1 - prior_prob) / prior_prob))\n+                    nn.init.xavier_uniform_(layer.weight)\n+                    nn.init.constant_(layer.bias, bias)\n+\n+            if module.bbox_embed is not None:\n+                for layer in module.bbox_embed:\n+                    nn.init.constant_(layer.layers[-1].weight, 0)\n+                    nn.init.constant_(layer.layers[-1].bias, 0)\n+\n+        if isinstance(module, DFineMultiscaleDeformableAttention):\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n+            grid_init = grid_init.reshape(module.n_heads, 1, 2).tile([1, sum(module.num_points_list), 1])\n+            scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n+            grid_init *= scaling\n+            with torch.no_grad():\n+                module.sampling_offsets.bias.data[...] = grid_init.flatten()\n+\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+\n+        if isinstance(module, DFineModel):\n+            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+            bias = float(-math.log((1 - prior_prob) / prior_prob))\n+            nn.init.xavier_uniform_(module.enc_score_head.weight)\n+            nn.init.constant_(module.enc_score_head.bias, bias)\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        if isinstance(module, DFineGate):\n+            bias = float(-math.log((1 - 0.5) / 0.5))\n+            init.constant_(module.gate.bias, bias)\n+            init.constant_(module.gate.weight, 0)\n+\n+        if isinstance(module, DFineLQE):\n+            init.constant_(module.reg_conf.layers[-1].bias, 0)\n+            init.constant_(module.reg_conf.layers[-1].weight, 0)\n+\n+        if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n+            nn.init.xavier_uniform_(module.weight_embedding.weight)\n+        if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n+            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n+\n+\n+class DFineIntegral(nn.Module):\n     \"\"\"\n-)\n-class DFineModelOutput(ModelOutput):\n-    r\"\"\"\n-    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-        Stacked intermediate hidden states (output of each layer of the decoder).\n-    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-        Stacked intermediate logits (logits of each layer of the decoder).\n-    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked intermediate reference points (reference points of each layer of the decoder).\n-    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-        Initial reference points used for the first decoder layer.\n-    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-        Initial reference points sent through the Transformer decoder.\n-    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n-        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-        picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n-        foreground and background).\n-    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n-        Logits of predicted bounding boxes coordinates in the encoder stage.\n-    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-        foreground and background).\n-    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the first stage.\n-    denoising_meta_values (`dict`):\n-        Extra dictionary for the denoising related values.\n+    A static layer that calculates integral results from a distribution.\n+\n+    This layer computes the target location using the formula: `sum{Pr(n) * W(n)}`,\n+    where Pr(n) is the softmax probability vector representing the discrete\n+    distribution, and W(n) is the non-uniform Weighting Function.\n+\n+    Args:\n+        max_num_bins (int): Max number of the discrete bins. Default is 32.\n+                       It can be adjusted based on the dataset or task requirements.\n     \"\"\"\n \n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_logits: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n-    initial_reference_points: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    init_reference_points: Optional[torch.FloatTensor] = None\n-    enc_topk_logits: Optional[torch.FloatTensor] = None\n-    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n-    enc_outputs_class: Optional[torch.FloatTensor] = None\n-    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n-    denoising_meta_values: Optional[dict] = None\n+    def __init__(self, config: DFineConfig):\n+        super().__init__()\n+        self.max_num_bins = config.max_num_bins\n+\n+    def forward(self, pred_corners: torch.Tensor, project: torch.Tensor) -> torch.Tensor:\n+        batch_size, num_queries, _ = pred_corners.shape\n+        pred_corners = F.softmax(pred_corners.reshape(-1, self.max_num_bins + 1), dim=1)\n+        pred_corners = F.linear(pred_corners, project.to(pred_corners.device)).reshape(-1, 4)\n+        pred_corners = pred_corners.reshape(batch_size, num_queries, -1)\n+        return pred_corners\n \n \n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n-    Output type of [`DFineForObjectDetection`].\n+    Base class for outputs of the DFineDecoder. This class adds two attributes to\n+    BaseModelOutputWithCrossAttentions, namely:\n+    - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n+    - a stacked tensor of intermediate reference points.\n     \"\"\"\n )\n-class DFineObjectDetectionOutput(ModelOutput):\n+class DFineDecoderOutput(ModelOutput):\n     r\"\"\"\n-    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-        scale-invariant IoU loss.\n-    loss_dict (`Dict`, *optional*):\n-        A dictionary containing the individual losses. Useful for logging.\n-    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-        Classification logits (including no-object) for all queries.\n-    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-        possible padding). You can use [`~DFineImageProcessor.post_process_object_detection`] to retrieve the\n-        unnormalized (absolute) bounding boxes.\n-    auxiliary_outputs (`list[Dict]`, *optional*):\n-        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-        `pred_boxes`) for each decoder layer.\n-    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n     intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n         Stacked intermediate hidden states (output of each layer of the decoder).\n-    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n         Stacked intermediate logits (logits of each layer of the decoder).\n-    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n         Stacked intermediate reference points (reference points of each layer of the decoder).\n     intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n         Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n     initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n         Stacked initial reference points (initial reference points of each layer of the decoder).\n-    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-        Initial reference points sent through the Transformer decoder.\n-    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the encoder.\n-    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the encoder.\n-    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-        foreground and background).\n-    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the first stage.\n-    denoising_meta_values (`dict`):\n-        Extra dictionary for the denoising related values\n+    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n+        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n+        used to compute the weighted average in the cross-attention heads.\n     \"\"\"\n \n-    loss: Optional[torch.FloatTensor] = None\n-    loss_dict: Optional[dict] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    pred_boxes: Optional[torch.FloatTensor] = None\n-    auxiliary_outputs: Optional[list[dict]] = None\n     last_hidden_state: Optional[torch.FloatTensor] = None\n     intermediate_hidden_states: Optional[torch.FloatTensor] = None\n     intermediate_logits: Optional[torch.FloatTensor] = None\n     intermediate_reference_points: Optional[torch.FloatTensor] = None\n     intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n     initial_reference_points: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    init_reference_points: Optional[tuple[torch.FloatTensor]] = None\n-    enc_topk_logits: Optional[torch.FloatTensor] = None\n-    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n-    enc_outputs_class: Optional[torch.FloatTensor] = None\n-    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n-    denoising_meta_values: Optional[dict] = None\n \n \n-class DFineFrozenBatchNorm2d(nn.Module):\n-    \"\"\"\n-    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n+def inverse_sigmoid(x, eps=1e-5):\n+    x = x.clamp(min=0, max=1)\n+    x1 = x.clamp(min=eps)\n+    x2 = (1 - x).clamp(min=eps)\n+    return torch.log(x1 / x2)\n \n-    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n-    torchvision.models.resnet[18,34,50,101] produce nans.\n+\n+def weighting_function(max_num_bins: int, up: torch.Tensor, reg_scale: int) -> torch.Tensor:\n     \"\"\"\n+    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n \n-    def __init__(self, n):\n-        super().__init__()\n-        self.register_buffer(\"weight\", torch.ones(n))\n-        self.register_buffer(\"bias\", torch.zeros(n))\n-        self.register_buffer(\"running_mean\", torch.zeros(n))\n-        self.register_buffer(\"running_var\", torch.ones(n))\n+    Args:\n+        max_num_bins (int): Max number of the discrete bins.\n+        up (Tensor): Controls upper bounds of the sequence,\n+                     where maximum offset is Â±up * H / W.\n+        reg_scale (float): Controls the curvature of the Weighting Function.\n+                           Larger values result in flatter weights near the central axis W(max_num_bins/2)=0\n+                           and steeper weights at both ends.\n+    Returns:\n+        Tensor: Sequence of Weighting Function.\n+    \"\"\"\n+    upper_bound1 = abs(up[0]) * abs(reg_scale)\n+    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n+    step = (upper_bound1 + 1) ** (2 / (max_num_bins - 2))\n+    left_values = [-((step) ** i) + 1 for i in range(max_num_bins // 2 - 1, 0, -1)]\n+    right_values = [(step) ** i - 1 for i in range(1, max_num_bins // 2)]\n+    values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n+    values = torch.cat(values, 0)\n+    return values\n \n-    def _load_from_state_dict(\n-        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-    ):\n-        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n-        if num_batches_tracked_key in state_dict:\n-            del state_dict[num_batches_tracked_key]\n \n-        super()._load_from_state_dict(\n-            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n-        )\n-\n-    def forward(self, x):\n-        # move reshapes to the beginning\n-        # to make it user-friendly\n-        weight = self.weight.reshape(1, -1, 1, 1)\n-        bias = self.bias.reshape(1, -1, 1, 1)\n-        running_var = self.running_var.reshape(1, -1, 1, 1)\n-        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n-        epsilon = 1e-5\n-        scale = weight * (running_var + epsilon).rsqrt()\n-        bias = bias - running_mean * scale\n-        return x * scale + bias\n-\n-\n-def replace_batch_norm(model):\n-    r\"\"\"\n-    Recursively replace all `torch.nn.BatchNorm2d` with `DFineFrozenBatchNorm2d`.\n+def distance2bbox(points, distance: torch.Tensor, reg_scale: float) -> torch.Tensor:\n+    \"\"\"\n+    Decodes edge-distances into bounding box coordinates.\n \n     Args:\n-        model (torch.nn.Module):\n-            input model\n+        points (`torch.Tensor`):\n+            (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n+        distance (`torch.Tensor`):\n+            (batch_size, num_boxes, 4) or (num_boxes, 4), representing distances from the point to the left, top, right, and bottom boundaries.\n+        reg_scale (`float`):\n+            Controls the curvature of the Weighting Function.\n+    Returns:\n+        `torch.Tensor`: Bounding boxes in (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n     \"\"\"\n-    for name, module in model.named_children():\n-        if isinstance(module, nn.BatchNorm2d):\n-            new_module = DFineFrozenBatchNorm2d(module.num_features)\n-\n-            if module.weight.device != torch.device(\"meta\"):\n-                new_module.weight.data.copy_(module.weight)\n-                new_module.bias.data.copy_(module.bias)\n-                new_module.running_mean.data.copy_(module.running_mean)\n-                new_module.running_var.data.copy_(module.running_var)\n+    reg_scale = abs(reg_scale)\n+    top_left_x = points[..., 0] - (0.5 * reg_scale + distance[..., 0]) * (points[..., 2] / reg_scale)\n+    top_left_y = points[..., 1] - (0.5 * reg_scale + distance[..., 1]) * (points[..., 3] / reg_scale)\n+    bottom_right_x = points[..., 0] + (0.5 * reg_scale + distance[..., 2]) * (points[..., 2] / reg_scale)\n+    bottom_right_y = points[..., 1] + (0.5 * reg_scale + distance[..., 3]) * (points[..., 3] / reg_scale)\n \n-            model._modules[name] = new_module\n+    bboxes = torch.stack([top_left_x, top_left_y, bottom_right_x, bottom_right_y], -1)\n \n-        if len(list(module.children())) > 0:\n-            replace_batch_norm(module)\n+    return corners_to_center_format(bboxes)\n \n \n-class DFineConvEncoder(nn.Module):\n+class DFineDecoder(DFinePreTrainedModel):\n     \"\"\"\n-    Convolutional backbone using the modeling_d_fine_resnet.py.\n+    D-FINE Decoder implementing Fine-grained Distribution Refinement (FDR).\n \n-    nn.BatchNorm2d layers are replaced by DFineFrozenBatchNorm2d as defined above.\n-    https://github.com/lyuwenyu/RT-DETR/blob/main/DFine_pytorch/src/nn/backbone/presnet.py#L142\n+    This decoder refines object detection predictions through iterative updates across multiple layers,\n+    utilizing attention mechanisms, location quality estimators, and distribution refinement techniques\n+    to improve bounding box accuracy and robustness.\n     \"\"\"\n \n-    def __init__(self, config):\n-        super().__init__()\n-\n-        backbone = load_backbone(config)\n-\n-        if config.freeze_backbone_batch_norms:\n-            # replace batch norm by frozen batch norm\n-            with torch.no_grad():\n-                replace_batch_norm(backbone)\n-        self.model = backbone\n-        self.intermediate_channel_sizes = self.model.channels\n-\n-    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n-        # send pixel_values through the model to get list of feature maps\n-        features = self.model(pixel_values).feature_maps\n-\n-        out = []\n-        for feature_map in features:\n-            # downsample pixel_mask to match shape of corresponding feature_map\n-            mask = nn.functional.interpolate(pixel_mask[None].float(), size=feature_map.shape[-2:]).to(torch.bool)[0]\n-            out.append((feature_map, mask))\n-        return out\n-\n-\n-class DFineEncoderLayer(nn.Module):\n     def __init__(self, config: DFineConfig):\n-        super().__init__()\n-        self.normalize_before = config.normalize_before\n+        super().__init__(config)\n+        self.eval_idx = config.eval_idx if config.eval_idx >= 0 else config.decoder_layers + config.eval_idx\n \n-        # self-attention\n-        self.self_attn = DFineMultiheadAttention(\n-            embed_dim=config.encoder_hidden_dim,\n-            num_heads=config.num_attention_heads,\n-            dropout=config.dropout,\n-        )\n-        self.self_attn_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n         self.dropout = config.dropout\n-        self.activation_fn = ACT2FN[config.encoder_activation_function]\n-        self.activation_dropout = config.activation_dropout\n-        self.fc1 = nn.Linear(config.encoder_hidden_dim, config.encoder_ffn_dim)\n-        self.fc2 = nn.Linear(config.encoder_ffn_dim, config.encoder_hidden_dim)\n-        self.final_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n+        self.layers = nn.ModuleList(\n+            [DFineDecoderLayer(config) for _ in range(config.decoder_layers)]\n+            + [DFineDecoderLayer(config) for _ in range(config.decoder_layers - self.eval_idx - 1)]\n+        )\n+        self.query_pos_head = DFineMLPPredictionHead(config, 4, 2 * config.d_model, config.d_model, num_layers=2)\n+\n+        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n+        self.bbox_embed = None\n+        self.class_embed = None\n+        self.reg_scale = nn.Parameter(torch.tensor([config.reg_scale]), requires_grad=False)\n+        self.max_num_bins = config.max_num_bins\n+        self.d_model = config.d_model\n+        self.layer_scale = config.layer_scale\n+        self.pre_bbox_head = DFineMLP(config.hidden_size, config.hidden_size, 4, 3)\n+        self.integral = DFineIntegral(config)\n+        self.num_head = config.decoder_attention_heads\n+        self.up = nn.Parameter(torch.tensor([config.up]), requires_grad=False)\n+        self.lqe_layers = nn.ModuleList([DFineLQE(config) for _ in range(config.decoder_layers)])\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n \n     def forward(\n         self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: torch.Tensor,\n-        position_embeddings: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-        **kwargs,\n-    ):\n-        \"\"\"\n+        encoder_hidden_states: torch.Tensor,\n+        reference_points: torch.Tensor,\n+        inputs_embeds: torch.Tensor,\n+        spatial_shapes,\n+        level_start_index=None,\n+        spatial_shapes_list=None,\n+        output_hidden_states=None,\n+        encoder_attention_mask=None,\n+        memory_mask=None,\n+        output_attentions=None,\n+        return_dict=None,\n+    ) -> DFineDecoderOutput:\n+        r\"\"\"\n         Args:\n-            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n-            attention_mask (`torch.FloatTensor`): attention mask of size\n-                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n-                values.\n-            position_embeddings (`torch.FloatTensor`, *optional*):\n-                Object queries (also called content embeddings), to be added to the hidden states.\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+                The query embeddings that are passed into the decoder.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n+                in `[0, 1]`:\n+                - 1 for pixels that are real (i.e. **not masked**),\n+                - 0 for pixels that are padding (i.e. **masked**).\n+            position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+                Position embeddings that are added to the queries and keys in each self-attention layer.\n+            reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)` is `as_two_stage` else `(batch_size, num_queries, 2)` or , *optional*):\n+                Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n+            spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n+                Spatial shapes of the feature maps.\n+            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n+                Indexes for the start of each feature level. In range `[0, sequence_length]`.\n+            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n+                Ratio of valid area in each feature level.\n+\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n         \"\"\"\n-        residual = hidden_states\n-        if self.normalize_before:\n-            hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        hidden_states, attn_weights = self.self_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=attention_mask,\n-            position_embeddings=position_embeddings,\n-            output_attentions=output_attentions,\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = residual + hidden_states\n-        if not self.normalize_before:\n-            hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        if self.normalize_before:\n-            hidden_states = self.final_layer_norm(hidden_states)\n-        residual = hidden_states\n+        if inputs_embeds is not None:\n+            hidden_states = inputs_embeds\n \n-        hidden_states = self.activation_fn(self.fc1(hidden_states))\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+        intermediate = ()\n+        intermediate_reference_points = ()\n+        intermediate_logits = ()\n+        intermediate_predicted_corners = ()\n+        initial_reference_points = ()\n \n-        hidden_states = self.fc2(hidden_states)\n+        output_detach = pred_corners_undetach = 0\n \n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        project = weighting_function(self.max_num_bins, self.up, self.reg_scale)\n+        ref_points_detach = F.sigmoid(reference_points)\n \n-        hidden_states = residual + hidden_states\n-        if not self.normalize_before:\n-            hidden_states = self.final_layer_norm(hidden_states)\n+        for i, decoder_layer in enumerate(self.layers):\n+            ref_points_input = ref_points_detach.unsqueeze(2)\n+            query_pos_embed = self.query_pos_head(ref_points_detach).clamp(min=-10, max=10)\n \n-        if self.training:\n-            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n-                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n-                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n \n-        outputs = (hidden_states,)\n+            output = decoder_layer(\n+                hidden_states=hidden_states,\n+                position_embeddings=query_pos_embed,\n+                reference_points=ref_points_input,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                encoder_hidden_states=encoder_hidden_states,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n \n-        if output_attentions:\n-            outputs += (attn_weights,)\n+            hidden_states = output[0]\n \n-        return outputs\n+            if i == 0:\n+                # Initial bounding box predictions with inverse sigmoid refinement\n+                new_reference_points = F.sigmoid(self.pre_bbox_head(output[0]) + inverse_sigmoid(ref_points_detach))\n+                ref_points_initial = new_reference_points.detach()\n \n+            # Refine bounding box corners using FDR, integrating previous layer's corrections\n+            if self.bbox_embed is not None:\n+                pred_corners = self.bbox_embed[i](hidden_states + output_detach) + pred_corners_undetach\n+                inter_ref_bbox = distance2bbox(\n+                    ref_points_initial, self.integral(pred_corners, project), self.reg_scale\n+                )\n+                pred_corners_undetach = pred_corners\n+                ref_points_detach = inter_ref_bbox.detach()\n \n-def inverse_sigmoid(x, eps=1e-5):\n-    x = x.clamp(min=0, max=1)\n-    x1 = x.clamp(min=eps)\n-    x2 = (1 - x).clamp(min=eps)\n-    return torch.log(x1 / x2)\n+            output_detach = hidden_states.detach()\n \n+            intermediate += (hidden_states,)\n \n-def get_contrastive_denoising_training_group(\n-    targets,\n-    num_classes,\n-    num_queries,\n+            if self.class_embed is not None and (self.training or i == self.eval_idx):\n+                scores = self.class_embed[i](hidden_states)\n+                # Add initial logits and reference points with pre-bbox head\n+                if i == 0:\n+                    intermediate_logits += (scores,)\n+                    intermediate_reference_points += (new_reference_points,)\n+                # Lqe does not affect the performance here.\n+                scores = self.lqe_layers[i](scores, pred_corners)\n+                intermediate_logits += (scores,)\n+                intermediate_reference_points += (inter_ref_bbox,)\n+                initial_reference_points += (ref_points_initial,)\n+                intermediate_predicted_corners += (pred_corners,)\n+\n+            if output_attentions:\n+                all_self_attns += (output[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (output[2],)\n+\n+        # Keep batch_size as first dimension\n+        intermediate = torch.stack(intermediate)\n+        if self.class_embed is not None and self.bbox_embed is not None:\n+            intermediate_logits = torch.stack(intermediate_logits, dim=1)\n+            intermediate_predicted_corners = torch.stack(intermediate_predicted_corners, dim=1)\n+            initial_reference_points = torch.stack(initial_reference_points, dim=1)\n+            intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    intermediate,\n+                    intermediate_logits,\n+                    intermediate_reference_points,\n+                    intermediate_predicted_corners,\n+                    initial_reference_points,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+\n+        return DFineDecoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate,\n+            intermediate_logits=intermediate_logits,\n+            intermediate_reference_points=intermediate_reference_points,\n+            intermediate_predicted_corners=intermediate_predicted_corners,\n+            initial_reference_points=initial_reference_points,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for outputs of the RT-DETR encoder-decoder model.\n+    \"\"\"\n+)\n+class DFineModelOutput(ModelOutput):\n+    r\"\"\"\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points used for the first decoder layer.\n+    init_reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the encoder stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`):\n+        Logits of predicted bounding boxes coordinates in the encoder stage.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values.\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    init_reference_points: Optional[torch.FloatTensor] = None\n+    enc_topk_logits: Optional[torch.FloatTensor] = None\n+    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n+    enc_outputs_class: Optional[torch.FloatTensor] = None\n+    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    denoising_meta_values: Optional[dict] = None\n+\n+\n+class DFineFrozenBatchNorm2d(nn.Module):\n+    \"\"\"\n+    BatchNorm2d where the batch statistics and the affine parameters are fixed.\n+\n+    Copy-paste from torchvision.misc.ops with added eps before rqsrt, without which any other models than\n+    torchvision.models.resnet[18,34,50,101] produce nans.\n+    \"\"\"\n+\n+    def __init__(self, n):\n+        super().__init__()\n+        self.register_buffer(\"weight\", torch.ones(n))\n+        self.register_buffer(\"bias\", torch.zeros(n))\n+        self.register_buffer(\"running_mean\", torch.zeros(n))\n+        self.register_buffer(\"running_var\", torch.ones(n))\n+\n+    def _load_from_state_dict(\n+        self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+    ):\n+        num_batches_tracked_key = prefix + \"num_batches_tracked\"\n+        if num_batches_tracked_key in state_dict:\n+            del state_dict[num_batches_tracked_key]\n+\n+        super()._load_from_state_dict(\n+            state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs\n+        )\n+\n+    def forward(self, x):\n+        # move reshapes to the beginning\n+        # to make it user-friendly\n+        weight = self.weight.reshape(1, -1, 1, 1)\n+        bias = self.bias.reshape(1, -1, 1, 1)\n+        running_var = self.running_var.reshape(1, -1, 1, 1)\n+        running_mean = self.running_mean.reshape(1, -1, 1, 1)\n+        epsilon = 1e-5\n+        scale = weight * (running_var + epsilon).rsqrt()\n+        bias = bias - running_mean * scale\n+        return x * scale + bias\n+\n+\n+def replace_batch_norm(model):\n+    r\"\"\"\n+    Recursively replace all `torch.nn.BatchNorm2d` with `DFineFrozenBatchNorm2d`.\n+\n+    Args:\n+        model (torch.nn.Module):\n+            input model\n+    \"\"\"\n+    for name, module in model.named_children():\n+        if isinstance(module, nn.BatchNorm2d):\n+            new_module = DFineFrozenBatchNorm2d(module.num_features)\n+\n+            if module.weight.device != torch.device(\"meta\"):\n+                new_module.weight.data.copy_(module.weight)\n+                new_module.bias.data.copy_(module.bias)\n+                new_module.running_mean.data.copy_(module.running_mean)\n+                new_module.running_var.data.copy_(module.running_var)\n+\n+            model._modules[name] = new_module\n+\n+        if len(list(module.children())) > 0:\n+            replace_batch_norm(module)\n+\n+\n+class DFineConvEncoder(nn.Module):\n+    \"\"\"\n+    Convolutional backbone using the modeling_d_fine_resnet.py.\n+\n+    nn.BatchNorm2d layers are replaced by DFineFrozenBatchNorm2d as defined above.\n+    https://github.com/lyuwenyu/RT-DETR/blob/main/DFine_pytorch/src/nn/backbone/presnet.py#L142\n+    \"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        backbone = load_backbone(config)\n+\n+        if config.freeze_backbone_batch_norms:\n+            # replace batch norm by frozen batch norm\n+            with torch.no_grad():\n+                replace_batch_norm(backbone)\n+        self.model = backbone\n+        self.intermediate_channel_sizes = self.model.channels\n+\n+    def forward(self, pixel_values: torch.Tensor, pixel_mask: torch.Tensor):\n+        # send pixel_values through the model to get list of feature maps\n+        features = self.model(pixel_values).feature_maps\n+\n+        out = []\n+        for feature_map in features:\n+            # downsample pixel_mask to match shape of corresponding feature_map\n+            mask = nn.functional.interpolate(pixel_mask[None].float(), size=feature_map.shape[-2:]).to(torch.bool)[0]\n+            out.append((feature_map, mask))\n+        return out\n+\n+\n+def get_contrastive_denoising_training_group(\n+    targets,\n+    num_classes,\n+    num_queries,\n     class_embed,\n     num_denoising_queries=100,\n     label_noise_ratio=0.5,\n@@ -829,442 +1051,55 @@ def get_contrastive_denoising_training_group(\n     negative_gt_mask = torch.zeros([batch_size, max_gt_num * 2, 1], device=device)\n     negative_gt_mask[:, max_gt_num:] = 1\n     negative_gt_mask = negative_gt_mask.tile([1, num_groups_denoising_queries, 1])\n-    positive_gt_mask = 1 - negative_gt_mask\n-    # contrastive denoising training positive index\n-    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n-    denoise_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n-    denoise_positive_idx = torch.split(\n-        denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n-    )\n-    # total denoising queries\n-    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n-\n-    if label_noise_ratio > 0:\n-        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n-        # randomly put a new one here\n-        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n-        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n-\n-    if box_noise_scale > 0:\n-        known_bbox = center_to_corners_format(input_query_bbox)\n-        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n-        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n-        rand_part = torch.rand_like(input_query_bbox)\n-        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n-        rand_part *= rand_sign\n-        known_bbox += rand_part * diff\n-        known_bbox.clip_(min=0.0, max=1.0)\n-        input_query_bbox = corners_to_center_format(known_bbox)\n-        input_query_bbox = inverse_sigmoid(input_query_bbox)\n-\n-    input_query_class = class_embed(input_query_class)\n-\n-    target_size = num_denoising_queries + num_queries\n-    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n-    # match query cannot see the reconstruction\n-    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n-\n-    # reconstructions cannot see each other\n-    for i in range(num_groups_denoising_queries):\n-        idx_block_start = max_gt_num * 2 * i\n-        idx_block_end = max_gt_num * 2 * (i + 1)\n-        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n-        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n-\n-    denoising_meta_values = {\n-        \"dn_positive_idx\": denoise_positive_idx,\n-        \"dn_num_group\": num_groups_denoising_queries,\n-        \"dn_num_split\": [num_denoising_queries, num_queries],\n-    }\n-\n-    return input_query_class, input_query_bbox, attn_mask, denoising_meta_values\n-\n-\n-def _get_clones(partial_module, N):\n-    return nn.ModuleList([partial_module() for i in range(N)])\n-\n-\n-@auto_docstring\n-class DFinePreTrainedModel(PreTrainedModel):\n-    config: DFineConfig\n-    base_model_prefix = \"d_fine\"\n-    main_input_name = \"pixel_values\"\n-    _no_split_modules = [r\"DFineHybridEncoder\", r\"DFineDecoderLayer\"]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        # initialize linear layer bias value according to a given probability value.\n-        if isinstance(module, (DFineForObjectDetection, DFineDecoder)):\n-            if module.class_embed is not None:\n-                for layer in module.class_embed:\n-                    prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n-                    bias = float(-math.log((1 - prior_prob) / prior_prob))\n-                    nn.init.xavier_uniform_(layer.weight)\n-                    nn.init.constant_(layer.bias, bias)\n-\n-            if module.bbox_embed is not None:\n-                for layer in module.bbox_embed:\n-                    nn.init.constant_(layer.layers[-1].weight, 0)\n-                    nn.init.constant_(layer.layers[-1].bias, 0)\n-\n-        if isinstance(module, DFineMultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n-            default_dtype = torch.get_default_dtype()\n-            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n-                2.0 * math.pi / module.n_heads\n-            )\n-            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-            grid_init = grid_init / grid_init.abs().max(-1, keepdim=True).values\n-            grid_init = grid_init.reshape(module.n_heads, 1, 2).tile([1, sum(module.num_points_list), 1])\n-            scaling = torch.concat([torch.arange(1, n + 1) for n in module.num_points_list]).reshape(1, -1, 1)\n-            grid_init *= scaling\n-            with torch.no_grad():\n-                module.sampling_offsets.bias.data[...] = grid_init.flatten()\n-\n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-\n-        if isinstance(module, DFineModel):\n-            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n-            bias = float(-math.log((1 - prior_prob) / prior_prob))\n-            nn.init.xavier_uniform_(module.enc_score_head.weight)\n-            nn.init.constant_(module.enc_score_head.bias, bias)\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n-        if isinstance(module, DFineGate):\n-            bias = float(-math.log((1 - 0.5) / 0.5))\n-            init.constant_(module.gate.bias, bias)\n-            init.constant_(module.gate.weight, 0)\n-\n-        if isinstance(module, DFineLQE):\n-            init.constant_(module.reg_conf.layers[-1].bias, 0)\n-            init.constant_(module.reg_conf.layers[-1].weight, 0)\n-\n-        if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n-            nn.init.xavier_uniform_(module.weight_embedding.weight)\n-        if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n-            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n-\n-\n-class DFineIntegral(nn.Module):\n-    \"\"\"\n-    A static layer that calculates integral results from a distribution.\n-\n-    This layer computes the target location using the formula: `sum{Pr(n) * W(n)}`,\n-    where Pr(n) is the softmax probability vector representing the discrete\n-    distribution, and W(n) is the non-uniform Weighting Function.\n-\n-    Args:\n-        max_num_bins (int): Max number of the discrete bins. Default is 32.\n-                       It can be adjusted based on the dataset or task requirements.\n-    \"\"\"\n-\n-    def __init__(self, config: DFineConfig):\n-        super().__init__()\n-        self.max_num_bins = config.max_num_bins\n-\n-    def forward(self, pred_corners: torch.Tensor, project: torch.Tensor) -> torch.Tensor:\n-        batch_size, num_queries, _ = pred_corners.shape\n-        pred_corners = F.softmax(pred_corners.reshape(-1, self.max_num_bins + 1), dim=1)\n-        pred_corners = F.linear(pred_corners, project.to(pred_corners.device)).reshape(-1, 4)\n-        pred_corners = pred_corners.reshape(batch_size, num_queries, -1)\n-        return pred_corners\n-\n-\n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for outputs of the DFineDecoder. This class adds two attributes to\n-    BaseModelOutputWithCrossAttentions, namely:\n-    - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n-    - a stacked tensor of intermediate reference points.\n-    \"\"\"\n-)\n-class DFineDecoderOutput(ModelOutput):\n-    r\"\"\"\n-    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-        Stacked intermediate hidden states (output of each layer of the decoder).\n-    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, config.num_labels)`):\n-        Stacked intermediate logits (logits of each layer of the decoder).\n-    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-        Stacked intermediate reference points (reference points of each layer of the decoder).\n-    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked initial reference points (initial reference points of each layer of the decoder).\n-    cross_attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` and `config.add_cross_attention=True` is passed or when `config.output_attentions=True`):\n-        Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n-        sequence_length)`. Attentions weights of the decoder's cross-attention layer, after the attention softmax,\n-        used to compute the weighted average in the cross-attention heads.\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_logits: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n-    initial_reference_points: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-\n-\n-def weighting_function(max_num_bins: int, up: torch.Tensor, reg_scale: int) -> torch.Tensor:\n-    \"\"\"\n-    Generates the non-uniform Weighting Function W(n) for bounding box regression.\n-\n-    Args:\n-        max_num_bins (int): Max number of the discrete bins.\n-        up (Tensor): Controls upper bounds of the sequence,\n-                     where maximum offset is Â±up * H / W.\n-        reg_scale (float): Controls the curvature of the Weighting Function.\n-                           Larger values result in flatter weights near the central axis W(max_num_bins/2)=0\n-                           and steeper weights at both ends.\n-    Returns:\n-        Tensor: Sequence of Weighting Function.\n-    \"\"\"\n-    upper_bound1 = abs(up[0]) * abs(reg_scale)\n-    upper_bound2 = abs(up[0]) * abs(reg_scale) * 2\n-    step = (upper_bound1 + 1) ** (2 / (max_num_bins - 2))\n-    left_values = [-((step) ** i) + 1 for i in range(max_num_bins // 2 - 1, 0, -1)]\n-    right_values = [(step) ** i - 1 for i in range(1, max_num_bins // 2)]\n-    values = [-upper_bound2] + left_values + [torch.zeros_like(up[0][None])] + right_values + [upper_bound2]\n-    values = torch.cat(values, 0)\n-    return values\n-\n-\n-def distance2bbox(points, distance: torch.Tensor, reg_scale: float) -> torch.Tensor:\n-    \"\"\"\n-    Decodes edge-distances into bounding box coordinates.\n-\n-    Args:\n-        points (`torch.Tensor`):\n-            (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n-        distance (`torch.Tensor`):\n-            (batch_size, num_boxes, 4) or (num_boxes, 4), representing distances from the point to the left, top, right, and bottom boundaries.\n-        reg_scale (`float`):\n-            Controls the curvature of the Weighting Function.\n-    Returns:\n-        `torch.Tensor`: Bounding boxes in (batch_size, num_boxes, 4) or (num_boxes, 4) format, representing [x_center, y_center, width, height]\n-    \"\"\"\n-    reg_scale = abs(reg_scale)\n-    top_left_x = points[..., 0] - (0.5 * reg_scale + distance[..., 0]) * (points[..., 2] / reg_scale)\n-    top_left_y = points[..., 1] - (0.5 * reg_scale + distance[..., 1]) * (points[..., 3] / reg_scale)\n-    bottom_right_x = points[..., 0] + (0.5 * reg_scale + distance[..., 2]) * (points[..., 2] / reg_scale)\n-    bottom_right_y = points[..., 1] + (0.5 * reg_scale + distance[..., 3]) * (points[..., 3] / reg_scale)\n-\n-    bboxes = torch.stack([top_left_x, top_left_y, bottom_right_x, bottom_right_y], -1)\n-\n-    return corners_to_center_format(bboxes)\n-\n-\n-class DFineDecoder(DFinePreTrainedModel):\n-    \"\"\"\n-    D-FINE Decoder implementing Fine-grained Distribution Refinement (FDR).\n-\n-    This decoder refines object detection predictions through iterative updates across multiple layers,\n-    utilizing attention mechanisms, location quality estimators, and distribution refinement techniques\n-    to improve bounding box accuracy and robustness.\n-    \"\"\"\n-\n-    def __init__(self, config: DFineConfig):\n-        super().__init__(config)\n-        self.eval_idx = config.eval_idx if config.eval_idx >= 0 else config.decoder_layers + config.eval_idx\n-\n-        self.dropout = config.dropout\n-        self.layers = nn.ModuleList(\n-            [DFineDecoderLayer(config) for _ in range(config.decoder_layers)]\n-            + [DFineDecoderLayer(config) for _ in range(config.decoder_layers - self.eval_idx - 1)]\n-        )\n-        self.query_pos_head = DFineMLPPredictionHead(config, 4, 2 * config.d_model, config.d_model, num_layers=2)\n-\n-        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n-        self.bbox_embed = None\n-        self.class_embed = None\n-        self.reg_scale = nn.Parameter(torch.tensor([config.reg_scale]), requires_grad=False)\n-        self.max_num_bins = config.max_num_bins\n-        self.d_model = config.d_model\n-        self.layer_scale = config.layer_scale\n-        self.pre_bbox_head = DFineMLP(config.hidden_size, config.hidden_size, 4, 3)\n-        self.integral = DFineIntegral(config)\n-        self.num_head = config.decoder_attention_heads\n-        self.up = nn.Parameter(torch.tensor([config.up]), requires_grad=False)\n-        self.lqe_layers = nn.ModuleList([DFineLQE(config) for _ in range(config.decoder_layers)])\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def forward(\n-        self,\n-        encoder_hidden_states: torch.Tensor,\n-        reference_points: torch.Tensor,\n-        inputs_embeds: torch.Tensor,\n-        spatial_shapes,\n-        level_start_index=None,\n-        spatial_shapes_list=None,\n-        output_hidden_states=None,\n-        encoder_attention_mask=None,\n-        memory_mask=None,\n-        output_attentions=None,\n-        return_dict=None,\n-    ) -> DFineDecoderOutput:\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-                The query embeddings that are passed into the decoder.\n-            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                of the decoder.\n-            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n-                in `[0, 1]`:\n-                - 1 for pixels that are real (i.e. **not masked**),\n-                - 0 for pixels that are padding (i.e. **masked**).\n-            position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-                Position embeddings that are added to the queries and keys in each self-attention layer.\n-            reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)` is `as_two_stage` else `(batch_size, num_queries, 2)` or , *optional*):\n-                Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n-            spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n-                Spatial shapes of the feature maps.\n-            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n-                Indexes for the start of each feature level. In range `[0, sequence_length]`.\n-            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n-                Ratio of valid area in each feature level.\n-\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if inputs_embeds is not None:\n-            hidden_states = inputs_embeds\n-\n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        intermediate = ()\n-        intermediate_reference_points = ()\n-        intermediate_logits = ()\n-        intermediate_predicted_corners = ()\n-        initial_reference_points = ()\n-\n-        output_detach = pred_corners_undetach = 0\n-\n-        project = weighting_function(self.max_num_bins, self.up, self.reg_scale)\n-        ref_points_detach = F.sigmoid(reference_points)\n-\n-        for i, decoder_layer in enumerate(self.layers):\n-            ref_points_input = ref_points_detach.unsqueeze(2)\n-            query_pos_embed = self.query_pos_head(ref_points_detach).clamp(min=-10, max=10)\n-\n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n-\n-            output = decoder_layer(\n-                hidden_states=hidden_states,\n-                position_embeddings=query_pos_embed,\n-                reference_points=ref_points_input,\n-                spatial_shapes=spatial_shapes,\n-                spatial_shapes_list=spatial_shapes_list,\n-                encoder_hidden_states=encoder_hidden_states,\n-                encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-            hidden_states = output[0]\n-\n-            if i == 0:\n-                # Initial bounding box predictions with inverse sigmoid refinement\n-                new_reference_points = F.sigmoid(self.pre_bbox_head(output[0]) + inverse_sigmoid(ref_points_detach))\n-                ref_points_initial = new_reference_points.detach()\n-\n-            # Refine bounding box corners using FDR, integrating previous layer's corrections\n-            if self.bbox_embed is not None:\n-                pred_corners = self.bbox_embed[i](hidden_states + output_detach) + pred_corners_undetach\n-                inter_ref_bbox = distance2bbox(\n-                    ref_points_initial, self.integral(pred_corners, project), self.reg_scale\n-                )\n-                pred_corners_undetach = pred_corners\n-                ref_points_detach = inter_ref_bbox.detach()\n-\n-            output_detach = hidden_states.detach()\n-\n-            intermediate += (hidden_states,)\n+    positive_gt_mask = 1 - negative_gt_mask\n+    # contrastive denoising training positive index\n+    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n+    denoise_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n+    denoise_positive_idx = torch.split(\n+        denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n+    )\n+    # total denoising queries\n+    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n \n-            if self.class_embed is not None and (self.training or i == self.eval_idx):\n-                scores = self.class_embed[i](hidden_states)\n-                # Add initial logits and reference points with pre-bbox head\n-                if i == 0:\n-                    intermediate_logits += (scores,)\n-                    intermediate_reference_points += (new_reference_points,)\n-                # Lqe does not affect the performance here.\n-                scores = self.lqe_layers[i](scores, pred_corners)\n-                intermediate_logits += (scores,)\n-                intermediate_reference_points += (inter_ref_bbox,)\n-                initial_reference_points += (ref_points_initial,)\n-                intermediate_predicted_corners += (pred_corners,)\n+    if label_noise_ratio > 0:\n+        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n+        # randomly put a new one here\n+        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n+        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n \n-            if output_attentions:\n-                all_self_attns += (output[1],)\n+    if box_noise_scale > 0:\n+        known_bbox = center_to_corners_format(input_query_bbox)\n+        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n+        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n+        rand_part = torch.rand_like(input_query_bbox)\n+        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n+        rand_part *= rand_sign\n+        known_bbox += rand_part * diff\n+        known_bbox.clip_(min=0.0, max=1.0)\n+        input_query_bbox = corners_to_center_format(known_bbox)\n+        input_query_bbox = inverse_sigmoid(input_query_bbox)\n \n-                if encoder_hidden_states is not None:\n-                    all_cross_attentions += (output[2],)\n+    input_query_class = class_embed(input_query_class)\n \n-        # Keep batch_size as first dimension\n-        intermediate = torch.stack(intermediate)\n-        if self.class_embed is not None and self.bbox_embed is not None:\n-            intermediate_logits = torch.stack(intermediate_logits, dim=1)\n-            intermediate_predicted_corners = torch.stack(intermediate_predicted_corners, dim=1)\n-            initial_reference_points = torch.stack(initial_reference_points, dim=1)\n-            intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n+    target_size = num_denoising_queries + num_queries\n+    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n+    # match query cannot see the reconstruction\n+    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    # reconstructions cannot see each other\n+    for i in range(num_groups_denoising_queries):\n+        idx_block_start = max_gt_num * 2 * i\n+        idx_block_end = max_gt_num * 2 * (i + 1)\n+        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n+        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    intermediate,\n-                    intermediate_logits,\n-                    intermediate_reference_points,\n-                    intermediate_predicted_corners,\n-                    initial_reference_points,\n-                    all_hidden_states,\n-                    all_self_attns,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n+    denoising_meta_values = {\n+        \"dn_positive_idx\": denoise_positive_idx,\n+        \"dn_num_group\": num_groups_denoising_queries,\n+        \"dn_num_split\": [num_denoising_queries, num_queries],\n+    }\n \n-        return DFineDecoderOutput(\n-            last_hidden_state=hidden_states,\n-            intermediate_hidden_states=intermediate,\n-            intermediate_logits=intermediate_logits,\n-            intermediate_reference_points=intermediate_reference_points,\n-            intermediate_predicted_corners=intermediate_predicted_corners,\n-            initial_reference_points=initial_reference_points,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            cross_attentions=all_cross_attentions,\n-        )\n+    return input_query_class, input_query_bbox, attn_mask, denoising_meta_values\n \n \n @auto_docstring(\n@@ -1615,6 +1450,84 @@ def forward(\n         )\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`DFineForObjectDetection`].\n+    \"\"\"\n+)\n+class DFineObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~DFineImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized (absolute) bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    loss_dict: Optional[dict] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    auxiliary_outputs: Optional[list[dict]] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    init_reference_points: Optional[tuple[torch.FloatTensor]] = None\n+    enc_topk_logits: Optional[torch.FloatTensor] = None\n+    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n+    enc_outputs_class: Optional[torch.FloatTensor] = None\n+    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    denoising_meta_values: Optional[dict] = None\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting bounding boxes and logits to be further\n@@ -1994,6 +1907,89 @@ def forward(self, input_features: torch.Tensor) -> torch.Tensor:\n         return input_features\n \n \n+class DFineEncoderLayer(nn.Module):\n+    def __init__(self, config: DFineConfig):\n+        super().__init__()\n+        self.normalize_before = config.normalize_before\n+\n+        # self-attention\n+        self.self_attn = DFineMultiheadAttention(\n+            embed_dim=config.encoder_hidden_dim,\n+            num_heads=config.num_attention_heads,\n+            dropout=config.dropout,\n+        )\n+        self.self_attn_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.encoder_activation_function]\n+        self.activation_dropout = config.activation_dropout\n+        self.fc1 = nn.Linear(config.encoder_hidden_dim, config.encoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.encoder_ffn_dim, config.encoder_hidden_dim)\n+        self.final_layer_norm = nn.LayerNorm(config.encoder_hidden_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        output_attentions: bool = False,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, target_len, source_len)` where padding elements are indicated by very large negative\n+                values.\n+            position_embeddings (`torch.FloatTensor`, *optional*):\n+                Object queries (also called content embeddings), to be added to the hidden states.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+        if self.normalize_before:\n+            hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_embeddings=position_embeddings,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        if not self.normalize_before:\n+            hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        if self.normalize_before:\n+            hidden_states = self.final_layer_norm(hidden_states)\n+        residual = hidden_states\n+\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+\n+        hidden_states = self.fc2(hidden_states)\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+\n+        hidden_states = residual + hidden_states\n+        if not self.normalize_before:\n+            hidden_states = self.final_layer_norm(hidden_states)\n+\n+        if self.training:\n+            if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n+                clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n+                hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n class DFineEncoder(nn.Module):\n     def __init__(self, config: DFineConfig):\n         super().__init__()"
        },
        {
            "sha": "c53fe5e1d1a361109fc72402d97de5c99fa3d2f9",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 101,
            "deletions": 101,
            "changes": 202,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -125,6 +125,45 @@ def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: boo\n         return embeddings\n \n \n+@auto_docstring\n+class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n+    config: InstructBlipVideoConfig\n+    base_model_prefix = \"blip\"\n+    supports_gradient_checkpointing = True\n+    _supports_attention_backend = True\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+\n+    _no_split_modules = [\n+        \"InstructBlipVideoQFormerEmbeddings\",\n+        \"InstructBlipVideoAttention\",\n+        \"InstructBlipVideoQFormerMultiHeadAttention\",\n+        \"InstructBlipVideoQFormerSelfOutput\",\n+    ]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_range\n+\n+        if isinstance(module, (nn.Linear, nn.Conv2d)):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, nn.Embedding):\n+            module.weight.data.normal_(mean=0.0, std=factor)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n+            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n+            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n+        elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n+            module.query_tokens.data.zero_()\n+\n+\n # Adapted from transformers.models.siglip.modeling_siglip.eager_attention_forward -> InstructBlipVideo doesn't cast attn weights to fp32\n def eager_attention_forward(\n     module: nn.Module,\n@@ -375,6 +414,68 @@ def forward(\n         )\n \n \n+class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+    config: InstructBlipVideoVisionConfig\n+\n+    def __init__(self, config: InstructBlipVideoVisionConfig):\n+        super().__init__(config)\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = InstructBlipVideoVisionEmbeddings(config)\n+        self.encoder = InstructBlipVideoEncoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+        self.post_init()\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> Union[tuple, BaseModelOutputWithPooling]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+    def get_input_embeddings(self):\n+        return self.embeddings\n+\n+\n class InstructBlipVideoQFormerMultiHeadAttention(nn.Module):\n     def __init__(self, config, is_cross_attention=False):\n         super().__init__()\n@@ -805,107 +906,6 @@ def forward(\n         return embeddings\n \n \n-@auto_docstring\n-class InstructBlipVideoPreTrainedModel(PreTrainedModel):\n-    config: InstructBlipVideoConfig\n-    base_model_prefix = \"blip\"\n-    supports_gradient_checkpointing = True\n-    _supports_attention_backend = True\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-    _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = True\n-\n-    _no_split_modules = [\n-        \"InstructBlipVideoQFormerEmbeddings\",\n-        \"InstructBlipVideoAttention\",\n-        \"InstructBlipVideoQFormerMultiHeadAttention\",\n-        \"InstructBlipVideoQFormerSelfOutput\",\n-    ]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        factor = self.config.initializer_range\n-\n-        if isinstance(module, (nn.Linear, nn.Conv2d)):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding):\n-            module.weight.data.normal_(mean=0.0, std=factor)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-        elif isinstance(module, InstructBlipVideoVisionEmbeddings):\n-            nn.init.trunc_normal_(module.position_embedding, mean=0.0, std=factor)\n-            nn.init.trunc_normal_(module.class_embedding, mean=0.0, std=factor)\n-        elif isinstance(module, (InstructBlipVideoForConditionalGeneration, InstructBlipVideoModel)):\n-            module.query_tokens.data.zero_()\n-\n-\n-class InstructBlipVideoVisionModel(InstructBlipVideoPreTrainedModel):\n-    main_input_name = \"pixel_values\"\n-    config: InstructBlipVideoVisionConfig\n-\n-    def __init__(self, config: InstructBlipVideoVisionConfig):\n-        super().__init__(config)\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = InstructBlipVideoVisionEmbeddings(config)\n-        self.encoder = InstructBlipVideoEncoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-\n-        self.post_init()\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        return_dict: Optional[bool] = None,\n-        interpolate_pos_encoding: bool = False,\n-    ) -> Union[tuple, BaseModelOutputWithPooling]:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n-\n-        encoder_outputs = self.encoder(\n-            inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-\n-        last_hidden_state = encoder_outputs[0]\n-        last_hidden_state = self.post_layernorm(last_hidden_state)\n-\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        if not return_dict:\n-            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-    def get_input_embeddings(self):\n-        return self.embeddings\n-\n-\n class InstructBlipVideoQFormerModel(InstructBlipVideoPreTrainedModel):\n     \"\"\"\n     Querying Transformer (Q-Former), used in InstructBlipVideo. Slightly modified from BLIP-2 as it also takes the"
        },
        {
            "sha": "50159634001580ad9600e907f0f745355166be91",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 21,
            "deletions": 59,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -39,15 +39,11 @@\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    logging,\n )\n from ..auto import AutoModel\n from .configuration_llava_onevision import LlavaOnevisionConfig\n \n \n-logger = logging.get_logger(__name__)\n-\n-\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -111,38 +107,31 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n     video_hidden_states: Optional[torch.FloatTensor] = None\n \n \n-class LlavaOnevisionPooler(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n+@auto_docstring\n+class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n+    config: LlavaOnevisionConfig\n+    base_model_prefix = \"\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"LlamaDecoderLayer\"]\n+    _skip_keys_device_placement = \"past_key_values\"\n \n-        mode = config.spatial_pool_mode\n-        stride = config.spatial_pool_stride\n-        out_channels = getattr(config, \"spatial_pool_out_channels\", config.vision_config.hidden_size)\n-        self.image_size = (config.vision_config.image_size // config.vision_config.patch_size) ** 2\n-\n-        if mode == \"average\":\n-            self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride)\n-        elif mode == \"max\":\n-            self.pool = nn.MaxPool2d(kernel_size=stride, stride=stride)\n-        elif mode == \"conv\":\n-            self.pool = nn.Conv2d(\n-                in_channels=config.vision_config.hidden_size,\n-                out_channels=out_channels,\n-                kernel_size=stride,\n-                stride=stride,\n-            )\n-        else:\n-            raise ValueError(f\"Unknown pooling mode: {mode}. Has to be one of [`average`, `max`, `conv`]\")\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n \n-    def forward(self, image_features):\n-        ori_width = int(math.sqrt(image_features.shape[1] * self.image_size // self.image_size))\n-        ori_height = int(ori_width * self.image_size // self.image_size)\n+    _can_compile_fullgraph = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n \n-        batch_size, _, dim = image_features.shape\n-        image_features_spatial = image_features.view(batch_size, ori_height, ori_height, dim).permute(0, 3, 1, 2)\n-        image_features_spatial_pool = self.pool(image_features_spatial)\n+    def _init_weights(self, module):\n+        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n \n-        return image_features_spatial_pool.flatten(2).transpose(1, 2).contiguous()\n+        if isinstance(module, nn.Linear):\n+            module.weight.data.normal_(mean=0.0, std=std)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+        elif isinstance(module, LlavaOnevisionModel):\n+            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n+            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n \n \n class LlavaOnevisionMultiModalProjector(nn.Module):\n@@ -274,33 +263,6 @@ def unpad_image(tensor, original_size):\n     return unpadded_tensor\n \n \n-@auto_docstring\n-class LlavaOnevisionPreTrainedModel(PreTrainedModel):\n-    config: LlavaOnevisionConfig\n-    base_model_prefix = \"\"\n-    supports_gradient_checkpointing = True\n-    _no_split_modules = [\"LlamaDecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n-\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-\n-    _can_compile_fullgraph = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-\n-    def _init_weights(self, module):\n-        std = getattr(self.config, \"initializer_range\", self.config.get_text_config().initializer_range)\n-\n-        if isinstance(module, nn.Linear):\n-            module.weight.data.normal_(mean=0.0, std=std)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-        elif isinstance(module, LlavaOnevisionModel):\n-            embed_std = 1 / math.sqrt(self.config.text_config.hidden_size)\n-            module.image_newline.data.normal_(mean=0.0, std=embed_std)\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The Llava-Next model which consists of a vision backbone and a language model without language modeling head."
        },
        {
            "sha": "56ab43c03010406324fa65e30aaab226f609b9c7",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "modified",
            "additions": 48,
            "deletions": 48,
            "changes": 96,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -715,54 +715,6 @@ def to_tuple(self) -> tuple[Any]:\n         )\n \n \n-class MetaClip2VisionTransformer(nn.Module):\n-    def __init__(self, config: MetaClip2VisionConfig):\n-        super().__init__()\n-        self.config = config\n-        embed_dim = config.hidden_size\n-\n-        self.embeddings = MetaClip2VisionEmbeddings(config)\n-        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-        self.encoder = MetaClip2Encoder(config)\n-        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        pixel_values: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = None,\n-        output_hidden_states: Optional[bool] = None,\n-        interpolate_pos_encoding: Optional[bool] = False,\n-    ) -> BaseModelOutputWithPooling:\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-\n-        if pixel_values is None:\n-            raise ValueError(\"You have to specify pixel_values\")\n-\n-        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n-        hidden_states = self.pre_layrnorm(hidden_states)\n-\n-        encoder_outputs: BaseModelOutput = self.encoder(\n-            inputs_embeds=hidden_states,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-        )\n-\n-        last_hidden_state = encoder_outputs.last_hidden_state\n-        pooled_output = last_hidden_state[:, 0, :]\n-        pooled_output = self.post_layernorm(pooled_output)\n-\n-        return BaseModelOutputWithPooling(\n-            last_hidden_state=last_hidden_state,\n-            pooler_output=pooled_output,\n-            hidden_states=encoder_outputs.hidden_states,\n-            attentions=encoder_outputs.attentions,\n-        )\n-\n-\n # contrastive loss function, adapted from\n # https://sachinruk.github.io/blog/2021-03-07-metaclip_2.html\n def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n@@ -1009,6 +961,54 @@ def forward(\n         )\n \n \n+class MetaClip2VisionTransformer(nn.Module):\n+    def __init__(self, config: MetaClip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = MetaClip2VisionEmbeddings(config)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.encoder = MetaClip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+    ) -> BaseModelOutputWithPooling:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     The vision model from METACLIP_2 without any head or projection on top."
        },
        {
            "sha": "70417e32e20bee640599e97f39d916426e3d91b8",
            "filename": "src/transformers/models/mm_grounding_dino/modeling_mm_grounding_dino.py",
            "status": "modified",
            "additions": 251,
            "deletions": 251,
            "changes": 502,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fmodeling_mm_grounding_dino.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -121,30 +121,6 @@ def forward(\n         return output.transpose(1, 2).contiguous()\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Base class for outputs of the MMGroundingDinoDecoder. This class adds two attributes to\n-    BaseModelOutputWithCrossAttentions, namely:\n-    - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n-    - a stacked tensor of intermediate reference points.\n-    \"\"\"\n-)\n-class MMGroundingDinoDecoderOutput(ModelOutput):\n-    r\"\"\"\n-    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-        Stacked intermediate hidden states (output of each layer of the decoder).\n-    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n-        Stacked intermediate reference points (reference points of each layer of the decoder).\n-    \"\"\"\n-\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n-\n-\n class MMGroundingDinoLearnedPositionEmbedding(nn.Module):\n     \"\"\"\n     This module learns positional embeddings up to a fixed maximum size.\n@@ -528,233 +504,6 @@ def forward(\n         return (vision_features, vision_attn), (text_features, text_attn)\n \n \n-class MMGroundingDinoMultiheadAttention(nn.Module):\n-    \"\"\"Equivalent implementation of nn.MultiheadAttention with `batch_first=True`.\"\"\"\n-\n-    def __init__(self, config, num_attention_heads=None):\n-        super().__init__()\n-        if config.hidden_size % num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n-            raise ValueError(\n-                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n-                f\"heads ({num_attention_heads})\"\n-            )\n-\n-        self.num_attention_heads = num_attention_heads\n-        self.attention_head_size = int(config.hidden_size / num_attention_heads)\n-        self.all_head_size = self.num_attention_heads * self.attention_head_size\n-\n-        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n-        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n-\n-        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n-\n-        self.dropout = nn.Dropout(config.attention_dropout)\n-\n-    def forward(\n-        self,\n-        queries: torch.Tensor,\n-        keys: torch.Tensor,\n-        values: torch.Tensor,\n-        attention_mask: Optional[torch.FloatTensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = queries.shape\n-        query_layer = (\n-            self.query(queries)\n-            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n-            .transpose(1, 2)\n-        )\n-        key_layer = (\n-            self.key(keys).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n-        value_layer = (\n-            self.value(values).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n-        )\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-        if attention_mask is not None:\n-            # Apply the attention mask is (precomputed for all layers in MMGroundingDinoModel forward() function)\n-            attention_scores = attention_scores + attention_mask\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        context_layer = self.out_proj(context_layer)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class MMGroundingDinoDecoderLayer(nn.Module):\n-    def __init__(self, config: MMGroundingDinoConfig):\n-        super().__init__()\n-        self.embed_dim = config.d_model\n-\n-        # self-attention\n-        self.self_attn = MMGroundingDinoMultiheadAttention(config, num_attention_heads=config.decoder_attention_heads)\n-\n-        self.dropout = config.dropout\n-        self.activation_fn = ACT2FN[config.activation_function]\n-        self.activation_dropout = config.activation_dropout\n-\n-        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n-        # cross-attention text\n-        self.encoder_attn_text = MMGroundingDinoMultiheadAttention(\n-            config, num_attention_heads=config.decoder_attention_heads\n-        )\n-        self.encoder_attn_text_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n-        # cross-attention\n-        self.encoder_attn = MMGroundingDinoMultiscaleDeformableAttention(\n-            config,\n-            num_heads=config.decoder_attention_heads,\n-            n_points=config.decoder_n_points,\n-        )\n-        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n-        # feedforward neural networks\n-        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n-        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n-        self.final_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n-\n-    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n-        return tensor if position_embeddings is None else tensor + position_embeddings\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        position_embeddings: Optional[torch.Tensor] = None,\n-        reference_points=None,\n-        spatial_shapes=None,\n-        spatial_shapes_list=None,\n-        level_start_index=None,\n-        vision_encoder_hidden_states: Optional[torch.Tensor] = None,\n-        vision_encoder_attention_mask: Optional[torch.Tensor] = None,\n-        text_encoder_hidden_states: Optional[torch.Tensor] = None,\n-        text_encoder_attention_mask: Optional[torch.Tensor] = None,\n-        self_attn_mask: Optional[torch.Tensor] = None,\n-        output_attentions: Optional[bool] = False,\n-    ):\n-        residual = hidden_states\n-\n-        # Self Attention\n-        queries = keys = self.with_pos_embed(hidden_states, position_embeddings)\n-        hidden_states, self_attn_weights = self.self_attn(\n-            queries=queries,\n-            keys=keys,\n-            values=hidden_states,\n-            attention_mask=self_attn_mask,\n-            output_attentions=True,\n-        )\n-\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.self_attn_layer_norm(hidden_states)\n-\n-        second_residual = hidden_states\n-\n-        # Cross-Attention Text\n-        queries = self.with_pos_embed(hidden_states, position_embeddings)\n-        hidden_states, text_cross_attn_weights = self.encoder_attn_text(\n-            queries=queries,\n-            keys=text_encoder_hidden_states,\n-            values=text_encoder_hidden_states,\n-            attention_mask=text_encoder_attention_mask,\n-            output_attentions=True,\n-        )\n-\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = second_residual + hidden_states\n-        hidden_states = self.encoder_attn_text_layer_norm(hidden_states)\n-\n-        third_residual = hidden_states\n-\n-        # Cross-Attention\n-        cross_attn_weights = None\n-        hidden_states, cross_attn_weights = self.encoder_attn(\n-            hidden_states=hidden_states,\n-            attention_mask=vision_encoder_attention_mask,\n-            encoder_hidden_states=vision_encoder_hidden_states,\n-            encoder_attention_mask=vision_encoder_attention_mask,\n-            position_embeddings=position_embeddings,\n-            reference_points=reference_points,\n-            spatial_shapes=spatial_shapes,\n-            spatial_shapes_list=spatial_shapes_list,\n-            level_start_index=level_start_index,\n-            output_attentions=output_attentions,\n-        )\n-\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = third_residual + hidden_states\n-        hidden_states = self.encoder_attn_layer_norm(hidden_states)\n-\n-        # Fully Connected\n-        residual = hidden_states\n-        hidden_states = self.activation_fn(self.fc1(hidden_states))\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n-        hidden_states = self.fc2(hidden_states)\n-        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n-        hidden_states = residual + hidden_states\n-        hidden_states = self.final_layer_norm(hidden_states)\n-\n-        outputs = (hidden_states,)\n-\n-        if output_attentions:\n-            outputs += (self_attn_weights, text_cross_attn_weights, cross_attn_weights)\n-\n-        return outputs\n-\n-\n-# Based on https://github.com/IDEA-Research/MMGroundingDino/blob/2b62f419c292ca9c518daae55512fabc3fead4a4/MMGroundingDino/models/MMGroundingDino/utils.py#L24\n-def get_sine_pos_embed(\n-    pos_tensor: torch.Tensor, num_pos_feats: int = 128, temperature: int = 10000, exchange_xy: bool = True\n-) -> Tensor:\n-    \"\"\"\n-    Generate sine position embeddings from a position tensor.\n-\n-    Args:\n-        pos_tensor (torch.Tensor):\n-            Tensor containing positions. Shape: [..., n].\n-        num_pos_feats (`int`, *optional*, defaults to 128):\n-            Projected shape for each float in the tensor.\n-        temperature (`int`, *optional*, defaults to 10000):\n-            Temperature in the sine/cosine function.\n-        exchange_xy (`bool`, *optional*, defaults to `True`):\n-            Exchange pos x and pos y. For example, input tensor is [x,y], the results will be [pos(y), pos(x)].\n-\n-    Returns:\n-        position_embeddings (torch.Tensor): shape: [..., n * hidden_size].\n-    \"\"\"\n-    scale = 2 * math.pi\n-    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos_tensor.device)\n-    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / num_pos_feats)\n-\n-    def sine_func(x: torch.Tensor):\n-        sin_x = x * scale / dim_t\n-        sin_x = torch.stack((sin_x[..., 0::2].sin(), sin_x[..., 1::2].cos()), dim=3).flatten(2)\n-        return sin_x\n-\n-    pos_tensor = pos_tensor.split([1] * pos_tensor.shape[-1], dim=-1)\n-    position_embeddings = [sine_func(x) for x in pos_tensor]\n-    if exchange_xy:\n-        position_embeddings[0], position_embeddings[1] = position_embeddings[1], position_embeddings[0]\n-    position_embeddings = torch.cat(position_embeddings, dim=-1)\n-    return position_embeddings\n-\n-\n @auto_docstring\n class MMGroundingDinoPreTrainedModel(PreTrainedModel):\n     config: MMGroundingDinoConfig\n@@ -1011,6 +760,78 @@ class MMGroundingDinoEncoderOutput(ModelOutput):\n     attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n \n \n+class MMGroundingDinoMultiheadAttention(nn.Module):\n+    \"\"\"Equivalent implementation of nn.MultiheadAttention with `batch_first=True`.\"\"\"\n+\n+    def __init__(self, config, num_attention_heads=None):\n+        super().__init__()\n+        if config.hidden_size % num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n+            raise ValueError(\n+                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n+                f\"heads ({num_attention_heads})\"\n+            )\n+\n+        self.num_attention_heads = num_attention_heads\n+        self.attention_head_size = int(config.hidden_size / num_attention_heads)\n+        self.all_head_size = self.num_attention_heads * self.attention_head_size\n+\n+        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n+        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n+\n+        self.out_proj = nn.Linear(config.hidden_size, config.hidden_size)\n+\n+        self.dropout = nn.Dropout(config.attention_dropout)\n+\n+    def forward(\n+        self,\n+        queries: torch.Tensor,\n+        keys: torch.Tensor,\n+        values: torch.Tensor,\n+        attention_mask: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor]:\n+        batch_size, seq_length, _ = queries.shape\n+        query_layer = (\n+            self.query(queries)\n+            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n+            .transpose(1, 2)\n+        )\n+        key_layer = (\n+            self.key(keys).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n+        value_layer = (\n+            self.value(values).view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(1, 2)\n+        )\n+\n+        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n+\n+        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n+        if attention_mask is not None:\n+            # Apply the attention mask is (precomputed for all layers in MMGroundingDinoModel forward() function)\n+            attention_scores = attention_scores + attention_mask\n+\n+        # Normalize the attention scores to probabilities.\n+        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n+\n+        # This is actually dropping out entire tokens to attend to, which might\n+        # seem a bit unusual, but is taken from the original Transformer paper.\n+        attention_probs = self.dropout(attention_probs)\n+\n+        context_layer = torch.matmul(attention_probs, value_layer)\n+\n+        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n+        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n+        context_layer = context_layer.view(new_context_layer_shape)\n+\n+        context_layer = self.out_proj(context_layer)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n+\n+\n class MMGroundingDinoTextEnhancerLayer(nn.Module):\n     \"\"\"Vanilla Transformer with text embeddings as input\"\"\"\n \n@@ -1177,6 +998,43 @@ def forward(\n         return hidden_states, attn_weights\n \n \n+# Based on https://github.com/IDEA-Research/MMGroundingDino/blob/2b62f419c292ca9c518daae55512fabc3fead4a4/MMGroundingDino/models/MMGroundingDino/utils.py#L24\n+def get_sine_pos_embed(\n+    pos_tensor: torch.Tensor, num_pos_feats: int = 128, temperature: int = 10000, exchange_xy: bool = True\n+) -> Tensor:\n+    \"\"\"\n+    Generate sine position embeddings from a position tensor.\n+\n+    Args:\n+        pos_tensor (torch.Tensor):\n+            Tensor containing positions. Shape: [..., n].\n+        num_pos_feats (`int`, *optional*, defaults to 128):\n+            Projected shape for each float in the tensor.\n+        temperature (`int`, *optional*, defaults to 10000):\n+            Temperature in the sine/cosine function.\n+        exchange_xy (`bool`, *optional*, defaults to `True`):\n+            Exchange pos x and pos y. For example, input tensor is [x,y], the results will be [pos(y), pos(x)].\n+\n+    Returns:\n+        position_embeddings (torch.Tensor): shape: [..., n * hidden_size].\n+    \"\"\"\n+    scale = 2 * math.pi\n+    dim_t = torch.arange(num_pos_feats, dtype=torch.float32, device=pos_tensor.device)\n+    dim_t = temperature ** (2 * torch.div(dim_t, 2, rounding_mode=\"floor\") / num_pos_feats)\n+\n+    def sine_func(x: torch.Tensor):\n+        sin_x = x * scale / dim_t\n+        sin_x = torch.stack((sin_x[..., 0::2].sin(), sin_x[..., 1::2].cos()), dim=3).flatten(2)\n+        return sin_x\n+\n+    pos_tensor = pos_tensor.split([1] * pos_tensor.shape[-1], dim=-1)\n+    position_embeddings = [sine_func(x) for x in pos_tensor]\n+    if exchange_xy:\n+        position_embeddings[0], position_embeddings[1] = position_embeddings[1], position_embeddings[0]\n+    position_embeddings = torch.cat(position_embeddings, dim=-1)\n+    return position_embeddings\n+\n+\n class MMGroundingDinoEncoderLayer(nn.Module):\n     def __init__(self, config) -> None:\n         super().__init__()\n@@ -1429,6 +1287,148 @@ def forward(\n         )\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for outputs of the MMGroundingDinoDecoder. This class adds two attributes to\n+    BaseModelOutputWithCrossAttentions, namely:\n+    - a stacked tensor of intermediate decoder hidden states (i.e. the output of each decoder layer)\n+    - a stacked tensor of intermediate reference points.\n+    \"\"\"\n+)\n+class MMGroundingDinoDecoderOutput(ModelOutput):\n+    r\"\"\"\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, sequence_length, hidden_size)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    \"\"\"\n+\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    attentions: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+\n+\n+class MMGroundingDinoDecoderLayer(nn.Module):\n+    def __init__(self, config: MMGroundingDinoConfig):\n+        super().__init__()\n+        self.embed_dim = config.d_model\n+\n+        # self-attention\n+        self.self_attn = MMGroundingDinoMultiheadAttention(config, num_attention_heads=config.decoder_attention_heads)\n+\n+        self.dropout = config.dropout\n+        self.activation_fn = ACT2FN[config.activation_function]\n+        self.activation_dropout = config.activation_dropout\n+\n+        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n+        # cross-attention text\n+        self.encoder_attn_text = MMGroundingDinoMultiheadAttention(\n+            config, num_attention_heads=config.decoder_attention_heads\n+        )\n+        self.encoder_attn_text_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n+        # cross-attention\n+        self.encoder_attn = MMGroundingDinoMultiscaleDeformableAttention(\n+            config,\n+            num_heads=config.decoder_attention_heads,\n+            n_points=config.decoder_n_points,\n+        )\n+        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n+        # feedforward neural networks\n+        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n+        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n+        self.final_layer_norm = nn.LayerNorm(self.embed_dim, config.layer_norm_eps)\n+\n+    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n+        return tensor if position_embeddings is None else tensor + position_embeddings\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Optional[torch.Tensor] = None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        vision_encoder_hidden_states: Optional[torch.Tensor] = None,\n+        vision_encoder_attention_mask: Optional[torch.Tensor] = None,\n+        text_encoder_hidden_states: Optional[torch.Tensor] = None,\n+        text_encoder_attention_mask: Optional[torch.Tensor] = None,\n+        self_attn_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ):\n+        residual = hidden_states\n+\n+        # Self Attention\n+        queries = keys = self.with_pos_embed(hidden_states, position_embeddings)\n+        hidden_states, self_attn_weights = self.self_attn(\n+            queries=queries,\n+            keys=keys,\n+            values=hidden_states,\n+            attention_mask=self_attn_mask,\n+            output_attentions=True,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.self_attn_layer_norm(hidden_states)\n+\n+        second_residual = hidden_states\n+\n+        # Cross-Attention Text\n+        queries = self.with_pos_embed(hidden_states, position_embeddings)\n+        hidden_states, text_cross_attn_weights = self.encoder_attn_text(\n+            queries=queries,\n+            keys=text_encoder_hidden_states,\n+            values=text_encoder_hidden_states,\n+            attention_mask=text_encoder_attention_mask,\n+            output_attentions=True,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = second_residual + hidden_states\n+        hidden_states = self.encoder_attn_text_layer_norm(hidden_states)\n+\n+        third_residual = hidden_states\n+\n+        # Cross-Attention\n+        cross_attn_weights = None\n+        hidden_states, cross_attn_weights = self.encoder_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=vision_encoder_attention_mask,\n+            encoder_hidden_states=vision_encoder_hidden_states,\n+            encoder_attention_mask=vision_encoder_attention_mask,\n+            position_embeddings=position_embeddings,\n+            reference_points=reference_points,\n+            spatial_shapes=spatial_shapes,\n+            spatial_shapes_list=spatial_shapes_list,\n+            level_start_index=level_start_index,\n+            output_attentions=output_attentions,\n+        )\n+\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = third_residual + hidden_states\n+        hidden_states = self.encoder_attn_layer_norm(hidden_states)\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.activation_fn(self.fc1(hidden_states))\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n+        hidden_states = self.fc2(hidden_states)\n+        hidden_states = nn.functional.dropout(hidden_states, p=self.dropout, training=self.training)\n+        hidden_states = residual + hidden_states\n+        hidden_states = self.final_layer_norm(hidden_states)\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (self_attn_weights, text_cross_attn_weights, cross_attn_weights)\n+\n+        return outputs\n+\n+\n class MMGroundingDinoDecoder(MMGroundingDinoPreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a [`MMGroundingDinoDecoderLayer`]."
        },
        {
            "sha": "140240cba6f744b3a27886c9f9aab35659d3b727",
            "filename": "src/transformers/models/modernbert_decoder/modeling_modernbert_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodeling_modernbert_decoder.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -397,9 +397,9 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertDecoderPredictionHead):\n             init_weight(module.dense, stds[\"out\"])\n-        elif module.__class__.__name__ == \"ModernBertDecoderForSequenceClassification\":\n+        elif isinstance(module, ModernBertDecoderForSequenceClassification):\n             init_weight(module.classifier, stds[\"final_out\"])\n-        elif module.__class__.__name__ == \"ModernBertDecoderForCausalLM\":\n+        elif isinstance(module, ModernBertDecoderForCausalLM):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n             module.weight.data.fill_(1.0)"
        },
        {
            "sha": "05bf9c98d01cec04c75bac108caa06f981eec24e",
            "filename": "src/transformers/models/modernbert_decoder/modular_modernbert_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert_decoder%2Fmodular_modernbert_decoder.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -459,9 +459,9 @@ def init_weight(module: nn.Module, std: float):\n             init_weight(module.Wo, stds[\"out\"])\n         elif isinstance(module, ModernBertDecoderPredictionHead):\n             init_weight(module.dense, stds[\"out\"])\n-        elif module.__class__.__name__ == \"ModernBertDecoderForSequenceClassification\":\n+        elif isinstance(module, ModernBertDecoderForSequenceClassification):\n             init_weight(module.classifier, stds[\"final_out\"])\n-        elif module.__class__.__name__ == \"ModernBertDecoderForCausalLM\":\n+        elif isinstance(module, ModernBertDecoderForCausalLM):\n             init_weight(module.decoder, stds[\"out\"])\n         elif isinstance(module, nn.LayerNorm):\n             module.weight.data.fill_(1.0)"
        },
        {
            "sha": "548a9378a2c0ca5d1a488cba55c356e1ff33f67f",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 416,
            "deletions": 420,
            "changes": 836,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -449,6 +449,72 @@ def forward(\n         return outputs\n \n \n+@auto_docstring\n+class RTDetrV2PreTrainedModel(PreTrainedModel):\n+    config: RTDetrV2Config\n+    base_model_prefix = \"rt_detr_v2\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, (RTDetrV2ForObjectDetection, RTDetrV2Decoder)):\n+            if module.class_embed is not None:\n+                for layer in module.class_embed:\n+                    prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+                    bias = float(-math.log((1 - prior_prob) / prior_prob))\n+                    nn.init.xavier_uniform_(layer.weight)\n+                    nn.init.constant_(layer.bias, bias)\n+\n+            if module.bbox_embed is not None:\n+                for layer in module.bbox_embed:\n+                    nn.init.constant_(layer.layers[-1].weight, 0)\n+                    nn.init.constant_(layer.layers[-1].bias, 0)\n+\n+        elif isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n+            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n+            default_dtype = torch.get_default_dtype()\n+            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n+                2.0 * math.pi / module.n_heads\n+            )\n+            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n+            grid_init = (\n+                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n+                .view(module.n_heads, 1, 1, 2)\n+                .repeat(1, module.n_levels, module.n_points, 1)\n+            )\n+            for i in range(module.n_points):\n+                grid_init[:, :, i, :] *= i + 1\n+            with torch.no_grad():\n+                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n+            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n+            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.value_proj.weight.data)\n+            nn.init.constant_(module.value_proj.bias.data, 0.0)\n+            nn.init.xavier_uniform_(module.output_proj.weight.data)\n+            nn.init.constant_(module.output_proj.bias.data, 0.0)\n+\n+        elif isinstance(module, RTDetrV2Model):\n+            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n+            bias = float(-math.log((1 - prior_prob) / prior_prob))\n+            nn.init.xavier_uniform_(module.enc_score_head.weight)\n+            nn.init.constant_(module.enc_score_head.bias, bias)\n+\n+        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n+            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n+\n+        elif isinstance(module, nn.LayerNorm):\n+            module.weight.data.fill_(1.0)\n+            module.bias.data.zero_()\n+\n+        if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n+            nn.init.xavier_uniform_(module.weight_embedding.weight)\n+        if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n+            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n+\n+\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -487,6 +553,172 @@ class RTDetrV2DecoderOutput(ModelOutput):\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n \n \n+def inverse_sigmoid(x, eps=1e-5):\n+    x = x.clamp(min=0, max=1)\n+    x1 = x.clamp(min=eps)\n+    x2 = (1 - x).clamp(min=eps)\n+    return torch.log(x1 / x2)\n+\n+\n+class RTDetrV2Decoder(RTDetrV2PreTrainedModel):\n+    def __init__(self, config: RTDetrV2Config):\n+        super().__init__(config)\n+\n+        self.dropout = config.dropout\n+        self.layers = nn.ModuleList([RTDetrV2DecoderLayer(config) for _ in range(config.decoder_layers)])\n+        self.query_pos_head = RTDetrV2MLPPredictionHead(config, 4, 2 * config.d_model, config.d_model, num_layers=2)\n+\n+        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n+        self.bbox_embed = None\n+        self.class_embed = None\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def forward(\n+        self,\n+        inputs_embeds=None,\n+        encoder_hidden_states=None,\n+        encoder_attention_mask=None,\n+        position_embeddings=None,\n+        reference_points=None,\n+        spatial_shapes=None,\n+        spatial_shapes_list=None,\n+        level_start_index=None,\n+        valid_ratios=None,\n+        output_attentions=None,\n+        output_hidden_states=None,\n+        return_dict=None,\n+    ):\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+                The query embeddings that are passed into the decoder.\n+            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n+                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n+                of the decoder.\n+            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n+                in `[0, 1]`:\n+                - 1 for pixels that are real (i.e. **not masked**),\n+                - 0 for pixels that are padding (i.e. **masked**).\n+            position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n+                Position embeddings that are added to the queries and keys in each self-attention layer.\n+            reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)` is `as_two_stage` else `(batch_size, num_queries, 2)` or , *optional*):\n+                Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n+            spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n+                Spatial shapes of the feature maps.\n+            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n+                Indexes for the start of each feature level. In range `[0, sequence_length]`.\n+            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n+                Ratio of valid area in each feature level.\n+\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if inputs_embeds is not None:\n+            hidden_states = inputs_embeds\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n+        intermediate = ()\n+        intermediate_reference_points = ()\n+        intermediate_logits = ()\n+\n+        reference_points = F.sigmoid(reference_points)\n+\n+        # https://github.com/lyuwenyu/RT-DETR/blob/94f5e16708329d2f2716426868ec89aa774af016/RTDetrV2_pytorch/src/zoo/RTDetrV2/RTDetrV2_decoder.py#L252\n+        for idx, decoder_layer in enumerate(self.layers):\n+            reference_points_input = reference_points.unsqueeze(2)\n+            position_embeddings = self.query_pos_head(reference_points)\n+\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                position_embeddings=position_embeddings,\n+                encoder_hidden_states=encoder_hidden_states,\n+                reference_points=reference_points_input,\n+                spatial_shapes=spatial_shapes,\n+                spatial_shapes_list=spatial_shapes_list,\n+                level_start_index=level_start_index,\n+                encoder_attention_mask=encoder_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            # hack implementation for iterative bounding box refinement\n+            if self.bbox_embed is not None:\n+                predicted_corners = self.bbox_embed[idx](hidden_states)\n+                new_reference_points = F.sigmoid(predicted_corners + inverse_sigmoid(reference_points))\n+                reference_points = new_reference_points.detach()\n+\n+            intermediate += (hidden_states,)\n+            intermediate_reference_points += (\n+                (new_reference_points,) if self.bbox_embed is not None else (reference_points,)\n+            )\n+\n+            if self.class_embed is not None:\n+                logits = self.class_embed[idx](hidden_states)\n+                intermediate_logits += (logits,)\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+                if encoder_hidden_states is not None:\n+                    all_cross_attentions += (layer_outputs[2],)\n+\n+        # Keep batch_size as first dimension\n+        intermediate = torch.stack(intermediate, dim=1)\n+        intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n+        if self.class_embed is not None:\n+            intermediate_logits = torch.stack(intermediate_logits, dim=1)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(\n+                v\n+                for v in [\n+                    hidden_states,\n+                    intermediate,\n+                    intermediate_logits,\n+                    intermediate_reference_points,\n+                    all_hidden_states,\n+                    all_self_attns,\n+                    all_cross_attentions,\n+                ]\n+                if v is not None\n+            )\n+        return RTDetrV2DecoderOutput(\n+            last_hidden_state=hidden_states,\n+            intermediate_hidden_states=intermediate,\n+            intermediate_logits=intermediate_logits,\n+            intermediate_reference_points=intermediate_reference_points,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+            cross_attentions=all_cross_attentions,\n+        )\n+\n+\n @dataclass\n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -545,85 +777,7 @@ class RTDetrV2ModelOutput(ModelOutput):\n     denoising_meta_values: Optional[dict] = None\n \n \n-@dataclass\n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    Output type of [`RTDetrV2ForObjectDetection`].\n-    \"\"\"\n-)\n-class RTDetrV2ObjectDetectionOutput(ModelOutput):\n-    r\"\"\"\n-    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n-        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n-        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n-        scale-invariant IoU loss.\n-    loss_dict (`Dict`, *optional*):\n-        A dictionary containing the individual losses. Useful for logging.\n-    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n-        Classification logits (including no-object) for all queries.\n-    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n-        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n-        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-        possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the\n-        unnormalized (absolute) bounding boxes.\n-    auxiliary_outputs (`list[Dict]`, *optional*):\n-        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n-        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n-        `pred_boxes`) for each decoder layer.\n-    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n-    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n-        Stacked intermediate hidden states (output of each layer of the decoder).\n-    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n-        Stacked intermediate logits (logits of each layer of the decoder).\n-    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked intermediate reference points (reference points of each layer of the decoder).\n-    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n-    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n-        Stacked initial reference points (initial reference points of each layer of the decoder).\n-    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n-        Initial reference points sent through the Transformer decoder.\n-    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the encoder.\n-    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the encoder.\n-    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n-        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n-        foreground and background).\n-    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n-        Logits of predicted bounding boxes coordinates in the first stage.\n-    denoising_meta_values (`dict`):\n-        Extra dictionary for the denoising related values\n-    \"\"\"\n-\n-    loss: Optional[torch.FloatTensor] = None\n-    loss_dict: Optional[dict] = None\n-    logits: Optional[torch.FloatTensor] = None\n-    pred_boxes: Optional[torch.FloatTensor] = None\n-    auxiliary_outputs: Optional[list[dict]] = None\n-    last_hidden_state: Optional[torch.FloatTensor] = None\n-    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n-    intermediate_logits: Optional[torch.FloatTensor] = None\n-    intermediate_reference_points: Optional[torch.FloatTensor] = None\n-    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n-    initial_reference_points: Optional[torch.FloatTensor] = None\n-    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n-    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n-    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n-    init_reference_points: Optional[tuple[torch.FloatTensor]] = None\n-    enc_topk_logits: Optional[torch.FloatTensor] = None\n-    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n-    enc_outputs_class: Optional[torch.FloatTensor] = None\n-    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n-    denoising_meta_values: Optional[dict] = None\n-\n-\n-class RTDetrV2FrozenBatchNorm2d(nn.Module):\n+class RTDetrV2FrozenBatchNorm2d(nn.Module):\n     \"\"\"\n     BatchNorm2d where the batch statistics and the affine parameters are fixed.\n \n@@ -1082,363 +1236,127 @@ def forward(\n         )\n \n \n-def inverse_sigmoid(x, eps=1e-5):\n-    x = x.clamp(min=0, max=1)\n-    x1 = x.clamp(min=eps)\n-    x2 = (1 - x).clamp(min=eps)\n-    return torch.log(x1 / x2)\n-\n-\n def get_contrastive_denoising_training_group(\n     targets,\n-    num_classes,\n-    num_queries,\n-    class_embed,\n-    num_denoising_queries=100,\n-    label_noise_ratio=0.5,\n-    box_noise_scale=1.0,\n-):\n-    \"\"\"\n-    Creates a contrastive denoising training group using ground-truth samples. It adds noise to labels and boxes.\n-\n-    Args:\n-        targets (`list[dict]`):\n-            The target objects, each containing 'class_labels' and 'boxes' for objects in an image.\n-        num_classes (`int`):\n-            Total number of classes in the dataset.\n-        num_queries (`int`):\n-            Number of query slots in the transformer.\n-        class_embed (`callable`):\n-            A function or a model layer to embed class labels.\n-        num_denoising_queries (`int`, *optional*, defaults to 100):\n-            Number of denoising queries.\n-        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n-            Ratio of noise applied to labels.\n-        box_noise_scale (`float`, *optional*, defaults to 1.0):\n-            Scale of noise applied to bounding boxes.\n-    Returns:\n-        `tuple` comprising various elements:\n-        - **input_query_class** (`torch.FloatTensor`) --\n-          Class queries with applied label noise.\n-        - **input_query_bbox** (`torch.FloatTensor`) --\n-          Bounding box queries with applied box noise.\n-        - **attn_mask** (`torch.FloatTensor`) --\n-           Attention mask for separating denoising and reconstruction queries.\n-        - **denoising_meta_values** (`dict`) --\n-          Metadata including denoising positive indices, number of groups, and split sizes.\n-    \"\"\"\n-\n-    if num_denoising_queries <= 0:\n-        return None, None, None, None\n-\n-    num_ground_truths = [len(t[\"class_labels\"]) for t in targets]\n-    device = targets[0][\"class_labels\"].device\n-\n-    max_gt_num = max(num_ground_truths)\n-    if max_gt_num == 0:\n-        return None, None, None, None\n-\n-    num_groups_denoising_queries = num_denoising_queries // max_gt_num\n-    num_groups_denoising_queries = 1 if num_groups_denoising_queries == 0 else num_groups_denoising_queries\n-    # pad gt to max_num of a batch\n-    batch_size = len(num_ground_truths)\n-\n-    input_query_class = torch.full([batch_size, max_gt_num], num_classes, dtype=torch.int32, device=device)\n-    input_query_bbox = torch.zeros([batch_size, max_gt_num, 4], device=device)\n-    pad_gt_mask = torch.zeros([batch_size, max_gt_num], dtype=torch.bool, device=device)\n-\n-    for i in range(batch_size):\n-        num_gt = num_ground_truths[i]\n-        if num_gt > 0:\n-            input_query_class[i, :num_gt] = targets[i][\"class_labels\"]\n-            input_query_bbox[i, :num_gt] = targets[i][\"boxes\"]\n-            pad_gt_mask[i, :num_gt] = 1\n-    # each group has positive and negative queries.\n-    input_query_class = input_query_class.tile([1, 2 * num_groups_denoising_queries])\n-    input_query_bbox = input_query_bbox.tile([1, 2 * num_groups_denoising_queries, 1])\n-    pad_gt_mask = pad_gt_mask.tile([1, 2 * num_groups_denoising_queries])\n-    # positive and negative mask\n-    negative_gt_mask = torch.zeros([batch_size, max_gt_num * 2, 1], device=device)\n-    negative_gt_mask[:, max_gt_num:] = 1\n-    negative_gt_mask = negative_gt_mask.tile([1, num_groups_denoising_queries, 1])\n-    positive_gt_mask = 1 - negative_gt_mask\n-    # contrastive denoising training positive index\n-    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n-    denoise_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n-    denoise_positive_idx = torch.split(\n-        denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n-    )\n-    # total denoising queries\n-    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n-\n-    if label_noise_ratio > 0:\n-        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n-        # randomly put a new one here\n-        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n-        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n-\n-    if box_noise_scale > 0:\n-        known_bbox = center_to_corners_format(input_query_bbox)\n-        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n-        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n-        rand_part = torch.rand_like(input_query_bbox)\n-        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n-        rand_part *= rand_sign\n-        known_bbox += rand_part * diff\n-        known_bbox.clip_(min=0.0, max=1.0)\n-        input_query_bbox = corners_to_center_format(known_bbox)\n-        input_query_bbox = inverse_sigmoid(input_query_bbox)\n-\n-    input_query_class = class_embed(input_query_class)\n-\n-    target_size = num_denoising_queries + num_queries\n-    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n-    # match query cannot see the reconstruction\n-    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n-\n-    # reconstructions cannot see each other\n-    for i in range(num_groups_denoising_queries):\n-        idx_block_start = max_gt_num * 2 * i\n-        idx_block_end = max_gt_num * 2 * (i + 1)\n-        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n-        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n-\n-    denoising_meta_values = {\n-        \"dn_positive_idx\": denoise_positive_idx,\n-        \"dn_num_group\": num_groups_denoising_queries,\n-        \"dn_num_split\": [num_denoising_queries, num_queries],\n-    }\n-\n-    return input_query_class, input_query_bbox, attn_mask, denoising_meta_values\n-\n-\n-def _get_clones(partial_module, N):\n-    return nn.ModuleList([partial_module() for i in range(N)])\n-\n-\n-@auto_docstring\n-class RTDetrV2PreTrainedModel(PreTrainedModel):\n-    config: RTDetrV2Config\n-    base_model_prefix = \"rt_detr_v2\"\n-    main_input_name = \"pixel_values\"\n-    _no_split_modules = [r\"RTDetrV2HybridEncoder\", r\"RTDetrV2DecoderLayer\"]\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, (RTDetrV2ForObjectDetection, RTDetrV2Decoder)):\n-            if module.class_embed is not None:\n-                for layer in module.class_embed:\n-                    prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n-                    bias = float(-math.log((1 - prior_prob) / prior_prob))\n-                    nn.init.xavier_uniform_(layer.weight)\n-                    nn.init.constant_(layer.bias, bias)\n-\n-            if module.bbox_embed is not None:\n-                for layer in module.bbox_embed:\n-                    nn.init.constant_(layer.layers[-1].weight, 0)\n-                    nn.init.constant_(layer.layers[-1].bias, 0)\n-\n-        elif isinstance(module, RTDetrV2MultiscaleDeformableAttention):\n-            nn.init.constant_(module.sampling_offsets.weight.data, 0.0)\n-            default_dtype = torch.get_default_dtype()\n-            thetas = torch.arange(module.n_heads, dtype=torch.int64).to(default_dtype) * (\n-                2.0 * math.pi / module.n_heads\n-            )\n-            grid_init = torch.stack([thetas.cos(), thetas.sin()], -1)\n-            grid_init = (\n-                (grid_init / grid_init.abs().max(-1, keepdim=True)[0])\n-                .view(module.n_heads, 1, 1, 2)\n-                .repeat(1, module.n_levels, module.n_points, 1)\n-            )\n-            for i in range(module.n_points):\n-                grid_init[:, :, i, :] *= i + 1\n-            with torch.no_grad():\n-                module.sampling_offsets.bias = nn.Parameter(grid_init.view(-1))\n-            nn.init.constant_(module.attention_weights.weight.data, 0.0)\n-            nn.init.constant_(module.attention_weights.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.value_proj.weight.data)\n-            nn.init.constant_(module.value_proj.bias.data, 0.0)\n-            nn.init.xavier_uniform_(module.output_proj.weight.data)\n-            nn.init.constant_(module.output_proj.bias.data, 0.0)\n-\n-        elif isinstance(module, RTDetrV2Model):\n-            prior_prob = self.config.initializer_bias_prior_prob or 1 / (self.config.num_labels + 1)\n-            bias = float(-math.log((1 - prior_prob) / prior_prob))\n-            nn.init.xavier_uniform_(module.enc_score_head.weight)\n-            nn.init.constant_(module.enc_score_head.bias, bias)\n-\n-        elif isinstance(module, (nn.Linear, nn.Conv2d, nn.BatchNorm2d)):\n-            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n-            if module.bias is not None:\n-                module.bias.data.zero_()\n-\n-        elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n-\n-        if hasattr(module, \"weight_embedding\") and self.config.learn_initial_query:\n-            nn.init.xavier_uniform_(module.weight_embedding.weight)\n-        if hasattr(module, \"denoising_class_embed\") and self.config.num_denoising > 0:\n-            nn.init.xavier_uniform_(module.denoising_class_embed.weight)\n-\n-\n-class RTDetrV2Decoder(RTDetrV2PreTrainedModel):\n-    def __init__(self, config: RTDetrV2Config):\n-        super().__init__(config)\n-\n-        self.dropout = config.dropout\n-        self.layers = nn.ModuleList([RTDetrV2DecoderLayer(config) for _ in range(config.decoder_layers)])\n-        self.query_pos_head = RTDetrV2MLPPredictionHead(config, 4, 2 * config.d_model, config.d_model, num_layers=2)\n-\n-        # hack implementation for iterative bounding box refinement and two-stage Deformable DETR\n-        self.bbox_embed = None\n-        self.class_embed = None\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    def forward(\n-        self,\n-        inputs_embeds=None,\n-        encoder_hidden_states=None,\n-        encoder_attention_mask=None,\n-        position_embeddings=None,\n-        reference_points=None,\n-        spatial_shapes=None,\n-        spatial_shapes_list=None,\n-        level_start_index=None,\n-        valid_ratios=None,\n-        output_attentions=None,\n-        output_hidden_states=None,\n-        return_dict=None,\n-    ):\n-        r\"\"\"\n-        Args:\n-            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n-                The query embeddings that are passed into the decoder.\n-            encoder_hidden_states (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\n-                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n-                of the decoder.\n-            encoder_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-                Mask to avoid performing cross-attention on padding pixel_values of the encoder. Mask values selected\n-                in `[0, 1]`:\n-                - 1 for pixels that are real (i.e. **not masked**),\n-                - 0 for pixels that are padding (i.e. **masked**).\n-            position_embeddings (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`, *optional*):\n-                Position embeddings that are added to the queries and keys in each self-attention layer.\n-            reference_points (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)` is `as_two_stage` else `(batch_size, num_queries, 2)` or , *optional*):\n-                Reference point in range `[0, 1]`, top-left (0,0), bottom-right (1, 1), including padding area.\n-            spatial_shapes (`torch.FloatTensor` of shape `(num_feature_levels, 2)`):\n-                Spatial shapes of the feature maps.\n-            level_start_index (`torch.LongTensor` of shape `(num_feature_levels)`, *optional*):\n-                Indexes for the start of each feature level. In range `[0, sequence_length]`.\n-            valid_ratios (`torch.FloatTensor` of shape `(batch_size, num_feature_levels, 2)`, *optional*):\n-                Ratio of valid area in each feature level.\n-\n-            output_attentions (`bool`, *optional*):\n-                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n-                returned tensors for more detail.\n-            output_hidden_states (`bool`, *optional*):\n-                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n-                for more detail.\n-            return_dict (`bool`, *optional*):\n-                Whether or not to return a [`~file_utils.ModelOutput`] instead of a plain tuple.\n-        \"\"\"\n-        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n-        output_hidden_states = (\n-            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n-        )\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-\n-        if inputs_embeds is not None:\n-            hidden_states = inputs_embeds\n+    num_classes,\n+    num_queries,\n+    class_embed,\n+    num_denoising_queries=100,\n+    label_noise_ratio=0.5,\n+    box_noise_scale=1.0,\n+):\n+    \"\"\"\n+    Creates a contrastive denoising training group using ground-truth samples. It adds noise to labels and boxes.\n \n-        # decoder layers\n-        all_hidden_states = () if output_hidden_states else None\n-        all_self_attns = () if output_attentions else None\n-        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n-        intermediate = ()\n-        intermediate_reference_points = ()\n-        intermediate_logits = ()\n+    Args:\n+        targets (`list[dict]`):\n+            The target objects, each containing 'class_labels' and 'boxes' for objects in an image.\n+        num_classes (`int`):\n+            Total number of classes in the dataset.\n+        num_queries (`int`):\n+            Number of query slots in the transformer.\n+        class_embed (`callable`):\n+            A function or a model layer to embed class labels.\n+        num_denoising_queries (`int`, *optional*, defaults to 100):\n+            Number of denoising queries.\n+        label_noise_ratio (`float`, *optional*, defaults to 0.5):\n+            Ratio of noise applied to labels.\n+        box_noise_scale (`float`, *optional*, defaults to 1.0):\n+            Scale of noise applied to bounding boxes.\n+    Returns:\n+        `tuple` comprising various elements:\n+        - **input_query_class** (`torch.FloatTensor`) --\n+          Class queries with applied label noise.\n+        - **input_query_bbox** (`torch.FloatTensor`) --\n+          Bounding box queries with applied box noise.\n+        - **attn_mask** (`torch.FloatTensor`) --\n+           Attention mask for separating denoising and reconstruction queries.\n+        - **denoising_meta_values** (`dict`) --\n+          Metadata including denoising positive indices, number of groups, and split sizes.\n+    \"\"\"\n \n-        reference_points = F.sigmoid(reference_points)\n+    if num_denoising_queries <= 0:\n+        return None, None, None, None\n \n-        # https://github.com/lyuwenyu/RT-DETR/blob/94f5e16708329d2f2716426868ec89aa774af016/RTDetrV2_pytorch/src/zoo/RTDetrV2/RTDetrV2_decoder.py#L252\n-        for idx, decoder_layer in enumerate(self.layers):\n-            reference_points_input = reference_points.unsqueeze(2)\n-            position_embeddings = self.query_pos_head(reference_points)\n+    num_ground_truths = [len(t[\"class_labels\"]) for t in targets]\n+    device = targets[0][\"class_labels\"].device\n \n-            if output_hidden_states:\n-                all_hidden_states += (hidden_states,)\n+    max_gt_num = max(num_ground_truths)\n+    if max_gt_num == 0:\n+        return None, None, None, None\n \n-            layer_outputs = decoder_layer(\n-                hidden_states,\n-                position_embeddings=position_embeddings,\n-                encoder_hidden_states=encoder_hidden_states,\n-                reference_points=reference_points_input,\n-                spatial_shapes=spatial_shapes,\n-                spatial_shapes_list=spatial_shapes_list,\n-                level_start_index=level_start_index,\n-                encoder_attention_mask=encoder_attention_mask,\n-                output_attentions=output_attentions,\n-            )\n+    num_groups_denoising_queries = num_denoising_queries // max_gt_num\n+    num_groups_denoising_queries = 1 if num_groups_denoising_queries == 0 else num_groups_denoising_queries\n+    # pad gt to max_num of a batch\n+    batch_size = len(num_ground_truths)\n \n-            hidden_states = layer_outputs[0]\n+    input_query_class = torch.full([batch_size, max_gt_num], num_classes, dtype=torch.int32, device=device)\n+    input_query_bbox = torch.zeros([batch_size, max_gt_num, 4], device=device)\n+    pad_gt_mask = torch.zeros([batch_size, max_gt_num], dtype=torch.bool, device=device)\n \n-            # hack implementation for iterative bounding box refinement\n-            if self.bbox_embed is not None:\n-                predicted_corners = self.bbox_embed[idx](hidden_states)\n-                new_reference_points = F.sigmoid(predicted_corners + inverse_sigmoid(reference_points))\n-                reference_points = new_reference_points.detach()\n+    for i in range(batch_size):\n+        num_gt = num_ground_truths[i]\n+        if num_gt > 0:\n+            input_query_class[i, :num_gt] = targets[i][\"class_labels\"]\n+            input_query_bbox[i, :num_gt] = targets[i][\"boxes\"]\n+            pad_gt_mask[i, :num_gt] = 1\n+    # each group has positive and negative queries.\n+    input_query_class = input_query_class.tile([1, 2 * num_groups_denoising_queries])\n+    input_query_bbox = input_query_bbox.tile([1, 2 * num_groups_denoising_queries, 1])\n+    pad_gt_mask = pad_gt_mask.tile([1, 2 * num_groups_denoising_queries])\n+    # positive and negative mask\n+    negative_gt_mask = torch.zeros([batch_size, max_gt_num * 2, 1], device=device)\n+    negative_gt_mask[:, max_gt_num:] = 1\n+    negative_gt_mask = negative_gt_mask.tile([1, num_groups_denoising_queries, 1])\n+    positive_gt_mask = 1 - negative_gt_mask\n+    # contrastive denoising training positive index\n+    positive_gt_mask = positive_gt_mask.squeeze(-1) * pad_gt_mask\n+    denoise_positive_idx = torch.nonzero(positive_gt_mask)[:, 1]\n+    denoise_positive_idx = torch.split(\n+        denoise_positive_idx, [n * num_groups_denoising_queries for n in num_ground_truths]\n+    )\n+    # total denoising queries\n+    num_denoising_queries = torch_int(max_gt_num * 2 * num_groups_denoising_queries)\n \n-            intermediate += (hidden_states,)\n-            intermediate_reference_points += (\n-                (new_reference_points,) if self.bbox_embed is not None else (reference_points,)\n-            )\n+    if label_noise_ratio > 0:\n+        mask = torch.rand_like(input_query_class, dtype=torch.float) < (label_noise_ratio * 0.5)\n+        # randomly put a new one here\n+        new_label = torch.randint_like(mask, 0, num_classes, dtype=input_query_class.dtype)\n+        input_query_class = torch.where(mask & pad_gt_mask, new_label, input_query_class)\n \n-            if self.class_embed is not None:\n-                logits = self.class_embed[idx](hidden_states)\n-                intermediate_logits += (logits,)\n+    if box_noise_scale > 0:\n+        known_bbox = center_to_corners_format(input_query_bbox)\n+        diff = torch.tile(input_query_bbox[..., 2:] * 0.5, [1, 1, 2]) * box_noise_scale\n+        rand_sign = torch.randint_like(input_query_bbox, 0, 2) * 2.0 - 1.0\n+        rand_part = torch.rand_like(input_query_bbox)\n+        rand_part = (rand_part + 1.0) * negative_gt_mask + rand_part * (1 - negative_gt_mask)\n+        rand_part *= rand_sign\n+        known_bbox += rand_part * diff\n+        known_bbox.clip_(min=0.0, max=1.0)\n+        input_query_bbox = corners_to_center_format(known_bbox)\n+        input_query_bbox = inverse_sigmoid(input_query_bbox)\n \n-            if output_attentions:\n-                all_self_attns += (layer_outputs[1],)\n+    input_query_class = class_embed(input_query_class)\n \n-                if encoder_hidden_states is not None:\n-                    all_cross_attentions += (layer_outputs[2],)\n+    target_size = num_denoising_queries + num_queries\n+    attn_mask = torch.full([target_size, target_size], 0, dtype=torch.float, device=device)\n+    # match query cannot see the reconstruction\n+    attn_mask[num_denoising_queries:, :num_denoising_queries] = -torch.inf\n \n-        # Keep batch_size as first dimension\n-        intermediate = torch.stack(intermediate, dim=1)\n-        intermediate_reference_points = torch.stack(intermediate_reference_points, dim=1)\n-        if self.class_embed is not None:\n-            intermediate_logits = torch.stack(intermediate_logits, dim=1)\n+    # reconstructions cannot see each other\n+    for i in range(num_groups_denoising_queries):\n+        idx_block_start = max_gt_num * 2 * i\n+        idx_block_end = max_gt_num * 2 * (i + 1)\n+        attn_mask[idx_block_start:idx_block_end, :idx_block_start] = -torch.inf\n+        attn_mask[idx_block_start:idx_block_end, idx_block_end:num_denoising_queries] = -torch.inf\n \n-        # add hidden states from the last decoder layer\n-        if output_hidden_states:\n-            all_hidden_states += (hidden_states,)\n+    denoising_meta_values = {\n+        \"dn_positive_idx\": denoise_positive_idx,\n+        \"dn_num_group\": num_groups_denoising_queries,\n+        \"dn_num_split\": [num_denoising_queries, num_queries],\n+    }\n \n-        if not return_dict:\n-            return tuple(\n-                v\n-                for v in [\n-                    hidden_states,\n-                    intermediate,\n-                    intermediate_logits,\n-                    intermediate_reference_points,\n-                    all_hidden_states,\n-                    all_self_attns,\n-                    all_cross_attentions,\n-                ]\n-                if v is not None\n-            )\n-        return RTDetrV2DecoderOutput(\n-            last_hidden_state=hidden_states,\n-            intermediate_hidden_states=intermediate,\n-            intermediate_logits=intermediate_logits,\n-            intermediate_reference_points=intermediate_reference_points,\n-            hidden_states=all_hidden_states,\n-            attentions=all_self_attns,\n-            cross_attentions=all_cross_attentions,\n-        )\n+    return input_query_class, input_query_bbox, attn_mask, denoising_meta_values\n \n \n @auto_docstring(\n@@ -1805,6 +1723,84 @@ def forward(self, x):\n         return x\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Output type of [`RTDetrV2ForObjectDetection`].\n+    \"\"\"\n+)\n+class RTDetrV2ObjectDetectionOutput(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` are provided)):\n+        Total loss as a linear combination of a negative log-likehood (cross-entropy) for class prediction and a\n+        bounding box loss. The latter is defined as a linear combination of the L1 loss and the generalized\n+        scale-invariant IoU loss.\n+    loss_dict (`Dict`, *optional*):\n+        A dictionary containing the individual losses. Useful for logging.\n+    logits (`torch.FloatTensor` of shape `(batch_size, num_queries, num_classes + 1)`):\n+        Classification logits (including no-object) for all queries.\n+    pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n+        Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n+        values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n+        possible padding). You can use [`~RTDetrV2ImageProcessor.post_process_object_detection`] to retrieve the\n+        unnormalized (absolute) bounding boxes.\n+    auxiliary_outputs (`list[Dict]`, *optional*):\n+        Optional, only returned when auxiliary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n+        and labels are provided. It is a list of dictionaries containing the two above keys (`logits` and\n+        `pred_boxes`) for each decoder layer.\n+    last_hidden_state (`torch.FloatTensor` of shape `(batch_size, num_queries, hidden_size)`):\n+        Sequence of hidden-states at the output of the last layer of the decoder of the model.\n+    intermediate_hidden_states (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, hidden_size)`):\n+        Stacked intermediate hidden states (output of each layer of the decoder).\n+    intermediate_logits (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, config.num_labels)`):\n+        Stacked intermediate logits (logits of each layer of the decoder).\n+    intermediate_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate reference points (reference points of each layer of the decoder).\n+    intermediate_predicted_corners (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked intermediate predicted corners (predicted corners of each layer of the decoder).\n+    initial_reference_points (`torch.FloatTensor` of shape `(batch_size, config.decoder_layers, num_queries, 4)`):\n+        Stacked initial reference points (initial reference points of each layer of the decoder).\n+    init_reference_points (`torch.FloatTensor` of shape  `(batch_size, num_queries, 4)`):\n+        Initial reference points sent through the Transformer decoder.\n+    enc_topk_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_topk_bboxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the encoder.\n+    enc_outputs_class (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Predicted bounding boxes scores where the top `config.two_stage_num_proposals` scoring bounding boxes are\n+        picked as region proposals in the first stage. Output of bounding box binary classification (i.e.\n+        foreground and background).\n+    enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.with_box_refine=True` and `config.two_stage=True`):\n+        Logits of predicted bounding boxes coordinates in the first stage.\n+    denoising_meta_values (`dict`):\n+        Extra dictionary for the denoising related values\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    loss_dict: Optional[dict] = None\n+    logits: Optional[torch.FloatTensor] = None\n+    pred_boxes: Optional[torch.FloatTensor] = None\n+    auxiliary_outputs: Optional[list[dict]] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    intermediate_hidden_states: Optional[torch.FloatTensor] = None\n+    intermediate_logits: Optional[torch.FloatTensor] = None\n+    intermediate_reference_points: Optional[torch.FloatTensor] = None\n+    intermediate_predicted_corners: Optional[torch.FloatTensor] = None\n+    initial_reference_points: Optional[torch.FloatTensor] = None\n+    decoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    decoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_last_hidden_state: Optional[torch.FloatTensor] = None\n+    encoder_hidden_states: Optional[tuple[torch.FloatTensor]] = None\n+    encoder_attentions: Optional[tuple[torch.FloatTensor]] = None\n+    init_reference_points: Optional[tuple[torch.FloatTensor]] = None\n+    enc_topk_logits: Optional[torch.FloatTensor] = None\n+    enc_topk_bboxes: Optional[torch.FloatTensor] = None\n+    enc_outputs_class: Optional[torch.FloatTensor] = None\n+    enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    denoising_meta_values: Optional[dict] = None\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     RT-DETR Model (consisting of a backbone and encoder-decoder) outputting bounding boxes and logits to be further"
        },
        {
            "sha": "fff7da3ee44d58042e97c5ef5b5947804807bbfe",
            "filename": "src/transformers/models/sam_hq/modeling_sam_hq.py",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fmodeling_sam_hq.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -418,6 +418,26 @@ def forward(self, hidden_states: torch.Tensor) -> tuple[torch.FloatTensor]:\n         return hidden_states\n \n \n+@auto_docstring\n+class SamHQPreTrainedModel(PreTrainedModel):\n+    config: SamHQConfig\n+    base_model_prefix = \"sam_hq\"\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"SamHQVisionAttention\"]\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module: nn.Module):\n+        super()._init_weights(module)\n+        if isinstance(module, SamHQVisionAttention):\n+            if module.use_rel_pos:\n+                module.rel_pos_h.data.zero_()\n+                module.rel_pos_w.data.zero_()\n+        elif isinstance(module, SamHQVisionEncoder):\n+            if self.config.use_abs_pos:\n+                module.pos_embed.data.zero_()\n+\n+\n class SamHQPatchEmbeddings(nn.Module):\n     \"\"\"\n     This class turns `pixel_values` of shape `(batch_size, num_channels, height, width)` into the initial\n@@ -473,26 +493,6 @@ def forward(self, hidden_states):\n         return hidden_states\n \n \n-@auto_docstring\n-class SamHQPreTrainedModel(PreTrainedModel):\n-    config: SamHQConfig\n-    base_model_prefix = \"sam_hq\"\n-    main_input_name = \"pixel_values\"\n-    _no_split_modules = [\"SamHQVisionAttention\"]\n-    supports_gradient_checkpointing = True\n-    _supports_sdpa = True\n-\n-    def _init_weights(self, module: nn.Module):\n-        super()._init_weights(module)\n-        if isinstance(module, SamHQVisionAttention):\n-            if module.use_rel_pos:\n-                module.rel_pos_h.data.zero_()\n-                module.rel_pos_w.data.zero_()\n-        elif isinstance(module, SamHQVisionEncoder):\n-            if self.config.use_abs_pos:\n-                module.pos_embed.data.zero_()\n-\n-\n class SamHQVisionEncoder(SamHQPreTrainedModel):\n     _can_record_outputs = {\n         \"hidden_states\": SamHQVisionLayer,"
        },
        {
            "sha": "6e984534afbac566e1a7f0535b6e3ab496b3c310",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "modified",
            "additions": 104,
            "deletions": 104,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -502,46 +502,6 @@ def forward(\n         )\n \n \n-class Siglip2TextEmbeddings(nn.Module):\n-    def __init__(self, config: Siglip2TextConfig):\n-        super().__init__()\n-        embed_dim = config.hidden_size\n-\n-        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n-        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n-\n-        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n-        self.register_buffer(\n-            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n-        )\n-\n-    def forward(\n-        self,\n-        input_ids: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        inputs_embeds: Optional[torch.FloatTensor] = None,\n-    ) -> torch.Tensor:\n-        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n-        max_position_embedding = self.position_embedding.weight.shape[0]\n-\n-        if seq_length > max_position_embedding:\n-            raise ValueError(\n-                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n-                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n-            )\n-\n-        if position_ids is None:\n-            position_ids = self.position_ids[:, :seq_length]\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.token_embedding(input_ids)\n-\n-        position_embeddings = self.position_embedding(position_ids)\n-        embeddings = inputs_embeds + position_embeddings\n-\n-        return embeddings\n-\n-\n def _trunc_normal_(tensor, mean, std, a, b):\n     # Cut & paste from PyTorch official master until it's in a few official releases - RW\n     # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n@@ -637,6 +597,110 @@ def default_flax_embed_init(tensor):\n     variance_scaling_(tensor, mode=\"fan_in\", distribution=\"normal\")\n \n \n+@auto_docstring\n+class Siglip2PreTrainedModel(PreTrainedModel):\n+    config: Siglip2Config\n+    base_model_prefix = \"siglip2\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\n+        \"Siglip2TextEmbeddings\",\n+        \"Siglip2VisionEmbeddings\",\n+        \"Siglip2EncoderLayer\",\n+        \"Siglip2MultiheadAttentionPoolingHead\",\n+    ]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Siglip2VisionEmbeddings):\n+            width = (\n+                self.config.vision_config.hidden_size\n+                if isinstance(self.config, Siglip2Config)\n+                else self.config.hidden_size\n+            )\n+            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+        elif isinstance(module, nn.Embedding):\n+            default_flax_embed_init(module.weight)\n+        elif isinstance(module, Siglip2Attention):\n+            nn.init.xavier_uniform_(module.q_proj.weight)\n+            nn.init.xavier_uniform_(module.k_proj.weight)\n+            nn.init.xavier_uniform_(module.v_proj.weight)\n+            nn.init.xavier_uniform_(module.out_proj.weight)\n+            nn.init.zeros_(module.q_proj.bias)\n+            nn.init.zeros_(module.k_proj.bias)\n+            nn.init.zeros_(module.v_proj.bias)\n+            nn.init.zeros_(module.out_proj.bias)\n+        elif isinstance(module, Siglip2MLP):\n+            nn.init.xavier_uniform_(module.fc1.weight)\n+            nn.init.xavier_uniform_(module.fc2.weight)\n+            nn.init.normal_(module.fc1.bias, std=1e-6)\n+            nn.init.normal_(module.fc2.bias, std=1e-6)\n+        elif isinstance(module, Siglip2MultiheadAttentionPoolingHead):\n+            nn.init.xavier_uniform_(module.probe.data)\n+            nn.init.xavier_uniform_(module.attention.in_proj_weight.data)\n+            nn.init.zeros_(module.attention.in_proj_bias.data)\n+        elif isinstance(module, Siglip2Model):\n+            logit_scale_init = torch.log(torch.tensor(1.0))\n+            module.logit_scale.data.fill_(logit_scale_init)\n+            module.logit_bias.data.zero_()\n+        elif isinstance(module, Siglip2ForImageClassification):\n+            nn.init.normal_(\n+                module.classifier.weight,\n+                std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            lecun_normal_(module.weight)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+class Siglip2TextEmbeddings(nn.Module):\n+    def __init__(self, config: Siglip2TextConfig):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+\n+        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n+        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.token_embedding(input_ids)\n+\n+        position_embeddings = self.position_embedding(position_ids)\n+        embeddings = inputs_embeds + position_embeddings\n+\n+        return embeddings\n+\n+\n class Siglip2TextTransformer(nn.Module):\n     def __init__(self, config: Siglip2TextConfig):\n         super().__init__()\n@@ -702,70 +766,6 @@ def forward(\n         )\n \n \n-@auto_docstring\n-class Siglip2PreTrainedModel(PreTrainedModel):\n-    config: Siglip2Config\n-    base_model_prefix = \"siglip2\"\n-    supports_gradient_checkpointing = True\n-\n-    _no_split_modules = [\n-        \"Siglip2TextEmbeddings\",\n-        \"Siglip2VisionEmbeddings\",\n-        \"Siglip2EncoderLayer\",\n-        \"Siglip2MultiheadAttentionPoolingHead\",\n-    ]\n-    _supports_flash_attn = True\n-    _supports_sdpa = True\n-    _supports_flex_attn = True\n-    _supports_attention_backend = True\n-\n-    def _init_weights(self, module):\n-        \"\"\"Initialize the weights\"\"\"\n-        if isinstance(module, Siglip2VisionEmbeddings):\n-            width = (\n-                self.config.vision_config.hidden_size\n-                if isinstance(self.config, Siglip2Config)\n-                else self.config.hidden_size\n-            )\n-            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n-        elif isinstance(module, nn.Embedding):\n-            default_flax_embed_init(module.weight)\n-        elif isinstance(module, Siglip2Attention):\n-            nn.init.xavier_uniform_(module.q_proj.weight)\n-            nn.init.xavier_uniform_(module.k_proj.weight)\n-            nn.init.xavier_uniform_(module.v_proj.weight)\n-            nn.init.xavier_uniform_(module.out_proj.weight)\n-            nn.init.zeros_(module.q_proj.bias)\n-            nn.init.zeros_(module.k_proj.bias)\n-            nn.init.zeros_(module.v_proj.bias)\n-            nn.init.zeros_(module.out_proj.bias)\n-        elif isinstance(module, Siglip2MLP):\n-            nn.init.xavier_uniform_(module.fc1.weight)\n-            nn.init.xavier_uniform_(module.fc2.weight)\n-            nn.init.normal_(module.fc1.bias, std=1e-6)\n-            nn.init.normal_(module.fc2.bias, std=1e-6)\n-        elif isinstance(module, Siglip2MultiheadAttentionPoolingHead):\n-            nn.init.xavier_uniform_(module.probe.data)\n-            nn.init.xavier_uniform_(module.attention.in_proj_weight.data)\n-            nn.init.zeros_(module.attention.in_proj_bias.data)\n-        elif isinstance(module, Siglip2Model):\n-            logit_scale_init = torch.log(torch.tensor(1.0))\n-            module.logit_scale.data.fill_(logit_scale_init)\n-            module.logit_bias.data.zero_()\n-        elif isinstance(module, Siglip2ForImageClassification):\n-            nn.init.normal_(\n-                module.classifier.weight,\n-                std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n-            )\n-        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n-            lecun_normal_(module.weight)\n-            if module.bias is not None:\n-                nn.init.zeros_(module.bias)\n-        elif isinstance(module, nn.LayerNorm):\n-            module.bias.data.zero_()\n-            module.weight.data.fill_(1.0)\n-\n-\n @auto_docstring(\n     custom_intro=\"\"\"\n     The text model from Siglip2 without any head or projection on top."
        },
        {
            "sha": "54520b6aed0d6c5dce605dab5cc7f4a69fabd4db",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/98289c5546cf167fb10178053b36719f9732fc03/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/98289c5546cf167fb10178053b36719f9732fc03/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=98289c5546cf167fb10178053b36719f9732fc03",
            "patch": "@@ -1568,6 +1568,11 @@ class node based on the inherited classes if needed. Also returns any new import\n             file_type, new_node_dependencies, mapper, model_name\n         )\n \n+        # Remove all classes explicitly defined in modular from the dependencies. Otherwise, if a class is referenced\n+        # before its new modular definition, it may be wrongly imported from elsewhere as a dependency if it matches\n+        # another class from a modeling file after renaming, even though it would be added after anyway (leading to duplicates)\n+        new_node_dependencies -= set(modular_mapper.classes.keys())\n+\n         # The node was modified -> look for all recursive dependencies of the new node\n         all_dependencies_to_add = find_all_dependencies(\n             dependency_mapping=mapper.class_dependency_mapping,"
        }
    ],
    "stats": {
        "total": 3339,
        "additions": 1649,
        "deletions": 1690
    }
}