{
    "author": "remi-or",
    "message": "Many small fixes for the CI (#42364)",
    "sha": "a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
    "files": [
        {
            "sha": "76029439d288fbae9ab757ca3b4a2e8de51ab756",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -49,6 +49,12 @@\n     _is_torch_available = True\n \n \n+# required for @can_return_tuple decorator to work with torchdynamo\n+_is_mlx_available = False\n+if is_mlx_available():\n+    _is_mlx_available = True\n+\n+\n # vendored from distutils.util\n def strtobool(val):\n     \"\"\"Convert a string representation of truth to true (1) or false (0).\n@@ -159,7 +165,7 @@ def is_mlx_array(x):\n     \"\"\"\n     Tests if `x` is a mlx array or not. Safe to call even when mlx is not installed.\n     \"\"\"\n-    return False if not is_mlx_available() else _is_mlx(x)\n+    return False if not _is_mlx_available else _is_mlx(x)\n \n \n def to_py_obj(obj):"
        },
        {
            "sha": "b9c96df3f5371c077cd0569dffaedb5c23ea0067",
            "filename": "tests/models/blt/test_modeling_blt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblt%2Ftest_modeling_blt.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -221,6 +221,11 @@ def test_eager_matches_sdpa_inference(\n             self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels, atols=atols\n         )\n \n+    @require_torch_accelerator\n+    @slow\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        self.skipTest(\"BLT always has an attention_mask input\")\n+\n \n @require_torch_accelerator\n class BltIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "de41fc067aa3ccc0a20984c28df22598237b0da0",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -88,7 +88,7 @@ class DbrxModelTest(CausalLMModelTest, unittest.TestCase):\n \n     @slow\n     def test_model_from_pretrained(self):\n-        model_name = \"eitanturok/dbrx-tiny\"\n+        model_name = \"trl-internal-testing/tiny-DbrxForCausalLM\"\n         model = DbrxModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n "
        },
        {
            "sha": "57e74b7af8276434522c2f34f2afbca1f1d63fed",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -271,6 +271,13 @@ def test_sdpa_can_dispatch_composite_models(self):\n                 ):\n                     self.assertTrue(submodule.config._attn_implementation == \"sdpa\")\n \n+    @require_torch_accelerator\n+    @slow\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        self.skipTest(\n+            \"deepseek_vl_hybrid uses SAM, which requires an attention_mask input for relative positional embeddings\"\n+        )\n+\n \n @require_torch\n @require_torch_accelerator"
        },
        {
            "sha": "01ff91cc716d6c6f5d7da2cd47e40a74bc8c7572",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -526,7 +526,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[2.3050, 2.8279, 0.6531], [-1.8457, -0.1455, -3.5661], [-1.0186, 0.4586, -2.2043]], device=torch_device\n         )\n-        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\").to(torch_device)"
        },
        {
            "sha": "ea2dc2217d452e7245606dca189ab7bc210bab46",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -556,7 +556,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.3461, 0.3624, 0.2689], [0.3461, 0.3624, 0.2689], [-0.1562, 1.1637, -0.3784]], device=torch_device\n         )\n-        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_summarization_inference(self):"
        },
        {
            "sha": "4359368a4fdba00e5faec586da097f14cb0185be",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a95d9975c8e9ba6366bb13e0b75d133c0cc49d98/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a95d9975c8e9ba6366bb13e0b75d133c0cc49d98",
            "patch": "@@ -3651,7 +3651,10 @@ def update_config_headdim(config, requested_dim):\n                 getattr(config, \"hidden_size\", None) is not None\n                 and getattr(config, \"num_attention_heads\", None) is not None\n             ):\n-                head_dim = head_dim if head_dim is not None else config.hidden_size // config.num_attention_heads\n+                # For some models, num_attention_heads is a list of ints: we take the max to maximize the multiplier\n+                num_attn_heads = getattr(config, \"num_attention_heads\")\n+                num_attn_heads = num_attn_heads if isinstance(num_attn_heads, int) else max(num_attn_heads)\n+                head_dim = head_dim if head_dim is not None else config.hidden_size // num_attn_heads\n                 config.hidden_size *= max(requested_dim // head_dim, 1)\n \n             if ("
        }
    ],
    "stats": {
        "total": 31,
        "additions": 26,
        "deletions": 5
    }
}