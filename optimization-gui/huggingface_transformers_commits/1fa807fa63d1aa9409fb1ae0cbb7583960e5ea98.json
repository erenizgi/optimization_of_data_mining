{
    "author": "ArthurZucker",
    "message": "Fix some fa2 tests (#35340)\n\n* remove fa2 test\n\n* remove other failing tests\n\n* style",
    "sha": "1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98",
    "files": [
        {
            "sha": "686544825c35513347fbe89e5b833867bdc76c14",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98",
            "patch": "@@ -14,14 +14,12 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Granite model.\"\"\"\n \n-import tempfile\n import unittest\n \n from parameterized import parameterized\n \n from transformers import GraniteConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n@@ -417,33 +415,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @slow\n-    def test_use_flash_attention_2_true(self):\n-        \"\"\"\n-        NOTE: this is the only test testing that the legacy `use_flash_attention=2` argument still works as intended.\n-        \"\"\"\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                model = model_class(config)\n-                model.save_pretrained(tmp_dir)\n-\n-                new_model = GraniteForCausalLM.from_pretrained(\n-                    tmp_dir, use_flash_attention_2=True, torch_dtype=torch.float16\n-                ).to(\"cuda\")\n-\n-                self.assertTrue(new_model.config._attn_implementation == \"flash_attention_2\")\n-\n-                has_flash = False\n-                for name, submodule in new_model.named_modules():\n-                    if \"FlashAttention\" in submodule.__class__.__name__:\n-                        has_flash = True\n-                        break\n-                if not has_flash:\n-                    raise ValueError(\"The flash model should have flash attention layers\")\n-\n \n @require_torch_gpu\n class GraniteIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "31307865a77da7c6dd3269dc73131d3a1514da61",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98",
            "patch": "@@ -14,14 +14,12 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch GraniteMoe model.\"\"\"\n \n-import tempfile\n import unittest\n \n from parameterized import parameterized\n \n from transformers import AutoTokenizer, GraniteMoeConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_gpu,\n@@ -416,33 +414,6 @@ def test_model_rope_scaling(self):\n         with self.assertRaises(AssertionError):\n             torch.testing.assert_close(yarn_sin_long, original_sin_long)\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @slow\n-    def test_use_flash_attention_2_true(self):\n-        \"\"\"\n-        NOTE: this is the only test testing that the legacy `use_flash_attention=2` argument still works as intended.\n-        \"\"\"\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                model = model_class(config)\n-                model.save_pretrained(tmp_dir)\n-\n-                new_model = GraniteMoeForCausalLM.from_pretrained(\n-                    tmp_dir, use_flash_attention_2=True, torch_dtype=torch.float16\n-                ).to(\"cuda\")\n-\n-                self.assertTrue(new_model.config._attn_implementation == \"flash_attention_2\")\n-\n-                has_flash = False\n-                for name, submodule in new_model.named_modules():\n-                    if \"FlashAttention\" in submodule.__class__.__name__:\n-                        has_flash = True\n-                        break\n-                if not has_flash:\n-                    raise ValueError(\"The flash model should have flash attention layers\")\n-\n \n @require_torch_gpu\n class GraniteMoeIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "feca640bb4a1190da0522352709c0b5562a3f2c1",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98",
            "patch": "@@ -14,18 +14,15 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch LLaMA model.\"\"\"\n \n-import tempfile\n import unittest\n \n-import pytest\n from packaging import version\n from parameterized import parameterized\n \n from transformers import AutoTokenizer, LlamaConfig, StaticCache, is_torch_available, set_seed\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     cleanup,\n-    require_flash_attn,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n@@ -543,34 +540,6 @@ def _reinitialize_config(base_config, new_kwargs):\n         with self.assertRaises(KeyError):\n             config = _reinitialize_config(base_config, {\"rope_scaling\": {\"rope_type\": \"linear\"}})  # missing \"factor\"\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @slow\n-    @pytest.mark.flash_attn_test\n-    def test_use_flash_attention_2_true(self):\n-        \"\"\"\n-        NOTE: this is the only test testing that the legacy `use_flash_attention=2` argument still works as intended.\n-        \"\"\"\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            with tempfile.TemporaryDirectory() as tmp_dir:\n-                model = model_class(config)\n-                model.save_pretrained(tmp_dir)\n-\n-                new_model = LlamaForCausalLM.from_pretrained(\n-                    tmp_dir, use_flash_attention_2=True, torch_dtype=torch.float16\n-                ).to(\"cuda\")\n-\n-                self.assertTrue(new_model.config._attn_implementation == \"flash_attention_2\")\n-\n-                has_flash = False\n-                for name, submodule in new_model.named_modules():\n-                    if \"FlashAttention\" in submodule.__class__.__name__:\n-                        has_flash = True\n-                        break\n-                if not has_flash:\n-                    raise ValueError(\"The flash model should have flash attention layers\")\n-\n \n @require_torch_gpu\n class LlamaIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "f150477c6231f4bba20d8a08f40e6cff7ccf9c2e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1fa807fa63d1aa9409fb1ae0cbb7583960e5ea98",
            "patch": "@@ -2769,8 +2769,6 @@ def check_pt_flax_outputs(self, fx_outputs, pt_outputs, model_class, tol=1e-4, n\n                 attributes = tuple([f\"{name}_{idx}\" for idx in range(len(fx_outputs))])\n \n             for fx_output, pt_output, attr in zip(fx_outputs, pt_outputs, attributes):\n-                if isinstance(pt_output, DynamicCache):\n-                    pt_output = pt_output.to_legacy_cache()\n                 self.check_pt_flax_outputs(fx_output, pt_output, model_class, tol=tol, name=attr)\n \n         elif isinstance(fx_outputs, jnp.ndarray):\n@@ -3612,34 +3610,6 @@ def test_model_is_small(self):\n                 num_params < 1000000\n             ), f\"{model_class} is too big for the common tests ({num_params})! It should have 1M max.\"\n \n-    @require_flash_attn\n-    @require_torch_gpu\n-    @mark.flash_attn_test\n-    @slow\n-    def test_flash_attn_2_conversion(self):\n-        if not self.has_attentions:\n-            self.skipTest(reason=\"Model architecture does not support attentions\")\n-\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            if not model_class._supports_flash_attn_2:\n-                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n-\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n-                ).to(torch_device)\n-\n-                for _, module in model.named_modules():\n-                    if \"FlashAttention\" in module.__class__.__name__:\n-                        return\n-\n-                self.assertTrue(False, \"FlashAttention2 modules not found in model\")\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        }
    ],
    "stats": {
        "total": 119,
        "additions": 0,
        "deletions": 119
    }
}