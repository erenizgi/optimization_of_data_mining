{
    "author": "a4lg",
    "message": "Qwen2/3 MoE + GGUF model support (restored) (#42854)\n\nThis commit restores Qwen2/3 MoE + GGUF support in Transformers v5.\n\nIn this version, handling of MoE tensors are significantly changed so\nthat support for all MoE + GGUF models ... (okay, only) Qwen2/3 MoE\nmodels in Transformers v4 is now broken.\n\nThis commit now adopts new tensor handling, along with extended\n`TensorProcessor` with capabilities to handle not only tensor data\nbut also tensor mappings.  In this process, Qwen2/3 MoE-specific hack\nis moved to `Qwen2MoeTensorProcessor`, making the main function to look\nmore model-agnostic.\n\nThis is fully tested on Qwen2 MoE `Qwen1.5-MoE-A2.7B` and partially on\nQwen3 MoE `Qwen3-30B-A3B-Thinking-2507` (due to memory constraints).\n\nSigned-off-by: Tsukasa OI <floss_llm@irq.a4lg.com>",
    "sha": "c67ec2c4c14477faf7b258eba532d2673ddad4a7",
    "files": [
        {
            "sha": "1262d2b3f1946571a7ef413038581523d5408818",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 66,
            "deletions": 19,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/c67ec2c4c14477faf7b258eba532d2673ddad4a7/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c67ec2c4c14477faf7b258eba532d2673ddad4a7/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=c67ec2c4c14477faf7b258eba532d2673ddad4a7",
            "patch": "@@ -63,6 +63,24 @@ class TensorProcessor:\n     def __init__(self, config=None):\n         self.config = config or {}\n \n+    def preprocess_name(self, hf_name: str) -> str:\n+        \"\"\"\n+        Preprocesses the tensor name to ease loading the GGUF tensors.\n+        \"\"\"\n+        return hf_name\n+\n+    def perform_fallback_tensor_mapping(\n+        self, gguf_to_hf_name_map: dict[str, str], suffix: str, qual_name: str, hf_name: str\n+    ):\n+        \"\"\"\n+        Called when get_gguf_hf_weights_map fails to map a HF parameter\n+        (tensor) and corresponding GGUF one.\n+\n+        This is particularly useful to resolve one-to-many\n+        HF-GGUF mappings sometimes appear in some MoE models.\n+        \"\"\"\n+        pass\n+\n     def process(self, weights, name, **kwargs):\n         return GGUFTensor(weights, name, {})\n \n@@ -98,33 +116,59 @@ def _reverse_permute_weights(\n \n \n class Qwen2MoeTensorProcessor(TensorProcessor):\n+    HF_EXPERT_RENAME_PATTERN = re.compile(r\"mlp.experts.\\d+.\")\n+    HF_MOE_W13_PATTERN = re.compile(r\"model\\.layers\\.(?P<bid>\\d+)\\.mlp\\.experts\\.gate_up_proj\")\n+    GGUF_MOE_WEIGHTS_PATTERN = re.compile(r\"(?P<name>.*\\.ffn_(?P<w>gate|down|up)_exps)\\.weight$\")\n+\n     def __init__(self, config=None):\n         super().__init__(config=config)\n \n-    def process(self, weights, name, **kwargs):\n-        if \"_exp\" in name:\n+    def preprocess_name(self, hf_name: str) -> str:\n+        return re.sub(self.HF_EXPERT_RENAME_PATTERN, \"mlp.experts.\", hf_name)\n+\n+    def perform_fallback_tensor_mapping(\n+        self, gguf_to_hf_name_map: dict[str, str], suffix: str, qual_name: str, hf_name: str\n+    ):\n+        # Map merged MoE weights (w1 (gate) and w3 (up)) separately.\n+        if m := re.fullmatch(self.HF_MOE_W13_PATTERN, hf_name):\n+            full_hf_name = qual_name + hf_name\n+            gguf_to_hf_name_map[f\"blk.{m['bid']}.ffn_gate_exps{suffix}\"] = full_hf_name\n+            gguf_to_hf_name_map[f\"blk.{m['bid']}.ffn_up_exps{suffix}\"] = full_hf_name\n+\n+    def process(self, weights, name: str, **kwargs):\n+        if m := re.fullmatch(self.GGUF_MOE_WEIGHTS_PATTERN, name):\n             tensor_key_mapping = kwargs.get(\"tensor_key_mapping\")\n             parsed_parameters = kwargs.get(\"parsed_parameters\")\n             if tensor_key_mapping:\n-                self._split_moe_expert_tensor(weights, parsed_parameters, name, tensor_key_mapping)\n+                self._set_moe_expert_tensor(weights, parsed_parameters, tensor_key_mapping[m[\"name\"]], m[\"w\"])\n                 return GGUFTensor(weights, None, {})\n         if \"ffn_gate_inp_shexp\" in name:\n             # for compatibility tensor shared_expert_gate must be (1, 2048) dim,\n             # quantized one is (2048)\n             weights = np.expand_dims(weights, axis=0)\n         return GGUFTensor(weights, name, {})\n \n-    def _split_moe_expert_tensor(\n-        self, weights: np.ndarray, parsed_parameters: dict[str, dict], name: str, tensor_key_mapping: dict\n-    ):\n-        # Original merge implementation\n-        # https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py#L1994-L2022\n-        name = tensor_key_mapping[name]\n-        w_counter = self.config.get(\"num_experts\", 60)\n-        for i in range(0, w_counter):\n-            temp_name = name.replace(\"mlp.experts.\", f\"mlp.experts.{i}.\")\n-            exp_weight = weights[i]\n-            parsed_parameters[\"tensors\"][temp_name] = torch.from_numpy(np.copy(exp_weight))\n+    def _set_moe_expert_tensor(self, weights: np.ndarray, parsed_parameters: dict[str, dict], hf_name: str, w: str):\n+        torch_weights = torch.from_numpy(np.copy(weights))\n+        if w == \"down\":\n+            parsed_parameters[\"tensors\"][hf_name] = torch_weights\n+        else:\n+            # Double the size of the second dimension to interleave w1 (gate) and w3 (up)\n+            # weights per expert (which is the first dimension).\n+            # w1 (gate) comes first and w3 (up) comes second.\n+            # ref: https://github.com/vllm-project/vllm/blob/8f8fda261a620234fdeea338f44093d5d8072879/vllm/model_executor/layers/fused_moe/layer.py#L988-L1015\n+            shape = list(weights.shape)\n+            shard_dim = 1\n+            shard_size = shape[shard_dim]\n+            shape[shard_dim] = shard_size * 2\n+            if hf_name not in parsed_parameters[\"tensors\"]:\n+                parsed_parameters[\"tensors\"][hf_name] = torch.zeros(shape, dtype=torch_weights.dtype)\n+            out: torch.Tensor = parsed_parameters[\"tensors\"][hf_name]\n+            if w == \"gate\":\n+                out = out.narrow(shard_dim, 0, shard_size)\n+            else:  # w == \"up\"\n+                out = out.narrow(shard_dim, shard_size, shard_size)\n+            out.copy_(torch_weights)\n \n \n class BloomTensorProcessor(TensorProcessor):\n@@ -281,6 +325,7 @@ def read_field(reader, field):\n # modified from https://github.com/vllm-project/vllm/blob/v0.6.4.post1/vllm/model_executor/model_loader/loader.py#L1115-L1147\n def get_gguf_hf_weights_map(\n     hf_model,\n+    processor: TensorProcessor,\n     model_type: Optional[str] = None,\n     num_layers: Optional[int] = None,\n     qual_name: str = \"\",\n@@ -334,9 +379,7 @@ def get_gguf_hf_weights_map(\n     gguf_to_hf_name_map = {}\n     state_dict = hf_model.state_dict()\n     for hf_name in state_dict:\n-        # An exception for qwen2moe/qwen3moe model, where the expert layers are packed\n-        if model_type in (\"qwen2moe\", \"qwen3moe\") and \"mlp.experts.\" in hf_name:\n-            hf_name = re.sub(r\"mlp.experts.\\d+.\", \"mlp.experts.\", hf_name)\n+        hf_name = processor.preprocess_name(hf_name)\n \n         name, suffix = hf_name, \"\"\n         if hf_name.endswith(\".weight\") or hf_name.endswith(\".bias\"):\n@@ -345,6 +388,7 @@ def get_gguf_hf_weights_map(\n \n         gguf_name = name_map.get_name(name)\n         if gguf_name is None:\n+            processor.perform_fallback_tensor_mapping(gguf_to_hf_name_map, suffix, qual_name, hf_name)\n             continue\n \n         gguf_to_hf_name_map[gguf_name + suffix] = qual_name + hf_name\n@@ -353,7 +397,9 @@ def get_gguf_hf_weights_map(\n     # Therefore, we need to check submodule as well to get a correct mapping\n     if named_children := hf_model.named_children():\n         for name, child in named_children:\n-            sub_map = get_gguf_hf_weights_map(child, model_type, num_layers, qual_name=f\"{qual_name}{name}.\")\n+            sub_map = get_gguf_hf_weights_map(\n+                child, processor, model_type, num_layers, qual_name=f\"{qual_name}{name}.\"\n+            )\n             # Ignore the keys that are already in the main map to avoid overwriting\n             sub_map = {k: v for k, v in sub_map.items() if k not in gguf_to_hf_name_map}\n             gguf_to_hf_name_map.update(sub_map)\n@@ -507,12 +553,13 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False, model_to_lo\n     if return_tensors:\n         parsed_parameters[\"tensors\"] = {}\n \n-        tensor_key_mapping = get_gguf_hf_weights_map(model_to_load)\n         config = parsed_parameters.get(\"config\", {})\n \n         ProcessorClass = TENSOR_PROCESSORS.get(architecture, TensorProcessor)\n         processor = ProcessorClass(config=config)\n \n+        tensor_key_mapping = get_gguf_hf_weights_map(model_to_load, processor)\n+\n         for tensor in tqdm(reader.tensors, desc=\"Converting and de-quantizing GGUF tensors...\"):\n             name = tensor.name\n             weights = dequantize(tensor.data, tensor.tensor_type)"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 66,
        "deletions": 19
    }
}