{
    "author": "Cyrilvallez",
    "message": "Remove deprecated objects (#43170)\n\n* remove stuff\n\n* oupsi\n\n* skip\n\n* fix",
    "sha": "29b0770abfa008ace7a4ef7716aff0082de11a80",
    "files": [
        {
            "sha": "d1c7f29976b4150e33e034153deb224310701b7a",
            "filename": "docs/source/en/model_doc/led.md",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fled.md?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -181,11 +181,6 @@ print(tokenizer.decode(output[0], skip_special_tokens=True))\n [[autodoc]] LEDForConditionalGeneration\n     - forward\n \n-## LEDForSequenceClassification\n-\n-[[autodoc]] LEDForSequenceClassification\n-    - forward\n-\n ## LEDForQuestionAnswering\n \n [[autodoc]] LEDForQuestionAnswering"
        },
        {
            "sha": "d25e2cdf7346c6b37bd38f4bfe4cb2bbb64ef7be",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -133,7 +133,6 @@\n         \"is_wandb_available\",\n     ],\n     \"loss\": [],\n-    \"modelcard\": [\"ModelCard\"],\n     \"pipelines\": [\n         \"AnyToAnyPipeline\",\n         \"AudioClassificationPipeline\",\n@@ -615,8 +614,7 @@\n     from .masking_utils import AttentionMaskInterface as AttentionMaskInterface\n     from .model_debugging_utils import model_addition_debugger_context as model_addition_debugger_context\n \n-    # Model Cards\n-    from .modelcard import ModelCard as ModelCard\n+    # Models\n     from .modeling_layers import GradientCheckpointingLayer as GradientCheckpointingLayer\n     from .modeling_rope_utils import ROPE_INIT_FUNCTIONS as ROPE_INIT_FUNCTIONS\n     from .modeling_rope_utils import RopeParameters as RopeParameters"
        },
        {
            "sha": "922dca4936454e5f5091e06bba2ef66152ebcc45",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 0,
            "deletions": 178,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -13,10 +13,7 @@\n # limitations under the License.\n \"\"\"Configuration base class and utilities.\"\"\"\n \n-import copy\n-import json\n import os\n-import warnings\n from dataclasses import dataclass\n from pathlib import Path\n from typing import Any\n@@ -47,8 +44,6 @@\n )\n from .training_args import ParallelMode\n from .utils import (\n-    MODEL_CARD_NAME,\n-    cached_file,\n     is_datasets_available,\n     is_tokenizers_available,\n     is_torch_available,\n@@ -76,179 +71,6 @@\n logger = logging.get_logger(__name__)\n \n \n-class ModelCard:\n-    r\"\"\"\n-    Structured Model Card class. Store model card as well as methods for loading/downloading/saving model cards.\n-\n-    Please read the following paper for details and explanation on the sections: \"Model Cards for Model Reporting\" by\n-    Margaret Mitchell, Simone Wu, Andrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben Hutchinson, Elena Spitzer,\n-    Inioluwa Deborah Raji and Timnit Gebru for the proposal behind model cards. Link: https://huggingface.co/papers/1810.03993\n-\n-    Note: A model card can be loaded and saved to disk.\n-    \"\"\"\n-\n-    def __init__(self, **kwargs):\n-        warnings.warn(\n-            \"The class `ModelCard` is deprecated and will be removed in version 5 of Transformers\", FutureWarning\n-        )\n-        # Recommended attributes from https://huggingface.co/papers/1810.03993 (see papers)\n-        self.model_details = kwargs.pop(\"model_details\", {})\n-        self.intended_use = kwargs.pop(\"intended_use\", {})\n-        self.factors = kwargs.pop(\"factors\", {})\n-        self.metrics = kwargs.pop(\"metrics\", {})\n-        self.evaluation_data = kwargs.pop(\"evaluation_data\", {})\n-        self.training_data = kwargs.pop(\"training_data\", {})\n-        self.quantitative_analyses = kwargs.pop(\"quantitative_analyses\", {})\n-        self.ethical_considerations = kwargs.pop(\"ethical_considerations\", {})\n-        self.caveats_and_recommendations = kwargs.pop(\"caveats_and_recommendations\", {})\n-\n-        # Open additional attributes\n-        for key, value in kwargs.items():\n-            try:\n-                setattr(self, key, value)\n-            except AttributeError as err:\n-                logger.error(f\"Can't set {key} with value {value} for {self}\")\n-                raise err\n-\n-    def save_pretrained(self, save_directory_or_file):\n-        \"\"\"Save a model card object to the directory or file `save_directory_or_file`.\"\"\"\n-        if os.path.isdir(save_directory_or_file):\n-            # If we save using the predefined names, we can load using `from_pretrained`\n-            output_model_card_file = os.path.join(save_directory_or_file, MODEL_CARD_NAME)\n-        else:\n-            output_model_card_file = save_directory_or_file\n-\n-        self.to_json_file(output_model_card_file)\n-        logger.info(f\"Model card saved in {output_model_card_file}\")\n-\n-    @classmethod\n-    def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`ModelCard`] from a pre-trained model model card.\n-\n-        Parameters:\n-            pretrained_model_name_or_path: either:\n-\n-                - a string, the *model id* of a pretrained model card hosted inside a model repo on huggingface.co.\n-                - a path to a *directory* containing a model card file saved using the [`~ModelCard.save_pretrained`]\n-                  method, e.g.: `./my_model_directory/`.\n-                - a path or url to a saved model card JSON *file*, e.g.: `./my_model_directory/modelcard.json`.\n-\n-            cache_dir: (*optional*) string:\n-                Path to a directory in which a downloaded pre-trained model card should be cached if the standard cache\n-                should not be used.\n-\n-            kwargs: (*optional*) dict: key/value pairs with which to update the ModelCard object after loading.\n-\n-                - The values in kwargs of any keys which are model card attributes will be used to override the loaded\n-                  values.\n-                - Behavior concerning key/value pairs whose keys are *not* model card attributes is controlled by the\n-                  *return_unused_kwargs* keyword parameter.\n-\n-            proxies: (*optional*) dict, default None:\n-                A dictionary of proxy servers to use by protocol or endpoint, e.g.: {'http': 'foo.bar:3128',\n-                'http://hostname': 'foo.bar:4012'}. The proxies are used on each request.\n-\n-            return_unused_kwargs: (*optional*) bool:\n-\n-                - If False, then this function returns just the final model card object.\n-                - If True, then this functions returns a tuple *(model card, unused_kwargs)* where *unused_kwargs* is a\n-                  dictionary consisting of the key/value pairs whose keys are not model card attributes: ie the part of\n-                  kwargs which has not been used to update *ModelCard* and is otherwise ignored.\n-\n-        Examples:\n-\n-        ```python\n-        # Download model card from huggingface.co and cache.\n-        modelcard = ModelCard.from_pretrained(\"google-bert/bert-base-uncased\")\n-        # Model card was saved using *save_pretrained('./test/saved_model/')*\n-        modelcard = ModelCard.from_pretrained(\"./test/saved_model/\")\n-        modelcard = ModelCard.from_pretrained(\"./test/saved_model/modelcard.json\")\n-        modelcard = ModelCard.from_pretrained(\"google-bert/bert-base-uncased\", output_attentions=True, foo=False)\n-        ```\"\"\"\n-        cache_dir = kwargs.pop(\"cache_dir\", None)\n-        proxies = kwargs.pop(\"proxies\", None)\n-        return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n-        from_pipeline = kwargs.pop(\"_from_pipeline\", None)\n-\n-        user_agent = {\"file_type\": \"model_card\"}\n-        if from_pipeline is not None:\n-            user_agent[\"using_pipeline\"] = from_pipeline\n-\n-        is_local = os.path.isdir(pretrained_model_name_or_path)\n-        if os.path.isfile(pretrained_model_name_or_path):\n-            resolved_model_card_file = pretrained_model_name_or_path\n-            is_local = True\n-        else:\n-            try:\n-                # Load from URL or cache if already cached\n-                resolved_model_card_file = cached_file(\n-                    pretrained_model_name_or_path,\n-                    filename=MODEL_CARD_NAME,\n-                    cache_dir=cache_dir,\n-                    proxies=proxies,\n-                    user_agent=user_agent,\n-                )\n-                if is_local:\n-                    logger.info(f\"loading model card file {resolved_model_card_file}\")\n-                else:\n-                    logger.info(f\"loading model card file {MODEL_CARD_NAME} from cache at {resolved_model_card_file}\")\n-                # Load model card\n-                modelcard = cls.from_json_file(resolved_model_card_file)\n-\n-            except (OSError, json.JSONDecodeError):\n-                # We fall back on creating an empty model card\n-                modelcard = cls()\n-\n-        # Update model card with kwargs if needed\n-        to_remove = []\n-        for key, value in kwargs.items():\n-            if hasattr(modelcard, key):\n-                setattr(modelcard, key, value)\n-                to_remove.append(key)\n-        for key in to_remove:\n-            kwargs.pop(key, None)\n-\n-        logger.info(f\"Model card: {modelcard}\")\n-        if return_unused_kwargs:\n-            return modelcard, kwargs\n-        else:\n-            return modelcard\n-\n-    @classmethod\n-    def from_dict(cls, json_object):\n-        \"\"\"Constructs a `ModelCard` from a Python dictionary of parameters.\"\"\"\n-        return cls(**json_object)\n-\n-    @classmethod\n-    def from_json_file(cls, json_file):\n-        \"\"\"Constructs a `ModelCard` from a json file of parameters.\"\"\"\n-        with open(json_file, encoding=\"utf-8\") as reader:\n-            text = reader.read()\n-        dict_obj = json.loads(text)\n-        return cls(**dict_obj)\n-\n-    def __eq__(self, other):\n-        return self.__dict__ == other.__dict__\n-\n-    def __repr__(self):\n-        return str(self.to_json_string())\n-\n-    def to_dict(self):\n-        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n-        output = copy.deepcopy(self.__dict__)\n-        return output\n-\n-    def to_json_string(self):\n-        \"\"\"Serializes this instance to a JSON string.\"\"\"\n-        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\"\n-\n-    def to_json_file(self, json_file_path):\n-        \"\"\"Save this instance to a json file.\"\"\"\n-        with open(json_file_path, \"w\", encoding=\"utf-8\") as writer:\n-            writer.write(self.to_json_string())\n-\n-\n AUTOGENERATED_TRAINER_COMMENT = \"\"\"\n <!-- This model card has been generated automatically according to the information the Trainer had access to. You\n should probably proofread and complete it, then remove this comment. -->"
        },
        {
            "sha": "4db902237b50bb92649b43ebecb3f1bd11421a5d",
            "filename": "src/transformers/modeling_outputs.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodeling_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodeling_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_outputs.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -12,7 +12,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import warnings\n from dataclasses import dataclass\n \n import torch\n@@ -1705,12 +1704,3 @@ class MaskedImageModelingOutput(ModelOutput):\n     reconstruction: torch.FloatTensor | None = None\n     hidden_states: tuple[torch.FloatTensor, ...] | None = None\n     attentions: tuple[torch.FloatTensor, ...] | None = None\n-\n-    @property\n-    def logits(self):\n-        warnings.warn(\n-            \"logits attribute is deprecated and will be removed in version 5 of Transformers.\"\n-            \" Please use the reconstruction attribute to retrieve the final output instead.\",\n-            FutureWarning,\n-        )\n-        return self.reconstruction"
        },
        {
            "sha": "652092dd354b9c53890fa72b484c1051741197b3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -1280,7 +1280,6 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"layoutlm\", \"LayoutLMForSequenceClassification\"),\n         (\"layoutlmv2\", \"LayoutLMv2ForSequenceClassification\"),\n         (\"layoutlmv3\", \"LayoutLMv3ForSequenceClassification\"),\n-        (\"led\", \"LEDForSequenceClassification\"),\n         (\"lilt\", \"LiltForSequenceClassification\"),\n         (\"llama\", \"LlamaForSequenceClassification\"),\n         (\"longformer\", \"LongformerForSequenceClassification\"),"
        },
        {
            "sha": "23079f97714553a73671bfb50f50751598a8eba1",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \"\"\"PyTorch BLIP model.\"\"\"\n \n-import warnings\n from dataclasses import dataclass\n from typing import Any\n \n@@ -84,15 +83,6 @@ class BlipForConditionalGenerationModelOutput(ModelOutput):\n     hidden_states: tuple[torch.FloatTensor, ...] | None = None\n     attentions: tuple[torch.FloatTensor, ...] | None = None\n \n-    @property\n-    def decoder_logits(self):\n-        warnings.warn(\n-            \"`decoder_logits` attribute is deprecated and will be removed in version 5 of Transformers.\"\n-            \" Please use the `logits` attribute to retrieve the final output instead.\",\n-            FutureWarning,\n-        )\n-        return self.logits\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "417c07ce7052a6e414980e82909476482d8a2ed0",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 1,
            "deletions": 153,
            "changes": 154,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -14,12 +14,11 @@\n \"\"\"PyTorch LED model.\"\"\"\n \n import math\n-import warnings\n from dataclasses import dataclass\n \n import torch\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn import CrossEntropyLoss\n \n from ... import initialization as init\n from ...activations import ACT2FN\n@@ -2086,156 +2085,6 @@ def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n         return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n \n \n-@auto_docstring(\n-    custom_intro=\"\"\"\n-    LED model with a sequence classification/head on top (a linear layer on top of the pooled output) e.g. for GLUE\n-    tasks.\n-    \"\"\"\n-)\n-class LEDForSequenceClassification(LEDPreTrainedModel):\n-    def __init__(self, config: LEDConfig, **kwargs):\n-        warnings.warn(\n-            \"The `transformers.LEDForSequenceClassification` class is deprecated and will be removed in version 5 of\"\n-            \" Transformers. No actual method were provided in the original paper on how to perform\"\n-            \" sequence classification.\",\n-            FutureWarning,\n-        )\n-        super().__init__(config, **kwargs)\n-        self.led = LEDModel(config)\n-        self.classification_head = LEDClassificationHead(\n-            config.d_model,\n-            config.d_model,\n-            config.num_labels,\n-            config.classifier_dropout,\n-        )\n-\n-        # Initialize weights and apply final processing\n-        self.post_init()\n-\n-    @auto_docstring\n-    def forward(\n-        self,\n-        input_ids: torch.LongTensor | None = None,\n-        attention_mask: torch.Tensor | None = None,\n-        decoder_input_ids: torch.LongTensor | None = None,\n-        decoder_attention_mask: torch.LongTensor | None = None,\n-        encoder_outputs: tuple[tuple[torch.FloatTensor]] | None = None,\n-        global_attention_mask: torch.FloatTensor | None = None,\n-        inputs_embeds: torch.FloatTensor | None = None,\n-        decoder_inputs_embeds: torch.FloatTensor | None = None,\n-        labels: torch.LongTensor | None = None,\n-        use_cache: bool | None = None,\n-        output_attentions: bool | None = None,\n-        output_hidden_states: bool | None = None,\n-        return_dict: bool | None = None,\n-        **kwargs,\n-    ) -> tuple[torch.Tensor] | LEDSeq2SeqSequenceClassifierOutput:\n-        r\"\"\"\n-        decoder_input_ids (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Indices of decoder input sequence tokens in the vocabulary.\n-\n-            Indices can be obtained using [`LedTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n-            [`PreTrainedTokenizer.__call__`] for details.\n-\n-            [What are input IDs?](../glossary#input-ids)\n-\n-            LED uses the `eos_token_id` as the starting token for `decoder_input_ids` generation. If `past_key_values`\n-            is used, optionally only the last `decoder_input_ids` have to be input (see `past_key_values`).\n-        decoder_attention_mask (`torch.LongTensor` of shape `(batch_size, target_sequence_length)`, *optional*):\n-            Default behavior: generate a tensor that ignores pad tokens in `decoder_input_ids`. Causal mask will also\n-            be used by default.\n-\n-            If you want to change padding behavior, you should read [`modeling_led._prepare_decoder_inputs`] and modify\n-            to your needs. See diagram 1 in [the paper](https://huggingface.co/papers/1910.13461) for more information on the\n-            default strategy.\n-        global_attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n-            Mask to decide the attention given on each token, local attention or global attention for the encoder.\n-            Tokens with global attention attends to all other tokens, and all other tokens attend to them. This is\n-            important for task-specific finetuning because it makes the model more flexible at representing the task.\n-            For example, for classification, the <s> token should be given global attention. For QA, all question\n-            tokens should also have global attention. Please refer to the [Longformer\n-            paper](https://huggingface.co/papers/2004.05150) for more details. Mask values selected in `[0, 1]`:\n-\n-            - 0 for local attention (a sliding window attention),\n-            - 1 for global attention (tokens that attend to all other tokens, and all other tokens attend to them).\n-        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n-            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n-            config.num_labels - 1]`. If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n-        \"\"\"\n-        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n-        if labels is not None:\n-            use_cache = False\n-\n-        if input_ids is None and inputs_embeds is not None:\n-            raise NotImplementedError(\n-                f\"Passing input embeddings is currently not supported for {self.__class__.__name__}\"\n-            )\n-\n-        outputs = self.led(\n-            input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            global_attention_mask=global_attention_mask,\n-            encoder_outputs=encoder_outputs,\n-            inputs_embeds=inputs_embeds,\n-            decoder_inputs_embeds=decoder_inputs_embeds,\n-            use_cache=use_cache,\n-            output_attentions=output_attentions,\n-            output_hidden_states=output_hidden_states,\n-            return_dict=return_dict,\n-        )\n-        hidden_states = outputs[0]  # last hidden state\n-\n-        eos_mask = input_ids.eq(self.config.eos_token_id).to(hidden_states.device)\n-\n-        if len(torch.unique_consecutive(eos_mask.sum(1))) > 1:\n-            raise ValueError(\"All examples must have the same number of <eos> tokens.\")\n-        sentence_representation = hidden_states[eos_mask, :].view(hidden_states.size(0), -1, hidden_states.size(-1))[\n-            :, -1, :\n-        ]\n-        logits = self.classification_head(sentence_representation)\n-\n-        loss = None\n-        if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.config.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.config.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.config.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.config.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n-        if not return_dict:\n-            output = (logits,) + outputs[1:]\n-            return ((loss,) + output) if loss is not None else output\n-\n-        return LEDSeq2SeqSequenceClassifierOutput(\n-            loss=loss,\n-            logits=logits,\n-            past_key_values=outputs.past_key_values,\n-            decoder_hidden_states=outputs.decoder_hidden_states,\n-            decoder_attentions=outputs.decoder_attentions,\n-            cross_attentions=outputs.cross_attentions,\n-            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n-            encoder_hidden_states=outputs.encoder_hidden_states,\n-            encoder_attentions=outputs.encoder_attentions,\n-            encoder_global_attentions=outputs.encoder_global_attentions,\n-        )\n-\n-\n @auto_docstring\n class LEDForQuestionAnswering(LEDPreTrainedModel):\n     def __init__(self, config):\n@@ -2366,7 +2215,6 @@ def forward(\n __all__ = [\n     \"LEDForConditionalGeneration\",\n     \"LEDForQuestionAnswering\",\n-    \"LEDForSequenceClassification\",\n     \"LEDModel\",\n     \"LEDPreTrainedModel\",\n ]"
        },
        {
            "sha": "e51e772bb5d7ee853976a1a39add6a6e8c9f58c3",
            "filename": "src/transformers/models/rag/tokenization_rag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 48,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Ftokenization_rag.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -14,9 +14,7 @@\n \"\"\"Tokenization classes for RAG.\"\"\"\n \n import os\n-import warnings\n \n-from ...tokenization_utils_base import BatchEncoding\n from ...utils import logging\n from .configuration_rag import RagConfig\n \n@@ -72,51 +70,5 @@ def _switch_to_input_mode(self):\n     def _switch_to_target_mode(self):\n         self.current_tokenizer = self.generator\n \n-    def prepare_seq2seq_batch(\n-        self,\n-        src_texts: list[str],\n-        tgt_texts: list[str] | None = None,\n-        max_length: int | None = None,\n-        max_target_length: int | None = None,\n-        padding: str = \"longest\",\n-        return_tensors: str | None = None,\n-        truncation: bool = True,\n-        **kwargs,\n-    ) -> BatchEncoding:\n-        warnings.warn(\n-            \"`prepare_seq2seq_batch` is deprecated and will be removed in version 5 of Hugging Face Transformers. Use the \"\n-            \"regular `__call__` method to prepare your inputs and the tokenizer under the `with_target_tokenizer` \"\n-            \"context manager to prepare your targets. See the documentation of your specific tokenizer for more \"\n-            \"details\",\n-            FutureWarning,\n-        )\n-        if max_length is None:\n-            max_length = self.current_tokenizer.model_max_length\n-        model_inputs = self(\n-            src_texts,\n-            add_special_tokens=True,\n-            return_tensors=return_tensors,\n-            max_length=max_length,\n-            padding=padding,\n-            truncation=truncation,\n-            **kwargs,\n-        )\n-        if tgt_texts is None:\n-            return model_inputs\n-        # Process tgt_texts\n-        if max_target_length is None:\n-            max_target_length = self.current_tokenizer.model_max_length\n-        labels = self(\n-            text_target=tgt_texts,\n-            add_special_tokens=True,\n-            return_tensors=return_tensors,\n-            padding=padding,\n-            max_length=max_target_length,\n-            truncation=truncation,\n-            **kwargs,\n-        )\n-        model_inputs[\"labels\"] = labels[\"input_ids\"]\n-        return model_inputs\n-\n \n __all__ = [\"RagTokenizer\"]"
        },
        {
            "sha": "4e94fc64cbf7ff5013f08a065156a43504726ec7",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -15,7 +15,6 @@\n \n import collections.abc\n import math\n-import warnings\n from dataclasses import dataclass\n \n import torch\n@@ -111,15 +110,6 @@ class SwinMaskedImageModelingOutput(ModelOutput):\n     attentions: tuple[torch.FloatTensor, ...] | None = None\n     reshaped_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n \n-    @property\n-    def logits(self):\n-        warnings.warn(\n-            \"logits attribute is deprecated and will be removed in version 5 of Transformers.\"\n-            \" Please use the reconstruction attribute to retrieve the final output instead.\",\n-            FutureWarning,\n-        )\n-        return self.reconstruction\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "703d841c0dc121723bed20d1ca547c12b9f89063",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -15,7 +15,6 @@\n \n import collections.abc\n import math\n-import warnings\n from dataclasses import dataclass\n \n import torch\n@@ -114,15 +113,6 @@ class Swinv2MaskedImageModelingOutput(ModelOutput):\n     attentions: tuple[torch.FloatTensor, ...] | None = None\n     reshaped_hidden_states: tuple[torch.FloatTensor, ...] | None = None\n \n-    @property\n-    def logits(self):\n-        warnings.warn(\n-            \"logits attribute is deprecated and will be removed in version 5 of Transformers.\"\n-            \" Please use the reconstruction attribute to retrieve the final output instead.\",\n-            FutureWarning,\n-        )\n-        return self.reconstruction\n-\n \n @dataclass\n @auto_docstring("
        },
        {
            "sha": "821b4a5b9b26fd1fdb83c060f0fa801e02a60cdd",
            "filename": "src/transformers/models/wav2vec2/tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 308,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Ftokenization_wav2vec2.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -15,20 +15,16 @@\n \n import json\n import os\n-import warnings\n from dataclasses import dataclass\n from itertools import groupby\n from typing import TYPE_CHECKING, Union\n \n import numpy as np\n \n from ...tokenization_python import PreTrainedTokenizer\n-from ...tokenization_utils_base import AddedToken, BatchEncoding\n+from ...tokenization_utils_base import AddedToken\n from ...utils import (\n     ModelOutput,\n-    PaddingStrategy,\n-    TensorType,\n-    add_end_docstrings,\n     logging,\n     to_py_obj,\n )\n@@ -447,7 +443,7 @@ def _decode(\n         output_char_offsets: bool | None = False,\n     ) -> str:\n         \"\"\"\n-        special _decode function is needed for Wav2Vec2Tokenizer because added tokens should be treated exactly the\n+        special _decode function is needed because added tokens should be treated exactly the\n         same as tokens of the base vocabulary and therefore the function `convert_tokens_to_string` has to be called on\n         the whole token list and not individually on added tokens\n         \"\"\"\n@@ -679,305 +675,4 @@ def save_vocabulary(self, save_directory: str, filename_prefix: str | None = Non\n         return (vocab_file,)\n \n \n-class Wav2Vec2Tokenizer(PreTrainedTokenizer):\n-    \"\"\"\n-    Constructs a Wav2Vec2 tokenizer.\n-\n-    This tokenizer inherits from [`PreTrainedTokenizer`] which contains some of the main methods. Users should refer to\n-    the superclass for more information regarding such methods.\n-\n-    Args:\n-        vocab_file (`str`):\n-            File containing the vocabulary.\n-        bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n-            The beginning of sentence token.\n-        eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n-            The end of sentence token.\n-        unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n-            The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n-            token instead.\n-        pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n-            The token used for padding, for example when batching sequences of different lengths.\n-        word_delimiter_token (`str`, *optional*, defaults to `\"|\"`):\n-            The token used for defining the end of a word.\n-        do_lower_case (`bool`, *optional*, defaults to `False`):\n-            Whether or not to lowercase the output when decoding.\n-        do_normalize (`bool`, *optional*, defaults to `False`):\n-            Whether or not to zero-mean unit-variance normalize the input. Normalizing can help to significantly\n-            improve the performance for some models, *e.g.*,\n-            [wav2vec2-lv60](https://huggingface.co/models?search=lv60).\n-        return_attention_mask (`bool`, *optional*, defaults to `False`):\n-            Whether or not [`~Wav2Vec2Tokenizer.__call__`] should return `attention_mask`.\n-\n-            <Tip>\n-\n-            Wav2Vec2 models that have set `config.feat_extract_norm == \"group\"`, such as\n-            [wav2vec2-base](https://huggingface.co/facebook/wav2vec2-base-960h), have **not** been trained using\n-            `attention_mask`. For such models, `input_values` should simply be padded with 0 and no `attention_mask`\n-            should be passed.\n-\n-            For Wav2Vec2 models that have set `config.feat_extract_norm == \"layer\"`, such as\n-            [wav2vec2-lv60](https://huggingface.co/facebook/wav2vec2-large-960h-lv60-self), `attention_mask` should be\n-            passed for batched inference.\n-\n-            </Tip>\n-\n-        **kwargs\n-            Additional keyword arguments passed along to [`PreTrainedTokenizer`]\n-    \"\"\"\n-\n-    vocab_files_names = VOCAB_FILES_NAMES\n-    pretrained_vocab_files_map = {\n-        \"vocab_file\": {\n-            \"facebook/wav2vec2-base-960h\": \"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/vocab.json\"\n-        },\n-        \"tokenizer_config_file\": {\n-            \"facebook/wav2vec2-base-960h\": (\n-                \"https://huggingface.co/facebook/wav2vec2-base-960h/resolve/main/tokenizer.json\"\n-            ),\n-        },\n-    }\n-    model_input_names = [\"input_values\", \"attention_mask\"]\n-\n-    def __init__(\n-        self,\n-        vocab_file,\n-        bos_token=\"<s>\",\n-        eos_token=\"</s>\",\n-        unk_token=\"<unk>\",\n-        pad_token=\"<pad>\",\n-        word_delimiter_token=\"|\",\n-        do_lower_case=False,\n-        do_normalize=False,\n-        return_attention_mask=False,\n-        **kwargs,\n-    ):\n-        warnings.warn(\n-            \"The class `Wav2Vec2Tokenizer` is deprecated and will be removed in version 5 of Transformers. Please use\"\n-            \" `Wav2Vec2Processor` or `Wav2Vec2CTCTokenizer` instead.\",\n-            FutureWarning,\n-        )\n-\n-        self._word_delimiter_token = word_delimiter_token\n-\n-        self.do_lower_case = do_lower_case\n-        self.return_attention_mask = return_attention_mask\n-        self.do_normalize = do_normalize\n-\n-        with open(vocab_file, encoding=\"utf-8\") as vocab_handle:\n-            self.encoder = json.load(vocab_handle)\n-\n-        self.decoder = {v: k for k, v in self.encoder.items()}\n-\n-        super().__init__(\n-            unk_token=unk_token,\n-            bos_token=bos_token,\n-            eos_token=eos_token,\n-            pad_token=pad_token,\n-            do_lower_case=do_lower_case,\n-            do_normalize=do_normalize,\n-            return_attention_mask=return_attention_mask,\n-            word_delimiter_token=word_delimiter_token,\n-            **kwargs,\n-        )\n-\n-    @property\n-    def word_delimiter_token(self) -> str:\n-        \"\"\"\n-        `str`: Padding token. Log an error if used while not having been set.\n-        \"\"\"\n-        if self._word_delimiter_token is None and self.verbose:\n-            logger.error(\"Using word_delimiter_token, but it is not set yet.\")\n-            return None\n-        return str(self._word_delimiter_token)\n-\n-    @property\n-    def word_delimiter_token_id(self) -> int | None:\n-        \"\"\"\n-        `Optional[int]`: Id of the word_delimiter_token in the vocabulary. Returns `None` if the token has not been\n-        set.\n-        \"\"\"\n-        if self._word_delimiter_token is None:\n-            return None\n-        return self.convert_tokens_to_ids(self.word_delimiter_token)\n-\n-    @word_delimiter_token.setter\n-    def word_delimiter_token(self, value):\n-        self._word_delimiter_token = value\n-\n-    @word_delimiter_token_id.setter\n-    def word_delimiter_token_id(self, value):\n-        self._word_delimiter_token = self.convert_tokens_to_ids(value)\n-\n-    @add_end_docstrings(WAV2VEC2_KWARGS_DOCSTRING)\n-    def __call__(\n-        self,\n-        raw_speech: np.ndarray | list[float] | list[np.ndarray] | list[list[float]],\n-        padding: bool | str | PaddingStrategy = False,\n-        max_length: int | None = None,\n-        pad_to_multiple_of: int | None = None,\n-        padding_side: str | None = None,\n-        return_tensors: str | TensorType | None = None,\n-        verbose: bool = True,\n-        **kwargs,\n-    ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n-        sequences.\n-\n-        Args:\n-            raw_speech (`np.ndarray`, `list[float]`, `list[np.ndarray]`, `list[list[float]]`):\n-                The sequence or batch of sequences to be padded. Each sequence can be a numpy array, a list of float\n-                values, a list of numpy array or a list of list of float values. Must be mono channel audio, not\n-                stereo, i.e. single float per timestep.\n-\n-            padding_side (`str`, *optional*):\n-                The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n-                Default value is picked from the class attribute of the same name.\n-        \"\"\"\n-\n-        is_batched_numpy = isinstance(raw_speech, np.ndarray) and len(raw_speech.shape) > 1\n-        if is_batched_numpy and len(raw_speech.shape) > 2:\n-            raise ValueError(f\"Only mono-channel audio is supported for input to {self}\")\n-        is_batched = is_batched_numpy or (\n-            isinstance(raw_speech, (list, tuple)) and (isinstance(raw_speech[0], (np.ndarray, tuple, list)))\n-        )\n-\n-        # make sure input is in list format\n-        if is_batched and not isinstance(raw_speech[0], np.ndarray):\n-            raw_speech = [np.asarray(speech) for speech in raw_speech]\n-        elif not is_batched and not isinstance(raw_speech, np.ndarray):\n-            raw_speech = np.asarray(raw_speech)\n-\n-        # always return batch\n-        if not is_batched:\n-            raw_speech = [raw_speech]\n-\n-        # zero-mean and unit-variance normalization\n-        if self.do_normalize:\n-            raw_speech = [(x - np.mean(x)) / np.sqrt(np.var(x) + 1e-5) for x in raw_speech]\n-\n-        # convert into correct format for padding\n-        encoded_inputs = BatchEncoding({\"input_values\": raw_speech})\n-\n-        padded_inputs = self.pad(\n-            encoded_inputs,\n-            padding=padding,\n-            max_length=max_length,\n-            pad_to_multiple_of=pad_to_multiple_of,\n-            padding_side=padding_side,\n-            return_attention_mask=self.return_attention_mask,\n-            return_tensors=return_tensors,\n-            verbose=verbose,\n-        )\n-\n-        return padded_inputs\n-\n-    @property\n-    def vocab_size(self) -> int:\n-        return len(self.decoder)\n-\n-    def get_vocab(self) -> dict:\n-        return dict(self.encoder, **self.added_tokens_encoder)\n-\n-    def _convert_token_to_id(self, token: str) -> int:\n-        \"\"\"Converts a token (str) in an index (integer) using the vocab.\"\"\"\n-        return self.encoder.get(token, self.encoder.get(self.unk_token))\n-\n-    def _convert_id_to_token(self, index: int) -> str:\n-        \"\"\"Converts an index (integer) in a token (str) using the vocab.\"\"\"\n-        result = self.decoder.get(index, self.unk_token)\n-        return result\n-\n-    def convert_tokens_to_string(self, tokens: list[str]) -> str:\n-        \"\"\"\n-        Converts a connectionist-temporal-classification (CTC) output tokens into a single string.\n-        \"\"\"\n-        # group same tokens into non-repeating tokens in CTC style decoding\n-        grouped_tokens = [token_group[0] for token_group in groupby(tokens)]\n-\n-        # filter self.pad_token which is used as CTC-blank token\n-        filtered_tokens = list(filter(lambda token: token != self.pad_token, grouped_tokens))\n-\n-        # replace delimiter token\n-        string = \"\".join([\" \" if token == self.word_delimiter_token else token for token in filtered_tokens]).strip()\n-\n-        if self.do_lower_case:\n-            string = string.lower()\n-\n-        return string\n-\n-    @staticmethod\n-    def clean_up_tokenization(out_string: str) -> str:\n-        \"\"\"\n-        Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms.\n-\n-        Args:\n-            out_string (`str`): The text to clean up.\n-\n-        Returns:\n-            `str`: The cleaned-up string.\n-        \"\"\"\n-        out_string = (\n-            out_string.replace(\" .\", \".\")\n-            .replace(\" ?\", \"?\")\n-            .replace(\" !\", \"!\")\n-            .replace(\" ,\", \",\")\n-            .replace(\" ' \", \"'\")\n-            .replace(\" n't\", \"n't\")\n-            .replace(\" 'm\", \"'m\")\n-            .replace(\" 's\", \"'s\")\n-            .replace(\" 've\", \"'ve\")\n-            .replace(\" 're\", \"'re\")\n-        )\n-        return out_string\n-\n-    def _decode(\n-        self,\n-        token_ids: list[int],\n-        skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: bool | None = None,\n-        **kwargs,\n-    ) -> str:\n-        \"\"\"\n-        special _decode function is needed for Wav2Vec2Tokenizer because added tokens should be treated exactly the\n-        same as tokens of the base vocabulary and therefore the function `convert_tokens_to_string` has to be called on\n-        the whole token list and not individually on added tokens\n-        \"\"\"\n-        # Don't skip special tokens in convert_ids_to_tokens so we can handle word_delimiter_token specially\n-        filtered_tokens = self.convert_ids_to_tokens(token_ids, skip_special_tokens=False)\n-\n-        result = []\n-        for token in filtered_tokens:\n-            if skip_special_tokens and token in self.all_special_tokens and token != self.word_delimiter_token:\n-                continue\n-            result.append(token)\n-\n-        text = self.convert_tokens_to_string(result)\n-\n-        clean_up_tokenization_spaces = (\n-            clean_up_tokenization_spaces\n-            if clean_up_tokenization_spaces is not None\n-            else self.clean_up_tokenization_spaces\n-        )\n-        if clean_up_tokenization_spaces:\n-            clean_text = self.clean_up_tokenization(text)\n-            return clean_text\n-        else:\n-            return text\n-\n-    def save_vocabulary(self, save_directory: str, filename_prefix: str | None = None) -> tuple[str]:\n-        if not os.path.isdir(save_directory):\n-            logger.error(f\"Vocabulary path ({save_directory}) should be a directory\")\n-            return\n-        vocab_file = os.path.join(\n-            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + VOCAB_FILES_NAMES[\"vocab_file\"]\n-        )\n-\n-        with open(vocab_file, \"w\", encoding=\"utf-8\") as f:\n-            f.write(json.dumps(self.encoder, indent=2, sort_keys=True, ensure_ascii=False) + \"\\n\")\n-\n-        return (vocab_file,)\n-\n-\n-__all__ = [\"Wav2Vec2CTCTokenizer\", \"Wav2Vec2Tokenizer\"]\n+__all__ = [\"Wav2Vec2CTCTokenizer\"]"
        },
        {
            "sha": "9876401b7f7d9d9adac64ff2791227afe42168ff",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -33,7 +33,6 @@\n from ..feature_extraction_utils import PreTrainedFeatureExtractor\n from ..generation import GenerationConfig\n from ..image_processing_utils import BaseImageProcessor\n-from ..modelcard import ModelCard\n from ..models.auto import AutoConfig, AutoTokenizer\n from ..processing_utils import ProcessorMixin\n from ..tokenization_python import PreTrainedTokenizer\n@@ -678,8 +677,6 @@ def build_pipeline_init_args(\n             [`ProcessorMixin`]. Processor is a composite object that might contain `tokenizer`, `feature_extractor`, and\n             `image_processor`.\"\"\"\n     docstring += r\"\"\"\n-        modelcard (`str` or [`ModelCard`], *optional*):\n-            Model card attributed to the model for this pipeline.\n         task (`str`, defaults to `\"\"`):\n             A task-identifier for the pipeline.\n         num_workers (`int`, *optional*, defaults to 8):\n@@ -783,7 +780,6 @@ def __init__(\n         feature_extractor: PreTrainedFeatureExtractor | None = None,\n         image_processor: BaseImageProcessor | None = None,\n         processor: ProcessorMixin | None = None,\n-        modelcard: ModelCard | None = None,\n         task: str = \"\",\n         device: int | torch.device | None = None,\n         binary_output: bool = False,\n@@ -798,7 +794,6 @@ def __init__(\n         self.feature_extractor = feature_extractor\n         self.image_processor = image_processor\n         self.processor = processor\n-        self.modelcard = modelcard\n \n         # `accelerate` device map\n         hf_device_map = getattr(self.model, \"hf_device_map\", None)"
        },
        {
            "sha": "aa8de57e715fcdaf80f8fbe63b6dba9faa657fcb",
            "filename": "src/transformers/pipelines/question_answering.py",
            "status": "modified",
            "additions": 2,
            "deletions": 17,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fquestion_answering.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -7,7 +7,6 @@\n import numpy as np\n \n from ..data import SquadExample, SquadFeatures, squad_convert_examples_to_features\n-from ..modelcard import ModelCard\n from ..tokenization_python import PreTrainedTokenizer\n from ..utils import (\n     PaddingStrategy,\n@@ -255,22 +254,8 @@ class QuestionAnsweringPipeline(ChunkPipeline):\n     default_input_names = \"question,context\"\n     handle_impossible_answer = False\n \n-    def __init__(\n-        self,\n-        model: \"PreTrainedModel\",\n-        tokenizer: PreTrainedTokenizer,\n-        modelcard: ModelCard | None = None,\n-        task: str = \"\",\n-        **kwargs,\n-    ):\n-        super().__init__(\n-            model=model,\n-            tokenizer=tokenizer,\n-            modelcard=modelcard,\n-            task=task,\n-            **kwargs,\n-        )\n-\n+    def __init__(self, model: \"PreTrainedModel\", tokenizer: PreTrainedTokenizer, task: str = \"\", **kwargs):\n+        super().__init__(model=model, tokenizer=tokenizer, task=task, **kwargs)\n         self._args_parser = QuestionAnsweringArgumentHandler()\n         self.check_model_type(MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES)\n "
        },
        {
            "sha": "4e2ff77cbe625707f1860064dc08cf22941bceeb",
            "filename": "src/transformers/pipelines/video_classification.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fvideo_classification.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -11,7 +11,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import warnings\n from io import BytesIO\n from typing import Any, overload\n \n@@ -88,7 +87,7 @@ def __call__(self, inputs: str, **kwargs: Any) -> list[dict[str, Any]]: ...\n     @overload\n     def __call__(self, inputs: list[str], **kwargs: Any) -> list[list[dict[str, Any]]]: ...\n \n-    def __call__(self, inputs: str | list[str] | None = None, **kwargs):\n+    def __call__(self, inputs: str | list[str] | None, **kwargs):\n         \"\"\"\n         Assign labels to the video(s) passed as inputs.\n \n@@ -126,13 +125,6 @@ def __call__(self, inputs: str | list[str] | None = None, **kwargs):\n             - **label** (`str`) -- The label identified by the model.\n             - **score** (`int`) -- The score attributed by the model for that label.\n         \"\"\"\n-        # After deprecation of this is completed, remove the default `None` value for `images`\n-        if \"videos\" in kwargs:\n-            warnings.warn(\n-                \"The `videos` argument has been renamed to `inputs`. In version 5 of Transformers, `videos` will no longer be accepted\",\n-                FutureWarning,\n-            )\n-            inputs = kwargs.pop(\"videos\")\n         if inputs is None:\n             raise ValueError(\"Cannot call the video-classification pipeline without an inputs argument!\")\n         return super().__call__(inputs, **kwargs)"
        },
        {
            "sha": "d129c5836538904b2259e14e23e25267f529165c",
            "filename": "src/transformers/pipelines/zero_shot_image_classification.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -1,4 +1,3 @@\n-import warnings\n from collections import UserDict\n from typing import Any, Union, overload\n \n@@ -132,12 +131,6 @@ def _sanitize_parameters(self, tokenizer_kwargs=None, **kwargs):\n             preprocess_params[\"timeout\"] = kwargs[\"timeout\"]\n         if \"hypothesis_template\" in kwargs:\n             preprocess_params[\"hypothesis_template\"] = kwargs[\"hypothesis_template\"]\n-        if tokenizer_kwargs is not None:\n-            warnings.warn(\n-                \"The `tokenizer_kwargs` argument is deprecated and will be removed in version 5 of Transformers\",\n-                FutureWarning,\n-            )\n-            preprocess_params[\"tokenizer_kwargs\"] = tokenizer_kwargs\n \n         return preprocess_params, {}, {}\n "
        },
        {
            "sha": "bc1add047802899351eb5cd13b2c5bd62f416131",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -42,7 +42,6 @@\n         MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n         LEDForConditionalGeneration,\n         LEDForQuestionAnswering,\n-        LEDForSequenceClassification,\n         LEDModel,\n         LEDTokenizer,\n     )\n@@ -264,19 +263,15 @@ def check_global_attention(self, config, inputs_dict):\n @require_torch\n class LEDModelTest(ModelTesterMixin, GenerationTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     all_model_classes = (\n-        (LEDModel, LEDForConditionalGeneration, LEDForSequenceClassification, LEDForQuestionAnswering)\n-        if is_torch_available()\n-        else ()\n+        (LEDModel, LEDForConditionalGeneration, LEDForQuestionAnswering) if is_torch_available() else ()\n     )\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": LEDModel,\n             \"question-answering\": LEDForQuestionAnswering,\n             \"summarization\": LEDForConditionalGeneration,\n-            \"text-classification\": LEDForSequenceClassification,\n             \"text2text-generation\": LEDForConditionalGeneration,\n             \"translation\": LEDForConditionalGeneration,\n-            \"zero-shot\": LEDForSequenceClassification,\n         }\n         if is_torch_available()\n         else {}\n@@ -336,11 +331,10 @@ def prepare_config_and_inputs_for_generate(self, *args, **kwargs):\n         inputs_dict.pop(\"global_attention_mask\")\n         return config, inputs_dict\n \n-    # LEDForSequenceClassification does not support inputs_embeds\n     def test_inputs_embeds(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n-        for model_class in (LEDModel, LEDForConditionalGeneration, LEDForQuestionAnswering):\n+        for model_class in self.all_model_classes:\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()"
        },
        {
            "sha": "41e74a6a0a663131e324875e20a550389f061607",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -176,7 +176,7 @@ def create_and_check_for_masked_image_modeling(self, config, pixel_values, label\n         model.eval()\n         result = model(pixel_values)\n         self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_channels, self.image_size, self.image_size)\n+            result.reconstruction.shape, (self.batch_size, self.num_channels, self.image_size, self.image_size)\n         )\n \n         # test greyscale images\n@@ -187,7 +187,7 @@ def create_and_check_for_masked_image_modeling(self, config, pixel_values, label\n \n         pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n         result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, 1, self.image_size, self.image_size))\n+        self.parent.assertEqual(result.reconstruction.shape, (self.batch_size, 1, self.image_size, self.image_size))\n \n     def create_and_check_for_image_classification(self, config, pixel_values, labels):\n         config.num_labels = self.type_sequence_label_size"
        },
        {
            "sha": "80498f9465350e8d5888eaaf95baa9c78e552da7",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -176,7 +176,7 @@ def create_and_check_for_masked_image_modeling(self, config, pixel_values, label\n         model.eval()\n         result = model(pixel_values)\n         self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_channels, self.image_size, self.image_size)\n+            result.reconstruction.shape, (self.batch_size, self.num_channels, self.image_size, self.image_size)\n         )\n \n         # test greyscale images\n@@ -187,7 +187,7 @@ def create_and_check_for_masked_image_modeling(self, config, pixel_values, label\n \n         pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n         result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, 1, self.image_size, self.image_size))\n+        self.parent.assertEqual(result.reconstruction.shape, (self.batch_size, 1, self.image_size, self.image_size))\n \n     def create_and_check_for_image_classification(self, config, pixel_values, labels):\n         config.num_labels = self.type_sequence_label_size"
        },
        {
            "sha": "d8e250fc23534ae612531a74ac2b465fb4d22d39",
            "filename": "tests/models/wav2vec2/test_tokenization_wav2vec2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 308,
            "changes": 310,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_tokenization_wav2vec2.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -13,24 +13,15 @@\n # limitations under the License.\n \"\"\"Tests for the Wav2Vec2 tokenizer.\"\"\"\n \n-import inspect\n import json\n import os\n import random\n-import shutil\n import tempfile\n import unittest\n \n-import numpy as np\n-\n-from transformers import (\n-    AddedToken,\n-    Wav2Vec2Config,\n-    Wav2Vec2CTCTokenizer,\n-    Wav2Vec2Tokenizer,\n-)\n+from transformers import AddedToken, Wav2Vec2CTCTokenizer\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES, Wav2Vec2CTCTokenizerOutput\n-from transformers.testing_utils import get_tests_dir, require_torch, slow\n+from transformers.testing_utils import get_tests_dir\n \n from ...test_tokenization_common import TokenizerTesterMixin\n \n@@ -53,303 +44,6 @@ def floats_list(shape, scale=1.0, rng=None, name=None):\n     return values\n \n \n-class Wav2Vec2TokenizerTest(unittest.TestCase):\n-    tokenizer_class = Wav2Vec2Tokenizer\n-\n-    @classmethod\n-    def setUpClass(cls):\n-        super().setUpClass()\n-\n-        vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n-        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-\n-        cls.special_tokens_map = {\"pad_token\": \"<pad>\", \"unk_token\": \"<unk>\", \"bos_token\": \"<s>\", \"eos_token\": \"</s>\"}\n-\n-        cls.tmpdirname = tempfile.mkdtemp()\n-        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-\n-    @classmethod\n-    def get_tokenizer(cls, pretrained_name=None, **kwargs):\n-        kwargs.update(cls.special_tokens_map)\n-        pretrained_name = pretrained_name or cls.tmpdirname\n-        return Wav2Vec2Tokenizer.from_pretrained(pretrained_name, **kwargs)\n-\n-    def test_tokenizer_decode(self):\n-        # TODO(PVP) - change to facebook\n-        tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n-\n-        sample_ids = [\n-            [11, 5, 15, tokenizer.pad_token_id, 15, 8, 98],\n-            [24, 22, 5, tokenizer.word_delimiter_token_id, 24, 22, 5, 77],\n-        ]\n-        tokens = tokenizer.decode(sample_ids[0])\n-        batch_tokens = tokenizer.batch_decode(sample_ids)\n-        self.assertEqual(tokens, batch_tokens[0])\n-        self.assertEqual(batch_tokens, [\"HELLO<unk>\", \"BYE BYE<unk>\"])\n-\n-    def test_tokenizer_decode_special(self):\n-        # TODO(PVP) - change to facebook\n-        tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n-\n-        sample_ids = [\n-            [11, 5, 15, tokenizer.pad_token_id, 15, 8, 98],\n-            [24, 22, 5, tokenizer.word_delimiter_token_id, 24, 22, 5, 77],\n-        ]\n-        sample_ids_2 = [\n-            [11, 5, 5, 5, 5, 5, 15, 15, 15, tokenizer.pad_token_id, 15, 8, 98],\n-            [\n-                24,\n-                22,\n-                5,\n-                tokenizer.pad_token_id,\n-                tokenizer.pad_token_id,\n-                tokenizer.pad_token_id,\n-                tokenizer.word_delimiter_token_id,\n-                24,\n-                22,\n-                5,\n-                77,\n-                tokenizer.word_delimiter_token_id,\n-            ],\n-        ]\n-\n-        batch_tokens = tokenizer.batch_decode(sample_ids)\n-        batch_tokens_2 = tokenizer.batch_decode(sample_ids_2)\n-        self.assertEqual(batch_tokens, batch_tokens_2)\n-        self.assertEqual(batch_tokens, [\"HELLO<unk>\", \"BYE BYE<unk>\"])\n-\n-    def test_tokenizer_decode_added_tokens(self):\n-        tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n-        tokenizer.add_tokens([\"!\", \"?\"])\n-        tokenizer.add_special_tokens({\"cls_token\": \"$$$\"})\n-\n-        sample_ids = [\n-            [\n-                11,\n-                5,\n-                15,\n-                tokenizer.pad_token_id,\n-                15,\n-                8,\n-                98,\n-                32,\n-                32,\n-                33,\n-                tokenizer.word_delimiter_token_id,\n-                32,\n-                32,\n-                33,\n-                34,\n-                34,\n-            ],\n-            [24, 22, 5, tokenizer.word_delimiter_token_id, 24, 22, 5, 77, tokenizer.pad_token_id, 34, 34],\n-        ]\n-        batch_tokens = tokenizer.batch_decode(sample_ids)\n-        batch_tokens_2 = tokenizer.batch_decode(sample_ids, skip_special_tokens=True)\n-\n-        self.assertEqual(batch_tokens, [\"HELLO<unk>!? !?$$$\", \"BYE BYE<unk>$$$\"])\n-        self.assertEqual(batch_tokens_2, [\"HELO!? !?\", \"BYE BYE\"])\n-\n-    def test_call(self):\n-        # Tests that all call wrap to encode_plus and batch_encode_plus\n-        tokenizer = self.get_tokenizer()\n-        # create three inputs of length 800, 1000, and 1200\n-        speech_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n-        np_speech_inputs = [np.asarray(speech_input) for speech_input in speech_inputs]\n-\n-        # Test not batched input\n-        encoded_sequences_1 = tokenizer(speech_inputs[0], return_tensors=\"np\").input_values\n-        encoded_sequences_2 = tokenizer(np_speech_inputs[0], return_tensors=\"np\").input_values\n-        self.assertTrue(np.allclose(encoded_sequences_1, encoded_sequences_2, atol=1e-3))\n-\n-        # Test batched\n-        encoded_sequences_1 = tokenizer(speech_inputs, return_tensors=\"np\").input_values\n-        encoded_sequences_2 = tokenizer(np_speech_inputs, return_tensors=\"np\").input_values\n-        for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n-            self.assertTrue(np.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n-\n-        # Test 2-D numpy arrays are batched.\n-        speech_inputs = [floats_list((1, x))[0] for x in (800, 800, 800)]\n-        np_speech_inputs = np.asarray(speech_inputs)\n-        encoded_sequences_1 = tokenizer(speech_inputs, return_tensors=\"np\").input_values\n-        encoded_sequences_2 = tokenizer(np_speech_inputs, return_tensors=\"np\").input_values\n-        for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n-            self.assertTrue(np.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n-\n-    def test_padding(self, max_length=50):\n-        def _input_values_have_equal_length(input_values):\n-            length = len(input_values[0])\n-            for input_values_slice in input_values[1:]:\n-                if len(input_values_slice) != length:\n-                    return False\n-            return True\n-\n-        def _input_values_are_equal(input_values_1, input_values_2):\n-            if len(input_values_1) != len(input_values_2):\n-                return False\n-\n-            for input_values_slice_1, input_values_slice_2 in zip(input_values_1, input_values_2):\n-                if not np.allclose(np.asarray(input_values_slice_1), np.asarray(input_values_slice_2), atol=1e-3):\n-                    return False\n-            return True\n-\n-        tokenizer = self.get_tokenizer()\n-        speech_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n-\n-        input_values_1 = tokenizer(speech_inputs).input_values\n-        input_values_2 = tokenizer(speech_inputs, padding=\"longest\").input_values\n-        input_values_3 = tokenizer(speech_inputs, padding=\"longest\", max_length=1600).input_values\n-\n-        self.assertFalse(_input_values_have_equal_length(input_values_1))\n-        self.assertTrue(_input_values_have_equal_length(input_values_2))\n-        self.assertTrue(_input_values_have_equal_length(input_values_3))\n-        self.assertTrue(_input_values_are_equal(input_values_2, input_values_3))\n-        self.assertTrue(len(input_values_1[0]) == 800)\n-        self.assertTrue(len(input_values_2[0]) == 1200)\n-        # padding should be 0.0\n-        self.assertTrue(abs(sum(np.asarray(input_values_2[0])[800:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_2[1])[1000:])) < 1e-3)\n-\n-        input_values_4 = tokenizer(speech_inputs, padding=\"max_length\").input_values\n-        input_values_5 = tokenizer(speech_inputs, padding=\"max_length\", max_length=1600).input_values\n-\n-        self.assertTrue(_input_values_are_equal(input_values_1, input_values_4))\n-        self.assertEqual(input_values_5.shape, (3, 1600))\n-        # padding should be 0.0\n-        self.assertTrue(abs(sum(np.asarray(input_values_5[0])[800:1200])) < 1e-3)\n-\n-        input_values_6 = tokenizer(speech_inputs, pad_to_multiple_of=500).input_values\n-        input_values_7 = tokenizer(speech_inputs, padding=\"longest\", pad_to_multiple_of=500).input_values\n-        input_values_8 = tokenizer(\n-            speech_inputs, padding=\"max_length\", pad_to_multiple_of=500, max_length=2400\n-        ).input_values\n-\n-        self.assertTrue(_input_values_are_equal(input_values_1, input_values_6))\n-        self.assertEqual(input_values_7.shape, (3, 1500))\n-        self.assertEqual(input_values_8.shape, (3, 2500))\n-        # padding should be 0.0\n-        self.assertTrue(abs(sum(np.asarray(input_values_7[0])[800:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_7[1])[1000:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_7[2])[1200:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_8[0])[800:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_8[1])[1000:])) < 1e-3)\n-        self.assertTrue(abs(sum(np.asarray(input_values_8[2])[1200:])) < 1e-3)\n-\n-    def test_get_vocab(self):\n-        tokenizer = self.get_tokenizer()\n-        vocab_dict = tokenizer.get_vocab()\n-        self.assertIsInstance(vocab_dict, dict)\n-        self.assertGreaterEqual(len(tokenizer), len(vocab_dict))\n-\n-        vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(len(tokenizer))]\n-        self.assertEqual(len(vocab), len(tokenizer))\n-\n-        tokenizer.add_tokens([\"asdfasdfasdfasdf\"])\n-        vocab = [tokenizer.convert_ids_to_tokens(i) for i in range(len(tokenizer))]\n-        self.assertEqual(len(vocab), len(tokenizer))\n-\n-    def test_save_and_load_tokenizer(self):\n-        tokenizer = self.get_tokenizer()\n-        # Isolate this from the other tests because we save additional tokens/etc\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        sample_ids = [0, 1, 4, 8, 9, 0, 12]\n-        before_tokens = tokenizer.decode(sample_ids)\n-        before_vocab = tokenizer.get_vocab()\n-        tokenizer.save_pretrained(tmpdirname)\n-\n-        after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n-        after_tokens = after_tokenizer.decode(sample_ids)\n-        after_vocab = after_tokenizer.get_vocab()\n-\n-        self.assertEqual(before_tokens, after_tokens)\n-        self.assertDictEqual(before_vocab, after_vocab)\n-\n-        shutil.rmtree(tmpdirname)\n-\n-        tokenizer = self.get_tokenizer()\n-\n-        # Isolate this from the other tests because we save additional tokens/etc\n-        tmpdirname = tempfile.mkdtemp()\n-\n-        before_len = len(tokenizer)\n-        sample_ids = [0, 1, 4, 8, 9, 0, 12, before_len, before_len + 1, before_len + 2]\n-        tokenizer.add_tokens([\"?\", \"!\"])\n-        extra_special_tokens = tokenizer.extra_special_tokens\n-        extra_special_tokens.append(\"&\")\n-        tokenizer.add_special_tokens(\n-            {\"extra_special_tokens\": extra_special_tokens}, replace_extra_special_tokens=False\n-        )\n-        before_tokens = tokenizer.decode(sample_ids)\n-        before_vocab = tokenizer.get_vocab()\n-        tokenizer.save_pretrained(tmpdirname)\n-\n-        after_tokenizer = tokenizer.__class__.from_pretrained(tmpdirname)\n-        after_tokens = after_tokenizer.decode(sample_ids)\n-        after_vocab = after_tokenizer.get_vocab()\n-\n-        self.assertEqual(before_tokens, after_tokens)\n-        self.assertDictEqual(before_vocab, after_vocab)\n-\n-        self.assertTrue(len(tokenizer), before_len + 3)\n-        self.assertTrue(len(tokenizer), len(after_tokenizer))\n-        shutil.rmtree(tmpdirname)\n-\n-    def test_tokenizer_slow_store_full_signature(self):\n-        signature = inspect.signature(self.tokenizer_class.__init__)\n-        tokenizer = self.get_tokenizer()\n-\n-        for parameter_name, parameter in signature.parameters.items():\n-            if parameter.default != inspect.Parameter.empty:\n-                self.assertIn(parameter_name, tokenizer.init_kwargs)\n-\n-    def test_zero_mean_unit_variance_normalization(self):\n-        tokenizer = self.get_tokenizer(do_normalize=True)\n-        speech_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n-        processed = tokenizer(speech_inputs, padding=\"longest\")\n-        input_values = processed.input_values\n-\n-        def _check_zero_mean_unit_variance(input_vector):\n-            self.assertTrue(np.abs(np.mean(input_vector)) < 1e-3)\n-            self.assertTrue(np.abs(np.var(input_vector) - 1) < 1e-3)\n-\n-        _check_zero_mean_unit_variance(input_values[0, :800])\n-        _check_zero_mean_unit_variance(input_values[1, :1000])\n-        _check_zero_mean_unit_variance(input_values[2])\n-\n-    def test_return_attention_mask(self):\n-        speech_inputs = [floats_list((1, x))[0] for x in range(800, 1400, 200)]\n-\n-        # default case -> no attention_mask is returned\n-        tokenizer = self.get_tokenizer()\n-        processed = tokenizer(speech_inputs)\n-        self.assertNotIn(\"attention_mask\", processed)\n-\n-        # wav2vec2-lv60 -> return attention_mask\n-        tokenizer = self.get_tokenizer(return_attention_mask=True)\n-        processed = tokenizer(speech_inputs, padding=\"longest\")\n-\n-        self.assertIn(\"attention_mask\", processed)\n-        self.assertListEqual(list(processed.attention_mask.shape), list(processed.input_values.shape))\n-        self.assertListEqual(processed.attention_mask.sum(-1).tolist(), [800, 1000, 1200])\n-\n-    @slow\n-    @require_torch\n-    def test_pretrained_checkpoints_are_set_correctly(self):\n-        # this test makes sure that models that are using\n-        # group norm don't have their tokenizer return the\n-        # attention_mask\n-        model_id = \"facebook/wav2vec2-base-960h\"\n-        config = Wav2Vec2Config.from_pretrained(model_id)\n-        tokenizer = Wav2Vec2Tokenizer.from_pretrained(model_id)\n-\n-        # only \"layer\" feature extraction norm should make use of\n-        # attention_mask\n-        self.assertEqual(tokenizer.return_attention_mask, config.feat_extract_norm == \"layer\")\n-\n-\n class Wav2Vec2CTCTokenizerTest(TokenizerTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"facebook/wav2vec2-base-960h\"\n     tokenizer_class = Wav2Vec2CTCTokenizer"
        },
        {
            "sha": "16dec39081caab579b76f7fce1e6716fe9e18a09",
            "filename": "tests/utils/test_model_card.py",
            "status": "removed",
            "additions": 0,
            "deletions": 88,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/942f110c2fdc3a17b400b0eb8fea33580f723b5a/tests%2Futils%2Ftest_model_card.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/942f110c2fdc3a17b400b0eb8fea33580f723b5a/tests%2Futils%2Ftest_model_card.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_model_card.py?ref=942f110c2fdc3a17b400b0eb8fea33580f723b5a",
            "patch": "@@ -1,88 +0,0 @@\n-# Copyright 2019 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import json\n-import os\n-import tempfile\n-import unittest\n-\n-from transformers.modelcard import ModelCard, TrainingSummary\n-\n-\n-class ModelCardTester(unittest.TestCase):\n-    def setUp(self):\n-        self.inputs_dict = {\n-            \"model_details\": {\n-                \"Organization\": \"testing\",\n-                \"Model date\": \"today\",\n-                \"Model version\": \"v2.1, Developed by Test Corp in 2019.\",\n-                \"Architecture\": \"Convolutional Neural Network.\",\n-            },\n-            \"metrics\": \"BLEU and ROUGE-1\",\n-            \"evaluation_data\": {\n-                \"Datasets\": {\"BLEU\": \"My-great-dataset-v1\", \"ROUGE-1\": \"My-short-dataset-v2.1\"},\n-                \"Preprocessing\": \"See details on https://huggingface.co/papers/1810.03993\",\n-            },\n-            \"training_data\": {\n-                \"Dataset\": \"English Wikipedia dump dated 2018-12-01\",\n-                \"Preprocessing\": (\n-                    \"Using SentencePiece vocabulary of size 52k tokens. See details on\"\n-                    \" https://huggingface.co/papers/1810.03993\"\n-                ),\n-            },\n-            \"quantitative_analyses\": {\"BLEU\": 55.1, \"ROUGE-1\": 76},\n-        }\n-\n-    def test_model_card_common_properties(self):\n-        modelcard = ModelCard.from_dict(self.inputs_dict)\n-        self.assertTrue(hasattr(modelcard, \"model_details\"))\n-        self.assertTrue(hasattr(modelcard, \"intended_use\"))\n-        self.assertTrue(hasattr(modelcard, \"factors\"))\n-        self.assertTrue(hasattr(modelcard, \"metrics\"))\n-        self.assertTrue(hasattr(modelcard, \"evaluation_data\"))\n-        self.assertTrue(hasattr(modelcard, \"training_data\"))\n-        self.assertTrue(hasattr(modelcard, \"quantitative_analyses\"))\n-        self.assertTrue(hasattr(modelcard, \"ethical_considerations\"))\n-        self.assertTrue(hasattr(modelcard, \"caveats_and_recommendations\"))\n-\n-    def test_model_card_to_json_string(self):\n-        modelcard = ModelCard.from_dict(self.inputs_dict)\n-        obj = json.loads(modelcard.to_json_string())\n-        for key, value in self.inputs_dict.items():\n-            self.assertEqual(obj[key], value)\n-\n-    def test_model_card_to_json_file(self):\n-        model_card_first = ModelCard.from_dict(self.inputs_dict)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            filename = os.path.join(tmpdirname, \"modelcard.json\")\n-            model_card_first.to_json_file(filename)\n-            model_card_second = ModelCard.from_json_file(filename)\n-\n-        self.assertEqual(model_card_second.to_dict(), model_card_first.to_dict())\n-\n-    def test_model_card_from_and_save_pretrained(self):\n-        model_card_first = ModelCard.from_dict(self.inputs_dict)\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            model_card_first.save_pretrained(tmpdirname)\n-            model_card_second = ModelCard.from_pretrained(tmpdirname)\n-\n-        self.assertEqual(model_card_second.to_dict(), model_card_first.to_dict())\n-\n-    def test_model_summary_modelcard_base_metadata(self):\n-        metadata = TrainingSummary(\"Model name\").create_metadata()\n-        self.assertTrue(\"library_name\" in metadata)\n-        self.assertTrue(metadata[\"library_name\"] == \"transformers\")"
        },
        {
            "sha": "7472fccd4ed17dc4c08fb089a9fea345362bf1fd",
            "filename": "tests/utils/tiny_model_summary.json",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Futils%2Ftiny_model_summary.json",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/tests%2Futils%2Ftiny_model_summary.json",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftiny_model_summary.json?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -2987,17 +2987,6 @@\n         ],\n         \"sha\": \"47c7a75a1e650dae60ff6e9bbab0f2386946670c\"\n     },\n-    \"LEDForSequenceClassification\": {\n-        \"tokenizer_classes\": [\n-            \"LEDTokenizer\",\n-            \"LEDTokenizerFast\"\n-        ],\n-        \"processor_classes\": [],\n-        \"model_classes\": [\n-            \"LEDForSequenceClassification\"\n-        ],\n-        \"sha\": \"3571e2c9d9f2f2ec0b8fe47090330b128be05126\"\n-    },\n     \"LEDModel\": {\n         \"tokenizer_classes\": [\n             \"LEDTokenizer\","
        },
        {
            "sha": "cf6dd9760b29f720b494fcefff6ddec825ba023f",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -73,6 +73,7 @@\n     \"UdopConfig\": [\"feed_forward_proj\"],\n     \"ZambaConfig\": [\"attn_layer_offset\", \"attn_layer_period\"],\n     \"MllamaVisionConfig\": [\"supported_aspect_ratios\"],\n+    \"LEDConfig\": [\"classifier_dropout\"],\n     \"GPTNeoXConfig\": [\"rotary_emb_base\"],\n     \"ShieldGemma2Config\": [\"mm_tokens_per_image\", \"vision_config\"],\n     \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],"
        },
        {
            "sha": "daae0536a5f009b2e649a8b356d982fdc807b977",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/29b0770abfa008ace7a4ef7716aff0082de11a80/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/29b0770abfa008ace7a4ef7716aff0082de11a80/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=29b0770abfa008ace7a4ef7716aff0082de11a80",
            "patch": "@@ -987,7 +987,6 @@ def find_all_documented_objects() -> list[str]:\n     \"SquadV1Processor\",\n     \"SquadV2Processor\",\n     \"Wav2Vec2ForMaskedLM\",\n-    \"Wav2Vec2Tokenizer\",\n     \"glue_compute_metrics\",\n     \"glue_convert_examples_to_features\",\n     \"glue_output_modes\",\n@@ -1010,7 +1009,6 @@ def find_all_documented_objects() -> list[str]:\n     \"DPRPretrainedReader\",  # Like an Encoder.\n     \"DummyObject\",  # Just picked by mistake sometimes.\n     \"MecabTokenizer\",  # Internal, should never have been in the main init.\n-    \"ModelCard\",  # Internal type.\n     \"SqueezeBertModule\",  # Internal building block (should have been called SqueezeBertLayer)\n     \"TransfoXLCorpus\",  # Internal type.\n     \"WordpieceTokenizer\",  # Internal, should never have been in the main init."
        }
    ],
    "stats": {
        "total": 1212,
        "additions": 17,
        "deletions": 1195
    }
}