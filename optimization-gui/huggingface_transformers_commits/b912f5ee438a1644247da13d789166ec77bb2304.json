{
    "author": "ArthurZucker",
    "message": "use torch.testing.assertclose instead to get more details about error in cis (#35659)\n\n* use torch.testing.assertclose instead to get more details about error in cis\n\n* fix\n\n* style\n\n* test_all\n\n* revert for I bert\n\n* fixes and updates\n\n* more image processing fixes\n\n* more image processors\n\n* fix mamba and co\n\n* style\n\n* less strick\n\n* ok I won't be strict\n\n* skip and be done\n\n* up",
    "sha": "b912f5ee438a1644247da13d789166ec77bb2304",
    "files": [
        {
            "sha": "adead75bf5de1e111aaabf81815661c53276d0d3",
            "filename": "src/transformers/models/wav2vec2_bert/convert_wav2vec2_seamless_checkpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fconvert_wav2vec2_seamless_checkpoint.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -183,7 +183,7 @@ def convert_wav2vec2_bert_checkpoint(\n         with torch.no_grad():\n             outputs = hf_wav2vec(**inputs)\n \n-        torch.testing.assert_close(original_output, outputs.last_hidden_state, atol=5e-3, rtol=5e-3)\n+        torch.testing.assert_close(original_output, outputs.last_hidden_state, rtol=5e-3, atol=5e-3)\n \n \n if __name__ == \"__main__\":"
        },
        {
            "sha": "0fb56417d5c3e1cf1d6946a8bac8fd253c0834ba",
            "filename": "tests/agents/test_agent_types.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fagents%2Ftest_agent_types.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fagents%2Ftest_agent_types.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fagents%2Ftest_agent_types.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -47,7 +47,7 @@ def test_from_tensor(self):\n         path = str(agent_type.to_string())\n \n         # Ensure that the tensor and the agent_type's tensor are the same\n-        self.assertTrue(torch.allclose(tensor, agent_type.to_raw(), atol=1e-4))\n+        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n \n         del agent_type\n \n@@ -56,7 +56,7 @@ def test_from_tensor(self):\n \n         # Ensure that the file contains the same value as the original tensor\n         new_tensor, _ = sf.read(path)\n-        self.assertTrue(torch.allclose(tensor, torch.tensor(new_tensor), atol=1e-4))\n+        torch.testing.assert_close(tensor, torch.tensor(new_tensor), rtol=1e-4, atol=1e-4)\n \n     def test_from_string(self):\n         tensor = torch.rand(12, dtype=torch.float64) - 0.5\n@@ -65,7 +65,7 @@ def test_from_string(self):\n \n         agent_type = AgentAudio(path)\n \n-        self.assertTrue(torch.allclose(tensor, agent_type.to_raw(), atol=1e-4))\n+        torch.testing.assert_close(tensor, agent_type.to_raw(), rtol=1e-4, atol=1e-4)\n         self.assertEqual(agent_type.to_string(), path)\n \n \n@@ -78,7 +78,7 @@ def test_from_tensor(self):\n         path = str(agent_type.to_string())\n \n         # Ensure that the tensor and the agent_type's tensor are the same\n-        self.assertTrue(torch.allclose(tensor, agent_type._tensor, atol=1e-4))\n+        torch.testing.assert_close(tensor, agent_type._tensor, rtol=1e-4, atol=1e-4)\n \n         self.assertIsInstance(agent_type.to_raw(), Image.Image)\n "
        },
        {
            "sha": "8f67852bfd055ab01ad5e5b9e14795ec8ffdecaf",
            "filename": "tests/bettertransformer/test_integration.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fbettertransformer%2Ftest_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fbettertransformer%2Ftest_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fbettertransformer%2Ftest_integration.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -66,7 +66,7 @@ def test_transform_and_reverse(self):\n             )\n \n             output_from_pretrained = model_reloaded.generate(**inp)\n-            self.assertTrue(torch.allclose(output, output_from_pretrained))\n+            torch.testing.assert_close(output, output_from_pretrained)\n \n     def test_error_save_pretrained(self):\n         r\"\"\""
        },
        {
            "sha": "89a8bd1d1bb6336e8177a1136599eb5c36b33aee",
            "filename": "tests/deepspeed/test_deepspeed.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fdeepspeed%2Ftest_deepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fdeepspeed%2Ftest_deepspeed.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -360,14 +360,14 @@ def bad_deepspeed_create_sinusoidal_positions(num_pos: int, dim: int) -> torch.T\n             model.config.max_position_embeddings, model.config.rotary_dim\n         )\n         self.assertFalse(torch.allclose(good_deepspeed_sin_cos, bad_deepspeed_sin_cos))\n-        self.assertTrue(torch.allclose(good_torch_sin_cos, good_deepspeed_sin_cos.cpu()))\n+        torch.testing.assert_close(good_torch_sin_cos, good_deepspeed_sin_cos.cpu())\n \n         # Finally, we can see that the incorrect pattern is okay on vanilla torch, demostrating that this issue is\n         # exclusive to DeepSpeed\n         bad_torch_sin_cos = bad_deepspeed_create_sinusoidal_positions(\n             model.config.max_position_embeddings, model.config.rotary_dim\n         )\n-        self.assertTrue(torch.allclose(bad_torch_sin_cos, good_torch_sin_cos))\n+        torch.testing.assert_close(bad_torch_sin_cos, good_torch_sin_cos)\n \n \n class TrainerIntegrationDeepSpeedWithCustomConfig(TestCasePlus):"
        },
        {
            "sha": "a922a71c22c68466a2c1775edbcdf1e45537b412",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -166,8 +166,8 @@ def test_temperature_dist_warper(self):\n         processed_scores = temp_dist_warper_smoother(input_ids, scores)\n \n         # uniform distribution stays uniform\n-        self.assertTrue(torch.allclose(probs[0, :], warped_prob_sharp[0, :], atol=1e-3))\n-        self.assertTrue(torch.allclose(probs[0, :], warped_prob_smooth[0, :], atol=1e-3))\n+        torch.testing.assert_close(probs[0, :], warped_prob_sharp[0, :], rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(probs[0, :], warped_prob_smooth[0, :], rtol=1e-3, atol=1e-3)\n \n         # sharp peaks get higher, valleys get lower\n         self.assertLess(probs[1, :].max(), warped_prob_sharp[1, :].max())\n@@ -288,7 +288,7 @@ def test_top_p_dist_warper(self):\n         EXPECTED_FILTERED_DIST = torch.tensor(\n             [[0.3, 0.0, 0.0, 0.5], [0.0, 0.3, 0.3, 0.25]], device=torch_device, dtype=torch.float\n         )\n-        self.assertTrue(torch.allclose(filtered_dist, EXPECTED_FILTERED_DIST, atol=1e-3))\n+        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n \n         # processor should not change logits in-place\n         self.assertFalse(torch.all(top_p_warp(input_ids, dist) == dist))\n@@ -335,7 +335,7 @@ def test_min_p_dist_warper(self):\n             device=torch_device,\n             dtype=torch.float,\n         )\n-        self.assertTrue(torch.allclose(filtered_dist, EXPECTED_FILTERED_DIST, atol=1e-3))\n+        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n \n         # processor should not change logits in-place\n         self.assertFalse(torch.all(min_p_warp(input_ids, dist) == dist))\n@@ -372,7 +372,7 @@ def test_typical_dist_warper(self):\n         EXPECTED_FILTERED_DIST = torch.tensor(\n             [[0.97, 0.0, 0.0, 0.0], [0.0, 0.2, 0.2, 0.2]], device=torch_device, dtype=torch.float\n         )\n-        self.assertTrue(torch.allclose(filtered_dist, EXPECTED_FILTERED_DIST, atol=1e-3))\n+        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n \n         # processor should not change logits in-place\n         self.assertFalse(torch.all(typical_warp(input_ids, dist) == dist))\n@@ -422,7 +422,7 @@ def test_epsilon_dist_warper(self):\n         EXPECTED_FILTERED_DIST = torch.tensor(\n             [[0.87, 0, 0, 0], [0.4, 0.299, 0.101, 0.2]], device=torch_device, dtype=torch.float\n         )\n-        self.assertTrue(torch.allclose(filtered_dist, EXPECTED_FILTERED_DIST, atol=1e-3))\n+        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n \n         # processor should not change logits in-place\n         self.assertFalse(torch.all(epsilon_warp(input_ids, dist) == dist))\n@@ -462,7 +462,7 @@ def test_eta_dist_warper(self):\n         EXPECTED_FILTERED_DIST = torch.tensor(\n             [[0.0, 0.1, 0.8, 0.1], [0.0, 0.0, 0.9, 0.0]], device=torch_device, dtype=torch.float\n         )\n-        self.assertTrue(torch.allclose(filtered_dist, EXPECTED_FILTERED_DIST, atol=1e-3))\n+        torch.testing.assert_close(filtered_dist, EXPECTED_FILTERED_DIST, rtol=1e-3, atol=1e-3)\n \n         # processor should not change logits in-place\n         self.assertFalse(torch.all(eta_warp(input_ids, dist) == dist))\n@@ -599,7 +599,7 @@ def test_no_bad_words_dist_processor(self):\n         # check edge case\n         no_bad_words_dist_proc = NoBadWordsLogitsProcessor(bad_words_ids=[[4]], eos_token_id=eos_token_id)\n         filtered_scores = no_bad_words_dist_proc(input_ids, scores)\n-        self.assertTrue(torch.allclose(scores, filtered_scores, atol=1e-3))\n+        torch.testing.assert_close(scores, filtered_scores, rtol=1e-3, atol=1e-3)\n \n     def test_bias_dist_processor(self):\n         vocab_size = 5\n@@ -674,7 +674,7 @@ def test_processor_list(self):\n         scores_comp = processor(input_ids, scores_comp)\n \n         # scores should be equal\n-        self.assertTrue(torch.allclose(scores, scores_comp, atol=1e-3))\n+        torch.testing.assert_close(scores, scores_comp, rtol=1e-3, atol=1e-3)\n \n         # input_ids should never be changed\n         self.assertListEqual(input_ids.tolist(), input_ids_comp.tolist())"
        },
        {
            "sha": "e613395c126d24f39670f6ba79b42c4636cba125",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1531,7 +1531,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, atol=1e-5, rtol=1e-5)\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @pytest.mark.generate\n     def test_past_key_values_format(self):\n@@ -2708,7 +2708,7 @@ def test_transition_scores_group_beam_search_encoder_decoder(self):\n         transition_scores = model.compute_transition_scores(outputs.sequences, outputs.scores, outputs.beam_indices)\n         transition_scores_sum = transition_scores.sum(-1)\n \n-        self.assertTrue(torch.allclose(transition_scores_sum, outputs.sequences_scores, atol=1e-3))\n+        torch.testing.assert_close(transition_scores_sum, outputs.sequences_scores, rtol=1e-3, atol=1e-3)\n \n     def test_beam_search_low_memory(self):\n         tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")"
        },
        {
            "sha": "8f3fe3f817a16fad6c6ce6d6e017afb7a5733ece",
            "filename": "tests/models/albert/test_modeling_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_albert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -350,7 +350,7 @@ def test_inference_no_head_absolute_embedding(self):\n             [[[-0.6513, 1.5035, -0.2766], [-0.6515, 1.5046, -0.2780], [-0.6512, 1.5049, -0.2784]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_export(self):"
        },
        {
            "sha": "3a3a33edf6093e4e6482fdd990a1418c140b3dce",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -651,4 +651,4 @@ def test_inference(self):\n             torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n         )\n         expected_logits = torch.tensor([[9.7093, 3.4679]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "c70269b1d196fcc558df799b5848fcd7c9d0b97a",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -612,7 +612,7 @@ def test_inference(self):\n         probs = outputs.logits_per_image.softmax(dim=1)\n         expected_probs = torch.tensor([[9.9942e-01, 5.7805e-04]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(probs, expected_probs, atol=5e-3))\n+        torch.testing.assert_close(probs, expected_probs, rtol=5e-3, atol=5e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -651,6 +651,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.3589, -0.5939, 0.3534], [0.4346, 0.1647, 0.7071], [1.1404, -0.4716, 0.1664]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n         )"
        },
        {
            "sha": "a59a6ba07e9ba396ec391a2c2433b42cdd3e00fd",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -239,7 +239,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @unittest.skip(\n         reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\""
        },
        {
            "sha": "7455d5ff8561e54033dc583eb6c9fc3d0f83d855",
            "filename": "tests/models/audio_spectrogram_transformer/test_feature_extraction_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_feature_extraction_audio_spectrogram_transformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -174,7 +174,7 @@ def test_integration(self):\n         feature_extractor = ASTFeatureExtractor()\n         input_values = feature_extractor(input_speech, return_tensors=\"pt\").input_values\n         self.assertEqual(input_values.shape, (1, 1024, 128))\n-        self.assertTrue(torch.allclose(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, atol=1e-4))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-4, atol=1e-4)\n \n     def test_feat_extract_from_and_save_pretrained(self):\n         feat_extract_first = self.feature_extraction_class(**self.feat_extract_dict)"
        },
        {
            "sha": "169ba3e8aeb476edb8b3a8a3c3335e78bf742915",
            "filename": "tests/models/audio_spectrogram_transformer/test_modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faudio_spectrogram_transformer%2Ftest_modeling_audio_spectrogram_transformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -266,4 +266,4 @@ def test_inference_audio_classification(self):\n \n         expected_slice = torch.tensor([-0.8760, -7.0042, -8.6602]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "fddd3d94d4b751c7751414fc20f95c0c33f78342",
            "filename": "tests/models/autoformer/test_modeling_autoformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fautoformer%2Ftest_modeling_autoformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -445,7 +445,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.3593, -1.3398, 0.6330], [0.2279, 1.5396, -0.1792], [0.0450, 1.3225, -0.2335]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = AutoformerForPrediction.from_pretrained(\"huggingface/autoformer-tourism-monthly\").to(torch_device)\n@@ -463,7 +463,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[-0.0734, -0.9036, 0.8358], [4.7186, 2.4113, 1.9581], [1.7953, 2.3558, 1.2970]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         model = AutoformerForPrediction.from_pretrained(\"huggingface/autoformer-tourism-monthly\").to(torch_device)\n@@ -481,4 +481,4 @@ def test_seq_to_seq_generation(self):\n \n         expected_slice = torch.tensor([3130.6763, 4056.5293, 7053.0786], device=torch_device)\n         mean_prediction = outputs.sequences.mean(dim=1)\n-        self.assertTrue(torch.allclose(mean_prediction[0, -3:], expected_slice, rtol=1e-1))\n+        torch.testing.assert_close(mean_prediction[0, -3:], expected_slice, rtol=1e-1)"
        },
        {
            "sha": "68da2fdf028a3d2f05f78482934ea27cefe7ab83",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -312,11 +312,11 @@ def test_initialization(self):\n             for name, param in model.named_parameters():\n                 if param.requires_grad:\n                     if \"A_log\" in name:\n-                        A = torch.arange(1, config.mamba_n_heads + 1, dtype=torch.float32)[None, :]\n-                        self.assertTrue(torch.allclose(param.data, torch.log(A), atol=1e-5, rtol=1e-5))\n+                        A = torch.arange(1, config.mamba_n_heads + 1, dtype=torch.float32)\n+                        torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n                     elif \"D\" in name:\n                         D = torch.ones(config.mamba_n_heads, dtype=torch.float32)\n-                        self.assertTrue(torch.allclose(param.data, D, atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, D, rtol=1e-5, atol=1e-5)\n                     else:\n                         self.assertIn(\n                             ((param.data.mean() * 1e9).round() / 1e9).item(),\n@@ -482,7 +482,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, atol=1e-5, rtol=1e-1)\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n \n @slow"
        },
        {
            "sha": "06638550951aa69a6332876be936ba816de8936e",
            "filename": "tests/models/bark/test_modeling_bark.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbark%2Ftest_modeling_bark.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -599,7 +599,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_embeds = model(**inputs)[0]\n \n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @require_torch_fp16\n     def test_generate_fp16(self):\n@@ -688,7 +688,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_embeds = model(**inputs)[0]\n \n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @require_torch_fp16\n     def test_generate_fp16(self):\n@@ -1252,8 +1252,8 @@ def test_generate_batching(self):\n         self.assertEqual(tuple(audio_lengths), (output1.shape[1], output2.shape[1]))\n \n         # then assert almost equal\n-        self.assertTrue(torch.allclose(outputs[0, : audio_lengths[0]], output1.squeeze(), atol=2e-3))\n-        self.assertTrue(torch.allclose(outputs[1, : audio_lengths[1]], output2.squeeze(), atol=2e-3))\n+        torch.testing.assert_close(outputs[0, : audio_lengths[0]], output1.squeeze(), rtol=2e-3, atol=2e-3)\n+        torch.testing.assert_close(outputs[1, : audio_lengths[1]], output2.squeeze(), rtol=2e-3, atol=2e-3)\n \n         # now test single input with return_output_lengths = True\n         outputs, _ = self.model.generate(**s1, **args, return_output_lengths=True)"
        },
        {
            "sha": "a795bfcabfb4091999254c5c7561f6fc8674577e",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -887,7 +887,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.7144, 0.8143, -1.2813], [0.7144, 0.8143, -1.2813], [-0.0467, 2.5911, -2.1845]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_base_mask_filling(self):"
        },
        {
            "sha": "6ed9182ad365e3d6557f44cdc90c4941d8dfca0d",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -634,7 +634,7 @@ def test_inference_masked_image_modeling_head(self):\n             [[-3.2437, 0.5072, -13.9174], [-3.2456, 0.4948, -13.9401], [-3.2033, 0.5121, -13.8550]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[bool_masked_pos][:3, :3], expected_slice, atol=1e-2))\n+        torch.testing.assert_close(logits[bool_masked_pos][:3, :3], expected_slice, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_inference_image_classification_head_imagenet_1k(self):\n@@ -655,7 +655,7 @@ def test_inference_image_classification_head_imagenet_1k(self):\n \n         expected_slice = torch.tensor([-1.2385, -1.0987, -1.0108]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_class_idx = 281\n         self.assertEqual(logits.argmax(-1).item(), expected_class_idx)\n@@ -681,7 +681,7 @@ def test_inference_image_classification_head_imagenet_22k(self):\n \n         expected_slice = torch.tensor([1.6881, -0.2787, 0.5901]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_class_idx = 2396\n         self.assertEqual(logits.argmax(-1).item(), expected_class_idx)\n@@ -727,7 +727,7 @@ def test_inference_semantic_segmentation(self):\n                 device=torch_device,\n             )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_post_processing_semantic_segmentation(self):"
        },
        {
            "sha": "db28e077b4ea4438a434b0fd87e940c6d2f02e35",
            "filename": "tests/models/bert/test_modeling_bert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_bert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -682,7 +682,7 @@ def test_inference_no_head_absolute_embedding(self):\n         self.assertEqual(output.shape, expected_shape)\n         expected_slice = torch.tensor([[[0.4249, 0.1008, 0.7531], [0.3771, 0.1188, 0.7467], [0.4152, 0.1098, 0.7108]]])\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head_relative_embedding_key(self):\n@@ -697,7 +697,7 @@ def test_inference_no_head_relative_embedding_key(self):\n             [[[0.0756, 0.3142, -0.5128], [0.3761, 0.3462, -0.5477], [0.2052, 0.3760, -0.1240]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head_relative_embedding_key_query(self):\n@@ -712,7 +712,7 @@ def test_inference_no_head_relative_embedding_key_query(self):\n             [[[0.6496, 0.3784, 0.8203], [0.8148, 0.5656, 0.2636], [-0.0681, 0.5597, 0.7045]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_sdpa_ignored_mask(self):\n         pkv = []"
        },
        {
            "sha": "06fa6b6b12e9fe1b18c08921057f520a9ba37004",
            "filename": "tests/models/bert_generation/test_modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert_generation%2Ftest_modeling_bert_generation.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -319,7 +319,7 @@ def test_inference_no_head_absolute_embedding(self):\n         expected_slice = torch.tensor(\n             [[[0.1775, 0.0083, -0.0321], [1.6002, 0.1287, 0.3912], [2.1473, 0.5791, 0.6066]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch\n@@ -335,4 +335,4 @@ def test_inference_no_head_absolute_embedding(self):\n         expected_slice = torch.tensor(\n             [[[-0.5788, -2.5994, -3.7054], [0.0438, 4.7997, 1.8795], [1.5862, 6.6409, 4.4638]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "8ca17eb14f18cfeb35ac33639a5f7bbeff092f91",
            "filename": "tests/models/big_bird/test_modeling_big_bird.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_big_bird.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -674,12 +674,12 @@ def test_inference_block_sparse_pretraining(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(\n-            torch.allclose(prediction_logits[0, 128:132, 128:132], expected_prediction_logits_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            prediction_logits[0, 128:132, 128:132], expected_prediction_logits_slice, rtol=1e-4, atol=1e-4\n         )\n \n         expected_seq_relationship_logits = torch.tensor([[46.9465, 47.9517]], device=torch_device)\n-        self.assertTrue(torch.allclose(seq_relationship_logits, expected_seq_relationship_logits, atol=1e-4))\n+        torch.testing.assert_close(seq_relationship_logits, expected_seq_relationship_logits, rtol=1e-4, atol=1e-4)\n \n     def test_inference_full_pretraining(self):\n         model = BigBirdForPreTraining.from_pretrained(\"google/bigbird-roberta-base\", attention_type=\"original_full\")\n@@ -703,12 +703,12 @@ def test_inference_full_pretraining(self):\n             ],\n             device=torch_device,\n         )\n-        self.assertTrue(\n-            torch.allclose(prediction_logits[0, 128:132, 128:132], expected_prediction_logits_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            prediction_logits[0, 128:132, 128:132], expected_prediction_logits_slice, rtol=1e-4, atol=1e-4\n         )\n \n         expected_seq_relationship_logits = torch.tensor([[41.4503, 41.2406]], device=torch_device)\n-        self.assertTrue(torch.allclose(seq_relationship_logits, expected_seq_relationship_logits, atol=1e-4))\n+        torch.testing.assert_close(seq_relationship_logits, expected_seq_relationship_logits, rtol=1e-4, atol=1e-4)\n \n     def test_block_sparse_attention_probs(self):\n         \"\"\"\n@@ -773,7 +773,7 @@ def test_block_sparse_attention_probs(self):\n             cl = torch.einsum(\"bhqk,bhkd->bhqd\", attention_probs, value_layer)\n             cl = cl.view(context_layer.size())\n \n-            self.assertTrue(torch.allclose(context_layer, cl, atol=0.001))\n+            torch.testing.assert_close(context_layer, cl, rtol=0.001, atol=0.001)\n \n     def test_block_sparse_context_layer(self):\n         model = BigBirdModel.from_pretrained(\n@@ -822,7 +822,7 @@ def test_block_sparse_context_layer(self):\n         context_layer = context_layer[0]\n \n         self.assertEqual(context_layer.shape, torch.Size((1, 128, 768)))\n-        self.assertTrue(torch.allclose(context_layer[0, 64:78, 300:310], targeted_cl, atol=0.0001))\n+        torch.testing.assert_close(context_layer[0, 64:78, 300:310], targeted_cl, rtol=0.0001, atol=0.0001)\n \n     def test_tokenizer_inference(self):\n         tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-roberta-base\")\n@@ -871,7 +871,7 @@ def test_tokenizer_inference(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(prediction[0, 52:64, 320:324], expected_prediction, atol=1e-4))\n+        torch.testing.assert_close(prediction[0, 52:64, 320:324], expected_prediction, rtol=1e-4, atol=1e-4)\n \n     def test_inference_question_answering(self):\n         tokenizer = BigBirdTokenizer.from_pretrained(\"google/bigbird-base-trivia-itc\")\n@@ -923,8 +923,8 @@ def test_inference_question_answering(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(start_logits[:, 64:96], target_start_logits, atol=1e-4))\n-        self.assertTrue(torch.allclose(end_logits[:, 64:96], target_end_logits, atol=1e-4))\n+        torch.testing.assert_close(start_logits[:, 64:96], target_start_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(end_logits[:, 64:96], target_end_logits, rtol=1e-4, atol=1e-4)\n \n         input_ids = inputs[\"input_ids\"].tolist()\n         answer = [\n@@ -966,4 +966,4 @@ def test_auto_padding(self):\n         # fmt: on\n \n         self.assertEqual(output.shape, torch.Size((1, 241, 768)))\n-        self.assertTrue(torch.allclose(output[0, 64:78, 300:310], target, atol=0.0001))\n+        torch.testing.assert_close(output[0, 64:78, 300:310], target, rtol=0.0001, atol=0.0001)"
        },
        {
            "sha": "5c8ae48e1b037de9aaee612c4d6d77de2a6f4603",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -418,12 +418,12 @@ def _check_batched_forward(self, attn_type, tolerance=1e-3):\n         with torch.no_grad():\n             logits_single_first = model(input_ids=input_ids[:1, :-chunk_length], labels=labels[:1]).logits\n \n-        self.assertTrue(torch.allclose(logits_batched[0, -3:], logits_single_first[0, -3:], atol=tolerance))\n+        torch.testing.assert_close(logits_batched[0, -3:], logits_single_first[0, -3:], rtol=tolerance, atol=tolerance)\n \n         with torch.no_grad():\n             logits_single_second = model(input_ids=input_ids[1:], labels=labels[1:, :-4]).logits\n \n-        self.assertTrue(torch.allclose(logits_batched[1, :3], logits_single_second[0, :3], atol=tolerance))\n+        torch.testing.assert_close(logits_batched[1, :3], logits_single_second[0, :3], rtol=tolerance, atol=tolerance)\n \n     def test_auto_padding(self):\n         ids = [[7, 6, 9] * 65]\n@@ -445,7 +445,7 @@ def test_auto_padding(self):\n             \"logits\"\n         ]\n \n-        self.assertTrue(torch.allclose(output1, output2, atol=1e-5))\n+        torch.testing.assert_close(output1, output2, rtol=1e-5, atol=1e-5)\n \n     def test_for_change_to_full_attn(self):\n         self.model_tester.seq_length = 9\n@@ -462,7 +462,7 @@ def test_for_change_to_full_attn(self):\n         model.load_state_dict(state_dict)\n         outputs2 = model(**input_dict)[\"logits\"]\n \n-        self.assertTrue(torch.allclose(outputs1, outputs2, atol=1e-5))\n+        torch.testing.assert_close(outputs1, outputs2, rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(\n         reason=\"This architecure has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n@@ -523,8 +523,8 @@ def test_inference_block_sparse(self):\n         )\n \n         # fmt: on\n-        self.assertTrue(\n-            torch.allclose(prediction_logits[0, 4:8, 128:156], expected_prediction_logits_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            prediction_logits[0, 4:8, 128:156], expected_prediction_logits_slice, rtol=1e-4, atol=1e-4\n         )\n \n     def test_inference_full_attn(self):\n@@ -544,8 +544,8 @@ def test_inference_full_attn(self):\n             device=torch_device,\n         )\n         # fmt: on\n-        self.assertTrue(\n-            torch.allclose(prediction_logits[0, 4:8, 128:156], expected_prediction_logits_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            prediction_logits[0, 4:8, 128:156], expected_prediction_logits_slice, rtol=1e-4, atol=1e-4\n         )\n \n     def test_seq_to_seq_generation(self):"
        },
        {
            "sha": "1082f901584ef93b3775f897ffa2f8b694b23564",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -432,7 +432,7 @@ def test_inference_lm_head_model(self):\n             [[[-9.5236, -9.8918, 10.4557], [-11.0469, -9.6423, 8.1022], [-8.8664, -7.8826, 5.5325]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_biogpt_generation(self):"
        },
        {
            "sha": "8e366f506abf3a19177a691a5ac669a2a93f80d1",
            "filename": "tests/models/bit/test_modeling_bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbit%2Ftest_modeling_bit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -296,7 +296,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([[-0.6526, -0.5263, -1.4398]]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "c4029e6377be2e35bd514a67fd7e4e43f965e4d1",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1431,5 +1431,5 @@ def test_inference_itm(self):\n \n         expected_scores = torch.Tensor([[0.0029, 0.9971]])\n \n-        self.assertTrue(torch.allclose(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3))\n-        self.assertTrue(torch.allclose(out[0].cpu(), torch.Tensor([[0.5162]]), rtol=1e-3, atol=1e-3))\n+        torch.testing.assert_close(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(out[0].cpu(), torch.Tensor([[0.5162]]), rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "628eaba738522ffed9700ffadfabd9bee4defde2",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -901,7 +901,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(\"BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\")\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n@@ -2215,8 +2215,8 @@ def test_inference_itm(self):\n \n         # verify\n         expected_scores = torch.Tensor([[0.0238, 0.9762]])\n-        self.assertTrue(torch.allclose(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3))\n-        self.assertTrue(torch.allclose(out[0].cpu(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n+        torch.testing.assert_close(torch.nn.Softmax()(out_itm[0].cpu()), expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(out[0].cpu(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3)\n \n     @require_torch_accelerator\n     @require_torch_fp16\n@@ -2235,10 +2235,8 @@ def test_inference_itm_fp16(self):\n \n         # verify\n         expected_scores = torch.Tensor([[0.0239, 0.9761]])\n-        self.assertTrue(\n-            torch.allclose(torch.nn.Softmax()(out_itm[0].cpu().float()), expected_scores, rtol=1e-3, atol=1e-3)\n-        )\n-        self.assertTrue(torch.allclose(out[0].cpu().float(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3))\n+        torch.testing.assert_close(torch.nn.Softmax()(out_itm[0].cpu().float()), expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(out[0].cpu().float(), torch.Tensor([[0.4406]]), rtol=1e-3, atol=1e-3)\n \n     @require_torch_accelerator\n     @require_torch_fp16"
        },
        {
            "sha": "66d0d82b6d750e4c9e37f30e1984dee542ef1916",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -689,4 +689,4 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.6518, 0.4978, -0.4544], [-2.6672, -0.0843, -0.4210], [-2.4510, -0.1002, -0.3458]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.image_features[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.image_features[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1dbf2a92fb4925b776d95c2f4e07d76e5e049c4d",
            "filename": "tests/models/bros/test_modeling_bros.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbros%2Ftest_modeling_bros.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -452,4 +452,4 @@ def test_inference_no_head(self):\n         ).to(torch_device)\n         torch.set_printoptions(sci_mode=False)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "bf41c8a9efc63d52af721aecb220eee6e5501fb9",
            "filename": "tests/models/camembert/test_modeling_camembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_modeling_camembert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -60,7 +60,7 @@ def test_output_embeds_base_model(self):\n         # camembert.eval()\n         # expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_torch_sdpa\n@@ -81,4 +81,4 @@ def test_output_embeds_base_model_sdpa(self):\n         with torch.no_grad():\n             output = model(input_ids)[\"last_hidden_state\"].detach()\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "31d02a2c041475cbf1a091a53c96197cc48fb335",
            "filename": "tests/models/canine/test_modeling_canine.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcanine%2Ftest_modeling_canine.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -562,12 +562,12 @@ def test_inference_no_head(self):\n             ]\n         )\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-2))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-2, atol=1e-2)\n \n         # verify pooled output\n         expected_shape = torch.Size((1, 768))\n         self.assertEqual(outputs.pooler_output.shape, expected_shape)\n \n         expected_slice = torch.tensor([-0.884311497, -0.529064834, 0.723164916])\n \n-        self.assertTrue(torch.allclose(outputs.pooler_output[0, :3], expected_slice, atol=1e-2))\n+        torch.testing.assert_close(outputs.pooler_output[0, :3], expected_slice, rtol=1e-2, atol=1e-2)"
        },
        {
            "sha": "f0d9107119fe61a74a0f15b92a1e0fae9abe332c",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -320,7 +320,7 @@ def test_model_rope_scaling(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n "
        },
        {
            "sha": "d63c152431cf944d57e7418d80b4b371e97d2e0a",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -739,7 +739,7 @@ def test_inference(self):\n         probs = outputs.logits_per_image.softmax(dim=1)\n         expected_probs = torch.tensor([[1.2686e-03, 5.4499e-02, 6.7968e-04, 9.4355e-01]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(probs, expected_probs, atol=5e-3))\n+        torch.testing.assert_close(probs, expected_probs, rtol=5e-3, atol=5e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -775,6 +775,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.3990, 0.2983, -0.1239], [-0.1452, -0.2759, 0.0403], [-0.3149, -0.4763, 0.8555]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n         )"
        },
        {
            "sha": "d0e2bb44a34477d303bef91bfec7e960893923fa",
            "filename": "tests/models/clap/test_feature_extraction_clap.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_feature_extraction_clap.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -285,8 +285,8 @@ def test_integration_fusion_short_input(self):\n             input_features = feature_extractor(input_speech, return_tensors=\"pt\", padding=padding).input_features\n             self.assertEqual(input_features.shape, (1, 4, 1001, 64))\n \n-            self.assertTrue(torch.allclose(input_features[0, 0, idx_in_mel[0]], EXPECTED_VALUES[0], atol=1e-4))\n-            self.assertTrue(torch.allclose(input_features[0, 0, idx_in_mel[1]], EXPECTED_VALUES[1], atol=1e-4))\n+            torch.testing.assert_close(input_features[0, 0, idx_in_mel[0]], EXPECTED_VALUES[0], rtol=1e-4, atol=1e-4)\n+            torch.testing.assert_close(input_features[0, 0, idx_in_mel[1]], EXPECTED_VALUES[1], rtol=1e-4, atol=1e-4)\n \n             self.assertTrue(torch.all(input_features[0, 0] == input_features[0, 1]))\n             self.assertTrue(torch.all(input_features[0, 0] == input_features[0, 2]))\n@@ -408,8 +408,8 @@ def test_integration_rand_trunc_short_input(self):\n                 input_speech, return_tensors=\"pt\", truncation=\"rand_trunc\", padding=padding\n             ).input_features\n             self.assertEqual(input_features.shape, (1, 1, 1001, 64))\n-            self.assertTrue(torch.allclose(input_features[0, 0, idx_in_mel[0]], EXPECTED_VALUES[0], atol=1e-4))\n-            self.assertTrue(torch.allclose(input_features[0, 0, idx_in_mel[1]], EXPECTED_VALUES[1], atol=1e-4))\n+            torch.testing.assert_close(input_features[0, 0, idx_in_mel[0]], EXPECTED_VALUES[0], rtol=1e-4, atol=1e-4)\n+            torch.testing.assert_close(input_features[0, 0, idx_in_mel[1]], EXPECTED_VALUES[1], rtol=1e-4, atol=1e-4)\n \n     def test_integration_fusion_long_input(self):\n         # fmt: off\n@@ -475,7 +475,7 @@ def test_integration_fusion_long_input(self):\n             set_seed(987654321)\n             input_features = feature_extractor(input_speech, return_tensors=\"pt\", padding=padding).input_features\n             self.assertEqual(input_features.shape, (1, 4, 1001, 64))\n-            self.assertTrue(torch.allclose(input_features[0, block_idx, MEL_BIN], EXPECTED_VALUES, atol=1e-3))\n+            torch.testing.assert_close(input_features[0, block_idx, MEL_BIN], EXPECTED_VALUES, rtol=1e-3, atol=1e-3)\n \n     def test_integration_rand_trunc_long_input(self):\n         # fmt: off\n@@ -544,4 +544,4 @@ def test_integration_rand_trunc_long_input(self):\n                 input_speech, return_tensors=\"pt\", truncation=\"rand_trunc\", padding=padding\n             ).input_features\n             self.assertEqual(input_features.shape, (1, 1, 1001, 64))\n-            self.assertTrue(torch.allclose(input_features[0, 0, MEL_BIN], EXPECTED_VALUES, atol=1e-4))\n+            torch.testing.assert_close(input_features[0, 0, MEL_BIN], EXPECTED_VALUES, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "75ee9a189ad48d01cc5f097b1ec320e6bc49b812",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1235,7 +1235,7 @@ def test_inference(self):\n \n         expected_logits = torch.tensor([[24.5701, 19.3049]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -1270,6 +1270,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.1538, 0.0322, -0.3235], [0.2893, 0.1135, -0.5708], [0.0461, 0.1540, -0.6018]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n         )"
        },
        {
            "sha": "4b712f199004448d3af882bce679d1b6f379bf98",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -814,13 +814,13 @@ def test_inference_image_segmentation(self):\n             [[-7.4613, -7.4785, -7.3628], [-7.3268, -7.0899, -7.1333], [-6.9838, -6.7900, -6.8913]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_masks_slice, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_masks_slice, rtol=1e-3, atol=1e-3)\n \n         # verify conditional and pooled output\n         expected_conditional = torch.tensor([0.5601, -0.0314, 0.1980]).to(torch_device)\n         expected_pooled_output = torch.tensor([0.5036, -0.2681, -0.2644]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.conditional_embeddings[0, :3], expected_conditional, atol=1e-3))\n-        self.assertTrue(torch.allclose(outputs.pooled_output[0, :3], expected_pooled_output, atol=1e-3))\n+        torch.testing.assert_close(outputs.conditional_embeddings[0, :3], expected_conditional, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(outputs.pooled_output[0, :3], expected_pooled_output, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -855,6 +855,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.1538, 0.0322, -0.3235], [0.2893, 0.1135, -0.5708], [0.0461, 0.1540, -0.6018]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n         )"
        },
        {
            "sha": "a536260f7ac2aed0a878099fa3b3feb2131d1739",
            "filename": "tests/models/clvp/test_feature_extraction_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_feature_extraction_clvp.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -238,4 +238,4 @@ def test_integration(self):\n         feature_extractor = ClvpFeatureExtractor.from_pretrained(\"susnato/clvp_dev\")\n         input_features = feature_extractor(input_speech, sampling_rate=sr[0], return_tensors=\"pt\").input_features\n         self.assertEqual(input_features.shape, (1, 80, 517))\n-        self.assertTrue(torch.allclose(input_features[0, 0, :30], EXPECTED_INPUT_FEATURES, atol=1e-4))\n+        torch.testing.assert_close(input_features[0, 0, :30], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "84a0101f6f287934143afbb6937506143aeb31f5",
            "filename": "tests/models/clvp/test_modeling_clvp.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_modeling_clvp.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -591,14 +591,14 @@ def test_conditional_encoder(self):\n             [[-0.8582, 0.5228, 1.9944], [-0.0465, -1.1017, -0.0093], [-0.0466, -0.6030, -0.1280]]\n         )\n \n-        self.assertTrue(torch.allclose(conditioning_encoder_outputs[0, :3, :3], EXPECTED_OUTPUTS, atol=1e-4))\n+        torch.testing.assert_close(conditioning_encoder_outputs[0, :3, :3], EXPECTED_OUTPUTS, rtol=1e-4, atol=1e-4)\n \n     def test_decoder_model_generate(self):\n         autoregressive_model_output = self.model.speech_decoder_model.generate(input_ids=self.text_tokens).cpu()\n \n         EXPECTED_OUTPUTS = torch.tensor([[147, 2, 54, 2, 43, 2, 169, 122, 29, 64, 2, 136, 37, 33, 9, 8193]])\n \n-        self.assertTrue(torch.allclose(autoregressive_model_output, EXPECTED_OUTPUTS))\n+        torch.testing.assert_close(autoregressive_model_output, EXPECTED_OUTPUTS)\n \n     def test_text_and_speech_encoder_models(self):\n         # check for text embeds\n@@ -608,7 +608,7 @@ def test_text_and_speech_encoder_models(self):\n         EXPECTED_TEXT_EMBEDS = torch.tensor([1.4798, -2.0005, 2.3902, -0.5042, 1.6401, -2.4135, -1.4800, 3.0118, -2.4422, 1.3266, 2.2339, 1.4761, -4.8983, -1.3592, 6.0251, 6.7364, 2.2576, 3.7229, -10.0436, 4.6676])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(text_embeds[0, :20], EXPECTED_TEXT_EMBEDS, atol=1e-4))\n+        torch.testing.assert_close(text_embeds[0, :20], EXPECTED_TEXT_EMBEDS, rtol=1e-4, atol=1e-4)\n \n         # check for speech embeds\n         speech_embeds = self.model.speech_encoder_model(input_ids=self.text_tokens, return_dict=True)[0].cpu()\n@@ -617,7 +617,7 @@ def test_text_and_speech_encoder_models(self):\n         EXPECTED_SPEECH_EMBEDS = torch.tensor([3.1202, -3.1183, -1.4264, -6.1339, 1.8885, -0.1983, 0.9461, -1.7414, 0.3320, -3.8400, -1.5715, 1.5096, -1.7576, 0.2387, 4.9758, 5.8450, -6.2534, 2.8587, -5.5816, 4.7821])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(speech_embeds[0, :20], EXPECTED_SPEECH_EMBEDS, atol=1e-4))\n+        torch.testing.assert_close(speech_embeds[0, :20], EXPECTED_SPEECH_EMBEDS, rtol=1e-4, atol=1e-4)\n \n     def test_full_model_integration(self):\n         full_model_output = self.model.generate(\n@@ -632,5 +632,5 @@ def test_full_model_integration(self):\n         EXPECTED_SPEECH_IDS = torch.tensor([[1953, 1080, 612], [1953, 612, 493], [1953, 612, 716]])\n         EXPECTED_SIMILARITY_SCORES = torch.tensor([[14.7660, 14.4569, 13.6472, 13.5683]])\n \n-        self.assertTrue(torch.allclose(full_model_output.speech_ids.cpu()[-3:, -3:], EXPECTED_SPEECH_IDS))\n-        self.assertTrue(torch.allclose(full_model_output.logits_per_text.cpu(), EXPECTED_SIMILARITY_SCORES))\n+        torch.testing.assert_close(full_model_output.speech_ids.cpu()[-3:, -3:], EXPECTED_SPEECH_IDS)\n+        torch.testing.assert_close(full_model_output.logits_per_text.cpu(), EXPECTED_SIMILARITY_SCORES)"
        },
        {
            "sha": "47cd68e3f728601e15217f83a94e90dafc93083a",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -363,4 +363,4 @@ def test_batched_small_model_logits(self):\n             output = model(**inputs)\n \n         logits = output.logits\n-        self.assertTrue(torch.allclose(EXPECTED_LOGITS, logits[:, :3, :3], rtol=1e-3, atol=1e-3))\n+        torch.testing.assert_close(EXPECTED_LOGITS, logits[:, :3, :3], rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "6f3ce6b96b41481d6b826b330d8c5e4c3336cf1a",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -238,7 +238,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @slow\n     @require_vision"
        },
        {
            "sha": "4f15b5e77bf846e61fcd274ef377a3ead5512d7a",
            "filename": "tests/models/conditional_detr/test_image_processing_conditional_detr.py",
            "status": "modified",
            "additions": 24,
            "deletions": 24,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_image_processing_conditional_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -179,31 +179,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n         self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n         expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify area\n         expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n         # verify boxes\n         expected_boxes_shape = torch.Size([6, 4])\n         self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n         expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n         # verify image_id\n         expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n         # verify is_crowd\n         expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n         # verify class_labels\n         expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n         # verify orig_size\n         expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n         # verify size\n         expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -225,34 +225,34 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n         self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n         expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify area\n         expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n         # verify boxes\n         expected_boxes_shape = torch.Size([6, 4])\n         self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n         expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n         # verify image_id\n         expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n         # verify is_crowd\n         expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n         # verify class_labels\n         expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n         # verify masks\n         expected_masks_sum = 822873\n         self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n         # verify orig_size\n         expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n         # verify size\n         expected_size = torch.tensor([800, 1066])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->ConditionalDetr, facebook/detr-resnet-50 ->microsoft/conditional-detr-resnet-50\n@@ -319,8 +319,8 @@ def test_batched_coco_detection_annotations(self):\n                     [0.5790, 0.4115, 0.3430, 0.7161],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -371,8 +371,8 @@ def test_batched_coco_detection_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->ConditionalDetr\n     def test_batched_coco_panoptic_annotations(self):\n@@ -442,8 +442,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     [0.2997, 0.2994, 0.5994, 0.5987],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -495,8 +495,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->ConditionalDetr\n     def test_max_width_max_height_resizing_and_pad_strategy(self):"
        },
        {
            "sha": "5a4357c40c08dbaed76a3fad87ac8836c0916b75",
            "filename": "tests/models/conditional_detr/test_modeling_conditional_detr.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconditional_detr%2Ftest_modeling_conditional_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -572,7 +572,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.4222, 0.7471, 0.8760], [0.6395, -0.2729, 0.7127], [-0.3090, 0.7642, 0.9529]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_object_detection_head(self):\n         model = ConditionalDetrForObjectDetection.from_pretrained(\"microsoft/conditional-detr-resnet-50\").to(\n@@ -594,14 +594,14 @@ def test_inference_object_detection_head(self):\n         expected_slice_logits = torch.tensor(\n             [[-10.4372, -5.7558, -8.6764], [-10.5410, -5.8704, -8.0590], [-10.6827, -6.3469, -8.3923]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n             [[0.7733, 0.6576, 0.4496], [0.5171, 0.1184, 0.9094], [0.8846, 0.5647, 0.2486]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n@@ -612,6 +612,6 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([38.3089, 72.1022, 177.6293, 118.4512]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes))\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)"
        },
        {
            "sha": "51daf2509c666f66527aee9ab5c0cf1577b06804",
            "filename": "tests/models/convbert/test_modeling_convbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvbert%2Ftest_modeling_convbert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -481,4 +481,4 @@ def test_inference_no_head(self):\n             [[[-0.0864, -0.4898, -0.3677], [0.1434, -0.2952, -0.7640], [-0.0112, -0.4432, -0.5432]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1965a76fad11f8d0ebce8e7f891e0e5572acd84d",
            "filename": "tests/models/convnext/test_modeling_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_convnext.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -288,7 +288,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-0.0260, -0.4739, 0.1911]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "18e7be96fbca5f005364d2b2f210cc6f1edd3a17",
            "filename": "tests/models/convnextv2/test_modeling_convnextv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_convnextv2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -336,4 +336,4 @@ def test_inference_image_classification_head(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([0.9996, 0.1966, -0.4386]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "e796d850a8d0a1b97ceb92be6fe693f754d35987",
            "filename": "tests/models/cpmant/test_modeling_cpmant.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcpmant%2Ftest_modeling_cpmant.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -185,7 +185,7 @@ def test_inference_masked_lm(self):\n         expected_slice = torch.tensor(\n             [[[6.1708, 5.9244, 1.0835], [6.5207, 6.2893, -11.3324], [-1.0107, -0.0576, -5.9577]]],\n         )\n-        self.assertTrue(torch.allclose(hidden_states[:, :3, :3], expected_slice, atol=1e-2))\n+        torch.testing.assert_close(hidden_states[:, :3, :3], expected_slice, rtol=1e-2, atol=1e-2)\n \n \n @require_torch\n@@ -202,7 +202,7 @@ def test_inference_casual(self):\n         expected_slice = torch.tensor(\n             [[[-6.4267, -6.4083, -6.3958], [-5.8802, -5.9447, -5.7811], [-5.3896, -5.4820, -5.4295]]],\n         )\n-        self.assertTrue(torch.allclose(hidden_states[:, :3, :3], expected_slice, atol=1e-2))\n+        torch.testing.assert_close(hidden_states[:, :3, :3], expected_slice, rtol=1e-2, atol=1e-2)\n \n     @tooslow\n     def test_simple_generation(self):"
        },
        {
            "sha": "fe02a166562d72e7a2c3e879e45a6a7f3c23ba4c",
            "filename": "tests/models/cvt/test_modeling_cvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_cvt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -267,4 +267,4 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([0.9285, 0.9015, -0.3150]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1bc5439046aa5014f7bd778695204b59049fc95a",
            "filename": "tests/models/dac/test_feature_extraction_dac.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_feature_extraction_dac.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -165,9 +165,9 @@ def test_integration(self):\n         feature_extractor = DacFeatureExtractor()\n         input_values = feature_extractor(input_audio, return_tensors=\"pt\")[\"input_values\"]\n         self.assertEqual(input_values.shape, (1, 1, 93696))\n-        self.assertTrue(torch.allclose(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, atol=1e-4))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-4, atol=1e-4)\n         audio_input_end = torch.tensor(input_audio[0][-30:], dtype=torch.float32)\n-        self.assertTrue(torch.allclose(input_values[0, 0, -46:-16], audio_input_end, atol=1e-4))\n+        torch.testing.assert_close(input_values[0, 0, -46:-16], audio_input_end, rtol=1e-4, atol=1e-4)\n \n     # Ignore copy\n     @unittest.skip(\"The DAC model doesn't support stereo logic\")"
        },
        {
            "sha": "729e40463ef87848ef1ad53e2bf436e4d2a45bb0",
            "filename": "tests/models/dac/test_modeling_dac.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdac%2Ftest_modeling_dac.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -438,14 +438,14 @@ def test_integration_16khz(self):\n             encoder_outputs_mean = torch.tensor([v.float().mean().cpu().item() for v in encoder_outputs.to_tuple()])\n \n             # make sure audio encoded codes are correct\n-            self.assertTrue(torch.allclose(encoder_outputs_mean, expected_encoder_sums, atol=1e-3))\n+            torch.testing.assert_close(encoder_outputs_mean, expected_encoder_sums, rtol=1e-3, atol=1e-3)\n \n             _, quantized_representation, _, _ = encoder_outputs.to_tuple()\n             input_values_dec = model.decode(quantized_representation)[0]\n             input_values_enc_dec = model(inputs[\"input_values\"])[1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"][0].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec[0].cpu().numpy()\n@@ -515,10 +515,10 @@ def test_integration_24khz(self):\n             input_values_from_codes = model.decode(audio_codes=encoder_outputs.audio_codes)[0]\n \n             # make sure decode from audio codes and quantized values give more or less the same results\n-            self.assertTrue(torch.allclose(input_values_from_codes, input_values_dec, atol=1e-5))\n+            torch.testing.assert_close(input_values_from_codes, input_values_dec, rtol=1e-5, atol=1e-5)\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"][0].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec[0].cpu().numpy()\n@@ -565,14 +565,14 @@ def test_integration_44khz(self):\n             encoder_outputs_mean = torch.tensor([v.float().mean().cpu().item() for v in encoder_outputs.to_tuple()])\n \n             # make sure audio encoded codes are correct\n-            self.assertTrue(torch.allclose(encoder_outputs_mean, expected_encoder_sums, atol=1e-3))\n+            torch.testing.assert_close(encoder_outputs_mean, expected_encoder_sums, rtol=1e-3, atol=1e-3)\n \n             _, quantized_representation, _, _ = encoder_outputs.to_tuple()\n             input_values_dec = model.decode(quantized_representation)[0]\n             input_values_enc_dec = model(inputs[\"input_values\"])[1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"][0].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec[0].cpu().numpy()\n@@ -622,14 +622,14 @@ def test_integration_batch_16khz(self):\n             encoder_outputs_mean = torch.tensor([v.float().mean().item() for v in encoder_outputs.to_tuple()])\n \n             # make sure audio encoded codes are correct\n-            self.assertTrue(torch.allclose(encoder_outputs_mean, expected_encoder_sums, atol=1e-3))\n+            torch.testing.assert_close(encoder_outputs_mean, expected_encoder_sums, rtol=1e-3, atol=1e-3)\n \n             _, quantized_representation, _, _ = encoder_outputs.to_tuple()\n             input_values_dec = model.decode(quantized_representation)[0]\n             input_values_enc_dec = model(inputs[\"input_values\"])[1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec.cpu().numpy()\n@@ -679,14 +679,14 @@ def test_integration_batch_24khz(self):\n             encoder_outputs_mean = torch.tensor([v.float().mean().cpu().item() for v in encoder_outputs.to_tuple()])\n \n             # make sure audio encoded codes are correct\n-            self.assertTrue(torch.allclose(encoder_outputs_mean, expected_encoder_sums, atol=1e-3))\n+            torch.testing.assert_close(encoder_outputs_mean, expected_encoder_sums, rtol=1e-3, atol=1e-3)\n \n             _, quantized_representation, _, _ = encoder_outputs.to_tuple()\n             input_values_dec = model.decode(quantized_representation)[0]\n             input_values_enc_dec = model(inputs[\"input_values\"])[1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec.cpu().numpy()\n@@ -736,14 +736,14 @@ def test_integration_batch_44khz(self):\n             encoder_outputs_mean = torch.tensor([v.float().mean().cpu().item() for v in encoder_outputs.to_tuple()])\n \n             # make sure audio encoded codes are correct\n-            self.assertTrue(torch.allclose(encoder_outputs_mean, expected_encoder_sums, atol=1e-3))\n+            torch.testing.assert_close(encoder_outputs_mean, expected_encoder_sums, rtol=1e-3, atol=1e-3)\n \n             _, quantized_representation, _, _ = encoder_outputs.to_tuple()\n             input_values_dec = model.decode(quantized_representation)[0]\n             input_values_enc_dec = model(inputs[\"input_values\"])[1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             arr = inputs[\"input_values\"].cpu().numpy()\n             arr_enc_dec = input_values_enc_dec.cpu().numpy()"
        },
        {
            "sha": "45482febd167dec734dd46f9d1fe0f607697a940",
            "filename": "tests/models/data2vec/test_modeling_data2vec_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_text.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -527,7 +527,7 @@ def test_inference_masked_lm(self):\n         # compare the actual values for a slice.\n         expected_slice = torch.tensor([[[0.2328, 0.0000, 1.1710], [2.2525, 0.0000, 1.9937], [2.1280, 0.0000, 1.8691]]])\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head(self):\n@@ -541,4 +541,4 @@ def test_inference_no_head(self):\n             [[[0.1998, -0.0379, 0.0024], [-0.0971, -0.2214, -0.1798], [-0.0789, -0.2400, -0.1898]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "0a9d1fd1812c35612bc11f98a5d3911a5ceaac3d",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -548,7 +548,7 @@ def test_inference_image_classification_head_imagenet_1k(self):\n \n         expected_slice = torch.tensor([0.3277, -0.1395, 0.0911]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_top2 = [model.config.label2id[i] for i in [\"remote control, remote\", \"tabby, tabby cat\"]]\n         self.assertEqual(logits[0].topk(2).indices.cpu().tolist(), expected_top2)"
        },
        {
            "sha": "556887bda1a9df1e6749e784539f2e0a8be9a0c7",
            "filename": "tests/models/dbrx/test_modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdbrx%2Ftest_modeling_dbrx.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -394,4 +394,4 @@ def test_tiny_model_logits(self):\n                 ]\n             ]\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "286dc940e080e1b97315a8670e49be74b1d72584",
            "filename": "tests/models/deberta/test_modeling_deberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_deberta.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -310,4 +310,4 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[[-0.5986, -0.8055, -0.8462], [1.4484, -0.9348, -0.8059], [0.3123, 0.0032, -1.4131]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4), f\"{output[:, 1:4, 1:4]}\")\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "02fd11ce4d5ccc40b647de4859df3fda40a78189",
            "filename": "tests/models/deberta_v2/test_modeling_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_deberta_v2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -328,4 +328,4 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[[0.2356, 0.1948, 0.0369], [-0.1063, 0.3586, -0.5152], [-0.6399, -0.0259, -0.2525]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4), f\"{output[:, 1:4, 1:4]}\")\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "b1a4d0938f71114a6f43b4a4e4894ce1aba940d1",
            "filename": "tests/models/decision_transformer/test_modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdecision_transformer%2Ftest_modeling_decision_transformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -231,7 +231,7 @@ def test_autoregressive_prediction(self):\n                 )\n \n             self.assertEqual(action_pred.shape, actions.shape)\n-            self.assertTrue(torch.allclose(action_pred[0, -1], expected_outputs[step], atol=1e-4))\n+            torch.testing.assert_close(action_pred[0, -1], expected_outputs[step], rtol=1e-4, atol=1e-4)\n             state, reward, _, _ = (  # env.step(action)\n                 torch.randn(1, 1, config.state_dim).to(device=torch_device, dtype=torch.float32),\n                 1.0,"
        },
        {
            "sha": "5a8825cc6c152fb1071e28743cb2ea07f3c50f54",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -186,31 +186,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -233,35 +233,35 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify masks\n             expected_masks_sum = 822873\n             relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n             self.assertTrue(relative_error < 1e-3)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->DeformableDetr\n@@ -328,8 +328,8 @@ def test_batched_coco_detection_annotations(self):\n                     [0.5790, 0.4115, 0.3430, 0.7161],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -380,8 +380,8 @@ def test_batched_coco_detection_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->DeformableDetr\n     def test_batched_coco_panoptic_annotations(self):\n@@ -451,8 +451,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     [0.2997, 0.2994, 0.5994, 0.5987],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -504,8 +504,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->DeformableDetr\n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n@@ -637,7 +637,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify area\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\"))\n         # verify boxes\n         self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n         self.assertTrue(\n@@ -646,12 +646,12 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify image_id\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\")\n         )\n         # verify is_crowd\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\")\n         )\n         # verify class_labels\n         self.assertTrue(\n@@ -660,11 +660,11 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify orig_size\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\")\n         )\n         # verify size\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))\n \n     @slow\n     @require_torch_gpu\n@@ -701,7 +701,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n             )\n         )\n         # verify area\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\"))\n         # verify boxes\n         self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n         self.assertTrue(\n@@ -710,12 +710,12 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n             )\n         )\n         # verify image_id\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\")\n         )\n         # verify is_crowd\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\")\n         )\n         # verify class_labels\n         self.assertTrue(\n@@ -729,8 +729,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n         relative_error = torch.abs(masks_sum_cpu - masks_sum_gpu) / masks_sum_cpu\n         self.assertTrue(relative_error < 1e-3)\n         # verify orig_size\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\")\n         )\n         # verify size\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))"
        },
        {
            "sha": "42f692864802f73053bbc87ddd0f9add89fbacf0",
            "filename": "tests/models/deformable_detr/test_modeling_deformable_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_modeling_deformable_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -697,11 +697,11 @@ def test_inference_object_detection_head(self):\n             [[0.8693, 0.2289, 0.2492], [0.3150, 0.5489, 0.5845], [0.5563, 0.7580, 0.8518]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n@@ -712,9 +712,9 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([16.5028, 52.8390, 318.2544, 470.7841]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes))\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)\n \n     def test_inference_object_detection_head_with_box_refine_two_stage(self):\n         model = DeformableDetrForObjectDetection.from_pretrained(\n@@ -740,11 +740,11 @@ def test_inference_object_detection_head_with_box_refine_two_stage(self):\n             [[0.2583, 0.5499, 0.4683], [0.7652, 0.9068, 0.4882], [0.5490, 0.2763, 0.0564]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n \n     @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):"
        },
        {
            "sha": "1637b22e95ef76cb5c85e779965b3930d52a958b",
            "filename": "tests/models/deit/test_modeling_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_deit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -421,7 +421,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-1.0266, 0.1912, -1.2861]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "91f958921740a60f435659fc59260e2faba04589",
            "filename": "tests/models/depth_anything/test_modeling_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdepth_anything%2Ftest_modeling_depth_anything.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -267,7 +267,7 @@ def test_inference(self):\n             [[8.8223, 8.6483, 8.6216], [8.3332, 8.6047, 8.7545], [8.6547, 8.6885, 8.7472]],\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(predicted_depth[0, :3, :3], expected_slice, atol=1e-6))\n+        torch.testing.assert_close(predicted_depth[0, :3, :3], expected_slice, rtol=1e-6, atol=1e-6)\n \n         # -- `metric` depth model --\n         image_processor = DPTImageProcessor.from_pretrained(\"depth-anything/depth-anything-V2-metric-indoor-small-hf\")\n@@ -290,7 +290,7 @@ def test_inference(self):\n             [[1.3349, 1.2947, 1.2802], [1.2794, 1.2338, 1.2901], [1.2630, 1.2219, 1.2478]],\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_export(self):\n         for strict in [True, False]:"
        },
        {
            "sha": "2dc84fe5e019b4912f78f9492fe802549b08c6f2",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 40,
            "deletions": 40,
            "changes": 80,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -248,31 +248,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -295,35 +295,35 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify masks\n             expected_masks_sum = 822873\n             relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n             self.assertTrue(relative_error < 1e-3)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_batched_coco_detection_annotations(self):\n@@ -389,8 +389,8 @@ def test_batched_coco_detection_annotations(self):\n                     [0.5790, 0.4115, 0.3430, 0.7161],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -441,8 +441,8 @@ def test_batched_coco_detection_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     def test_batched_coco_panoptic_annotations(self):\n         # prepare image, target and masks_path\n@@ -511,8 +511,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     [0.2997, 0.2994, 0.5994, 0.5987],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -564,8 +564,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     def test_max_width_max_height_resizing_and_pad_strategy(self):\n         for image_processing_class in self.image_processor_list:\n@@ -694,7 +694,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify area\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\"))\n         # verify boxes\n         self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n         self.assertTrue(\n@@ -703,12 +703,12 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify image_id\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\")\n         )\n         # verify is_crowd\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\")\n         )\n         # verify class_labels\n         self.assertTrue(\n@@ -717,11 +717,11 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify orig_size\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\")\n         )\n         # verify size\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))\n \n     @slow\n     @require_torch_gpu\n@@ -756,7 +756,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n             )\n         )\n         # verify area\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\"))\n         # verify boxes\n         self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n         self.assertTrue(\n@@ -765,12 +765,12 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n             )\n         )\n         # verify image_id\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\")\n         )\n         # verify is_crowd\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\")\n         )\n         # verify class_labels\n         self.assertTrue(\n@@ -784,8 +784,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_panoptic_annotations(self):\n         relative_error = torch.abs(masks_sum_cpu - masks_sum_gpu) / masks_sum_cpu\n         self.assertTrue(relative_error < 1e-3)\n         # verify orig_size\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\")\n         )\n         # verify size\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))"
        },
        {
            "sha": "1451eaeb80d8e7f596cc6603a8205ef84d945445",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -588,7 +588,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.0616, -0.5146, -0.4032], [-0.7629, -0.4934, -1.7153], [-0.4768, -0.6403, -0.7826]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_object_detection_head(self):\n         model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(torch_device)\n@@ -608,14 +608,14 @@ def test_inference_object_detection_head(self):\n         expected_slice_logits = torch.tensor(\n             [[-19.1194, -0.0893, -11.0154], [-17.3640, -1.8035, -14.0219], [-20.0461, -0.5837, -11.1060]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n             [[0.4433, 0.5302, 0.8853], [0.5494, 0.2517, 0.0529], [0.4998, 0.5360, 0.9956]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n@@ -626,9 +626,9 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([40.1633, 70.8115, 175.5471, 117.9841]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes))\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)\n \n     def test_inference_panoptic_segmentation_head(self):\n         model = DetrForSegmentation.from_pretrained(\"facebook/detr-resnet-50-panoptic\").to(torch_device)\n@@ -648,21 +648,21 @@ def test_inference_panoptic_segmentation_head(self):\n         expected_slice_logits = torch.tensor(\n             [[-18.1565, -1.7568, -13.5029], [-16.8888, -1.4138, -14.1028], [-17.5709, -2.5080, -11.8654]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n         expected_slice_boxes = torch.tensor(\n             [[0.5344, 0.1789, 0.9285], [0.4420, 0.0572, 0.0875], [0.6630, 0.6887, 0.1017]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         expected_shape_masks = torch.Size((1, model.config.num_queries, 200, 267))\n         self.assertEqual(outputs.pred_masks.shape, expected_shape_masks)\n         expected_slice_masks = torch.tensor(\n             [[-7.7558, -10.8788, -11.9797], [-11.8881, -16.4329, -17.7451], [-14.7316, -19.7383, -20.3004]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, atol=1e-3))\n+        torch.testing.assert_close(outputs.pred_masks[0, 0, :3, :3], expected_slice_masks, rtol=1e-3, atol=1e-3)\n \n         # verify postprocessing\n         results = image_processor.post_process_panoptic_segmentation(\n@@ -681,7 +681,7 @@ def test_inference_panoptic_segmentation_head(self):\n             number_of_unique_segments, expected_number_of_segments + 1\n         )  # we add 1 for the background class\n         self.assertTrue(results[\"segmentation\"].shape, expected_shape)\n-        self.assertTrue(torch.allclose(results[\"segmentation\"][:3, :3], expected_slice_segmentation, atol=1e-4))\n+        torch.testing.assert_close(results[\"segmentation\"][:3, :3], expected_slice_segmentation, rtol=1e-4, atol=1e-4)\n         self.assertTrue(len(results[\"segments_info\"]), expected_number_of_segments)\n         self.assertDictEqual(results[\"segments_info\"][0], expected_first_segment)\n \n@@ -713,4 +713,4 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.0616, -0.5146, -0.4032], [-0.7629, -0.4934, -1.7153], [-0.4768, -0.6403, -0.7826]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "da1fe24bda4f61188b3b321ede5dc74ce568fd29",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -420,7 +420,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n "
        },
        {
            "sha": "27a3eafddc36335f4b01895f4836c9e5b8ff51c8",
            "filename": "tests/models/dinat/test_modeling_dinat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinat%2Ftest_modeling_dinat.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -365,7 +365,7 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n         expected_slice = torch.tensor([-0.1545, -0.7667, 0.4642]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "9d849373349c837db0cde0cc1c1225419ba9c4bd",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -330,7 +330,7 @@ def test_inference_no_head(self):\n             [[-2.1747, -0.4729, 1.0936], [-3.2780, -0.8269, -0.9210], [-2.9129, 1.1284, -0.7306]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "185492d6d405baa0451f0e3f6142777d1de989a6",
            "filename": "tests/models/dinov2_with_registers/test_modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2_with_registers%2Ftest_modeling_dinov2_with_registers.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -355,7 +355,7 @@ def test_inference_no_head(self):\n             [[-0.4636, -1.4582, -0.0274], [-1.4738, -0.8858, 0.3002], [0.0714, -0.2407, -1.5940]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "367785b5e265fce029716ac22d6ecc4aaca33bf9",
            "filename": "tests/models/distilbert/test_modeling_distilbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_distilbert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -340,15 +340,15 @@ def test_flash_attn_2_inference_equivalence(self):\n                 logits = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n                 logits_fa = model_fa(dummy_input, output_hidden_states=True).hidden_states[-1]\n \n-                self.assertTrue(torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2))\n+                torch.testing.assert_close(logits_fa, logits, rtol=4e-2, atol=4e-2)\n \n                 output_fa = model_fa(dummy_input, attention_mask=dummy_attention_mask, output_hidden_states=True)\n                 logits_fa = output_fa.hidden_states[-1]\n \n                 output = model(dummy_input, attention_mask=dummy_attention_mask, output_hidden_states=True)\n                 logits = output.hidden_states[-1]\n \n-                self.assertTrue(torch.allclose(logits_fa[1:], logits[1:], atol=4e-2, rtol=4e-2))\n+                torch.testing.assert_close(logits_fa[1:], logits[1:], rtol=4e-2, atol=4e-2)\n \n     # Because DistilBertForMultipleChoice requires inputs with different shapes we need to override this test.\n     @require_flash_attn\n@@ -395,15 +395,15 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n                 logits = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n                 logits_fa = model_fa(dummy_input, output_hidden_states=True).hidden_states[-1]\n \n-                self.assertTrue(torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2))\n+                torch.testing.assert_close(logits_fa, logits, rtol=4e-2, atol=4e-2)\n \n                 output_fa = model_fa(dummy_input, attention_mask=dummy_attention_mask, output_hidden_states=True)\n                 logits_fa = output_fa.hidden_states[-1]\n \n                 output = model(dummy_input, attention_mask=dummy_attention_mask, output_hidden_states=True)\n                 logits = output.hidden_states[-1]\n \n-                self.assertTrue(torch.allclose(logits_fa[:-1], logits[:-1], atol=4e-2, rtol=4e-2))\n+                torch.testing.assert_close(logits_fa[:-1], logits[:-1], rtol=4e-2, atol=4e-2)\n \n \n @require_torch\n@@ -421,7 +421,7 @@ def test_inference_no_head_absolute_embedding(self):\n             [[[-0.1639, 0.3299, 0.1648], [-0.1746, 0.3289, 0.1710], [-0.1884, 0.3357, 0.1810]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_export(self):"
        },
        {
            "sha": "d0f9f79b325330fbf19e3ab0663f91af6b9c8e14",
            "filename": "tests/models/dit/test_modeling_dit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdit%2Ftest_modeling_dit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdit%2Ftest_modeling_dit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdit%2Ftest_modeling_dit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -58,4 +58,4 @@ def test_for_image_classification(self):\n             device=torch_device,\n             dtype=torch.float,\n         )\n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "3f05bc242fdb336e9e7ec1a330aa0099def082fb",
            "filename": "tests/models/dpr/test_modeling_dpr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpr%2Ftest_modeling_dpr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -272,7 +272,7 @@ def test_inference_no_head(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output[:, :10], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :10], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_reader_inference(self):\n@@ -303,5 +303,5 @@ def test_reader_inference(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(outputs.start_logits[:, :10], expected_start_logits, atol=1e-4))\n-        self.assertTrue(torch.allclose(outputs.end_logits[:, :10], expected_end_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.start_logits[:, :10], expected_start_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.end_logits[:, :10], expected_end_logits, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "c00b810cfe315b0080b90a7941738eeeae595964",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -342,7 +342,7 @@ def test_inference_depth_estimation(self):\n             [[6.3199, 6.3629, 6.4148], [6.3850, 6.3615, 6.4166], [6.3519, 6.3176, 6.3575]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_semantic_segmentation(self):\n         image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large-ade\")\n@@ -363,7 +363,7 @@ def test_inference_semantic_segmentation(self):\n             [[4.0480, 4.2420, 4.4360], [4.3124, 4.5693, 4.8261], [4.5768, 4.8965, 5.2163]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_post_processing_semantic_segmentation(self):\n         image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-large-ade\")\n@@ -410,7 +410,7 @@ def test_post_processing_depth_estimation(self):\n             predicted_depth.unsqueeze(0).unsqueeze(1), size=(500, 500), mode=\"bicubic\", align_corners=False\n         ).squeeze()\n         self.assertTrue(output_enlarged.shape == expected_shape)\n-        self.assertTrue(torch.allclose(predicted_depth_l, output_enlarged, rtol=1e-3))\n+        torch.testing.assert_close(predicted_depth_l, output_enlarged, rtol=1e-3)\n \n     def test_export(self):\n         for strict in [True, False]:\n@@ -431,4 +431,4 @@ def test_export(self):\n                     eager_outputs = model(**inputs)\n                     exported_outputs = exported_program.module().forward(inputs[\"pixel_values\"])\n                 self.assertEqual(eager_outputs.logits.shape, exported_outputs.logits.shape)\n-                self.assertTrue(torch.allclose(eager_outputs.logits, exported_outputs.logits, atol=1e-4))\n+                torch.testing.assert_close(eager_outputs.logits, exported_outputs.logits, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "6b30ed323d4c38bec7a4627a7ee7120032c37844",
            "filename": "tests/models/dpt/test_modeling_dpt_auto_backbone.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_auto_backbone.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -277,7 +277,7 @@ def test_inference_depth_estimation_dinov2(self):\n             [[6.0336, 7.1502, 7.4130], [6.8977, 7.2383, 7.2268], [7.9180, 8.0525, 8.0134]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_depth_estimation_beit(self):\n         image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-beit-base-384\")\n@@ -299,7 +299,7 @@ def test_inference_depth_estimation_beit(self):\n             [[2669.7061, 2663.7144, 2674.9399], [2633.9326, 2650.9092, 2665.4270], [2621.8271, 2632.0129, 2637.2290]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_depth_estimation_swinv2(self):\n         image_processor = DPTImageProcessor.from_pretrained(\"Intel/dpt-swinv2-tiny-256\")\n@@ -321,4 +321,4 @@ def test_inference_depth_estimation_swinv2(self):\n             [[1032.7719, 1025.1886, 1030.2661], [1023.7619, 1021.0075, 1024.9121], [1022.5667, 1018.8522, 1021.4145]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1229f3e40faf746d75afb65a517a79b6d9ccefc7",
            "filename": "tests/models/dpt/test_modeling_dpt_hybrid.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt_hybrid.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -335,4 +335,4 @@ def test_inference_depth_estimation(self):\n             [[[5.6437, 5.6146, 5.6511], [5.4371, 5.5649, 5.5958], [5.5215, 5.5184, 5.5293]]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[:3, :3, :3] / 100, expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[:3, :3, :3] / 100, expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "796c5a149a20019b401fdcb65d0f5435d48c14db",
            "filename": "tests/models/efficientnet/test_modeling_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_modeling_efficientnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -259,4 +259,4 @@ def test_inference_image_classification_head(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([-0.2962, 0.4487, 0.4499]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "e2aa0d41f219927ae137d9e4e03b0f7644384ff2",
            "filename": "tests/models/electra/test_modeling_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_electra.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -485,4 +485,4 @@ def test_inference_no_head_absolute_embedding(self):\n             [[[0.4471, 0.6821, -0.3265], [0.4627, 0.5255, -0.3668], [0.4532, 0.3313, -0.4344]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "c0e84f5c5e44fa58c9881fbead3815286d2cbc22",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -167,7 +167,7 @@ def test_model_rope_scaling(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -368,7 +368,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @unittest.skip(\n         \"Emu3 has a VQ module that uses `weight.data` directly in forward which prevent offloding on that module\""
        },
        {
            "sha": "d809a90f120f3e3a7f0baed0a1afdcfee899d80d",
            "filename": "tests/models/encodec/test_feature_extraction_encodec.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_feature_extraction_encodec.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -159,7 +159,7 @@ def test_integration(self):\n         feature_extractor = EncodecFeatureExtractor()\n         input_values = feature_extractor(input_audio, return_tensors=\"pt\").input_values\n         self.assertEqual(input_values.shape, (1, 1, 93680))\n-        self.assertTrue(torch.allclose(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, atol=1e-6))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-6, atol=1e-6)\n \n     def test_integration_stereo(self):\n         # fmt: off\n@@ -178,8 +178,8 @@ def test_integration_stereo(self):\n         feature_extractor = EncodecFeatureExtractor(feature_size=2)\n         input_values = feature_extractor(input_audio, return_tensors=\"pt\").input_values\n         self.assertEqual(input_values.shape, (1, 2, 93680))\n-        self.assertTrue(torch.allclose(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, atol=1e-6))\n-        self.assertTrue(torch.allclose(input_values[0, 1, :30], EXPECTED_INPUT_VALUES * 0.5, atol=1e-6))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-6, atol=1e-6)\n+        torch.testing.assert_close(input_values[0, 1, :30], EXPECTED_INPUT_VALUES * 0.5, rtol=1e-6, atol=1e-6)\n \n     def test_truncation_and_padding(self):\n         input_audio = self._load_datasamples(2)"
        },
        {
            "sha": "2d5eca4b83ae7e022edd84fe9cff3297446a3d05",
            "filename": "tests/models/encodec/test_modeling_encodec.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencodec%2Ftest_modeling_encodec.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -324,7 +324,7 @@ def test_feed_forward_chunking(self):\n             inputs = self._prepare_for_class(inputs_dict, model_class)\n             inputs[\"input_values\"] = inputs[\"input_values\"].repeat(1, 1, 10)\n \n-            hidden_states_no_chunk = model(**inputs)[0]\n+            hidden_states_no_chunk = model(**inputs)[1]\n \n             torch.manual_seed(0)\n             config.chunk_length_s = 1\n@@ -335,8 +335,8 @@ def test_feed_forward_chunking(self):\n             model.to(torch_device)\n             model.eval()\n \n-            hidden_states_with_chunk = model(**inputs)[0]\n-            self.assertTrue(torch.allclose(hidden_states_no_chunk, hidden_states_with_chunk, atol=1e-3))\n+            hidden_states_with_chunk = model(**inputs)[1]\n+            torch.testing.assert_close(hidden_states_no_chunk, hidden_states_with_chunk, rtol=1e-1, atol=1e-2)\n \n     @unittest.skip(\n         reason=\"The EncodecModel is not transformers based, thus it does not have the usual `hidden_states` logic\"\n@@ -507,7 +507,7 @@ def test_integration_24kHz(self):\n                 )[-1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             # make sure shape matches\n             self.assertTrue(inputs[\"input_values\"].shape == input_values_enc_dec.shape)\n@@ -563,7 +563,7 @@ def test_integration_48kHz(self):\n                 )[-1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             # make sure shape matches\n             self.assertTrue(inputs[\"input_values\"].shape == input_values_enc_dec.shape)\n@@ -622,7 +622,7 @@ def test_batch_48kHz(self):\n                 input_values_enc_dec = model(input_values, bandwidth=float(bandwidth))[-1]\n \n             # make sure forward and decode gives same result\n-            self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec, atol=1e-3))\n+            torch.testing.assert_close(input_values_dec, input_values_enc_dec, rtol=1e-3, atol=1e-3)\n \n             # make sure shape matches\n             self.assertTrue(input_values.shape == input_values_enc_dec.shape)"
        },
        {
            "sha": "1bffcca2221109f7e69cce1f6e49f2bb6482aa1f",
            "filename": "tests/models/esm/test_modeling_esm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -317,7 +317,7 @@ def test_inference_masked_lm(self):\n             expected_slice = torch.tensor(\n                 [[[8.9215, -10.5898, -6.4671], [-6.3967, -13.9114, -1.1212], [-7.7812, -13.9516, -3.7406]]]\n             )\n-            self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_no_head(self):\n         with torch.no_grad():\n@@ -330,7 +330,7 @@ def test_inference_no_head(self):\n             expected_slice = torch.tensor(\n                 [[[0.1444, 0.5413, 0.3248], [0.3034, 0.0053, 0.3108], [0.3228, -0.2499, 0.3415]]]\n             )\n-            self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @require_bitsandbytes\n     def test_inference_bitsandbytes(self):"
        },
        {
            "sha": "ada6b773b589eb9f8ca914324ec8e93f9742f350",
            "filename": "tests/models/esm/test_modeling_esmfold.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_esmfold.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -282,4 +282,4 @@ def test_inference_protein_folding(self):\n         input_ids = torch.tensor([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]])\n         position_outputs = model(input_ids)[\"positions\"]\n         expected_slice = torch.tensor([2.5828, 0.7993, -10.9334], dtype=torch.float32)\n-        self.assertTrue(torch.allclose(position_outputs[0, 0, 0, 0], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(position_outputs[0, 0, 0, 0], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "b92b4782998c9afa40b3d0c25e7fad0cac2baad9",
            "filename": "tests/models/falcon/test_modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon%2Ftest_modeling_falcon.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -446,7 +446,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -628,4 +628,4 @@ def test_falcon_alibi_sdpa_matches_eager(self):\n             falcon_output_eager = falcon(input_ids, output_attentions=True)[0]\n             falcon_output_sdpa = falcon(input_ids)[0]\n \n-        self.assertTrue(torch.allclose(falcon_output_eager, falcon_output_sdpa, atol=1e-3))\n+        torch.testing.assert_close(falcon_output_eager, falcon_output_sdpa, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "75835da1a4684f63e28fffaf18e65050167e3bd1",
            "filename": "tests/models/falcon_mamba/test_modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_mamba%2Ftest_modeling_falcon_mamba.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -354,11 +354,12 @@ def test_initialization(self):\n                         self.assertTrue(param.data.min().item() >= inv_dt[0])\n                 elif \"A_log\" in name:\n                     A = torch.arange(1, config.state_size + 1, dtype=torch.float32)[None, :]\n-                    self.assertTrue(torch.allclose(param.data, torch.log(A), atol=1e-5, rtol=1e-5))\n+                    A = A.expand(config.intermediate_size, -1).contiguous()\n+                    torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n                 elif \"D\" in name:\n                     if param.requires_grad:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n \n     @slow\n     # Ignore copy"
        },
        {
            "sha": "cc413b94a63e0fa76113e9228dddff55eaa68fe7",
            "filename": "tests/models/fastspeech2_conformer/test_modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffastspeech2_conformer%2Ftest_modeling_fastspeech2_conformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -390,7 +390,7 @@ def test_inference_integration(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(spectrogram[0, :10, :10], expected_mel_spectrogram, atol=1e-4))\n+        torch.testing.assert_close(spectrogram[0, :10, :10], expected_mel_spectrogram, rtol=1e-4, atol=1e-4)\n         self.assertEqual(spectrogram.shape, (1, 205, model.config.num_mel_bins))\n \n     def test_training_integration(self):\n@@ -447,8 +447,8 @@ def test_training_integration(self):\n \n         expected_loss = torch.tensor(74.4595, device=torch_device)\n \n-        self.assertTrue(torch.allclose(spectrogram[0, :10, :10], expected_mel_spectrogram, atol=1e-3))\n-        self.assertTrue(torch.allclose(loss, expected_loss, atol=1e-4))\n+        torch.testing.assert_close(spectrogram[0, :10, :10], expected_mel_spectrogram, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(loss, expected_loss, rtol=1e-4, atol=1e-4)\n         self.assertEqual(spectrogram.shape, (1, 224, model.config.num_mel_bins))\n \n \n@@ -803,5 +803,5 @@ def test_inference_integration(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(waveform[0, :100], expected_waveform, atol=1e-4))\n+        torch.testing.assert_close(waveform[0, :100], expected_waveform, rtol=1e-4, atol=1e-4)\n         self.assertEqual(waveform.shape, (1, 52480))"
        },
        {
            "sha": "2ba0b509e47e10327109b6276eb3a2b5923c94a4",
            "filename": "tests/models/flaubert/test_modeling_flaubert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_modeling_flaubert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -514,4 +514,4 @@ def test_inference_no_head_absolute_embedding(self):\n             [[[-2.6251, -1.4298, -0.0227], [-2.8510, -1.6387, 0.2258], [-2.8114, -1.1832, -0.3066]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "e4949c54ca21a2b55447a495a8f674ba36377a70",
            "filename": "tests/models/flava/test_modeling_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_modeling_flava.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1346,7 +1346,7 @@ def test_inference(self):\n         )\n \n         expected_logits = torch.tensor([[16.1291, 8.4033], [16.1291, 8.4033]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.contrastive_logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.contrastive_logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n         self.assertAlmostEqual(outputs.loss_info.mmm_text.item(), 2.0727925, places=4)\n         self.assertAlmostEqual(outputs.loss_info.mmm_image.item(), 7.0282096, places=4)\n         self.assertAlmostEqual(outputs.loss.item(), 11.3792324, places=4)\n@@ -1397,7 +1397,7 @@ def test_inference_with_itm_labels(self):\n         )\n \n         expected_logits = torch.tensor([[16.1291, 8.4033], [16.1291, 8.4033]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.contrastive_logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.contrastive_logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n         self.assertAlmostEqual(outputs.loss_info.mmm_text.item(), 2.0727925, places=4)\n         self.assertAlmostEqual(outputs.loss_info.mmm_image.item(), 6.8965902, places=4)\n         self.assertAlmostEqual(outputs.loss.item(), 9.6084213, places=4)"
        },
        {
            "sha": "26eec0f29908e285a92a1841cc1325adb4057052",
            "filename": "tests/models/fnet/test_modeling_fnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffnet%2Ftest_modeling_fnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -550,7 +550,7 @@ def test_inference_for_masked_lm(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_tokenizers\n@@ -592,7 +592,7 @@ def test_inference_for_next_sentence_prediction(self):\n \n         expected_slice = torch.tensor([[-0.2234, -0.0226]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(output, expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output, expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_model(self):\n@@ -610,4 +610,4 @@ def test_inference_model(self):\n             [[[4.1541, -0.1051, -0.1667], [-0.9144, 0.2939, -0.0086], [-0.8472, -0.7281, 0.0256]]], device=torch_device\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "2d3d8b6f3ac5361f26fb72b8044af030b634a508",
            "filename": "tests/models/focalnet/test_modeling_focalnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffocalnet%2Ftest_modeling_focalnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -426,7 +426,7 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n         expected_slice = torch.tensor([0.2166, -0.4368, 0.2191]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n         self.assertTrue(outputs.logits.argmax(dim=-1).item(), 281)\n \n "
        },
        {
            "sha": "0d7f4d0cab7201fa7a97e7abb878ec249ade5b20",
            "filename": "tests/models/fsmt/test_modeling_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffsmt%2Ftest_modeling_fsmt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -513,7 +513,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[-1.5753, -1.5753, 2.8975], [-0.9540, -0.9540, 1.0299], [-3.3131, -3.3131, 0.5219]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def translation_setup(self, pair):\n         text = {\n@@ -608,6 +608,6 @@ def test_positional_emb_weights_against_marian(self):\n         )\n         no_cache_pad_zero = emb1(input_ids)[0]\n         # XXX: only the 1st line matches the 3rd\n-        self.assertTrue(\n-            torch.allclose(torch.tensor(desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], atol=1e-3)\n+        torch.testing.assert_close(\n+            torch.tensor(desired_weights, device=torch_device), no_cache_pad_zero[:3, :5], rtol=1e-3, atol=1e-3\n         )"
        },
        {
            "sha": "6bd0826861004ea34e0a0fb22697ad4d9796eb2d",
            "filename": "tests/models/funnel/test_modeling_funnel.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffunnel%2Ftest_modeling_funnel.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -501,16 +501,16 @@ def test_inference_tiny_model(self):\n \n         expected_output_sum = torch.tensor(2344.8352)\n         expected_output_mean = torch.tensor(0.8052)\n-        self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))\n-        self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))\n+        torch.testing.assert_close(output.sum(), expected_output_sum, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output.mean(), expected_output_mean, rtol=1e-4, atol=1e-4)\n \n         attention_mask = torch.tensor([[1] * 7, [1] * 4 + [0] * 3] * 6 + [[0, 1, 1, 0, 0, 1, 1]])\n         output = model(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0].abs()\n \n         expected_output_sum = torch.tensor(2343.8425)\n         expected_output_mean = torch.tensor(0.8049)\n-        self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))\n-        self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))\n+        torch.testing.assert_close(output.sum(), expected_output_sum, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output.mean(), expected_output_mean, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_model(self):\n@@ -521,5 +521,5 @@ def test_inference_model(self):\n \n         expected_output_sum = torch.tensor(235.7246)\n         expected_output_mean = torch.tensor(0.0256)\n-        self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))\n-        self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))\n+        torch.testing.assert_close(output.sum(), expected_output_sum, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output.mean(), expected_output_mean, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "ff9c086bdb2b86a5810f9580c1ff22cde6b00169",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -555,7 +555,7 @@ def test_forward_pass(self):\n             [[-0.9514, -0.9512, -0.9507], [-0.5454, -0.5453, -0.5453], [-0.8862, -0.8857, -0.8848]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_image_captioning(self):\n         processor = GitProcessor.from_pretrained(\"microsoft/git-base\")\n@@ -576,7 +576,7 @@ def test_inference_image_captioning(self):\n         self.assertEqual(generated_caption, \"two cats laying on a pink blanket\")\n         self.assertTrue(outputs.scores[-1].shape, expected_shape)\n         expected_slice = torch.tensor([[-0.8805, -0.8803, -0.8799]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.scores[-1][0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.scores[-1][0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_visual_question_answering(self):\n         processor = GitProcessor.from_pretrained(\"microsoft/git-base-textvqa\")\n@@ -653,4 +653,4 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-1.0296, 2.5960, 0.8703], [1.7027, 1.3302, -0.4543], [-1.4932, -0.1084, 0.0502]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "f6bd7b146c376b706a6c99b7093cf80818ce4ec3",
            "filename": "tests/models/glpn/test_modeling_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_modeling_glpn.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -342,4 +342,4 @@ def test_inference_depth_estimation(self):\n             [[3.4291, 2.7865, 2.5151], [3.2841, 2.7021, 2.3502], [3.1147, 2.4625, 2.2481]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1ac2db40812379d27430676352f4abf2b36e0e1e",
            "filename": "tests/models/gpt_bigcode/test_modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_bigcode%2Ftest_modeling_gpt_bigcode.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -589,4 +589,4 @@ def test_mqa_reduces_to_mha(self, seed, is_train_mode=True):\n         attention_mqa_result = attention_mqa(hidden_states)[0]\n \n         # CHECK THAT ALL OUTPUTS ARE THE SAME\n-        self.assertTrue(torch.allclose(attention_mha_result, attention_mqa_result, atol=1e-5))\n+        torch.testing.assert_close(attention_mha_result, attention_mqa_result, rtol=1e-5, atol=1e-5)"
        },
        {
            "sha": "97403cb8e5cbcdb8f49c49dd882a5b18f222d948",
            "filename": "tests/models/gpt_neox/test_modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neox%2Ftest_modeling_gpt_neox.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -359,7 +359,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -472,4 +472,4 @@ def pythia_integration_test(self):\n         # alternative: tokenizer('<|im_start|>system\\nA chat between')\n         input_ids = torch.as_tensor(input_ids)[None].to(torch_device)\n         outputs = model(input_ids)[\"logits\"][:, -1][0, :30]\n-        self.assertTrue(torch.allclose(EXPECTED_LOGITS, outputs, atol=1e-5))\n+        torch.testing.assert_close(EXPECTED_LOGITS, outputs, rtol=1e-5, atol=1e-5)"
        },
        {
            "sha": "a7ef3024de6b0e9acb55b3b39ee3d5eaae505bb4",
            "filename": "tests/models/granite/test_modeling_granite.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranite%2Ftest_modeling_granite.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -343,7 +343,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -444,7 +444,7 @@ def test_model_3b_logits_bf16(self):\n         # fmt: off\n         EXPECTED_MEAN = torch.tensor([[-1.9798, -3.1626, -2.8062, -2.3777, -2.7091, -2.2338, -2.5924, -2.3974]])\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1), atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICE = torch.tensor([[4.8750, -2.1875, -2.1875, -2.1875, -2.1875, -2.8438, -2.1875, -2.1875,\n@@ -474,4 +474,4 @@ def test_model_3b_logits(self):\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.0984, -3.1294, -2.8153, -2.3568, -2.7337, -2.2624, -2.6016, -2.4022]])\n \n-        self.assertTrue(torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)"
        },
        {
            "sha": "9e7b7c944214fc052144ee33cb3c2767c6ce141b",
            "filename": "tests/models/granitemoe/test_modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgranitemoe%2Ftest_modeling_granitemoe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -342,7 +342,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -441,9 +441,7 @@ def test_model_3b_logits(self):\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.2122, -1.6632, -2.9269, -2.3344, -2.0143, -3.0146, -2.6839, -2.5610]])\n \n-        self.assertTrue(\n-            torch.allclose(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), atol=1e-2, rtol=1e-2)\n-        )\n+        torch.testing.assert_close(EXPECTED_MEAN.to(torch_device), out.logits.float().mean(-1), rtol=1e-2, atol=1e-2)\n \n         # slicing logits[0, 0, 0:15]\n         EXPECTED_SLICE = torch.tensor([[4.8785, -2.2890, -2.2892, -2.2885, -2.2890, -3.5007, -2.2897, -2.2892,"
        },
        {
            "sha": "4ac70a4e0fb84bd44625812993fd82f60842018e",
            "filename": "tests/models/grounding_dino/test_image_processing_grounding_dino.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_image_processing_grounding_dino.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -193,10 +193,10 @@ def test_post_process_object_detection(self):\n         self.assertEqual(results[0][\"scores\"].shape, (self.image_processor_tester.num_queries,))\n \n         expected_scores = torch.tensor([0.7050, 0.7222, 0.7222, 0.6829, 0.7220])\n-        self.assertTrue(torch.allclose(results[0][\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[0][\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n \n         expected_box_slice = torch.tensor([0.6908, 0.4354, 1.0737, 1.3947])\n-        self.assertTrue(torch.allclose(results[0][\"boxes\"][0], expected_box_slice, atol=1e-4))\n+        torch.testing.assert_close(results[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_call_pytorch_with_coco_detection_annotations with DeformableDetr->GroundingDino\n@@ -218,31 +218,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([5887.9600, 11250.2061, 489353.8438, 837122.7500, 147967.5156, 165732.3438])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_detection_annotations with Detr->GroundingDino\n@@ -309,8 +309,8 @@ def test_batched_coco_detection_annotations(self):\n                     [0.5790, 0.4115, 0.3430, 0.7161],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -361,8 +361,8 @@ def test_batched_coco_detection_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     @slow\n     # Copied from tests.models.deformable_detr.test_image_processing_deformable_detr.DeformableDetrImageProcessingTest.test_call_pytorch_with_coco_panoptic_annotations with DeformableDetr->GroundingDino\n@@ -386,35 +386,35 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([147979.6875, 165527.0469, 484638.5938, 11292.9375, 5879.6562, 7634.1147])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify masks\n             expected_masks_sum = 822873\n             relative_error = torch.abs(encoding[\"labels\"][0][\"masks\"].sum() - expected_masks_sum) / expected_masks_sum\n             self.assertTrue(relative_error < 1e-3)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([800, 1066])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_batched_coco_panoptic_annotations with Detr->GroundingDino\n@@ -485,8 +485,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     [0.2997, 0.2994, 0.5994, 0.5987],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check the masks have also been padded\n             self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1066]))\n@@ -538,8 +538,8 @@ def test_batched_coco_panoptic_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->GroundingDino\n     def test_max_width_max_height_resizing_and_pad_strategy(self):"
        },
        {
            "sha": "b102c357e518393b6fdf08f732c819d2e07016bd",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -645,11 +645,11 @@ def test_inference_object_detection_head(self):\n             [[-4.8913, -0.1900, -0.2161], [-4.9653, -0.3719, -0.3950], [-5.9599, -3.3765, -3.3104]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-3, atol=1e-3)\n \n         expected_shape_boxes = torch.Size((1, model.config.num_queries, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = processor.image_processor.post_process_object_detection(\n@@ -659,8 +659,8 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([344.8143, 23.1796, 637.4004, 373.8295]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 2)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-3))\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n \n         # verify grounded postprocessing\n         expected_labels = [\"a cat\", \"a cat\"]\n@@ -672,8 +672,8 @@ def test_inference_object_detection_head(self):\n             target_sizes=[(image.height, image.width)],\n         )[0]\n \n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-3))\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n         self.assertListEqual(results[\"text_labels\"], expected_labels)\n \n     @require_torch_accelerator\n@@ -697,12 +697,12 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n \n         # 3. assert equivalence\n         for key in cpu_outputs.keys():\n-            self.assertTrue(torch.allclose(cpu_outputs[key], gpu_outputs[key].cpu(), atol=1e-3))\n+            torch.testing.assert_close(cpu_outputs[key], gpu_outputs[key].cpu(), rtol=1e-3, atol=1e-3)\n \n         expected_logits = torch.tensor(\n             [[-4.8915, -0.1900, -0.2161], [-4.9658, -0.3716, -0.3948], [-5.9596, -3.3763, -3.3103]]\n         )\n-        self.assertTrue(torch.allclose(cpu_outputs.logits[0, :3, :3], expected_logits, atol=1e-3))\n+        torch.testing.assert_close(cpu_outputs.logits[0, :3, :3], expected_logits, rtol=1e-3, atol=1e-3)\n \n         # assert postprocessing\n         results_cpu = processor.image_processor.post_process_object_detection(\n@@ -713,8 +713,8 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n             gpu_outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n         )[0]\n \n-        self.assertTrue(torch.allclose(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), atol=1e-3))\n-        self.assertTrue(torch.allclose(results_cpu[\"boxes\"], result_gpu[\"boxes\"].cpu(), atol=1e-3))\n+        torch.testing.assert_close(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(results_cpu[\"boxes\"], result_gpu[\"boxes\"].cpu(), rtol=1e-3, atol=1e-3)\n \n     def test_cross_attention_mask(self):\n         model = GroundingDinoForObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\").to(torch_device)\n@@ -738,6 +738,6 @@ def test_cross_attention_mask(self):\n             outputs2 = model(**encoding2)\n             outputs_batched = model(**encoding_batched)\n \n-        self.assertTrue(torch.allclose(outputs1.logits, outputs_batched.logits[:1], atol=1e-3))\n+        torch.testing.assert_close(outputs1.logits, outputs_batched.logits[:1], rtol=1e-3, atol=1e-3)\n         # For some reason 12 elements are > 1e-3, but the rest are fine\n-        self.assertTrue(torch.allclose(outputs2.logits, outputs_batched.logits[1:], atol=1.8e-3))\n+        torch.testing.assert_close(outputs2.logits, outputs_batched.logits[1:], rtol=1.8e-3, atol=1.8e-3)"
        },
        {
            "sha": "d527853b1eca4e8412fd15502d4fdaf92bd54721",
            "filename": "tests/models/grounding_dino/test_processor_grounding_dino.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -136,10 +136,10 @@ def test_post_process_grounded_object_detection(self):\n         self.assertEqual(post_processed[0][\"scores\"].shape, (self.num_queries,))\n \n         expected_scores = torch.tensor([0.7050, 0.7222, 0.7222, 0.6829, 0.7220])\n-        self.assertTrue(torch.allclose(post_processed[0][\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(post_processed[0][\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n \n         expected_box_slice = torch.tensor([0.6908, 0.4354, 1.0737, 1.3947])\n-        self.assertTrue(torch.allclose(post_processed[0][\"boxes\"][0], expected_box_slice, atol=1e-4))\n+        torch.testing.assert_close(post_processed[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n \n     # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.test_save_load_pretrained_default with CLIP->GroundingDino,GroundingDinoTokenizer->BertTokenizer\n     def test_save_load_pretrained_default(self):"
        },
        {
            "sha": "da1db5a1fc82798091720dcf5e8d7b6a74852154",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -765,4 +765,4 @@ def test_inference(self):\n \n         expected_logits = torch.tensor([[13.3523, 6.3629]])\n \n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "923bdd1156365f8445354bdbb5eb9a596f0a2cb9",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -545,7 +545,7 @@ def test_inference_image_classification_head(self):\n             ]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(inputs.pixel_values[0, :3, :3, :3], expected_pixel_values, atol=1e-4))\n+        torch.testing.assert_close(inputs.pixel_values[0, :3, :3, :3], expected_pixel_values, rtol=1e-4, atol=1e-4)\n \n         # forward pass\n         with torch.no_grad():\n@@ -557,7 +557,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([[0.8028, 0.2409, -0.2254, -0.3712, -0.2848]]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :5], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :5], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_interpolate_pos_encoding(self):\n         model = HieraModel.from_pretrained(\"facebook/hiera-tiny-224-hf\").to(torch_device)\n@@ -581,7 +581,7 @@ def test_inference_interpolate_pos_encoding(self):\n             [[1.7853, 0.0690, 0.3177], [2.6853, -0.2334, 0.0889], [1.5445, -0.1515, -0.0300]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_for_pretraining(self):\n@@ -619,7 +619,7 @@ def test_inference_for_pretraining(self):\n             ]\n         )\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :5, :5], expected_slice.to(torch_device), atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :5, :5], expected_slice.to(torch_device), rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "9f77379befe20d8c9df8705892bc792d8f1a36d0",
            "filename": "tests/models/hubert/test_modeling_hubert.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_hubert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -812,7 +812,7 @@ def test_inference_keyword_spotting(self):\n         expected_logits = torch.tensor([7.6692, 17.7795, 11.1562, 11.8232], dtype=torch.float16, device=torch_device)\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=3e-2))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=3e-2, atol=3e-2)\n \n     def test_inference_intent_classification(self):\n         model = HubertForSequenceClassification.from_pretrained(\n@@ -849,9 +849,9 @@ def test_inference_intent_classification(self):\n         self.assertListEqual(predicted_ids_location.tolist(), expected_labels_location)\n \n         # TODO: lower the tolerance after merging the padding fix https://github.com/pytorch/fairseq/pull/3572\n-        self.assertTrue(torch.allclose(predicted_logits_action, expected_logits_action, atol=3e-1))\n-        self.assertTrue(torch.allclose(predicted_logits_object, expected_logits_object, atol=3e-1))\n-        self.assertTrue(torch.allclose(predicted_logits_location, expected_logits_location, atol=3e-1))\n+        torch.testing.assert_close(predicted_logits_action, expected_logits_action, rtol=3e-1, atol=3e-1)\n+        torch.testing.assert_close(predicted_logits_object, expected_logits_object, rtol=3e-1, atol=3e-1)\n+        torch.testing.assert_close(predicted_logits_location, expected_logits_location, rtol=3e-1, atol=3e-1)\n \n     def test_inference_speaker_identification(self):\n         model = HubertForSequenceClassification.from_pretrained(\n@@ -877,7 +877,7 @@ def test_inference_speaker_identification(self):\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n         # TODO: lower the tolerance after merging the padding fix https://github.com/pytorch/fairseq/pull/3572\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=10))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=10, atol=10)\n \n     def test_inference_emotion_recognition(self):\n         model = HubertForSequenceClassification.from_pretrained(\n@@ -899,7 +899,7 @@ def test_inference_emotion_recognition(self):\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n         # TODO: lower the tolerance after merging the padding fix https://github.com/pytorch/fairseq/pull/3572\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=1e-1))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=1e-1, atol=1e-1)\n \n     def test_inference_distilhubert(self):\n         model = HubertModel.from_pretrained(\"ntu-spml/distilhubert\").to(torch_device)\n@@ -940,8 +940,8 @@ def test_inference_distilhubert(self):\n         )\n         expected_output_sum = -3776.0730\n \n-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))\n-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))\n+        torch.testing.assert_close(outputs[:, :4, :4], expected_outputs_first, rtol=5e-3, atol=5e-3)\n+        torch.testing.assert_close(outputs[:, -4:, -4:], expected_outputs_last, rtol=5e-3, atol=5e-3)\n         self.assertTrue(abs(outputs.sum() - expected_output_sum) < 0.1)\n \n     def test_inference_hubert_25hz(self):\n@@ -977,6 +977,6 @@ def test_inference_hubert_25hz(self):\n         )\n         expected_output_sum = 1681.7603\n \n-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))\n-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))\n+        torch.testing.assert_close(outputs[:, :4, :4], expected_outputs_first, rtol=5e-3, atol=5e-3)\n+        torch.testing.assert_close(outputs[:, -4:, -4:], expected_outputs_last, rtol=5e-3, atol=5e-3)\n         self.assertTrue(abs(outputs.sum() - expected_output_sum) < 0.1)"
        },
        {
            "sha": "50b286ca51abb95e9d04a02fde505add068d2cf9",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -666,7 +666,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, image_attention_mask, signa\n             next_logits_with_padding = model(**model_kwargs, **inputs_dict).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @pytest.mark.generate\n     def test_generate_continue_from_past_key_values(self):"
        },
        {
            "sha": "8cafc606bd2cd245cce1b1e5752ee81517fcd37e",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -562,7 +562,7 @@ def test_inputs_embeds_matches_input_ids_with_generate(self):\n             out_ids = model.generate(input_ids=input_ids, **inputs, max_new_tokens=2)\n             out_embeds = model.generate(input_ids=input_ids, inputs_embeds=inputs_embeds, **inputs, max_new_tokens=2)\n \n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n \n @require_torch"
        },
        {
            "sha": "147e576036ce232702c760e83ec3f311856442fe",
            "filename": "tests/models/ijepa/test_modeling_ijepa.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fijepa%2Ftest_modeling_ijepa.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -288,7 +288,7 @@ def test_inference_no_head(self):\n             [[-0.0621, -0.0054, -2.7513], [-0.1952, 0.0909, -3.9536], [0.0942, -0.0331, -1.2833]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_accelerate\n@@ -338,4 +338,4 @@ def test_inference_interpolate_pos_encoding(self):\n             [[-0.0621, -0.0054, -2.7513], [-0.1952, 0.0909, -3.9536], [0.0942, -0.0331, -1.2833]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "a8bcb8d1802a4f259c065558ac5b90fa027099ec",
            "filename": "tests/models/imagegpt/test_modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_modeling_imagegpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -354,4 +354,4 @@ def test_inference_causal_lm_head(self):\n             [[2.3445, 2.6889, 2.7313], [1.0530, 1.2416, 0.5699], [0.2205, 0.7749, 0.3953]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "d6a5220aac27f7a11bf1e34e4dc6ec7449287bbb",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -504,7 +504,7 @@ def test_inference_no_head(self):\n             [[0.4699, 0.7295, 0.8967], [0.4858, 0.3810, 0.9641], [-0.0233, 0.3608, 1.0303]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = InformerForPrediction.from_pretrained(\"huggingface/informer-tourism-monthly\").to(torch_device)\n@@ -527,7 +527,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[0.4170, 0.9067, 0.8153], [0.3004, 0.7574, 0.7066], [0.6803, -0.6323, 1.2802]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         model = InformerForPrediction.from_pretrained(\"huggingface/informer-tourism-monthly\").to(torch_device)\n@@ -547,4 +547,4 @@ def test_seq_to_seq_generation(self):\n \n         expected_slice = torch.tensor([3400.8005, 4289.2637, 7101.9209], device=torch_device)\n         mean_prediction = outputs.sequences.mean(dim=1)\n-        self.assertTrue(torch.allclose(mean_prediction[0, -3:], expected_slice, rtol=1e-1))\n+        torch.testing.assert_close(mean_prediction[0, -3:], expected_slice, rtol=1e-1)"
        },
        {
            "sha": "d472274fabb23aa4440368b7db28aede161698f5",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -748,7 +748,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             ).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(\n         \"InstructBLIP cannot generate only from input ids, and requires pixel values in all cases to be present\""
        },
        {
            "sha": "ef95aab8bf931ca9789e15b16505cd9e702e924a",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -768,7 +768,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             ).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(\n         \"InstructBLIPVideo cannot generate only from input ids, and requires pixel values in all cases to be present\""
        },
        {
            "sha": "263b35345ba0ebe969dc26cfefad2e5e60a0eb0a",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -415,10 +415,11 @@ def test_initialization(self):\n                 if param.requires_grad:\n                     if \"A_log\" in name:\n                         A = torch.arange(1, config.mamba_d_state + 1, dtype=torch.float32)[None, :]\n-                        self.assertTrue(torch.allclose(param.data, torch.log(A), atol=1e-5, rtol=1e-5))\n+                        A = A.expand(config.mamba_expand * config.hidden_size, -1).contiguous()\n+                        torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n                     elif \"D\" in name:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n                     else:\n                         self.assertIn(\n                             ((param.data.mean() * 1e9).round() / 1e9).item(),"
        },
        {
            "sha": "757783950b90026a6db9a302b514f3739c02a627",
            "filename": "tests/models/jetmoe/test_modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjetmoe%2Ftest_modeling_jetmoe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -390,10 +390,10 @@ def test_model_8b_logits(self):\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[0.2507, -2.7073, -1.3445, -1.9363, -1.7216, -1.7370, -1.9054, -1.9792]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-3.3689,  5.9006,  5.7450, -1.7012, -4.7072, -4.7071, -4.7071, -4.7071, -4.7072, -4.7072, -4.7072, -4.7071,  3.8321,  9.1746, -4.7071, -4.7072, -4.7071, -4.7072, -4.7071, -4.7072, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071, -4.7071])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n         del model\n         backend_empty_cache(torch_device)"
        },
        {
            "sha": "bb318ba1322150759fa2e0abfaf119331ad376da",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -515,7 +515,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, pad_size, signature):\n             next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-3))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_model_from_pretrained(self):\n@@ -892,6 +892,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[0.9148, -1.4148, 3.8040], [3.3443, 1.9478, 0.2080], [1.6604, 2.8184, -0.3618]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-1)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-2, atol=1e-2\n         )"
        },
        {
            "sha": "ab2a9cd5135930761362048abcd9c78a67da12f9",
            "filename": "tests/models/layoutlm/test_modeling_layoutlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_layoutlm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -329,12 +329,12 @@ def test_forward_pass_no_head(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n         # test the pooled output on [1, :3]\n         expected_slice = torch.tensor([-0.6580, -0.0214, 0.8552], device=torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.pooler_output[1, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(outputs.pooler_output[1, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_forward_pass_sequence_classification(self):"
        },
        {
            "sha": "a8b5083ebd512b3221c5b2df949cfe286e395f9f",
            "filename": "tests/models/layoutlmv2/test_modeling_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_modeling_layoutlmv2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -559,7 +559,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[-0.1087, 0.0727, -0.3075], [0.0799, -0.0427, -0.0751], [-0.0367, 0.0480, -0.1358]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n         # verify the pooled output\n         expected_shape = torch.Size((2, model.config.hidden_size))"
        },
        {
            "sha": "af0301cf6b83b576c76fe1a7e5ab6780e3ace592",
            "filename": "tests/models/layoutlmv3/test_modeling_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_layoutlmv3.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -417,4 +417,4 @@ def test_inference_no_head(self):\n             [[-0.0529, 0.3618, 0.1632], [-0.1587, -0.1667, -0.0400], [-0.1557, -0.1671, -0.0505]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "3d21fa0a69d5d1882340bde2b30bd4fd0979e027",
            "filename": "tests/models/led/test_modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fled%2Ftest_modeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_led.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -540,7 +540,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[2.3050, 2.8279, 0.6531], [-1.8457, -0.1455, -3.5661], [-1.0186, 0.4586, -2.2043]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-base-16384\").to(torch_device)\n@@ -557,7 +557,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[33.6507, 6.4572, 16.8089], [5.8739, -2.4238, 11.2902], [-3.2139, -4.3149, 4.2783]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         # this test requires 16GB of RAM"
        },
        {
            "sha": "b35967a84eeb6e7fed4acd9285268e11339913a9",
            "filename": "tests/models/levit/test_modeling_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flevit%2Ftest_modeling_levit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -409,4 +409,4 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([1.0448, -0.3745, -1.8317]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "9bfbb1c520c880d4e17766e9d2b2736a29b27d52",
            "filename": "tests/models/lilt/test_modeling_lilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flilt%2Ftest_modeling_lilt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -326,4 +326,4 @@ def test_inference_no_head(self):\n         )\n \n         self.assertTrue(outputs.last_hidden_state.shape, expected_shape)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :, :3], expected_slice, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "c2abf19b224110239162614a7ac5e3c93d5fc1b9",
            "filename": "tests/models/llama/test_modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_llama.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -413,7 +413,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n "
        },
        {
            "sha": "23663ee649a759f9335a9cd834234a6998706515",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -242,7 +242,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     def test_mismatching_num_image_tokens(self):\n         \"\"\""
        },
        {
            "sha": "ce86a56958108f9c607a3373a7f809a48c86678a",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -288,7 +288,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     def test_mismatching_num_image_tokens(self):\n         \"\"\""
        },
        {
            "sha": "f7cf66b248f3a5ed8aa80c33eaad43ea7e3cef75",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -305,7 +305,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     def test_mismatching_num_image_tokens(self):\n         \"\"\""
        },
        {
            "sha": "2674aaabd8ccc7a1ebacff5971f1147d5c77f073",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -291,7 +291,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @parameterized.expand(\n         ["
        },
        {
            "sha": "304a0e47446aa0943da7171822f2571598854a1c",
            "filename": "tests/models/longformer/test_modeling_longformer.py",
            "status": "modified",
            "additions": 21,
            "deletions": 13,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_longformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -466,7 +466,9 @@ def test_diagonalize(self):\n         self.assertTrue(padded_hidden_states.shape[-1] == chunked_hidden_states.shape[-1] + window_overlap_size - 1)\n \n         # first row => [0.4983,  2.6918, -0.0071,  1.0492, 0.0000,  0.0000,  0.0000]\n-        self.assertTrue(torch.allclose(padded_hidden_states[0, 0, 0, :4], chunked_hidden_states[0, 0, 0], atol=1e-3))\n+        torch.testing.assert_close(\n+            padded_hidden_states[0, 0, 0, :4], chunked_hidden_states[0, 0, 0], rtol=1e-3, atol=1e-3\n+        )\n         self.assertTrue(\n             torch.allclose(\n                 padded_hidden_states[0, 0, 0, 4:],\n@@ -475,7 +477,9 @@ def test_diagonalize(self):\n             )\n         )\n         # last row => [0.0000,  0.0000,  0.0000, 2.0514, -1.1600,  0.5372,  0.2629]\n-        self.assertTrue(torch.allclose(padded_hidden_states[0, 0, -1, 3:], chunked_hidden_states[0, 0, -1], atol=1e-3))\n+        torch.testing.assert_close(\n+            padded_hidden_states[0, 0, -1, 3:], chunked_hidden_states[0, 0, -1], rtol=1e-3, atol=1e-3\n+        )\n         self.assertTrue(\n             torch.allclose(\n                 padded_hidden_states[0, 0, -1, :3],\n@@ -493,8 +497,10 @@ def test_pad_and_transpose_last_two_dims(self):\n         self.assertEqual(padded_hidden_states.shape, (1, 8, 5))\n \n         expected_added_dim = torch.zeros((5,), device=torch_device, dtype=torch.float32)\n-        self.assertTrue(torch.allclose(expected_added_dim, padded_hidden_states[0, -1, :], atol=1e-6))\n-        self.assertTrue(torch.allclose(hidden_states[0, -1, :], padded_hidden_states.view(1, -1)[0, 24:32], atol=1e-6))\n+        torch.testing.assert_close(expected_added_dim, padded_hidden_states[0, -1, :], rtol=1e-6, atol=1e-6)\n+        torch.testing.assert_close(\n+            hidden_states[0, -1, :], padded_hidden_states.view(1, -1)[0, 24:32], rtol=1e-6, atol=1e-6\n+        )\n \n     def test_chunk(self):\n         hidden_states = self._get_hidden_states()\n@@ -513,8 +519,10 @@ def test_chunk(self):\n             [0.4983, -1.8348, -0.7584, 2.0514], device=torch_device, dtype=torch.float32\n         )\n \n-        self.assertTrue(torch.allclose(chunked_hidden_states[0, :, 0, 0], expected_slice_along_seq_length, atol=1e-3))\n-        self.assertTrue(torch.allclose(chunked_hidden_states[0, 0, :, 0], expected_slice_along_chunk, atol=1e-3))\n+        torch.testing.assert_close(\n+            chunked_hidden_states[0, :, 0, 0], expected_slice_along_seq_length, rtol=1e-3, atol=1e-3\n+        )\n+        torch.testing.assert_close(chunked_hidden_states[0, 0, :, 0], expected_slice_along_chunk, rtol=1e-3, atol=1e-3)\n         self.assertEqual(chunked_hidden_states.shape, (1, 3, 4, 4))\n \n     def test_mask_invalid_locations(self):\n@@ -728,8 +736,8 @@ def test_inference_no_head(self):\n         output_without_mask = model(input_ids)[0]\n \n         expected_output_slice = torch.tensor([0.0549, 0.1087, -0.1119, -0.0368, 0.0250], device=torch_device)\n-        self.assertTrue(torch.allclose(output[0, 0, -5:], expected_output_slice, atol=1e-4))\n-        self.assertTrue(torch.allclose(output_without_mask[0, 0, -5:], expected_output_slice, atol=1e-4))\n+        torch.testing.assert_close(output[0, 0, -5:], expected_output_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output_without_mask[0, 0, -5:], expected_output_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head_long(self):\n@@ -749,8 +757,8 @@ def test_inference_no_head_long(self):\n \n         expected_output_sum = torch.tensor(74585.8594, device=torch_device)\n         expected_output_mean = torch.tensor(0.0243, device=torch_device)\n-        self.assertTrue(torch.allclose(output.sum(), expected_output_sum, atol=1e-4))\n-        self.assertTrue(torch.allclose(output.mean(), expected_output_mean, atol=1e-4))\n+        torch.testing.assert_close(output.sum(), expected_output_sum, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output.mean(), expected_output_mean, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_masked_lm_long(self):\n@@ -769,6 +777,6 @@ def test_inference_masked_lm_long(self):\n         expected_prediction_scores_sum = torch.tensor(-6.1048e08, device=torch_device)\n         expected_prediction_scores_mean = torch.tensor(-3.0348, device=torch_device)\n \n-        self.assertTrue(torch.allclose(loss, expected_loss, atol=1e-4))\n-        self.assertTrue(torch.allclose(prediction_scores.sum(), expected_prediction_scores_sum, atol=1e-4))\n-        self.assertTrue(torch.allclose(prediction_scores.mean(), expected_prediction_scores_mean, atol=1e-4))\n+        torch.testing.assert_close(loss, expected_loss, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(prediction_scores.sum(), expected_prediction_scores_sum, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(prediction_scores.mean(), expected_prediction_scores_mean, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "c2c2563b5506059d75900ad5924f323313277874",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1362,8 +1362,10 @@ def test_inference_hidden_states(self):\n \n         # check if encoder_outputs match\n         expected_output_slice = torch.tensor([0.0629, -0.1294, -0.0089, 0.0772, 0.0663], device=torch_device)\n-        self.assertTrue(torch.allclose(output.encoder_hidden_states[-1][0, 0, :5], expected_output_slice, atol=1e-4))\n+        torch.testing.assert_close(\n+            output.encoder_hidden_states[-1][0, 0, :5], expected_output_slice, rtol=1e-4, atol=1e-4\n+        )\n \n         # check if logits match\n         expected_output_slice = torch.tensor([5.5231, 6.1058, 3.1766, 8.2391, -5.9453], device=torch_device)\n-        self.assertTrue(torch.allclose(output.logits[0, 0, :5], expected_output_slice, atol=1e-4))\n+        torch.testing.assert_close(output.logits[0, 0, :5], expected_output_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "1a0b63d33e0c18507acda33f89aecc93ced1a109",
            "filename": "tests/models/luke/test_modeling_luke.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fluke%2Ftest_modeling_luke.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -909,14 +909,14 @@ def test_inference_base_model(self):\n         expected_slice = torch.tensor(\n             [[0.0037, 0.1368, -0.0091], [0.1099, 0.3329, -0.1095], [0.0765, 0.5335, 0.1179]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # Verify entity hidden states\n         expected_shape = torch.Size((1, 1, 768))\n         self.assertEqual(outputs.entity_last_hidden_state.shape, expected_shape)\n \n         expected_slice = torch.tensor([[0.1457, 0.1044, 0.0174]]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_large_model(self):\n@@ -944,11 +944,11 @@ def test_inference_large_model(self):\n         expected_slice = torch.tensor(\n             [[0.0133, 0.0865, 0.0095], [0.3093, -0.2576, -0.7418], [-0.1720, -0.2117, -0.2869]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # Verify entity hidden states\n         expected_shape = torch.Size((1, 1, 1024))\n         self.assertEqual(outputs.entity_last_hidden_state.shape, expected_shape)\n \n         expected_slice = torch.tensor([[0.0466, -0.0106, -0.0179]]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.entity_last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "50be9cce9b2a8dba68bf9b04d9348df24f3b38f5",
            "filename": "tests/models/lxmert/test_modeling_lxmert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_lxmert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -803,4 +803,4 @@ def test_inference_no_head_absolute_embedding(self):\n             [[[0.2417, -0.9807, 0.1480], [1.2541, -0.8320, 0.5112], [1.4070, -1.1052, 0.6990]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "b4ddb483d8c7fec66d7e4b98bfc2b7d0a886837f",
            "filename": "tests/models/m2m_100/test_modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fm2m_100%2Ftest_modeling_m2m_100.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -374,7 +374,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[-0.7780, -0.1676, 0.1038], [-6.7556, -1.3992, 0.0567], [-7.5383, -0.5920, -0.2779]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to(torch_device)\n@@ -391,7 +391,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[-1.0448, -1.0411, 3.7992], [-3.2191, -3.2386, -1.3451], [-3.6210, -3.5993, 0.4925]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\").to(torch_device)"
        },
        {
            "sha": "1ac0a25b680509f3cace45ec2f26c9ab3ea4eda8",
            "filename": "tests/models/mamba/test_modeling_mamba.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba%2Ftest_modeling_mamba.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -344,11 +344,12 @@ def test_initialization(self):\n                         self.assertTrue(param.data.min().item() >= inv_dt[0])\n                 elif \"A_log\" in name:\n                     A = torch.arange(1, config.state_size + 1, dtype=torch.float32)[None, :]\n-                    self.assertTrue(torch.allclose(param.data, torch.log(A), atol=1e-5, rtol=1e-5))\n+                    A = A.expand(config.intermediate_size, -1).contiguous()\n+                    torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n                 elif \"D\" in name:\n                     if param.requires_grad:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n \n     @slow\n     def test_model_from_pretrained(self):"
        },
        {
            "sha": "c5b787f6438994899bd6b9f8727c3931c1ef993e",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -250,7 +250,7 @@ def test_initialization(self):\n                 if \"D\" in name:\n                     if param.requires_grad:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n \n     @unittest.skip(reason=\"Mamba 2 weights are not tied\")\n     def test_tied_weights_keys(self):\n@@ -439,4 +439,4 @@ def test_mamba2_mixer_train_vs_eval_equivalence(self):\n                 mixer.eval()\n                 out_eval = mixer(hidden_states)\n \n-                self.assertTrue(torch.allclose(out_train, out_eval, atol=1e-3))\n+                torch.testing.assert_close(out_train, out_eval, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "7e2ce0b7c18f58fdd97200aa526be058c0695225",
            "filename": "tests/models/markuplm/test_modeling_markuplm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_modeling_markuplm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -388,4 +388,4 @@ def test_forward_pass_no_head(self):\n             [[0.0675, -0.0052, 0.5001], [-0.2281, 0.0802, 0.2192], [-0.0583, -0.3311, 0.1185]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "aaca13dbc367ea0d323fb5cbb30e146f55911aa2",
            "filename": "tests/models/mask2former/test_image_processing_mask2former.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_image_processing_mask2former.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -339,8 +339,8 @@ def get_instance_segmentation_and_mapping(annotation):\n \n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor([30, 55])))\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55])))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n@@ -381,8 +381,8 @@ def test_integration_semantic_segmentation(self):\n \n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60])))\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143])))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n@@ -441,9 +441,9 @@ def create_panoptic_map(annotation, segments_info):\n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n         expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels)))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n         expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)"
        },
        {
            "sha": "af7704b1efaf29c914d46b0824afda9c10e7e468",
            "filename": "tests/models/mask2former/test_modeling_mask2former.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmask2former%2Ftest_modeling_mask2former.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -436,7 +436,7 @@ def test_inference_universal_segmentation_head(self):\n             [-6.6105, -6.3427, -6.4675],\n         ]\n         expected_slice = torch.tensor(expected_slice).to(torch_device)\n-        self.assertTrue(torch.allclose(masks_queries_logits[0, 0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(masks_queries_logits[0, 0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n         # class_queries_logits\n         class_queries_logits = outputs.class_queries_logits\n         self.assertEqual(class_queries_logits.shape, (1, model.config.num_queries, model.config.num_labels + 1))\n@@ -447,7 +447,9 @@ def test_inference_universal_segmentation_head(self):\n                 [0.3045, -7.7293, -3.0275],\n             ]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.class_queries_logits[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(\n+            outputs.class_queries_logits[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE\n+        )\n \n     @require_torch_accelerator\n     @require_torch_fp16\n@@ -500,10 +502,10 @@ def test_export(self):\n             eager_outputs = model(**inputs)\n             exported_outputs = exported_program.module().forward(inputs[\"pixel_values\"], inputs[\"pixel_mask\"])\n         self.assertEqual(eager_outputs.masks_queries_logits.shape, exported_outputs.masks_queries_logits.shape)\n-        self.assertTrue(\n-            torch.allclose(eager_outputs.masks_queries_logits, exported_outputs.masks_queries_logits, atol=TOLERANCE)\n+        torch.testing.assert_close(\n+            eager_outputs.masks_queries_logits, exported_outputs.masks_queries_logits, rtol=TOLERANCE, atol=TOLERANCE\n         )\n         self.assertEqual(eager_outputs.class_queries_logits.shape, exported_outputs.class_queries_logits.shape)\n-        self.assertTrue(\n-            torch.allclose(eager_outputs.class_queries_logits, exported_outputs.class_queries_logits, atol=TOLERANCE)\n+        torch.testing.assert_close(\n+            eager_outputs.class_queries_logits, exported_outputs.class_queries_logits, rtol=TOLERANCE, atol=TOLERANCE\n         )"
        },
        {
            "sha": "d042c702a6012e9303b459734344eed910aa4669",
            "filename": "tests/models/maskformer/test_image_processing_maskformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_image_processing_maskformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -293,8 +293,8 @@ def get_instance_segmentation_and_mapping(annotation):\n \n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor([30, 55])))\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55])))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([30, 55]))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([4, 4, 23, 55]))\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n@@ -335,8 +335,8 @@ def test_integration_semantic_segmentation(self):\n \n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60])))\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143])))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor([2, 4, 60]))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], torch.tensor([0, 3, 7, 8, 15, 28, 30, 143]))\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)\n@@ -395,9 +395,9 @@ def create_panoptic_map(annotation, segments_info):\n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n         expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels)))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], torch.tensor(expected_class_labels))\n         expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n \n         # verify the mask labels\n         self.assertEqual(len(inputs[\"mask_labels\"]), 2)"
        },
        {
            "sha": "9298fe2d1c3c1485401b83db3675a931b0a09096",
            "filename": "tests/models/maskformer/test_modeling_maskformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmaskformer%2Ftest_modeling_maskformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -567,7 +567,7 @@ def test_inference_instance_segmentation_head(self):\n             [-1.5795398, -1.9269832, -2.093942],\n         ]\n         expected_slice = torch.tensor(expected_slice).to(torch_device)\n-        self.assertTrue(torch.allclose(masks_queries_logits[0, 0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(masks_queries_logits[0, 0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n         # class_queries_logits\n         class_queries_logits = outputs.class_queries_logits\n         self.assertEqual(\n@@ -580,7 +580,9 @@ def test_inference_instance_segmentation_head(self):\n                 [1.0766e-04, -7.7630e00, -5.1263e00],\n             ]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.class_queries_logits[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(\n+            outputs.class_queries_logits[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE\n+        )\n \n     def test_inference_instance_segmentation_head_resnet_backbone(self):\n         model = (\n@@ -607,7 +609,7 @@ def test_inference_instance_segmentation_head_resnet_backbone(self):\n         )\n         expected_slice = [[-0.9046, -2.6366, -4.6062], [-3.4179, -5.7890, -8.8057], [-4.9179, -7.6560, -10.7711]]\n         expected_slice = torch.tensor(expected_slice).to(torch_device)\n-        self.assertTrue(torch.allclose(masks_queries_logits[0, 0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(masks_queries_logits[0, 0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n         # class_queries_logits\n         class_queries_logits = outputs.class_queries_logits\n         self.assertEqual(\n@@ -616,7 +618,9 @@ def test_inference_instance_segmentation_head_resnet_backbone(self):\n         expected_slice = torch.tensor(\n             [[4.7188, -3.2585, -2.8857], [6.6871, -2.9181, -1.2487], [7.2449, -2.2764, -2.1874]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.class_queries_logits[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(\n+            outputs.class_queries_logits[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE\n+        )\n \n     @require_torch_accelerator\n     @require_torch_fp16"
        },
        {
            "sha": "465444f6927e156c0deb4f97b1876b6f4b4c5efa",
            "filename": "tests/models/mgp_str/test_modeling_mgp_str.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_modeling_mgp_str.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -259,4 +259,4 @@ def test_inference(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(outputs.logits[0][:, 1:4, 1:4], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0][:, 1:4, 1:4], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "4542fe3bbacef7b32d64ba0e22b59ebe581f7b1a",
            "filename": "tests/models/mimi/test_modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmimi%2Ftest_modeling_mimi.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -846,7 +846,7 @@ def test_integration(self):\n                     )[1]\n \n                 # make sure forward and decode gives same result\n-                self.assertTrue(torch.allclose(input_values_dec, input_values_enc_dec))\n+                torch.testing.assert_close(input_values_dec, input_values_enc_dec)\n \n                 # make sure shape matches\n                 self.assertTrue(inputs[\"input_values\"].shape == input_values_enc_dec.shape)"
        },
        {
            "sha": "c4003da462301f38143e554b349a5a9d8d812944",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -450,7 +450,7 @@ def test_model_7b_logits(self):\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.5548, -2.5737, -3.0600, -2.5906, -2.8478, -2.8118, -2.9325, -2.7694]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n \n         # Key 9 for MI300, Key 8 for A100/A10, and Key 7 for T4.\n         #"
        },
        {
            "sha": "e1ded5e934bb461637d7be3b5185b418b11a4e1b",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -320,7 +320,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     def _check_attentions_for_generate(\n         self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1"
        },
        {
            "sha": "ab588cf20f26a8632793ea5de0a00baccac36693",
            "filename": "tests/models/mobilenet_v1/test_modeling_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v1%2Ftest_modeling_mobilenet_v1.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -248,4 +248,4 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-4.1739, -1.1233, 3.1205]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "7e96dea4fefe2ebbe80d94474b00ff34f53f2458",
            "filename": "tests/models/mobilenet_v2/test_modeling_mobilenet_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_modeling_mobilenet_v2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -303,7 +303,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([0.2445, -1.1993, 0.1905]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_semantic_segmentation(self):\n@@ -333,4 +333,4 @@ def test_inference_semantic_segmentation(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "a14a5fb445d5fad1acba8d777fc75e32646af088",
            "filename": "tests/models/mobilevit/test_modeling_mobilevit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_mobilevit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -306,7 +306,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-1.9364, -1.2327, -0.4653]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_semantic_segmentation(self):\n@@ -336,7 +336,7 @@ def test_inference_semantic_segmentation(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_post_processing_semantic_segmentation(self):"
        },
        {
            "sha": "136bb5131214688ce705587ecbb0319f09b37c4c",
            "filename": "tests/models/mobilevitv2/test_modeling_mobilevitv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevitv2%2Ftest_modeling_mobilevitv2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -319,7 +319,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-1.6336e00, -7.3204e-02, -5.1883e-01]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_semantic_segmentation(self):\n@@ -349,7 +349,7 @@ def test_inference_semantic_segmentation(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_post_processing_semantic_segmentation(self):"
        },
        {
            "sha": "c7c34bf061508020a1019fb6414d77cd8c53214b",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -383,7 +383,7 @@ def test_inference_masked_lm(self):\n         expected_slice = torch.tensor(\n             [[[3.8387, -0.2017, 12.2839], [3.6300, 0.6869, 14.7123], [-5.1137, -3.8122, 11.9874]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head(self):\n@@ -405,7 +405,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[[0.3151, -0.6417, -0.7027], [-0.7834, -1.5810, 0.4576], [1.0614, -0.7268, -0.0871]]]\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_token_classification(self):\n@@ -428,7 +428,7 @@ def test_inference_token_classification(self):\n         expected = torch.tensor(\n             [[[2.0159, 4.6569], [-0.9430, 3.1595], [-3.8770, 3.2653], [1.5752, 4.5167], [-1.6939, 1.2524]]]\n         )\n-        self.assertTrue(torch.allclose(output, expected, atol=1e-4))\n+        torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_sequence_classification(self):\n@@ -451,7 +451,7 @@ def test_inference_sequence_classification(self):\n         self.assertEqual(output.shape, expected_shape)\n \n         expected = torch.tensor([[1.6466, 4.5662]])\n-        self.assertTrue(torch.allclose(output, expected, atol=1e-4))\n+        torch.testing.assert_close(output, expected, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_export(self):"
        },
        {
            "sha": "465ecec2083c07ce5391b2ad481cd333604a8e17",
            "filename": "tests/models/moonshine/test_modeling_moonshine.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoonshine%2Ftest_modeling_moonshine.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -489,7 +489,7 @@ def test_tiny_logits_single(self):\n             -8.0796, -7.3300, -7.3672, -6.8765, -7.6876, -7.2682, -6.9866, -6.7457, -7.6855, -7.3050,\n         ])\n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_base_logits_single(self):\n@@ -507,7 +507,7 @@ def test_base_logits_single(self):\n             -7.9310, -8.1024, -7.8699, -7.8231, -8.0752, -7.9764, -7.8127, -8.0536, -7.9492, -7.9290,\n         ])\n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0][0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_tiny_logits_batch(self):\n@@ -525,7 +525,7 @@ def test_tiny_logits_batch(self):\n             [-10.8078, 4.0030, -0.0633, -5.0505, -5.3906, -5.4590, -5.2420, -5.4746, -5.2665, -5.3158]\n         ])\n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_base_logits_batch(self):\n@@ -545,7 +545,7 @@ def test_base_logits_batch(self):\n         ])\n \n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0][:, :10].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_tiny_generation_single(self):"
        },
        {
            "sha": "adaf0fcc34acaf5b71954aef19fb3eba58647873",
            "filename": "tests/models/moshi/test_modeling_moshi.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmoshi%2Ftest_modeling_moshi.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -244,7 +244,7 @@ def test_resize_tokens_embeddings(self):\n             else:\n                 old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n                 new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n-            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, rtol=1e-3, atol=1e-3)\n \n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             if not is_deepspeed_zero3_enabled():\n@@ -344,7 +344,7 @@ def test_resize_tokens_embeddings(self):\n             else:\n                 old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n                 new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n-            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, rtol=1e-3, atol=1e-3)\n \n     @unittest.skip(reason=\"Some undefined behavior encountered with test versions of this model. Skip for now.\")\n     def test_cpu_offload(self):\n@@ -733,7 +733,7 @@ def test_left_padding_compatibility(self):\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n     @require_torch_sdpa\n     @slow\n@@ -810,8 +810,8 @@ def test_eager_matches_sdpa_generate(self):\n                     depth_decoder_do_sample=False,\n                 )\n \n-                self.assertTrue(torch.allclose(res_eager.sequences, res_sdpa.sequences))\n-                self.assertTrue(torch.allclose(res_eager.audio_sequences, res_sdpa.audio_sequences))\n+                torch.testing.assert_close(res_eager.sequences, res_sdpa.sequences)\n+                torch.testing.assert_close(res_eager.audio_sequences, res_sdpa.audio_sequences)\n \n     @pytest.mark.generate\n     def test_generate_without_input_ids(self):"
        },
        {
            "sha": "e71ec7566d85548d2348dbee3ac7e60aea2a7f4f",
            "filename": "tests/models/mpnet/test_modeling_mpnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpnet%2Ftest_modeling_mpnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -264,4 +264,4 @@ def test_inference_no_head(self):\n             [[[-0.0550, 0.1943, -0.0740], [-0.0562, 0.2211, -0.0579], [-0.0437, 0.3337, -0.0641]]]\n         )\n         # compare the actual values for a slice.\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "5507c127bb945ea795580df4a013a109d7cc39ad",
            "filename": "tests/models/mpt/test_modeling_mpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpt%2Ftest_modeling_mpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -513,4 +513,4 @@ def test_model_logits(self):\n         expected_slice = torch.Tensor([-0.2520, -0.2178, -0.1953]).to(torch_device, torch.bfloat16)\n         predicted_slice = outputs.hidden_states[-1][0, 0, :3]\n \n-        self.assertTrue(torch.allclose(expected_slice, predicted_slice, atol=1e-3, rtol=1e-3))\n+        torch.testing.assert_close(expected_slice, predicted_slice, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "cb5713bc2b592e30b80133b80cce996fc1a5fa81",
            "filename": "tests/models/mra/test_modeling_mra.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmra%2Ftest_modeling_mra.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -401,7 +401,7 @@ def test_inference_no_head(self):\n             [[[-0.0140, 0.0830, -0.0381], [0.1546, 0.1402, 0.0220], [0.1162, 0.0851, 0.0165]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_masked_lm(self):\n@@ -420,7 +420,7 @@ def test_inference_masked_lm(self):\n             [[[9.2595, -3.6038, 11.8819], [9.3869, -3.2693, 11.0956], [11.8524, -3.4938, 13.1210]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_masked_lm_long_input(self):\n@@ -439,4 +439,4 @@ def test_inference_masked_lm_long_input(self):\n             [[[5.4789, -2.3564, 7.5064], [7.9067, -1.3369, 9.9668], [9.0712, -1.8106, 7.0380]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "c1aba2838273d8385adb6fcd246ff39cc22d5f02",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1821,7 +1821,7 @@ def test_logits_text_prompt(self):\n         # fmt: on\n \n         self.assertTrue(logits.shape == (*decoder_input_ids.shape, model.decoder.config.vocab_size))\n-        self.assertTrue(torch.allclose(logits[0, 0, :16].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :16].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_logits_text_audio_prompt(self):\n@@ -1859,7 +1859,7 @@ def test_logits_text_audio_prompt(self):\n         # fmt: on\n \n         self.assertTrue(logits.shape == (8, 50, 2048))\n-        self.assertTrue(torch.allclose(logits[0, -1, :16].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0, -1, :16].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_unconditional_greedy(self):\n@@ -1881,7 +1881,7 @@ def test_generate_unconditional_greedy(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (1, 1, 3200))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_unconditional_sampling(self):\n@@ -1904,7 +1904,7 @@ def test_generate_unconditional_sampling(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_greedy(self):\n@@ -1931,7 +1931,7 @@ def test_generate_text_prompt_greedy(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :10].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :10].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_greedy_with_classifier_free_guidance(self):\n@@ -1958,7 +1958,7 @@ def test_generate_text_prompt_greedy_with_classifier_free_guidance(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_sampling(self):\n@@ -1986,7 +1986,7 @@ def test_generate_text_prompt_sampling(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_audio_prompt(self):\n@@ -2013,7 +2013,7 @@ def test_generate_text_audio_prompt(self):\n         self.assertTrue(\n             output_values.shape == (2, 1, 36480)\n         )  # input values take shape 32000 and we generate from there\n-        self.assertTrue(torch.allclose(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n \n @require_torch\n@@ -2053,8 +2053,8 @@ def test_generate_unconditional_greedy(self):\n \n         # (bsz, channels, seq_len)\n         self.assertTrue(output_values.shape == (1, 2, 5760))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT, atol=1e-4))\n-        self.assertTrue(torch.allclose(output_values[0, 1, :16].cpu(), EXPECTED_VALUES_RIGHT, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output_values[0, 1, :16].cpu(), EXPECTED_VALUES_RIGHT, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_audio_prompt(self):\n@@ -2088,5 +2088,5 @@ def test_generate_text_audio_prompt(self):\n         # (bsz, channels, seq_len)\n         self.assertTrue(output_values.shape == (2, 2, 37760))\n         # input values take shape 32000 and we generate from there - we check the last (generated) values\n-        self.assertTrue(torch.allclose(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES_LEFT, atol=1e-4))\n-        self.assertTrue(torch.allclose(output_values[0, 1, -16:].cpu(), EXPECTED_VALUES_RIGHT, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, -16:].cpu(), EXPECTED_VALUES_LEFT, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output_values[0, 1, -16:].cpu(), EXPECTED_VALUES_RIGHT, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "0066c08720d64f2493c238d1166b126f8c0f12de",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 16,
            "deletions": 12,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1799,7 +1799,7 @@ def test_logits_text_prompt(self):\n         )\n \n         self.assertTrue(logits.shape == logits_shape)\n-        self.assertTrue(torch.allclose(logits[0, -1, :16].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0, -1, :16].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_logits_text_audio_prompt(self):\n@@ -1841,7 +1841,7 @@ def test_logits_text_audio_prompt(self):\n         # fmt: on\n \n         self.assertTrue(logits.shape == (8, 240, 2048))\n-        self.assertTrue(torch.allclose(logits[1:3, -1, 32:40].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[1:3, -1, 32:40].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_unconditional_greedy(self):\n@@ -1863,7 +1863,7 @@ def test_generate_unconditional_greedy(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (1, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_unconditional_sampling(self):\n@@ -1888,7 +1888,7 @@ def test_generate_unconditional_sampling(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_greedy(self):\n@@ -1915,7 +1915,7 @@ def test_generate_text_prompt_greedy(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :10].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :10].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_greedy_with_classifier_free_guidance(self):\n@@ -1943,7 +1943,7 @@ def test_generate_text_prompt_greedy_with_classifier_free_guidance(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_prompt_sampling(self):\n@@ -1977,7 +1977,7 @@ def test_generate_text_prompt_sampling(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_text_audio_prompt(self):\n@@ -2002,7 +2002,7 @@ def test_generate_text_audio_prompt(self):\n         # fmt: on\n \n         self.assertTrue(output_values.shape == (2, 1, 4480))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, atol=1e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES, rtol=1e-4, atol=1e-4)\n \n \n @require_torch\n@@ -2039,8 +2039,8 @@ def test_generate_unconditional_greedy(self):\n \n         # (bsz, channels, seq_len)\n         self.assertTrue(output_values.shape == (1, 2, 5760))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT, atol=6e-4))\n-        self.assertTrue(torch.allclose(output_values[0, 1, :16].cpu(), EXPECTED_VALUES_LEFT, atol=6e-4))\n+        torch.testing.assert_close(output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT, rtol=6e-4, atol=6e-4)\n+        torch.testing.assert_close(output_values[0, 1, :16].cpu(), EXPECTED_VALUES_LEFT, rtol=6e-4, atol=6e-4)\n \n     @slow\n     def test_generate_text_audio_prompt(self):\n@@ -2071,5 +2071,9 @@ def test_generate_text_audio_prompt(self):\n \n         # (bsz, channels, seq_len)\n         self.assertTrue(output_values.shape == (2, 2, 5760))\n-        self.assertTrue(torch.allclose(output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT_FIRST_SAMPLE, atol=1e-4))\n-        self.assertTrue(torch.allclose(output_values[1, 1, :16].cpu(), EXPECTED_VALUES_RIGHT_SECOND_SAMPLE, atol=1e-4))\n+        torch.testing.assert_close(\n+            output_values[0, 0, :16].cpu(), EXPECTED_VALUES_LEFT_FIRST_SAMPLE, rtol=1e-4, atol=1e-4\n+        )\n+        torch.testing.assert_close(\n+            output_values[1, 1, :16].cpu(), EXPECTED_VALUES_RIGHT_SECOND_SAMPLE, rtol=1e-4, atol=1e-4\n+        )"
        },
        {
            "sha": "92aa2d27d73153b134a8274938dc1850f1bf1290",
            "filename": "tests/models/mvp/test_modeling_mvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmvp%2Ftest_modeling_mvp.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -571,7 +571,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.3461, 0.3624, 0.2689], [0.3461, 0.3624, 0.2689], [-0.1562, 1.1637, -0.3784]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-3))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_summarization_inference(self):"
        },
        {
            "sha": "851bb25edfb5fc0ad52ade2b34f9cee048989ed4",
            "filename": "tests/models/nllb_moe/test_modeling_nllb_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnllb_moe%2Ftest_modeling_nllb_moe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -519,7 +519,7 @@ def test_top_2_routing(self):\n         hidden_states = masked_hidden_states.sum(dim=0).reshape(self.batch_size, self.sequence_length, hidden_dim)\n \n         EXPECTED_MEAN_FAIRSEQ_HIDDEN_STATES = torch.Tensor([[ 7.0340e-04,  2.7997e-03, -1.3351e-02, -7.6705e-03, -3.5089e-03,3.9773e-03,  7.4593e-03,  1.2566e-02,  3.5860e-03, -2.7448e-02,-1.3731e-02, -1.0534e-02, -1.3606e-02, -1.5048e-02, -2.8914e-03,-5.0371e-03, -1.3963e-03,  6.0076e-03, -1.1380e-02, -1.4620e-02, 5.2401e-03,  8.4660e-04, -1.5319e-03, -1.6735e-02,  1.1302e-02, 3.6119e-03,  4.6084e-03, -1.3458e-02,  7.7792e-05,  1.4312e-02, 4.9107e-03, -5.0936e-03], [-4.4538e-03,  3.1026e-03,  1.4121e-04, -4.8121e-03, -5.6279e-03, 7.2493e-03,  3.9769e-03,  1.1114e-02, -1.5666e-03, -2.3477e-02, 8.7268e-03,  1.3446e-02, -2.8845e-05, -1.7287e-02,  8.7619e-03, -4.5316e-03, -1.2164e-02,  5.7461e-03, -4.5861e-03, -9.3907e-03, 2.9808e-02,  8.9206e-04, -7.6232e-04, -1.4173e-02,  3.0208e-03, 1.5310e-02,  9.7717e-03,  3.1014e-03,  7.8042e-03,  8.0197e-03, 3.4784e-03, -7.1728e-03]])  # fmt: skip\n-        self.assertTrue(torch.allclose(hidden_states.mean(1), EXPECTED_MEAN_FAIRSEQ_HIDDEN_STATES, 1e-4))\n+        torch.testing.assert_close(hidden_states.mean(1), EXPECTED_MEAN_FAIRSEQ_HIDDEN_STATES, atol=1e-4, rtol=1e-4)\n \n     def test_batch_prioritized_routing(self):\n         set_seed(0)"
        },
        {
            "sha": "f923a2f159e0066b5a1bb2067c10ac24151f2ecc",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -142,7 +142,7 @@ def test_expected_output(self):\n         dummy_image = self.image_processor_tester.prepare_dummy_image()\n         image_processor = self.image_processor\n         inputs = image_processor(dummy_image, return_tensors=\"pt\")\n-        self.assertTrue(torch.allclose(inputs[\"pixel_values\"].mean(), torch.tensor(0.4906), atol=1e-3, rtol=1e-3))\n+        torch.testing.assert_close(inputs[\"pixel_values\"].mean(), torch.tensor(0.4906), rtol=1e-3, atol=1e-3)\n \n     def test_crop_margin_all_white(self):\n         image = np.uint8(np.ones((100, 100, 3)) * 255)"
        },
        {
            "sha": "73da6cec18f2e78ca81a92ce6347c100f7c3387d",
            "filename": "tests/models/nystromformer/test_modeling_nystromformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnystromformer%2Ftest_modeling_nystromformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -304,7 +304,7 @@ def test_inference_no_head(self):\n             [[[-0.4532, -0.0936, 0.5137], [-0.2676, 0.0628, 0.6186], [-0.3629, -0.1726, 0.4716]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_masked_lm_end_to_end(self):"
        },
        {
            "sha": "22e85bc339d86eb9175fc4dc16031d5a2e3213ad",
            "filename": "tests/models/olmo/test_modeling_olmo.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo%2Ftest_modeling_olmo.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -339,7 +339,7 @@ def test_model_rope_scaling(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -356,10 +356,10 @@ def test_model_1b_logits(self):\n         out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[2.2869, 0.3315, 0.9876, 1.4146, 1.8804, 2.0430, 1.7055, 1.2065]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([2.5551, -1.1230, 11.0510, 12.4977, 7.9651, 7.2342, 6.1885, 7.8340, 9.9847, 12.6695, 12.2345, 10.7970, 8.4749, 14.2483, 12.9588, 13.9233, 11.0496, 5.5749, 7.4466, 7.7914, 6.8440, 5.8951, 4.8180, 4.1935, 4.5216, 4.7256, 3.9553, 12.2870, 12.4990, 8.1591])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_logits(self):\n@@ -368,10 +368,10 @@ def test_model_7b_logits(self):\n         out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[0.0271, 0.0249, -0.0578, -0.0870, 0.0167, 0.0710, 0.1002, 0.0677]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-1.7433, -1.6685, 7.4941, 6.1506, 0.1364, -0.1127, 1.3224, 4.5458, 4.2068, 5.8296, 7.4723, 2.7925, 3.1245, 10.8872, 10.0758, 10.6717, 7.0945, 1.2398, 3.6766, 4.2365, 2.5655, 2.2222, 1.7418, 0.5223, 0.7753, 1.0938, 0.6723, 6.2522, 6.2264, 1.8105])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_twin_2t_logits(self):\n@@ -380,10 +380,10 @@ def test_model_7b_twin_2t_logits(self):\n         out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-0.3636, -0.3825, -0.4800, -0.3696, -0.8388, -0.9737, -0.9849, -0.8356]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-2.0833, -1.9234, 8.7312, 7.8049, 1.0372, 0.8941, 3.1548, 1.8502, 5.5511, 5.5793, 8.1166, 4.5906, 1.8691, 11.6377, 8.9858, 11.6447, 7.4549, 1.4725, 2.8399, 2.7568, 1.4011, 1.6958, 0.5572, 0.5231, 0.3068, 0.5364, 0.6769, 7.9636, 8.2379, 1.7950])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_greedy_generation(self):"
        },
        {
            "sha": "ce2bd05193148c2dd5b2fc28d32d72649a9a9646",
            "filename": "tests/models/olmo2/test_modeling_olmo2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo2%2Ftest_modeling_olmo2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -338,7 +338,7 @@ def test_model_rope_scaling(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -357,10 +357,10 @@ def test_model_7b_logits(self):\n         EXPECTED_MEAN = torch.tensor(\n             [[-13.0244, -13.9564, -11.8270, -11.3047, -12.3794, -12.4215, -15.6030, -12.7962]]\n         )\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-5.3909, -13.9841, -13.6123, -14.5780, -13.9455, -13.2265, -13.4734, -11.9079, -9.2879, -12.6139, -11.4819, -5.9607, -11.9657, -6.3618, -11.1065, -7.3075, -6.5674, -6.7154, -7.3409, -7.9662, -8.0863, -8.1682, -8.7341, -8.7665, -8.8742, -9.7813, -8.0620, -12.5937, -7.6440, -11.3966])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_greedy_generation(self):"
        },
        {
            "sha": "c95b68625d0a870c17c9b3f5f8392841689698ae",
            "filename": "tests/models/olmoe/test_modeling_olmoe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmoe%2Ftest_modeling_olmoe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -352,7 +352,7 @@ def test_model_rope_scaling(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -369,10 +369,10 @@ def test_model_7b_logits(self):\n         out = model(torch.tensor(input_ids)).logits.float()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-1.3814, -3.4450, -2.2990, -1.9542, -2.4387, -2.7941, -2.9312, -2.8309]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([-2.3874, -2.4076, -2.4995, 4.2278, 1.4004, -0.0252, 0.4189, -2.7560, 0.3531, 1.6678, -0.7941, -1.1818, -0.2920, 0.7131, -1.4173, 1.6723, 0.5406, 0.1345, -0.1800, 0.2304, 1.2791, 0.7489, 0.6341, -0.0151, -1.3693, -1.2532, -2.3921, 0.7376, 1.6876, 0.5483])  # fmt: skip\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n \n     @slow\n     def test_model_7b_greedy_generation(self):"
        },
        {
            "sha": "e3996ade0396c847ab5f99598d35f0026a9c6785",
            "filename": "tests/models/omdet_turbo/test_modeling_omdet_turbo.py",
            "status": "modified",
            "additions": 30,
            "deletions": 22,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_modeling_omdet_turbo.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -701,8 +701,8 @@ def test_inference_object_detection_head(self):\n             [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n-        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.decoder_class_logits[:3, :3], expected_class_logits, rtol=1e-1, atol=1e-1)\n+        torch.testing.assert_close(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, rtol=1e-3, atol=1e-3)\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n@@ -712,8 +712,8 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 4)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n \n         expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n         self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n@@ -745,8 +745,8 @@ def test_inference_object_detection_head_fp16(self):\n             [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n         ).to(torch_device, dtype=torch.float16)\n \n-        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n-        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.decoder_class_logits[:3, :3], expected_class_logits, rtol=1e-1, atol=1e-1)\n+        torch.testing.assert_close(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, rtol=1e-3, atol=1e-3)\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n@@ -758,8 +758,8 @@ def test_inference_object_detection_head_fp16(self):\n         )\n \n         self.assertEqual(len(results[\"scores\"]), 4)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-1))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-1, atol=1e-1)\n \n         expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n         self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n@@ -787,8 +787,8 @@ def test_inference_object_detection_head_no_task(self):\n             [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n-        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.decoder_class_logits[:3, :3], expected_class_logits, rtol=1e-1, atol=1e-1)\n+        torch.testing.assert_close(outputs.decoder_coord_logits[:3, :3], expected_coord_logits, rtol=1e-3, atol=1e-3)\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n@@ -798,8 +798,8 @@ def test_inference_object_detection_head_no_task(self):\n         expected_slice_boxes = torch.tensor([39.8870, 70.3522, 176.7424, 118.0354]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 4)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-2))\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes, rtol=1e-2, atol=1e-2)\n \n         expected_text_labels = [\"remote\", \"cat\", \"remote\", \"cat\"]\n         self.assertListEqual(results[\"text_labels\"], expected_text_labels)\n@@ -831,8 +831,12 @@ def test_inference_object_detection_head_batched(self):\n             [[[0.2550, 0.5501, 0.4738]], [[0.2535, 0.6006, 0.0353]], [[0.3742, 0.3337, 0.0666]]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.decoder_class_logits[:, :1, :3], expected_class_logits, atol=1e-1))\n-        self.assertTrue(torch.allclose(outputs.decoder_coord_logits[:, :1, :3], expected_coord_logits, atol=1e-3))\n+        torch.testing.assert_close(\n+            outputs.decoder_class_logits[:, :1, :3], expected_class_logits, rtol=1e-1, atol=1e-1\n+        )\n+        torch.testing.assert_close(\n+            outputs.decoder_coord_logits[:, :1, :3], expected_coord_logits, rtol=1e-3, atol=1e-3\n+        )\n \n         # verify grounded postprocessing\n         results = processor.post_process_grounded_object_detection(\n@@ -851,11 +855,11 @@ def test_inference_object_detection_head_batched(self):\n         ).to(torch_device)\n \n         self.assertListEqual([len(result[\"scores\"]) for result in results], [4, 4, 6])\n-        self.assertTrue(\n-            torch.allclose(torch.stack([result[\"scores\"][0] for result in results]), expected_scores, atol=1e-2)\n+        torch.testing.assert_close(\n+            torch.stack([result[\"scores\"][0] for result in results]), expected_scores, rtol=1e-2, atol=1e-2\n         )\n-        self.assertTrue(\n-            torch.allclose(torch.stack([result[\"boxes\"][0, :] for result in results]), expected_slice_boxes, atol=1e-2)\n+        torch.testing.assert_close(\n+            torch.stack([result[\"boxes\"][0, :] for result in results]), expected_slice_boxes, rtol=1e-2, atol=1e-2\n         )\n \n         expected_text_labels = [\n@@ -889,8 +893,12 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n             [[[0.2550, 0.5501, 0.4738, 0.8745], [0.7695, 0.4121, 0.4603, 0.7244], [0.7691, 0.4117, 0.4603, 0.7214]]]\n         )\n \n-        self.assertTrue(torch.allclose(cpu_outputs.decoder_class_logits[:3, :3], expected_class_logits, atol=1e-1))\n-        self.assertTrue(torch.allclose(cpu_outputs.decoder_coord_logits[:3, :3], expected_coord_logits, atol=1e-3))\n+        torch.testing.assert_close(\n+            cpu_outputs.decoder_class_logits[:3, :3], expected_class_logits, rtol=1e-1, atol=1e-1\n+        )\n+        torch.testing.assert_close(\n+            cpu_outputs.decoder_coord_logits[:3, :3], expected_coord_logits, rtol=1e-3, atol=1e-3\n+        )\n \n         # verify grounded postprocessing\n         results_cpu = processor.post_process_grounded_object_detection(\n@@ -900,5 +908,5 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n             gpu_outputs, text_labels=[text_labels], target_sizes=[image.size[::-1]]\n         )[0]\n \n-        self.assertTrue(torch.allclose(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), atol=1e-2))\n-        self.assertTrue(torch.allclose(results_cpu[\"boxes\"][0, :], result_gpu[\"boxes\"][0, :].cpu(), atol=1e-2))\n+        torch.testing.assert_close(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(results_cpu[\"boxes\"][0, :], result_gpu[\"boxes\"][0, :].cpu(), rtol=1e-2, atol=1e-2)"
        },
        {
            "sha": "66dbc4eb017647330e5db65983a3041109aaa04e",
            "filename": "tests/models/omdet_turbo/test_processor_omdet_turbo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processor_omdet_turbo.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -106,10 +106,10 @@ def test_post_process_grounded_object_detection(self):\n         self.assertEqual(post_processed[0][\"boxes\"].shape, (self.num_queries, 4))\n         self.assertEqual(post_processed[0][\"scores\"].shape, (self.num_queries,))\n         expected_scores = torch.tensor([0.7310, 0.6579, 0.6513, 0.6444, 0.6252])\n-        self.assertTrue(torch.allclose(post_processed[0][\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(post_processed[0][\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n \n         expected_box_slice = torch.tensor([14.9657, 141.2052, 30.0000, 312.9670])\n-        self.assertTrue(torch.allclose(post_processed[0][\"boxes\"][0], expected_box_slice, atol=1e-4))\n+        torch.testing.assert_close(post_processed[0][\"boxes\"][0], expected_box_slice, rtol=1e-4, atol=1e-4)\n \n     def test_save_load_pretrained_additional_features(self):\n         processor = OmDetTurboProcessor(tokenizer=self.get_tokenizer(), image_processor=self.get_image_processor())"
        },
        {
            "sha": "8f1df74ea6274103678334f9d70426aba179c3f2",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -576,7 +576,7 @@ def test_inference_universal_segmentation_head(self):\n         )\n         expected_slice = [[[3.1848, 4.2141, 4.1993], [2.9000, 3.5721, 3.6603], [2.5358, 3.0883, 3.6168]]]\n         expected_slice = torch.tensor(expected_slice).to(torch_device)\n-        self.assertTrue(torch.allclose(masks_queries_logits[0, 0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(masks_queries_logits[0, 0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n         # class_queries_logits\n         class_queries_logits = outputs.class_queries_logits\n         self.assertEqual(\n@@ -586,7 +586,7 @@ def test_inference_universal_segmentation_head(self):\n         expected_slice = torch.tensor(\n             [[3.0668, -1.1833, -5.1103], [3.3440, -3.3620, -5.1101], [2.6017, -4.3613, -4.1444]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(class_queries_logits[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(class_queries_logits[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     @require_torch_accelerator\n     @require_torch_fp16"
        },
        {
            "sha": "2a9d9f859cc96511c5ce5cbbfe0859c1d2304f0b",
            "filename": "tests/models/oneformer/test_processor_oneformer.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Foneformer%2Ftest_processor_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Foneformer%2Ftest_processor_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_processor_oneformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -504,9 +504,9 @@ def create_panoptic_map(annotation, segments_info):\n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n         expected_class_labels = torch.tensor([4, 17, 32, 42, 12, 3, 5, 0, 43, 96, 104, 31, 125, 138, 87, 149])  # noqa: E231  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], expected_class_labels)\n         expected_class_labels = torch.tensor([19, 67, 82, 17, 12, 42, 3, 14, 5, 0, 115, 43, 8, 138, 125, 143])  # noqa: E231  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n \n         # verify the task inputs\n         self.assertEqual(len(inputs[\"task_inputs\"]), 2)\n@@ -592,9 +592,9 @@ def create_panoptic_map(annotation, segments_info):\n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n         expected_class_labels = torch.tensor([32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 43, 43, 43, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], expected_class_labels)\n         expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 12, 12, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n \n         # verify the task inputs\n         self.assertEqual(len(inputs[\"task_inputs\"]), 2)\n@@ -680,9 +680,9 @@ def create_panoptic_map(annotation, segments_info):\n         # verify the class labels\n         self.assertEqual(len(inputs[\"class_labels\"]), 2)\n         expected_class_labels = torch.tensor([4, 17, 32, 42, 42, 42, 42, 42, 42, 42, 32, 12, 12, 12, 12, 12, 42, 42, 12, 12, 12, 42, 12, 12, 12, 12, 12, 3, 12, 12, 12, 12, 42, 42, 42, 12, 42, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 12, 5, 12, 12, 12, 12, 12, 12, 12, 0, 43, 43, 43, 96, 43, 104, 43, 31, 125, 31, 125, 138, 87, 125, 149, 138, 125, 87, 87])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][0], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][0], expected_class_labels)\n         expected_class_labels = torch.tensor([19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 67, 82, 19, 19, 17, 19, 19, 19, 19, 19, 19, 19, 19, 19, 12, 12, 42, 12, 12, 12, 12, 3, 14, 12, 12, 12, 12, 12, 12, 12, 12, 14, 5, 12, 12, 0, 115, 43, 43, 115, 43, 43, 43, 8, 8, 8, 138, 138, 125, 143])  # fmt: skip\n-        self.assertTrue(torch.allclose(inputs[\"class_labels\"][1], expected_class_labels))\n+        torch.testing.assert_close(inputs[\"class_labels\"][1], expected_class_labels)\n \n         # verify the task inputs\n         self.assertEqual(len(inputs[\"task_inputs\"]), 2)"
        },
        {
            "sha": "e6aff1c7021cfd4c96fc10fc4c41b6f75743aadf",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -826,7 +826,7 @@ def test_inference(self):\n             torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n         )\n         expected_logits = torch.tensor([[-6.2229, -8.2601]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -858,7 +858,7 @@ def test_inference_interpolate_pos_encoding(self):\n             torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n         )\n         expected_logits = torch.tensor([[-6.2520, -8.2970]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n         expected_shape = torch.Size((1, 4097, 768))\n         self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n \n@@ -874,7 +874,7 @@ def test_inference_interpolate_pos_encoding(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.2407, 0.0553, 0.4636], [0.1082, 0.0494, 0.1861], [0.2459, 0.0527, 0.4398]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         model = Owlv2ForObjectDetection.from_pretrained(model_name).to(torch_device)\n         query_image = prepare_img()\n@@ -920,7 +920,7 @@ def test_inference_interpolate_pos_encoding(self):\n             ]\n         )\n \n-        self.assertTrue(torch.allclose(model.box_bias[:3, :4], expected_default_box_bias, atol=1e-4))\n+        torch.testing.assert_close(model.box_bias[:3, :4], expected_default_box_bias, rtol=1e-4, atol=1e-4)\n \n         # Interpolate with any resolution size.\n         processor.image_processor.size = {\"height\": 1264, \"width\": 1024}\n@@ -945,7 +945,7 @@ def test_inference_interpolate_pos_encoding(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.2438, 0.0945, 0.4675], [0.1361, 0.0431, 0.2406], [0.2465, 0.0428, 0.4429]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         query_image = prepare_img()\n         inputs = processor(\n@@ -992,13 +992,11 @@ def test_inference_object_detection(self):\n         expected_slice_logits = torch.tensor(\n             [[-21.413497, -21.612638], [-19.008193, -19.548841], [-20.958896, -21.382694]]\n         ).to(torch_device)\n-        resulted_slice_logits = outputs.logits[0, :3, :3]\n-        max_diff = torch.max(torch.abs(resulted_slice_logits - expected_slice_logits)).item()\n-        self.assertLess(max_diff, 3e-4)\n-\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n         expected_slice_boxes = torch.tensor(\n             [[0.241309, 0.051896, 0.453267], [0.139474, 0.045701, 0.250660], [0.233022, 0.050479, 0.427671]],\n         ).to(torch_device)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n         resulted_slice_boxes = outputs.pred_boxes[0, :3, :3]\n         max_diff = torch.max(torch.abs(resulted_slice_boxes - expected_slice_boxes)).item()\n         self.assertLess(max_diff, 3e-4)\n@@ -1044,7 +1042,7 @@ def test_inference_one_shot_object_detection(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.2413, 0.0519, 0.4533], [0.1395, 0.0457, 0.2507], [0.2330, 0.0505, 0.4277]],\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.target_pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.target_pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_torch_accelerator"
        },
        {
            "sha": "81034df4cbc9e0d8ebafd05b026e8ef9185f2d1b",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -819,7 +819,7 @@ def test_inference(self):\n             torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n         )\n         expected_logits = torch.tensor([[3.4613, 0.9403]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -851,7 +851,7 @@ def test_inference_interpolate_pos_encoding(self):\n             torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n         )\n         expected_logits = torch.tensor([[3.6278, 0.8861]], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n \n         expected_shape = torch.Size((1, 626, 768))\n         self.assertEqual(outputs.vision_model_output.last_hidden_state.shape, expected_shape)\n@@ -868,7 +868,7 @@ def test_inference_interpolate_pos_encoding(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.0680, 0.0422, 0.1347], [0.2071, 0.0450, 0.4146], [0.2000, 0.0418, 0.3476]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         model = OwlViTForObjectDetection.from_pretrained(model_name).to(torch_device)\n         query_image = prepare_img()\n@@ -913,7 +913,7 @@ def test_inference_interpolate_pos_encoding(self):\n                 [-1.9452, -3.1332, -3.1332, -3.1332],\n             ]\n         )\n-        self.assertTrue(torch.allclose(model.box_bias[:3, :4], expected_default_box_bias, atol=1e-4))\n+        torch.testing.assert_close(model.box_bias[:3, :4], expected_default_box_bias, rtol=1e-4, atol=1e-4)\n \n         # Interpolate with any resolution size.\n         processor.image_processor.size = {\"height\": 1264, \"width\": 1024}\n@@ -938,7 +938,7 @@ def test_inference_interpolate_pos_encoding(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.0499, 0.0301, 0.0983], [0.2244, 0.0365, 0.4663], [0.1387, 0.0314, 0.1859]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         query_image = prepare_img()\n         inputs = processor(\n@@ -985,7 +985,7 @@ def test_inference_object_detection(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         # test post-processing\n         post_processed_output = processor.post_process_grounded_object_detection(outputs)\n@@ -1028,7 +1028,7 @@ def test_inference_one_shot_object_detection(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.0691, 0.0445, 0.1373], [0.1592, 0.0456, 0.3192], [0.1632, 0.0423, 0.2478]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.target_pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.target_pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_torch_accelerator"
        },
        {
            "sha": "7c72b03a5d5ad3dfc8992da14fed6ad3bec3ee2e",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -232,7 +232,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n     def test_mismatching_num_image_tokens(self):"
        },
        {
            "sha": "7c8e0be72be14c2b280b137e71d761d4fe666b8c",
            "filename": "tests/models/patchtsmixer/test_modeling_patchtsmixer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtsmixer%2Ftest_modeling_patchtsmixer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -483,7 +483,7 @@ def test_pretrain_head(self):\n         self.assertEqual(output.shape, expected_shape)\n \n         expected_slice = torch.tensor([[[[-0.9106]],[[1.5326]],[[-0.8245]],[[0.7439]],[[-0.7830]],[[2.6256]],[[-0.6485]],]],device=torch_device)  # fmt: skip\n-        self.assertTrue(torch.allclose(output[0, :7, :1, :1], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :7, :1, :1], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_forecasting_head(self):\n         model = PatchTSMixerForPrediction.from_pretrained(\"ibm/patchtsmixer-etth1-forecasting\").to(torch_device)\n@@ -504,7 +504,7 @@ def test_forecasting_head(self):\n             [[0.2471, 0.5036, 0.3596, 0.5401, -0.0985, 0.3423, -0.8439]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output[0, :1, :7], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :1, :7], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_prediction_generation(self):\n         model = PatchTSMixerForPrediction.from_pretrained(\"ibm/patchtsmixer-etth1-generate\").to(torch_device)\n@@ -526,7 +526,7 @@ def test_prediction_generation(self):\n \n         mean_prediction = outputs.sequences.mean(dim=1)\n \n-        self.assertTrue(torch.allclose(mean_prediction[0, -1:], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(mean_prediction[0, -1:], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n \n @require_torch"
        },
        {
            "sha": "0f6f019dc3ef538336bef0888281e9aaffaca747",
            "filename": "tests/models/patchtst/test_modeling_patchtst.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpatchtst%2Ftest_modeling_patchtst.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -329,7 +329,7 @@ def test_pretrain_head(self):\n             [[[-0.0173]], [[-1.0379]], [[-0.1030]], [[0.3642]], [[0.1601]], [[-1.3136]], [[0.8780]]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output[0, :7, :1, :1], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :7, :1, :1], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     # Publishing of pretrained weights are under internal review. Pretrained model is not yet downloadable.\n     def test_prediction_head(self):\n@@ -349,7 +349,7 @@ def test_prediction_head(self):\n             [[0.5142, 0.6928, 0.6118, 0.5724, -0.3735, -0.1336, -0.7124]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output[0, :1, :7], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :1, :7], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_prediction_generation(self):\n         model = PatchTSTForPrediction.from_pretrained(\"namctin/patchtst_etth1_forecast\").to(torch_device)\n@@ -367,7 +367,7 @@ def test_prediction_generation(self):\n             device=torch_device,\n         )\n         mean_prediction = outputs.sequences.mean(dim=1)\n-        self.assertTrue(torch.allclose(mean_prediction[0, -1:], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(mean_prediction[0, -1:], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_regression_generation(self):\n         model = PatchTSTForRegression.from_pretrained(\"ibm/patchtst-etth1-regression-distribution\").to(torch_device)\n@@ -385,4 +385,4 @@ def test_regression_generation(self):\n             device=torch_device,\n         )\n         mean_prediction = outputs.sequences.mean(dim=1)\n-        self.assertTrue(torch.allclose(mean_prediction[-5:], expected_slice, rtol=TOLERANCE))\n+        torch.testing.assert_close(mean_prediction[-5:], expected_slice, rtol=TOLERANCE)"
        },
        {
            "sha": "2463b7ab2613ed3d195c32bb9b69ab50256ba7c5",
            "filename": "tests/models/pegasus_x/test_modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus_x%2Ftest_modeling_pegasus_x.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -595,7 +595,7 @@ def test_inference_no_head(self):\n             [[0.0702, -0.1552, 0.1192], [0.0836, -0.1848, 0.1304], [0.0673, -0.1686, 0.1045]], device=torch_device\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base\").to(torch_device)\n@@ -612,7 +612,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[0.0, 9.5705185, 1.5897303], [0.0, 9.833374, 1.5828674], [0.0, 10.429961, 1.5643371]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         hf = PegasusXForConditionalGeneration.from_pretrained(\"google/pegasus-x-base-arxiv\").to(torch_device)"
        },
        {
            "sha": "e6bcb930ec61de5ca850298ca196494cf8fe2f06",
            "filename": "tests/models/perceiver/test_modeling_perceiver.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fperceiver%2Ftest_modeling_perceiver.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -683,7 +683,7 @@ def test_feed_forward_chunking(self):\n                         torch.allclose(hidden_states_no_chunk[modality], hidden_states_with_chunk[modality], atol=1e-3)\n                     )\n             else:\n-                self.assertTrue(torch.allclose(hidden_states_no_chunk, hidden_states_with_chunk, atol=1e-3))\n+                torch.testing.assert_close(hidden_states_no_chunk, hidden_states_with_chunk, rtol=1e-3, atol=1e-3)\n \n     def test_save_load(self):\n         for model_class in self.all_model_classes:\n@@ -909,7 +909,7 @@ def test_inference_masked_lm(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_greedy_predictions = [38, 115, 111, 121, 121, 111, 116, 109, 52]\n         masked_tokens_predictions = logits[0, 52:61].argmax(dim=-1).tolist()\n@@ -938,7 +938,7 @@ def test_inference_image_classification(self):\n         expected_slice = torch.tensor([-1.1652, -0.1992, -0.7520], device=torch_device)\n \n         atol = 1e-3 if IS_ROCM_SYSTEM else 1e-4\n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=atol))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=atol, atol=atol)\n \n     @slow\n     def test_inference_image_classification_fourier(self):\n@@ -962,7 +962,7 @@ def test_inference_image_classification_fourier(self):\n \n         expected_slice = torch.tensor([-1.1295, -0.2832, 0.3226], device=torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_image_classification_conv(self):\n@@ -986,7 +986,7 @@ def test_inference_image_classification_conv(self):\n \n         expected_slice = torch.tensor([-1.1186, 0.0554, 0.0897], device=torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_optical_flow(self):\n@@ -1030,7 +1030,7 @@ def test_inference_optical_flow(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "c8725a5badce13ff2bee60ed134ac01991fbf60c",
            "filename": "tests/models/persimmon/test_modeling_persimmon.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpersimmon%2Ftest_modeling_persimmon.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -410,7 +410,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -483,14 +483,14 @@ def test_model_8b_chat_logits(self):\n             [[-11.4726, -11.1495, -11.2694, -11.2223, -10.9452, -11.0663, -11.0031, -11.1028]]\n         )\n         # change dtype to `torch.float32` before calling `mean` to avoid `nan` values\n-        torch.testing.assert_close(out.cpu().to(torch.float32).mean(-1), EXPECTED_MEAN, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(out.cpu().to(torch.float32).mean(-1), EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n         # fmt: off\n         EXPECTED_SLICE = torch.tensor(\n             [-16.9062, -16.9062, -16.9062, -16.9062, -16.8906, -16.9062, -16.9531, -16.9062, -16.9062, -16.9062, -16.9531, -16.9062, -16.9531, -16.9062, -16.9062, -16.9062, -16.9062, -16.9062, -16.9531, -16.9062, -16.9062, -16.9062, -16.9062, -16.9062, -16.9062, -16.9531, -16.9062, -16.9531, -16.9062, -16.9062],\n             dtype=torch.float16\n         )\n         # fmt: on\n-        torch.testing.assert_close(out.cpu()[0, 0, :30], EXPECTED_SLICE, atol=1e-5, rtol=1e-5)\n+        torch.testing.assert_close(out.cpu()[0, 0, :30], EXPECTED_SLICE, rtol=1e-5, atol=1e-5)\n \n         backend_empty_cache(torch_device)\n         del model"
        },
        {
            "sha": "c54d4ebee5c33936b828cdc9c141dfd5e5ebf329",
            "filename": "tests/models/phi/test_modeling_phi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi%2Ftest_modeling_phi.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -389,7 +389,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -463,7 +463,7 @@ def test_model_phi_1_logits(self):\n \n         EXPECTED_OUTPUT = torch.tensor([[2.2671,  6.7684, -2.0107, -1.2440, -1.5335, -2.3828,  6.9186,  6.4245, 3.1548,  0.9998,  0.0760,  4.4653,  4.9857,  4.2956,  1.2308, -1.4178, 0.1361,  0.5191, -0.5699, -2.2201, -3.0750, -3.9600, -4.5936, -3.7394, -2.7777,  6.1874, -0.4148, -1.5684, -0.5967,  0.2395], [1.7004,  4.0383,  0.0546,  0.4530, -0.3619, -0.9021,  1.8355,  1.3587, 1.2406,  2.5775, -0.8834,  5.1910,  4.2565,  4.1406,  3.0752, -0.9099, 1.1595,  0.0264,  0.3243, -1.1803, -1.3945, -2.1406, -3.9939, -1.4438, -2.9546,  3.9204,  1.0851, -1.0598, -1.7819, -0.4827]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n \n     def test_model_phi_1_5_logits(self):\n         input_ids = {\n@@ -479,7 +479,7 @@ def test_model_phi_1_5_logits(self):\n \n         EXPECTED_OUTPUT = torch.tensor([[12.2922, 13.3507,  8.6963,  9.1355,  9.3502,  9.2667, 14.2027, 13.1363, 13.5446, 11.1337,  9.9279, 16.7195, 13.0768, 14.9141, 11.9965,  8.0233, 10.3129, 10.6118, 10.0204,  9.3827,  8.8344,  8.2806,  8.0153,  8.0540, 7.0964, 16.5743, 11.1256,  9.6987, 11.4770, 10.5440], [12.3323, 14.6050,  8.9986,  8.1580,  9.5654,  6.6728, 12.5966, 12.6662, 12.2784, 11.7522,  8.2039, 16.3102, 11.2203, 13.6088, 12.0125,  9.1021, 9.8216, 10.0987,  9.0926,  8.4260,  8.8009,  7.6547,  6.8075,  7.7881, 7.4501, 15.7451, 10.5053,  8.3129, 10.0027,  9.2612]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n \n     def test_model_phi_2_logits(self):\n         input_ids = {\n@@ -495,7 +495,7 @@ def test_model_phi_2_logits(self):\n \n         EXPECTED_OUTPUT = torch.tensor([[6.4830,  6.1644,  3.4055,  2.2848,  5.4654,  2.8360,  5.5975,  5.5391, 7.3101,  4.2498,  2.5913, 10.3885,  6.4359,  8.7982,  5.6534,  0.5150, 2.7498,  3.1930,  2.4334,  1.7781,  1.5613,  1.3067,  0.8291,  0.5633, 0.6522,  9.8191,  5.5771,  2.7987,  4.2845,  3.7030], [6.0642,  7.8242,  3.4634,  1.9259,  4.3169,  2.0913,  6.0446,  3.6804, 6.6736,  4.0727,  2.1791, 11.4139,  5.6795,  7.5652,  6.2039,  2.7174, 4.3266,  3.6930,  2.8058,  2.6721,  2.3047,  2.0848,  2.0972,  2.0441, 1.3160,  9.2085,  4.5557,  3.0296,  2.6045,  2.4059]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-3, rtol=1e-3))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-3, atol=1e-3)\n \n     def test_phi_2_generation(self):\n         model = PhiForCausalLM.from_pretrained(\"microsoft/phi-2\")"
        },
        {
            "sha": "1b27178587847f0aab6e0599b87df51cc5ef3e9a",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -491,7 +491,7 @@ def test_model_rope_scaling_short_long_factor(self, scaling_type):\n         # KV cache is re-computed after reaching the (`config.original_max_position_embeddings`+1)th token position\n         self.assertFalse(torch.allclose(keys_with_short_factor, keys_with_long_factor, atol=1e-2, rtol=1e-2))\n         # Last token generated using long factor\n-        self.assertTrue(torch.allclose(last_token_logits, regenerated_last_token_logits, atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(last_token_logits, regenerated_last_token_logits, rtol=1e-2, atol=1e-2)\n \n \n @slow\n@@ -511,7 +511,7 @@ def test_model_phi3_mini_4k_instruct_logits(self):\n \n         EXPECTED_OUTPUT = torch.tensor([[ 0.9979, -1.9449, -2.5613, -2.2110, -0.9323, -2.2726, -3.2468, -2.0122,-1.0021, -1.2764, -1.0876, -1.2358,  3.9385,  6.2152, -0.3695, -2.3285,-1.2907, -1.8238, -1.9941, -2.2098, -0.6923, -1.6793, -1.1660, -2.0469,-0.7369, -1.4101, -1.4091, -3.1694, -1.8383, -1.1952],[ 3.0525,  1.9178,  3.7016,  0.9263,  0.3397,  1.9584,  2.1347,  0.3482, 1.3773,  0.2153,  0.2798,  0.8360,  9.0936, 11.4944, -0.3575, -0.9442,-0.1246,  1.3869,  0.9846,  1.7243,  0.9150,  1.0823,  0.4313,  1.5742, 0.2566, -0.1401, -1.3019,  0.4967,  0.6941,  0.7214]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n \n     def test_phi3_mini_4k_instruct_generation(self):\n         model = Phi3ForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n@@ -572,7 +572,7 @@ def test_model_phi3_mini_128k_instruct_logits(self):\n \n         EXPECTED_OUTPUT = torch.tensor([[ 1.8478, -0.5709, -1.6792, -1.2133, -0.7809, -0.8817, -2.0969, -1.1191,-0.7731, -1.0483, -0.5961, -1.3067,  3.1325,  6.9442, -0.4803, -0.9154,-1.3085, -1.0822, -1.1433, -0.7660, -0.8531, -0.9150, -0.6179, -1.6153,-0.2239, -1.3207, -1.1187, -2.4795, -1.4733, -0.4931],[ 3.5839,  2.4722,  3.7130,  1.2032,  0.7356,  2.7777,  2.5256,  0.9157, 1.6431,  0.3533,  0.5100,  1.3512,  8.9873, 10.9815,  0.3530,  0.1473, 0.2051,  1.8553,  1.5988,  2.2268,  1.1897,  1.2829,  0.7894,  1.8895, 0.7666,  0.4122, -0.9316,  0.9936,  1.2722,  0.8263]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n \n     def test_phi3_mini_128k_instruct_generation(self):\n         model = Phi3ForCausalLM.from_pretrained(\"microsoft/phi-3-mini-128k-instruct\")"
        },
        {
            "sha": "9ce4ae0091042fb0c1ba0c811bfee3c731ce0db3",
            "filename": "tests/models/phimoe/test_modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphimoe%2Ftest_modeling_phimoe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -491,7 +491,7 @@ def test_model_rope_scaling_short_long_factor(self, scaling_type):\n         # KV cache is re-computed after reaching the (`config.original_max_position_embeddings`+1)th token position\n         self.assertFalse(torch.allclose(keys_with_short_factor, keys_with_long_factor, atol=1e-3, rtol=1e-3))\n         # Last token generated using long factor\n-        self.assertTrue(torch.allclose(last_token_logits, regenerated_last_token_logits, atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(last_token_logits, regenerated_last_token_logits, rtol=1e-2, atol=1e-2)\n \n \n @slow\n@@ -518,7 +518,7 @@ def test_model_phimoe_instruct_logits(self):\n          -4.9375,  0.7148, -0.0972,  1.7656, -0.0801,  0.2217,  0.1875, -0.4629,\n           1.5781,  0.3535,  0.0874,  0.6836, -0.0518, -1.2969]]).to(torch_device)  # fmt: skip\n \n-        self.assertTrue(torch.allclose(EXPECTED_OUTPUT, output[0, :2, :30], atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(EXPECTED_OUTPUT, output[0, :2, :30], rtol=1e-4, atol=1e-4)\n \n     def test_phimoe_instruct_generation(self):\n         model = PhimoeForCausalLM.from_pretrained(\"microsoft/Phi-3.5-MoE-instruct\")"
        },
        {
            "sha": "2650b3503b59903fe9d1b64467968616e3df9353",
            "filename": "tests/models/pix2struct/test_image_processing_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -106,7 +106,7 @@ def test_expected_patches(self):\n         max_patch = 2048\n \n         inputs = image_processor(dummy_image, return_tensors=\"pt\", max_patches=max_patch)\n-        self.assertTrue(torch.allclose(inputs.flattened_patches.mean(), torch.tensor(0.0606), atol=1e-3, rtol=1e-3))\n+        torch.testing.assert_close(inputs.flattened_patches.mean(), torch.tensor(0.0606), rtol=1e-3, atol=1e-3)\n \n     def test_call_pil(self):\n         # Initialize image_processor"
        },
        {
            "sha": "19bfde038f2a5252f1fae3c9267dfbfd82bbfead",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -281,7 +281,9 @@ def test_slow_fast_equivalence(self):\n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0], atol=1e-2))\n+        torch.testing.assert_close(\n+            encoding_slow.pixel_values[0][0], encoding_fast.pixel_values[0][0], rtol=1e-2, atol=1e-2\n+        )\n \n     @slow\n     @require_torch_gpu\n@@ -300,7 +302,9 @@ def test_can_compile_fast_image_processor(self):\n         image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n         output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(output_eager.pixel_values[0][0], output_compiled.pixel_values[0][0], atol=1e-4))\n+        torch.testing.assert_close(\n+            output_eager.pixel_values[0][0], output_compiled.pixel_values[0][0], rtol=1e-4, atol=1e-4\n+        )\n \n     @unittest.skip(reason=\"PixtralImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")  # FIXME Amy\n     def test_call_numpy_4_channels(self):"
        },
        {
            "sha": "775df97cde303658e29ee554236b32474f08c34a",
            "filename": "tests/models/poolformer/test_modeling_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpoolformer%2Ftest_modeling_poolformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -236,4 +236,4 @@ def test_inference_image_classification_head(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([-0.6113, 0.1685, -0.0492]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "2a1a767c2c6371409a99a110c2fd36db7dcaf9c5",
            "filename": "tests/models/pop2piano/test_feature_extraction_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_feature_extraction_pop2piano.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -148,7 +148,7 @@ def test_integration(self):\n         EXPECTED_INPUT_FEATURES = torch.tensor(\n             [[-7.1493, -6.8701, -4.3214], [-5.9473, -5.7548, -3.8438], [-6.1324, -5.9018, -4.3778]]\n         )\n-        self.assertTrue(torch.allclose(input_features[0, :3, :3], EXPECTED_INPUT_FEATURES, atol=1e-4))\n+        torch.testing.assert_close(input_features[0, :3, :3], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)\n \n     def test_attention_mask(self):\n         feature_extractor = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())"
        },
        {
            "sha": "47cf47f6adaa58187ae41de6e937ff5026aded13",
            "filename": "tests/models/pop2piano/test_modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_modeling_pop2piano.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -691,7 +691,7 @@ def test_mel_conditioner_integration(self):\n             [[1.0475305318832397, 0.29052114486694336, -0.47778210043907166], [1.0, 1.0, 1.0], [1.0, 1.0, 1.0]]\n         )\n \n-        self.assertTrue(torch.allclose(outputs[0, :3, :3], EXPECTED_OUTPUTS, atol=1e-4))\n+        torch.testing.assert_close(outputs[0, :3, :3], EXPECTED_OUTPUTS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_essentia"
        },
        {
            "sha": "6ee6721d81c730021dbd45fa74a046bc498df6bd",
            "filename": "tests/models/pop2piano/test_tokenization_pop2piano.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_tokenization_pop2piano.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -87,8 +87,8 @@ def test_call(self):\n         )\n         expected_output_attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 0, 0, 0, 0]])\n \n-        self.assertTrue(torch.allclose(output[\"token_ids\"], expected_output_token_ids, atol=1e-4))\n-        self.assertTrue(torch.allclose(output[\"attention_mask\"], expected_output_attention_mask, atol=1e-4))\n+        torch.testing.assert_close(output[\"token_ids\"], expected_output_token_ids, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(output[\"attention_mask\"], expected_output_attention_mask, rtol=1e-4, atol=1e-4)\n \n     def test_batch_decode(self):\n         # test batch decode with model, feature-extractor outputs(beatsteps, extrapolated_beatstep)\n@@ -174,7 +174,7 @@ def test_batch_decode_outputs(self):\n         )\n         predicted_start_timings = torch.tensor(predicted_start_timings)\n \n-        self.assertTrue(torch.allclose(expected_start_timings, predicted_start_timings, atol=1e-4))\n+        torch.testing.assert_close(expected_start_timings, predicted_start_timings, rtol=1e-4, atol=1e-4)\n \n         # Checking note end timings\n         expected_end_timings = torch.tensor(\n@@ -187,7 +187,7 @@ def test_batch_decode_outputs(self):\n         )\n         predicted_end_timings = torch.tensor(predicted_end_timings)\n \n-        self.assertTrue(torch.allclose(expected_end_timings, predicted_end_timings, atol=1e-4))\n+        torch.testing.assert_close(expected_end_timings, predicted_end_timings, rtol=1e-4, atol=1e-4)\n \n     def test_get_vocab(self):\n         vocab_dict = self.tokenizer.get_vocab()"
        },
        {
            "sha": "1f86a7662c8fd69b6bfb42cac8645bb5393c9ecb",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1227,7 +1227,7 @@ def test_pretrained_checkpoint_hidden_states(self):\n         expected_slice = torch.tensor(\n             [[[-7.7729, -8.0343, -8.26001], [-7.74213, -7.8629, -8.6000], [-7.7328, -7.8269, -8.5264]]]\n         ).to(torch_device)\n-        #        self.assertTrue(torch.allclose(output_predited_logits[:, :3, :3], expected_slice, atol=1e-4))\n+        #        torch.testing.assert_close(output_predited_logits[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n         assert torch.allclose(output_predited_logits[:, :3, :3], expected_slice, atol=1e-4)\n \n         # encoder outputs\n@@ -1237,15 +1237,15 @@ def test_pretrained_checkpoint_hidden_states(self):\n         ).to(torch_device)\n         expected_shape_encoder = torch.Size((1, 28, 1024))\n         self.assertEqual(encoder_outputs.shape, expected_shape_encoder)\n-        #        self.assertTrue(torch.allclose(encoder_outputs[:, :3, :3], expected_encoder_outputs_slice, atol=1e-4))\n+        #        torch.testing.assert_close(encoder_outputs[:, :3, :3], expected_encoder_outputs_slice, rtol=1e-4, atol=1e-4)\n         assert torch.allclose(encoder_outputs[:, :3, :3], expected_encoder_outputs_slice, atol=1e-4)\n \n         # decoder outputs\n         decoder_outputs = model.prophetnet.decoder(decoder_prev_ids, encoder_hidden_states=encoder_outputs)\n         predicting_streams = decoder_outputs[1].view(1, model.config.ngram, 12, -1)\n         predicting_streams_logits = model.lm_head(predicting_streams)\n         next_first_stream_logits = predicting_streams_logits[:, 0]\n-        #        self.assertTrue(torch.allclose(next_first_stream_logits[:, :3, :3], expected_slice, atol=1e-4))\n+        #        torch.testing.assert_close(next_first_stream_logits[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n         assert torch.allclose(next_first_stream_logits[:, :3, :3], expected_slice, atol=1e-4)\n \n     @slow"
        },
        {
            "sha": "3bc5e3892d42083ad94cd48b4923d1ec467dcf57",
            "filename": "tests/models/pvt/test_modeling_pvt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt%2Ftest_modeling_pvt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -277,7 +277,7 @@ def test_inference_image_classification(self):\n \n         expected_slice = torch.tensor([-1.4192, -1.9158, -0.9702]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_model(self):\n@@ -300,7 +300,7 @@ def test_inference_model(self):\n             [[-0.3086, 1.0402, 1.1816], [-0.2880, 0.5781, 0.6124], [0.1480, 0.6129, -0.0590]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_accelerate"
        },
        {
            "sha": "1c69385745f0d8dd2965bec90cc1b06efd0f8c2c",
            "filename": "tests/models/pvt_v2/test_modeling_pvt_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt_v2%2Ftest_modeling_pvt_v2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -336,7 +336,7 @@ def test_inference_image_classification(self):\n \n         expected_slice = torch.tensor([-1.4192, -1.9158, -0.9702]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_model(self):\n@@ -359,7 +359,7 @@ def test_inference_model(self):\n             [[-0.3086, 1.0402, 1.1816], [-0.2880, 0.5781, 0.6124], [0.1480, 0.6129, -0.0590]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_accelerate"
        },
        {
            "sha": "e426aee98c24ce5985b4ccf380bf475542cf280b",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -446,11 +446,11 @@ def test_model_450m_logits(self):\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-1.9537, -1.6193, -1.4123, -1.4673, -1.8511, -1.9309, -1.9826, -2.1776]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([3.2025, 7.1265, 4.6058, 3.6423, 1.6357, 3.9265, 5.1883, 5.8760, 2.7942, 4.4823, 3.2571, 2.1063, 3.4275, 4.2028, 1.9767, 5.2115, 6.6756, 6.3999, 6.0483, 5.7378, 5.6660, 5.2298, 5.4103, 5.1248, 5.4376, 2.4570, 2.6107, 5.4039, 2.8077, 4.7777])  # fmt: skip\n         print(out[0, 0, :30])\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n         del model\n         backend_empty_cache(torch_device)"
        },
        {
            "sha": "126450eacc5b4b6ed2625eb43491d150dfec52a1",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -508,11 +508,11 @@ def test_model_a2_7b_logits(self):\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-4.2125, -3.6416, -4.9136, -4.3005, -4.9938, -3.4393, -3.5195, -4.1621]])\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n         # slicing logits[0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([2.3013, -0.6595, -0.1389, -1.4095, -1.7381, -1.7609, -2.0449, -2.4289, -3.0271, -2.1351, -0.6568, -4.6012, -1.9102, -0.7475, -3.1377, 4.6904, 7.1936, 7.0991, 6.4414, 6.1720, 6.2617, 5.8751, 5.6997, 5.6011, 5.5828, -3.9505, -0.5384, -0.3392, 1.2445, 2.0714])  # fmt: skip\n         print(out[0, 0, :30])\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n         del model\n         backend_empty_cache(torch_device)"
        },
        {
            "sha": "bfa4dca85e321e6be9ed3c2ba60c150490697b82",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import numpy as np\n+import requests\n \n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n@@ -296,3 +297,26 @@ def test_custom_patch_size(self):\n                 encoded_video = prcocess_out.pixel_values_videos\n                 expected_output_video_shape = (171500, 1176)\n                 self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(\n+            encoding_slow.pixel_values, encoding_fast.pixel_values, rtol=100, atol=1e-2\n+        )  # @yoni bit weird that we have such diffs"
        },
        {
            "sha": "fde19b74543fc931a9b27d7a87f2a5348529c494",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 19,
            "deletions": 15,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1095,7 +1095,7 @@ def test_lsh_layer_forward(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_lsh_layer_forward_complex(self):\n         config = self._get_basic_config_and_input()\n@@ -1118,7 +1118,7 @@ def test_lsh_layer_forward_complex(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_local_layer_forward(self):\n         config = self._get_basic_config_and_input()\n@@ -1136,7 +1136,7 @@ def test_local_layer_forward(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_local_layer_forward_complex(self):\n         config = self._get_basic_config_and_input()\n@@ -1158,7 +1158,7 @@ def test_local_layer_forward_complex(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_lsh_model_forward(self):\n         config = self._get_basic_config_and_input()\n@@ -1175,7 +1175,7 @@ def test_lsh_model_forward(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_local_model_forward(self):\n         config = self._get_basic_config_and_input()\n@@ -1191,7 +1191,7 @@ def test_local_model_forward(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_lm_model_forward(self):\n         config = self._get_basic_config_and_input()\n@@ -1210,7 +1210,7 @@ def test_lm_model_forward(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(output_slice, expected_output_slice, atol=1e-3))\n+        torch.testing.assert_close(output_slice, expected_output_slice, rtol=1e-3, atol=1e-3)\n \n     def test_local_lm_model_grad(self):\n         config = self._get_basic_config_and_input()\n@@ -1224,7 +1224,9 @@ def test_local_lm_model_grad(self):\n         input_ids, _ = self._get_input_ids_and_mask()\n         loss = model(input_ids=input_ids, labels=input_ids)[0]\n \n-        self.assertTrue(torch.allclose(loss, torch.tensor(5.8019, dtype=torch.float, device=torch_device), atol=1e-3))\n+        torch.testing.assert_close(\n+            loss, torch.tensor(5.8019, dtype=torch.float, device=torch_device), rtol=1e-3, atol=1e-3\n+        )\n         loss.backward()\n \n         # check last grads to cover all proable errors\n@@ -1246,9 +1248,9 @@ def test_local_lm_model_grad(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(grad_slice_word, expected_grad_slice_word, atol=1e-3))\n-        self.assertTrue(torch.allclose(grad_slice_position_factor_1, expected_grad_slice_pos_fac_1, atol=1e-3))\n-        self.assertTrue(torch.allclose(grad_slice_position_factor_2, expected_grad_slice_pos_fac_2, atol=1e-3))\n+        torch.testing.assert_close(grad_slice_word, expected_grad_slice_word, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(grad_slice_position_factor_1, expected_grad_slice_pos_fac_1, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(grad_slice_position_factor_2, expected_grad_slice_pos_fac_2, rtol=1e-3, atol=1e-3)\n \n     def test_lsh_lm_model_grad(self):\n         config = self._get_basic_config_and_input()\n@@ -1264,7 +1266,9 @@ def test_lsh_lm_model_grad(self):\n         input_ids, _ = self._get_input_ids_and_mask()\n         loss = model(input_ids=input_ids, labels=input_ids)[0]\n \n-        self.assertTrue(torch.allclose(loss, torch.tensor(5.7854, dtype=torch.float, device=torch_device), atol=1e-3))\n+        torch.testing.assert_close(\n+            loss, torch.tensor(5.7854, dtype=torch.float, device=torch_device), rtol=1e-3, atol=1e-3\n+        )\n         loss.backward()\n         # check last grads to cover all proable errors\n         grad_slice_word = model.reformer.embeddings.word_embeddings.weight.grad[0, :5]\n@@ -1285,9 +1289,9 @@ def test_lsh_lm_model_grad(self):\n             dtype=torch.float,\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(grad_slice_word, expected_grad_slice_word, atol=1e-3))\n-        self.assertTrue(torch.allclose(grad_slice_position_factor_1, expected_grad_slice_pos_fac_1, atol=1e-3))\n-        self.assertTrue(torch.allclose(grad_slice_position_factor_2, expected_grad_slice_pos_fac_2, atol=1e-3))\n+        torch.testing.assert_close(grad_slice_word, expected_grad_slice_word, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(grad_slice_position_factor_1, expected_grad_slice_pos_fac_1, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(grad_slice_position_factor_2, expected_grad_slice_pos_fac_2, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_pretrained_generate_crime_and_punish(self):"
        },
        {
            "sha": "371e699d233c82b3ce71e75e1168105666bc8ea0",
            "filename": "tests/models/regnet/test_modeling_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fregnet%2Ftest_modeling_regnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -250,4 +250,4 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-0.4180, -1.5051, -3.4836]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "f0e9e7a050b15e8ce30b26161018789eb8b2af85",
            "filename": "tests/models/rembert/test_modeling_rembert.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_rembert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -507,4 +507,6 @@ def test_inference_model(self):\n         #     [-0.15887849032878876, -0.054529931396245956, 0.5356100797653198]\n         # ]]\n \n-        self.assertTrue(torch.allclose(output[\"last_hidden_state\"][:, :, :3], expected_implementation, atol=1e-4))\n+        torch.testing.assert_close(\n+            output[\"last_hidden_state\"][:, :, :3], expected_implementation, rtol=1e-4, atol=1e-4\n+        )"
        },
        {
            "sha": "c940521a8d18501df4d737e448a89ed8ee5b33bb",
            "filename": "tests/models/resnet/test_modeling_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_resnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -303,7 +303,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-11.1069, -9.7877, -8.3777]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "11171ee93453add5e0f07acf2e44d9f99100d644",
            "filename": "tests/models/roberta/test_modeling_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_roberta.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -541,7 +541,7 @@ def test_inference_masked_lm(self):\n         # roberta.eval()\n         # expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head(self):\n@@ -559,7 +559,7 @@ def test_inference_no_head(self):\n         # roberta.eval()\n         # expected_slice = roberta.extract_features(input_ids)[:, :3, :3].detach()\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_classification_head(self):\n@@ -576,7 +576,7 @@ def test_inference_classification_head(self):\n         # roberta.eval()\n         # expected_tensor = roberta.predict(\"mnli\", input_ids, return_logits=True).detach()\n \n-        self.assertTrue(torch.allclose(output, expected_tensor, atol=1e-4))\n+        torch.testing.assert_close(output, expected_tensor, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_export(self):"
        },
        {
            "sha": "1333e2d5989f49779bf6a8c0da9499e180f3b6c6",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_roberta_prelayernorm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -544,7 +544,7 @@ def test_inference_masked_lm(self):\n             [[[40.4880, 18.0199, -5.2367], [-1.8877, -4.0885, 10.7085], [-2.2613, -5.6110, 7.2665]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], EXPECTED_SLICE, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_no_head(self):\n@@ -558,4 +558,4 @@ def test_inference_no_head(self):\n             [[[0.0208, -0.0356, 0.0237], [-0.1569, -0.0411, -0.2626], [0.1879, 0.0125, -0.0089]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], EXPECTED_SLICE, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "bdae0aea13dc9c1df0c6decde3f9bebf0c8d9e9a",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -523,7 +523,7 @@ def test_inference_masked_lm(self):\n             [[[-0.1205, -1.0265, 0.2922], [-1.5134, 0.1974, 0.1519], [-5.0135, -3.9003, -0.8404]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_torch"
        },
        {
            "sha": "97718d97406f575e46774ff855d32d68f90a7a13",
            "filename": "tests/models/rt_detr/test_image_processing_rt_detr.py",
            "status": "modified",
            "additions": 22,
            "deletions": 22,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -171,31 +171,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n             self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n             expected_slice = torch.tensor([0.5490, 0.5647, 0.5725])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n             # verify area\n             expected_area = torch.tensor([2827.9883, 5403.4761, 235036.7344, 402070.2188, 71068.8281, 79601.2812])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n             # verify boxes\n             expected_boxes_shape = torch.Size([6, 4])\n             self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n             expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n             # verify image_id\n             expected_image_id = torch.tensor([39769])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n             # verify is_crowd\n             expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n             # verify class_labels\n             expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n             # verify orig_size\n             expected_orig_size = torch.tensor([480, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n             # verify size\n             expected_size = torch.tensor([640, 640])\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_image_processor_outputs(self):\n@@ -211,7 +211,7 @@ def test_image_processor_outputs(self):\n \n             # verify pixel values: output values\n             expected_slice = torch.tensor([0.5490196347236633, 0.5647059082984924, 0.572549045085907])\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-5))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-5, atol=1e-5)\n \n     def test_multiple_images_processor_outputs(self):\n         images_urls = [\n@@ -255,7 +255,7 @@ def test_multiple_images_processor_outputs(self):\n                     [0.19607844948768616, 0.21176472306251526, 0.3607843220233917],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"pixel_values\"][:, 1, 0, :3], expected_slices, atol=1e-5))\n+            torch.testing.assert_close(encoding[\"pixel_values\"][:, 1, 0, :3], expected_slices, rtol=1e-5, atol=1e-5)\n \n     @slow\n     def test_batched_coco_detection_annotations(self):\n@@ -321,8 +321,8 @@ def test_batched_coco_detection_annotations(self):\n                     [0.7715, 0.4115, 0.4570, 0.7161],\n                 ]\n             )\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3, rtol=1e-3)\n \n             # Check if do_convert_annotations=False, then the annotations are not converted to centre_x, centre_y, width, height\n             # format and not in the range [0, 1]\n@@ -369,8 +369,8 @@ def test_batched_coco_detection_annotations(self):\n                     unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n                 ]\n             ).T\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-            self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+            torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+            torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     @slow\n     @require_torch_gpu\n@@ -400,7 +400,7 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify area\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"area\"], encoding_gpu[\"labels\"][0][\"area\"].to(\"cpu\"))\n         # verify boxes\n         self.assertEqual(encoding_cpu[\"labels\"][0][\"boxes\"].shape, encoding_gpu[\"labels\"][0][\"boxes\"].shape)\n         self.assertTrue(\n@@ -409,12 +409,12 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify image_id\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"image_id\"], encoding_gpu[\"labels\"][0][\"image_id\"].to(\"cpu\")\n         )\n         # verify is_crowd\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"iscrowd\"], encoding_gpu[\"labels\"][0][\"iscrowd\"].to(\"cpu\")\n         )\n         # verify class_labels\n         self.assertTrue(\n@@ -423,8 +423,8 @@ def test_fast_processor_equivalence_cpu_gpu_coco_detection_annotations(self):\n             )\n         )\n         # verify orig_size\n-        self.assertTrue(\n-            torch.allclose(encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\"))\n+        torch.testing.assert_close(\n+            encoding_cpu[\"labels\"][0][\"orig_size\"], encoding_gpu[\"labels\"][0][\"orig_size\"].to(\"cpu\")\n         )\n         # verify size\n-        self.assertTrue(torch.allclose(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\")))\n+        torch.testing.assert_close(encoding_cpu[\"labels\"][0][\"size\"], encoding_gpu[\"labels\"][0][\"size\"].to(\"cpu\"))"
        },
        {
            "sha": "c3ccc89efc26fd7cb035b9438a566d9a4d9b42f3",
            "filename": "tests/models/rt_detr/test_modeling_rt_detr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_modeling_rt_detr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -745,11 +745,11 @@ def test_inference_object_detection_head(self):\n             ]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n \n         expected_shape_boxes = torch.Size((1, 300, 4))\n         self.assertEqual(outputs.pred_boxes.shape, expected_shape_boxes)\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n@@ -769,6 +769,6 @@ def test_inference_object_detection_head(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(results[\"scores\"][:4], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[\"scores\"][:4], expected_scores, rtol=1e-4, atol=1e-4)\n         self.assertSequenceEqual(results[\"labels\"][:4].tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(results[\"boxes\"][:4], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(results[\"boxes\"][:4], expected_slice_boxes, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "6517c96509c470b07f3d16ff00fe12317009eda1",
            "filename": "tests/models/rwkv/test_modeling_rwkv.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frwkv%2Ftest_modeling_rwkv.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -297,7 +297,7 @@ def test_initialization(self):\n                 elif \"time_first\" in name:\n                     if param.requires_grad:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n                 elif any(x in name for x in [\"time_mix_key\", \"time_mix_receptance\"]):\n                     if param.requires_grad:\n                         self.assertInterval("
        },
        {
            "sha": "c44046bd8161524579b7b5d8708371e133497b57",
            "filename": "tests/models/sam/test_modeling_sam.py",
            "status": "modified",
            "additions": 19,
            "deletions": 17,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_sam.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -539,8 +539,10 @@ def test_inference_mask_generation_no_point(self):\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n         masks = outputs.pred_masks[0, 0, 0, 0, :3]\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.4515), atol=2e-4))\n-        self.assertTrue(torch.allclose(masks, torch.tensor([-4.1800, -3.4948, -3.4481]).to(torch_device), atol=2e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.4515), rtol=2e-4, atol=2e-4)\n+        torch.testing.assert_close(\n+            masks, torch.tensor([-4.1800, -3.4948, -3.4481]).to(torch_device), rtol=2e-4, atol=2e-4\n+        )\n \n     def test_inference_mask_generation_one_point_one_bb(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -561,9 +563,9 @@ def test_inference_mask_generation_one_point_one_bb(self):\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n         masks = outputs.pred_masks[0, 0, 0, 0, :3]\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9566), atol=2e-4))\n-        self.assertTrue(\n-            torch.allclose(masks, torch.tensor([-12.7729, -12.3665, -12.6061]).to(torch_device), atol=2e-4)\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9566), rtol=2e-4, atol=2e-4)\n+        torch.testing.assert_close(\n+            masks, torch.tensor([-12.7729, -12.3665, -12.6061]).to(torch_device), rtol=2e-4, atol=2e-4\n         )\n \n     def test_inference_mask_generation_batched_points_batched_images(self):\n@@ -605,8 +607,8 @@ def test_inference_mask_generation_batched_points_batched_images(self):\n             ]\n         )\n         EXPECTED_MASKS = torch.tensor([-2.8550, -2.7988, -2.9625])\n-        self.assertTrue(torch.allclose(scores, EXPECTED_SCORES, atol=1e-3))\n-        self.assertTrue(torch.allclose(masks, EXPECTED_MASKS, atol=1e-3))\n+        torch.testing.assert_close(scores, EXPECTED_SCORES, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(masks, EXPECTED_MASKS, rtol=1e-3, atol=1e-3)\n \n     def test_inference_mask_generation_one_point_one_bb_zero(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -632,7 +634,7 @@ def test_inference_mask_generation_one_point_one_bb_zero(self):\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n \n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.7894), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.7894), rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_one_point(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -653,7 +655,7 @@ def test_inference_mask_generation_one_point(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9675), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9675), rtol=1e-4, atol=1e-4)\n \n         # With no label\n         input_points = [[[400, 650]]]\n@@ -663,7 +665,7 @@ def test_inference_mask_generation_one_point(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9675), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9675), rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_two_points(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -684,7 +686,7 @@ def test_inference_mask_generation_two_points(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9762), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9762), rtol=1e-4, atol=1e-4)\n \n         # no labels\n         inputs = processor(images=raw_image, input_points=input_points, return_tensors=\"pt\").to(torch_device)\n@@ -693,7 +695,7 @@ def test_inference_mask_generation_two_points(self):\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n \n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.9762), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.9762), rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_two_points_batched(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -714,8 +716,8 @@ def test_inference_mask_generation_two_points_batched(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores[0][-1], torch.tensor(0.9762), atol=1e-4))\n-        self.assertTrue(torch.allclose(scores[1][-1], torch.tensor(0.9637), atol=1e-4))\n+        torch.testing.assert_close(scores[0][-1], torch.tensor(0.9762), rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(scores[1][-1], torch.tensor(0.9637), rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_one_box(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -733,7 +735,7 @@ def test_inference_mask_generation_one_box(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores[-1], torch.tensor(0.7937), atol=1e-4))\n+        torch.testing.assert_close(scores[-1], torch.tensor(0.7937), rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_batched_image_one_point(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -762,7 +764,7 @@ def test_inference_mask_generation_batched_image_one_point(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n         scores_single = outputs.iou_scores.squeeze()\n-        self.assertTrue(torch.allclose(scores_batched[1, :], scores_single, atol=1e-4))\n+        torch.testing.assert_close(scores_batched[1, :], scores_single, rtol=1e-4, atol=1e-4)\n \n     def test_inference_mask_generation_two_points_point_batch(self):\n         model = SamModel.from_pretrained(\"facebook/sam-vit-base\")\n@@ -812,7 +814,7 @@ def test_inference_mask_generation_three_boxes_point_batch(self):\n \n         iou_scores = outputs.iou_scores.cpu()\n         self.assertTrue(iou_scores.shape == (1, 3, 3))\n-        torch.testing.assert_close(iou_scores, EXPECTED_IOU, atol=1e-4, rtol=1e-4)\n+        torch.testing.assert_close(iou_scores, EXPECTED_IOU, rtol=1e-4, atol=1e-4)\n \n     def test_dummy_pipeline_generation(self):\n         generator = pipeline(\"mask-generation\", model=\"facebook/sam-vit-base\", device=torch_device)"
        },
        {
            "sha": "f5e59e49fcd10694c9cc1b68b96ba60ef4360077",
            "filename": "tests/models/seamless_m4t/test_feature_extraction_seamless_m4t.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseamless_m4t%2Ftest_feature_extraction_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseamless_m4t%2Ftest_feature_extraction_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseamless_m4t%2Ftest_feature_extraction_seamless_m4t.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -283,21 +283,21 @@ def test_call_torch(self):\n         # Test not batched input\n         encoded_sequences_1 = feature_extractor(speech_inputs[0], return_tensors=\"pt\").input_features\n         encoded_sequences_2 = feature_extractor(pt_speech_inputs[0], return_tensors=\"pt\").input_features\n-        self.assertTrue(torch.allclose(encoded_sequences_1, encoded_sequences_2, atol=1e-3))\n+        torch.testing.assert_close(encoded_sequences_1, encoded_sequences_2, rtol=1e-3, atol=1e-3)\n \n         # Test batched\n         encoded_sequences_1 = feature_extractor(speech_inputs, return_tensors=\"pt\").input_features\n         encoded_sequences_2 = feature_extractor(pt_speech_inputs, return_tensors=\"pt\").input_features\n         for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n-            self.assertTrue(torch.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n+            torch.testing.assert_close(enc_seq_1, enc_seq_2, rtol=1e-3, atol=1e-3)\n \n         # Test 2-D numpy arrays are batched.\n         speech_inputs = [floats_list((1, x))[0] for x in (800, 800, 800)]\n         pt_speech_inputs = torch.tensor(speech_inputs)\n         encoded_sequences_1 = feature_extractor(speech_inputs, return_tensors=\"pt\").input_features\n         encoded_sequences_2 = feature_extractor(pt_speech_inputs, return_tensors=\"pt\").input_features\n         for enc_seq_1, enc_seq_2 in zip(encoded_sequences_1, encoded_sequences_2):\n-            self.assertTrue(torch.allclose(enc_seq_1, enc_seq_2, atol=1e-3))\n+            torch.testing.assert_close(enc_seq_1, enc_seq_2, rtol=1e-3, atol=1e-3)\n \n     @require_torch\n     # Copied from tests.models.whisper.test_feature_extraction_whisper.WhisperFeatureExtractionTest.test_double_precision_pad\n@@ -339,7 +339,7 @@ def test_integration(self):\n \n         feature_extractor(input_speech, return_tensors=\"pt\").input_features[0, 5, :30]\n         self.assertEqual(input_features.shape, (1, 279, 160))\n-        self.assertTrue(torch.allclose(input_features[0, 5, :30], EXPECTED_INPUT_FEATURES, atol=1e-4))\n+        torch.testing.assert_close(input_features[0, 5, :30], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)\n \n     def test_zero_mean_unit_variance_normalization_trunc_np_longest(self):\n         feat_extract = self.feature_extraction_class(**self.feat_extract_tester.prepare_feat_extract_dict())"
        },
        {
            "sha": "5f6493a36cbce830e19ba3331b06c54a3bdf6af6",
            "filename": "tests/models/segformer/test_modeling_segformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_segformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -373,7 +373,7 @@ def test_inference_image_segmentation_ade(self):\n                 [[-12.5134, -13.4686, -14.4915], [-12.8669, -14.4343, -14.7758], [-13.2523, -14.5819, -15.0694]],\n             ]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_image_segmentation_city(self):\n@@ -402,7 +402,7 @@ def test_inference_image_segmentation_city(self):\n                 [[-3.6456, -3.0209, -1.4203], [-3.0797, -3.1959, -2.0000], [-1.8757, -1.9217, -1.6997]],\n             ]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3, :3], expected_slice, atol=1e-1))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3, :3], expected_slice, rtol=1e-1, atol=1e-1)\n \n     @slow\n     def test_post_processing_semantic_segmentation(self):"
        },
        {
            "sha": "5c58f4846ba95e9bc3c3ba7a33f9be2568c87c5b",
            "filename": "tests/models/seggpt/test_image_processing_seggpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseggpt%2Ftest_image_processing_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseggpt%2Ftest_image_processing_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseggpt%2Ftest_image_processing_seggpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -231,11 +231,11 @@ def test_pixel_values(self):\n             ]\n         )\n \n-        self.assertTrue(torch.allclose(inputs.pixel_values[0, :, :3, :3], expected_pixel_values, atol=1e-4))\n-        self.assertTrue(\n-            torch.allclose(inputs.prompt_pixel_values[0, :, :3, :3], expected_prompt_pixel_values, atol=1e-4)\n+        torch.testing.assert_close(inputs.pixel_values[0, :, :3, :3], expected_pixel_values, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(\n+            inputs.prompt_pixel_values[0, :, :3, :3], expected_prompt_pixel_values, rtol=1e-4, atol=1e-4\n         )\n-        self.assertTrue(torch.allclose(inputs.prompt_masks[0, :, :3, :3], expected_prompt_masks, atol=1e-4))\n+        torch.testing.assert_close(inputs.prompt_masks[0, :, :3, :3], expected_prompt_masks, rtol=1e-4, atol=1e-4)\n \n     def test_prompt_mask_equivalence(self):\n         image_processor = self.image_processing_class(**self.image_processor_dict)"
        },
        {
            "sha": "c8b7362b6048d608fe29ff8be568c9c4b00855c7",
            "filename": "tests/models/seggpt/test_modeling_seggpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fseggpt%2Ftest_modeling_seggpt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -313,7 +313,7 @@ def test_seggpt_loss(self):\n         loss_value = loss(prompt_masks, pred_masks, label, bool_masked_pos)\n         expected_loss_value = torch.tensor(0.3340)\n \n-        self.assertTrue(torch.allclose(loss_value, expected_loss_value, atol=1e-4))\n+        torch.testing.assert_close(loss_value, expected_loss_value, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_from_pretrained(self):\n@@ -386,7 +386,7 @@ def test_one_shot_inference(self):\n             ]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.pred_masks[0, :, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.pred_masks[0, :, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         result = image_processor.post_process_semantic_segmentation(outputs, [input_image.size[::-1]])[0]\n \n@@ -428,7 +428,7 @@ def test_few_shot_inference(self):\n         ).to(torch_device)\n \n         self.assertEqual(outputs.pred_masks.shape, expected_shape)\n-        self.assertTrue(torch.allclose(outputs.pred_masks[0, :, 448:451, :3], expected_slice, atol=4e-4))\n+        torch.testing.assert_close(outputs.pred_masks[0, :, 448:451, :3], expected_slice, rtol=4e-4, atol=4e-4)\n \n     @slow\n     def test_one_shot_with_label(self):\n@@ -461,4 +461,4 @@ def test_one_shot_with_label(self):\n             outputs = model(**inputs, labels=labels, bool_masked_pos=bool_masked_pos)\n \n         expected_loss = torch.tensor(0.0074).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-4))\n+        torch.testing.assert_close(outputs.loss, expected_loss, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "9893fcf0b3b900dd8ebf928ca698369f481d48ad",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -553,8 +553,8 @@ def test_inference_pretrained_batched(self):\n         )\n         expected_output_sum = 62146.7422\n \n-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=5e-3))\n-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=5e-3))\n+        torch.testing.assert_close(outputs[:, :4, :4], expected_outputs_first, rtol=5e-3, atol=5e-3)\n+        torch.testing.assert_close(outputs[:, -4:, -4:], expected_outputs_last, rtol=5e-3, atol=5e-3)\n         self.assertTrue(abs(outputs.sum() - expected_output_sum) < 5)\n \n     def test_inference_ctc_batched(self):"
        },
        {
            "sha": "43bd31d92a0be395ed61da583f13da85bb307832",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -567,8 +567,8 @@ def test_inference_pretrained_batched(self):\n         )\n         expected_output_sum = 54201.0469\n \n-        self.assertTrue(torch.allclose(outputs[:, :4, :4], expected_outputs_first, atol=1e-3))\n-        self.assertTrue(torch.allclose(outputs[:, -4:, -4:], expected_outputs_last, atol=1e-3))\n+        torch.testing.assert_close(outputs[:, :4, :4], expected_outputs_first, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(outputs[:, -4:, -4:], expected_outputs_last, rtol=1e-3, atol=1e-3)\n         self.assertTrue(abs(outputs.sum() - expected_output_sum) < 1)\n \n     def test_inference_ctc_batched(self):"
        },
        {
            "sha": "3dec33018476454fc08e4e011a31fe23fb55e1e9",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1014,12 +1014,12 @@ def test_inference(self):\n \n         expected_logits = torch.tensor([[-0.7567, -10.3354]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)\n \n         # verify the probs\n         probs = torch.sigmoid(logits_per_image)  # these are the probabilities\n         expected_probs = torch.tensor([[3.1937e-01, 3.2463e-05]], device=torch_device)\n-        self.assertTrue(torch.allclose(probs, expected_probs, atol=1e-3))\n+        torch.testing.assert_close(probs, expected_probs, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "9b3f53947695325e70354eb37dbc8cfab2e997f8",
            "filename": "tests/models/speecht5/test_feature_extraction_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_feature_extraction_speecht5.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -402,7 +402,7 @@ def test_integration(self):\n         feature_extractor = SpeechT5FeatureExtractor()\n         input_values = feature_extractor(input_speech, return_tensors=\"pt\").input_values\n         self.assertEqual(input_values.shape, (1, 93680))\n-        self.assertTrue(torch.allclose(input_values[0, :30], EXPECTED_INPUT_VALUES, atol=1e-6))\n+        torch.testing.assert_close(input_values[0, :30], EXPECTED_INPUT_VALUES, rtol=1e-6, atol=1e-6)\n \n     def test_integration_target(self):\n         # fmt: off\n@@ -418,4 +418,4 @@ def test_integration_target(self):\n         feature_extractor = SpeechT5FeatureExtractor()\n         input_values = feature_extractor(audio_target=input_speech, return_tensors=\"pt\").input_values\n         self.assertEqual(input_values.shape, (1, 366, 80))\n-        self.assertTrue(torch.allclose(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, atol=1e-4))\n+        torch.testing.assert_close(input_values[0, 0, :30], EXPECTED_INPUT_VALUES, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "15b849722b78c1747b68500660e19770bc9af38a",
            "filename": "tests/models/squeezebert/test_modeling_squeezebert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsqueezebert%2Ftest_modeling_squeezebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsqueezebert%2Ftest_modeling_squeezebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsqueezebert%2Ftest_modeling_squeezebert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -294,4 +294,4 @@ def test_inference_classification_head(self):\n         expected_shape = torch.Size((1, 3))\n         self.assertEqual(output.shape, expected_shape)\n         expected_tensor = torch.tensor([[0.6401, -0.0349, -0.6041]])\n-        self.assertTrue(torch.allclose(output, expected_tensor, atol=1e-4))\n+        torch.testing.assert_close(output, expected_tensor, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "7c237b0bcfc18cc02b6d7d683d68c4f5c5b17e8b",
            "filename": "tests/models/stablelm/test_modeling_stablelm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fstablelm%2Ftest_modeling_stablelm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -395,7 +395,7 @@ def test_model_rope_scaling_from_config(self, scaling_type):\n         # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n         # maximum sequence length, so the outputs for the short input should match.\n         if scaling_type == \"dynamic\":\n-            self.assertTrue(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n         else:\n             self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n \n@@ -465,11 +465,11 @@ def test_model_stablelm_3b_4e1t_logits(self):\n \n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[2.7146, 2.4245, 1.5616, 1.4424, 2.6790]]).to(torch_device)\n-        self.assertTrue(torch.allclose(output.mean(dim=-1), EXPECTED_MEAN, atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(output.mean(dim=-1), EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n \n         # Expected logits sliced from [0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([7.1030, -1.4195,  9.9206,  7.7008,  4.9891,  4.2169,  5.5426,  3.7878, 6.7593,  5.7360,  8.4691,  5.5448,  5.0544, 10.4129,  8.5573, 13.0405, 7.3265,  3.5868,  6.1106,  5.9406,  5.6376,  5.7490,  5.4850,  4.8124, 5.1991,  4.6419,  4.5719,  9.9588,  6.7222,  4.5070]).to(torch_device)  # fmt: skip\n-        self.assertTrue(torch.allclose(output[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(output[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_stablelm_3b_4e1t_generation(self):\n@@ -498,11 +498,11 @@ def test_model_tiny_random_stablelm_2_logits(self):\n \n         # Expected mean on dim = -1\n         EXPECTED_MEAN = torch.tensor([[-2.7196, -3.6099, -2.6877, -3.1973, -3.9344]]).to(torch_device)\n-        self.assertTrue(torch.allclose(output.mean(dim=-1), EXPECTED_MEAN, atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(output.mean(dim=-1), EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n \n         # Expected logits sliced from [0, 0, 0:30]\n         EXPECTED_SLICE = torch.tensor([2.8364, 5.3811, 5.1659, 7.5485, 4.3219, 6.3315, 1.3967, 6.9147, 3.9679, 6.4786, 5.9176, 3.3067, 5.2917, 0.1485, 3.9630, 7.9947,10.6727, 9.6757, 8.8772, 8.3527, 7.8445, 6.6025, 5.5786, 7.0985,6.1369, 3.4259, 1.9397, 4.6157, 4.8105, 3.1768]).to(torch_device)  # fmt: skip\n-        self.assertTrue(torch.allclose(output[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4))\n+        torch.testing.assert_close(output[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_tiny_random_stablelm_2_generation(self):"
        },
        {
            "sha": "11f4fe11fc762c7c51090cd4f851585c024fb743",
            "filename": "tests/models/superpoint/test_modeling_superpoint.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -297,7 +297,7 @@ def test_inference(self):\n                 atol=1e-4,\n             )\n         )\n-        self.assertTrue(torch.allclose(predicted_scores_image0_values, expected_scores_image0_values, atol=1e-4))\n+        torch.testing.assert_close(predicted_scores_image0_values, expected_scores_image0_values, rtol=1e-4, atol=1e-4)\n         self.assertTrue(\n             torch.allclose(\n                 predicted_descriptors_image0_value,"
        },
        {
            "sha": "234c8aa15fe9a0668800c1bf7b6ceb8f005e6256",
            "filename": "tests/models/swiftformer/test_modeling_swiftformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_swiftformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -288,4 +288,4 @@ def test_inference_image_classification_head(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([[-2.1703e00, 2.1107e00, -2.0811e00]]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "92c06de971b074c2a597e33cdbc635fd134bb512",
            "filename": "tests/models/swin/test_modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_swin.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -488,7 +488,7 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n         expected_slice = torch.tensor([-0.0948, -0.6454, -0.0921]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "91d04915d13546b51286f9ad874b8836e9fd94ff",
            "filename": "tests/models/swin2sr/test_modeling_swin2sr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_modeling_swin2sr.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -332,7 +332,7 @@ def test_inference_image_super_resolution_head(self):\n         expected_slice = torch.tensor(\n             [[0.5458, 0.5546, 0.5638], [0.5526, 0.5565, 0.5651], [0.5396, 0.5426, 0.5621]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.reconstruction[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.reconstruction[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_fp16(self):\n         processor = Swin2SRImageProcessor()\n@@ -353,4 +353,4 @@ def test_inference_fp16(self):\n         expected_slice = torch.tensor(\n             [[0.5454, 0.5542, 0.5640], [0.5518, 0.5562, 0.5649], [0.5391, 0.5425, 0.5620]], dtype=model.dtype\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.reconstruction[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.reconstruction[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "4bf309cc6abbaebc10676f2ee5f1efb92c953a24",
            "filename": "tests/models/swinv2/test_modeling_swinv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswinv2%2Ftest_modeling_swinv2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -485,7 +485,7 @@ def test_inference_image_classification_head(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n         expected_slice = torch.tensor([-0.3947, -0.4306, 0.0026]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_fp16(self):\n@@ -505,7 +505,7 @@ def test_inference_fp16(self):\n         expected_shape = torch.Size((1, 1000))\n         self.assertEqual(outputs.logits.shape, expected_shape)\n         expected_slice = torch.tensor([-0.3938, -0.4290, 0.0020], dtype=model.dtype).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "854a73f16b37a101a9ab9b4f381be0c679a0f3b6",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1703,7 +1703,7 @@ def test_compile_static_cache_encoder(self):\n \n         model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n         logits_compiled = model(**inputs)\n-        self.assertTrue(torch.allclose(logits[0][:, -3:, -3], logits_compiled[0][:, -3:, -3], atol=1e-5))\n+        torch.testing.assert_close(logits[0][:, -3:, -3], logits_compiled[0][:, -3:, -3], rtol=1e-5, atol=1e-5)\n \n \n @require_torch"
        },
        {
            "sha": "50165cbe1a84ec0aad3127f620fdbf6668da2c1d",
            "filename": "tests/models/table_transformer/test_modeling_table_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftable_transformer%2Ftest_modeling_table_transformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -595,9 +595,9 @@ def test_table_detection(self):\n             [[-6.7329, -16.9590, 6.7447], [-8.0038, -22.3071, 6.9288], [-7.2445, -20.9855, 7.3465]],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_logits, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_logits, rtol=1e-4, atol=1e-4)\n \n         expected_boxes = torch.tensor(\n             [[0.4868, 0.1764, 0.6729], [0.6674, 0.4621, 0.3864], [0.4720, 0.1757, 0.6362]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_boxes, atol=1e-3))\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_boxes, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "980ff28b9ee9450f055b77f8fc304c24845a47a8",
            "filename": "tests/models/tapas/test_modeling_tapas.py",
            "status": "modified",
            "additions": 12,
            "deletions": 12,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tapas.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -605,12 +605,12 @@ def test_inference_no_head(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[:, :3, :3], expected_slice, atol=0.0005))\n+        torch.testing.assert_close(outputs.last_hidden_state[:, :3, :3], expected_slice, rtol=0.0005, atol=0.0005)\n \n         # test the pooled output\n         expected_slice = torch.tensor([[0.987518311, -0.970520139, -0.994303405]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.pooler_output[:, :3], expected_slice, atol=0.0005))\n+        torch.testing.assert_close(outputs.pooler_output[:, :3], expected_slice, rtol=0.0005, atol=0.0005)\n \n     @unittest.skip(reason=\"Model not available yet\")\n     def test_inference_masked_lm(self):\n@@ -666,7 +666,7 @@ def test_inference_question_answering_head_conversational(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits, expected_tensor, atol=0.015))\n+        torch.testing.assert_close(logits, expected_tensor, rtol=0.015, atol=0.015)\n \n     @slow\n     def test_inference_question_answering_head_conversational_absolute_embeddings(self):\n@@ -716,7 +716,7 @@ def test_inference_question_answering_head_conversational_absolute_embeddings(se\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits, expected_tensor, atol=0.01))\n+        torch.testing.assert_close(logits, expected_tensor, rtol=0.01, atol=0.01)\n \n     @slow\n     def test_inference_question_answering_head_weak_supervision(self):\n@@ -744,7 +744,7 @@ def test_inference_question_answering_head_weak_supervision(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[:, -6:], expected_slice, atol=0.4))\n+        torch.testing.assert_close(logits[:, -6:], expected_slice, rtol=0.4, atol=0.4)\n \n         # test the aggregation logits\n         logits_aggregation = outputs.logits_aggregation\n@@ -755,7 +755,7 @@ def test_inference_question_answering_head_weak_supervision(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits_aggregation, expected_tensor, atol=0.001))\n+        torch.testing.assert_close(logits_aggregation, expected_tensor, rtol=0.001, atol=0.001)\n \n         # test the predicted answer coordinates and aggregation indices\n         EXPECTED_PREDICTED_ANSWER_COORDINATES = [[(0, 0)], [(1, 2)]]\n@@ -813,7 +813,7 @@ def test_training_question_answering_head_weak_supervision(self):\n         # test the loss\n         loss = outputs.loss\n         expected_loss = torch.tensor(3.3527612686157227e-08, device=torch_device)\n-        self.assertTrue(torch.allclose(loss, expected_loss, atol=1e-6))\n+        torch.testing.assert_close(loss, expected_loss, rtol=1e-6, atol=1e-6)\n \n         # test the logits on the first example\n         logits = outputs.logits\n@@ -834,15 +834,15 @@ def test_training_question_answering_head_weak_supervision(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits[0, -9:], expected_slice, atol=1e-6))\n+        torch.testing.assert_close(logits[0, -9:], expected_slice, rtol=1e-6, atol=1e-6)\n \n         # test the aggregation logits on the second example\n         logits_aggregation = outputs.logits_aggregation\n         expected_shape = torch.Size((2, 4))\n         self.assertEqual(logits_aggregation.shape, expected_shape)\n         expected_slice = torch.tensor([-4.0538, 40.0304, -5.3554, 23.3965], device=torch_device)\n \n-        self.assertTrue(torch.allclose(logits_aggregation[1, -4:], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits_aggregation[1, -4:], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_question_answering_head_strong_supervision(self):\n@@ -890,7 +890,7 @@ def test_inference_question_answering_head_strong_supervision(self):\n             device=torch_device,\n         )\n \n-        self.assertTrue(torch.allclose(logits, expected_tensor, atol=0.02))\n+        torch.testing.assert_close(logits, expected_tensor, rtol=0.02, atol=0.02)\n \n         # test the aggregation logits\n         logits_aggregation = outputs.logits_aggregation\n@@ -900,7 +900,7 @@ def test_inference_question_answering_head_strong_supervision(self):\n             [[16.5659733, -3.06624889, -2.34152961, -0.970244825]], device=torch_device\n         )  # PyTorch model outputs [[16.5679, -3.0668, -2.3442, -0.9674]]\n \n-        self.assertTrue(torch.allclose(logits_aggregation, expected_tensor, atol=0.003))\n+        torch.testing.assert_close(logits_aggregation, expected_tensor, rtol=0.003, atol=0.003)\n \n     @slow\n     def test_inference_classification_head(self):\n@@ -922,7 +922,7 @@ def test_inference_classification_head(self):\n             [[0.795137286, 9.5572]], device=torch_device\n         )  # Note that the PyTorch model outputs [[0.8057, 9.5281]]\n \n-        self.assertTrue(torch.allclose(outputs.logits, expected_tensor, atol=0.05))\n+        torch.testing.assert_close(outputs.logits, expected_tensor, rtol=0.05, atol=0.05)\n \n \n @require_torch"
        },
        {
            "sha": "0f02cfcaaf2176379c79ef8c8356bde447750051",
            "filename": "tests/models/textnet/test_modeling_textnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -333,7 +333,9 @@ def test_inference_no_head(self):\n             [0.9210, 0.6099, 0.0000, 0.0000, 0.0000, 0.0000, 3.2207, 2.6602, 1.8925, 0.0000],\n             device=torch_device,\n         )\n-        self.assertTrue(torch.allclose(output.feature_maps[-1][0][10][12][:10], expected_slice_backbone, atol=1e-3))\n+        torch.testing.assert_close(\n+            output.feature_maps[-1][0][10][12][:10], expected_slice_backbone, rtol=1e-3, atol=1e-3\n+        )\n \n \n @require_torch"
        },
        {
            "sha": "5f049bd9246658cb637be498b1b4a273467a43a7",
            "filename": "tests/models/time_series_transformer/test_modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftime_series_transformer%2Ftest_modeling_time_series_transformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -512,7 +512,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[0.8196, -1.5131, 1.4620], [1.1268, -1.3238, 1.5997], [1.5098, -1.0715, 1.7359]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_inference_head(self):\n         model = TimeSeriesTransformerForPrediction.from_pretrained(\n@@ -534,7 +534,7 @@ def test_inference_head(self):\n         expected_slice = torch.tensor(\n             [[-1.2957, -1.0280, -0.6045], [-0.7017, -0.8193, -0.3717], [-1.0449, -0.8149, 0.1405]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(output[0, :3, :3], expected_slice, atol=TOLERANCE))\n+        torch.testing.assert_close(output[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     def test_seq_to_seq_generation(self):\n         model = TimeSeriesTransformerForPrediction.from_pretrained(\n@@ -555,4 +555,4 @@ def test_seq_to_seq_generation(self):\n \n         expected_slice = torch.tensor([2825.2749, 3584.9207, 6763.9951], device=torch_device)\n         mean_prediction = outputs.sequences.mean(dim=1)\n-        self.assertTrue(torch.allclose(mean_prediction[0, -3:], expected_slice, rtol=1e-1))\n+        torch.testing.assert_close(mean_prediction[0, -3:], expected_slice, rtol=1e-1)"
        },
        {
            "sha": "ec8b34e5e27a251940815c00f020d4734a3eb72f",
            "filename": "tests/models/timesformer/test_modeling_timesformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimesformer%2Ftest_modeling_timesformer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -352,4 +352,4 @@ def test_inference_for_video_classification(self):\n \n         expected_slice = torch.tensor([-0.3016, -0.7713, -0.4205]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "6ab0dffde4eb4088fe7ef592576b8ecf91b28126",
            "filename": "tests/models/tvp/test_modeling_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_modeling_tvp.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -277,7 +277,7 @@ def test_inference_no_head(self):\n         expected_slice = torch.tensor(\n             [[-0.4902, -0.4121, -1.7872], [-0.2184, 2.1211, -0.9371], [0.1180, 0.5003, -0.1727]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_with_head(self):\n         model = TvpForVideoGrounding.from_pretrained(\"Jiqing/tiny-random-tvp\").to(torch_device)\n@@ -296,7 +296,7 @@ def test_inference_with_head(self):\n         expected_shape = torch.Size((1, 2))\n         assert outputs.logits.shape == expected_shape\n         expected_slice = torch.tensor([[0.5061, 0.4988]]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits, expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits, expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_interpolate_inference_no_head(self):\n         model = TvpModel.from_pretrained(\"Jiqing/tiny-random-tvp\").to(torch_device)"
        },
        {
            "sha": "2b18aa65887347d2a6761f900107608cd2f92f03",
            "filename": "tests/models/unispeech/test_modeling_unispeech.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech%2Ftest_modeling_unispeech.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -597,4 +597,4 @@ def test_inference_pretraining(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(cosine_sim[:, :5], expected_cosine_sim_slice, atol=1e-3))\n+        torch.testing.assert_close(cosine_sim[:, :5], expected_cosine_sim_slice, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "ce8cd4180bfe0516ae5788c99655b1ab0d3b212e",
            "filename": "tests/models/unispeech_sat/test_modeling_unispeech_sat.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funispeech_sat%2Ftest_modeling_unispeech_sat.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -845,7 +845,9 @@ def test_inference_encoder_base(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[:, :2, -2:], expected_hidden_states_slice, atol=1e-3))\n+        torch.testing.assert_close(\n+            outputs.last_hidden_state[:, :2, -2:], expected_hidden_states_slice, rtol=1e-3, atol=1e-3\n+        )\n \n     def test_inference_encoder_large(self):\n         model = UniSpeechSatModel.from_pretrained(\"microsoft/unispeech-sat-large\")\n@@ -871,7 +873,9 @@ def test_inference_encoder_large(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[:, :2, -2:], expected_hidden_states_slice, atol=1e-3))\n+        torch.testing.assert_close(\n+            outputs.last_hidden_state[:, :2, -2:], expected_hidden_states_slice, rtol=1e-3, atol=1e-3\n+        )\n \n     def test_inference_diarization(self):\n         model = UniSpeechSatForAudioFrameClassification.from_pretrained(\"microsoft/unispeech-sat-base-plus-sd\").to(\n@@ -900,7 +904,7 @@ def test_inference_diarization(self):\n         )\n         self.assertEqual(labels[0, :, 0].sum(), 270)\n         self.assertEqual(labels[0, :, 1].sum(), 647)\n-        self.assertTrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))\n+        torch.testing.assert_close(outputs.logits[:, :4], expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_inference_speaker_verification(self):\n         model = UniSpeechSatForXVector.from_pretrained(\"microsoft/unispeech-sat-base-plus-sv\").to(torch_device)"
        },
        {
            "sha": "85a26bee874c45c678dd4ffcab5dab18ff1b5b3e",
            "filename": "tests/models/univnet/test_feature_extraction_univnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_feature_extraction_univnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -360,6 +360,6 @@ def test_integration(self):\n         EXPECTED_MEAN = torch.tensor(-6.18862009)\n         EXPECTED_STDDEV = torch.tensor(2.80845642)\n \n-        torch.testing.assert_close(input_features_mean, EXPECTED_MEAN, atol=5e-5, rtol=5e-6)\n+        torch.testing.assert_close(input_features_mean, EXPECTED_MEAN, rtol=5e-5, atol=5e-5)\n         torch.testing.assert_close(input_features_stddev, EXPECTED_STDDEV)\n-        torch.testing.assert_close(input_features[0, :30, 0], EXPECTED_INPUT_FEATURES, atol=1e-4, rtol=1e-5)\n+        torch.testing.assert_close(input_features[0, :30, 0], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "9a7ade71552782953540bcc893889dd01509d55d",
            "filename": "tests/models/univnet/test_modeling_univnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Funivnet%2Ftest_modeling_univnet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -276,9 +276,9 @@ def test_model_inference_batched(self):\n         EXPECTED_STDDEV = torch.tensor(0.35230172)\n         EXPECTED_SLICE = torch.tensor([-0.3408, -0.6045, -0.5052, 0.1160, -0.1556, -0.0405, -0.3024, -0.5290, -0.5019])\n \n-        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, atol=1e-4, rtol=1e-5)\n-        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, atol=1e-4, rtol=1e-5)\n-        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, atol=5e-4, rtol=1e-5)\n+        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, rtol=5e-4, atol=5e-4)\n \n     def test_model_inference_unbatched(self):\n         # Load sample checkpoint from Tortoise TTS\n@@ -300,9 +300,9 @@ def test_model_inference_unbatched(self):\n         EXPECTED_STDDEV = torch.tensor(0.33986747)\n         EXPECTED_SLICE = torch.tensor([-0.3276, -0.5504, -0.3484, 0.3574, -0.0373, -0.1826, -0.4880, -0.6431, -0.5162])\n \n-        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, atol=1e-4, rtol=1e-5)\n-        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, atol=1e-4, rtol=1e-5)\n-        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, atol=1e-3, rtol=1e-5)\n+        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, rtol=1e-3, atol=1e-3)\n \n     def test_integration(self):\n         feature_extractor = UnivNetFeatureExtractor.from_pretrained(\"dg845/univnet-dev\")\n@@ -331,6 +331,6 @@ def test_integration(self):\n         EXPECTED_SLICE = torch.tensor([-4.3934e-04, -1.8203e-04, -3.3033e-04, -3.8716e-04, -1.6125e-04, 3.5389e-06, -3.3149e-04, -3.7613e-04, -2.3331e-04])\n         # fmt: on\n \n-        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, atol=5e-6, rtol=1e-5)\n-        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, atol=1e-4, rtol=1e-5)\n-        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, atol=5e-6, rtol=1e-5)\n+        torch.testing.assert_close(waveform_mean, EXPECTED_MEAN, rtol=5e-6, atol=5e-6)\n+        torch.testing.assert_close(waveform_stddev, EXPECTED_STDDEV, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(waveform_slice, EXPECTED_SLICE, rtol=5e-6, atol=5e-6)"
        },
        {
            "sha": "94ddae0ee760c1ab92ad27ac92a2747c56c7646a",
            "filename": "tests/models/upernet/test_modeling_upernet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fupernet%2Ftest_modeling_upernet.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -310,7 +310,7 @@ def test_inference_swin_backbone(self):\n         expected_slice = torch.tensor(\n             [[-7.5958, -7.5958, -7.4302], [-7.5958, -7.5958, -7.4302], [-7.4797, -7.4797, -7.3068]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_convnext_backbone(self):\n         processor = AutoImageProcessor.from_pretrained(\"openmmlab/upernet-convnext-tiny\")\n@@ -328,4 +328,4 @@ def test_inference_convnext_backbone(self):\n         expected_slice = torch.tensor(\n             [[-8.8110, -8.8110, -8.6521], [-8.8110, -8.8110, -8.6521], [-8.7746, -8.7746, -8.6130]]\n         ).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "cead90bd869b10637ca3fcaa51f51e21be33afc7",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -389,7 +389,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     def test_mismatching_num_image_tokens(self):\n         \"\"\""
        },
        {
            "sha": "1e470e2d7845452c969c0216bfc836628e48b282",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -385,7 +385,7 @@ def test_inference_for_video_classification(self):\n \n         expected_slice = torch.tensor([0.3669, -0.0688, -0.2421]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_for_pretraining(self):\n@@ -409,11 +409,11 @@ def test_inference_for_pretraining(self):\n             [[0.7994, 0.9612, 0.8508], [0.7401, 0.8958, 0.8302], [0.5862, 0.7468, 0.7325]], device=torch_device\n         )\n         self.assertEqual(outputs.logits.shape, expected_shape)\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify the loss (`config.norm_pix_loss` = `True`)\n         expected_loss = torch.tensor([0.5142], device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-4))\n+        torch.testing.assert_close(outputs.loss, expected_loss, rtol=1e-4, atol=1e-4)\n \n         # verify the loss (`config.norm_pix_loss` = `False`)\n         model = VideoMAEForPreTraining.from_pretrained(\"MCG-NJU/videomae-base-short\", norm_pix_loss=False).to(\n@@ -424,4 +424,4 @@ def test_inference_for_pretraining(self):\n             outputs = model(**inputs)\n \n         expected_loss = torch.tensor(torch.tensor([0.6469]), device=torch_device)\n-        self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-4))\n+        torch.testing.assert_close(outputs.loss, expected_loss, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "7977d6298fc38be70b59dd7778a8629a39862223",
            "filename": "tests/models/vilt/test_modeling_vilt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_modeling_vilt.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -587,7 +587,7 @@ def test_inference_masked_lm(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([-12.5061, -12.5123, -12.5174]).to(torch_device)\n-        self.assertTrue(torch.allclose(outputs.logits[0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify masked token prediction equals \"cats\"\n         predicted_id = outputs.logits[0, 4, :].argmax(-1).item()\n@@ -612,7 +612,7 @@ def test_inference_visual_question_answering(self):\n \n         expected_slice = torch.tensor([-15.9495, -18.1472, -10.3041]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # compute loss\n         vqa_labels = [[2, 3, 155, 800]]\n@@ -673,4 +673,4 @@ def test_inference_natural_language_visual_reasoning(self):\n                 device=torch_device,\n             )\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "44d2550bb492422ed7f39d71d937289b42ef8734",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -226,7 +226,7 @@ def test_inputs_embeds_matches_input_ids(self):\n             with torch.no_grad():\n                 out_ids = model(input_ids=input_ids, **inputs)[0]\n                 out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n     def test_mismatching_num_image_tokens(self):"
        },
        {
            "sha": "a680e504cd635de282cf9b0155eeb0e90c975da1",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1142,7 +1142,7 @@ def test_inference_handwritten(self):\n             [-1.4502, -4.6683, -0.5347, -2.9291, 9.1435, -3.0571, 8.9764, 1.7560, 8.7358, -1.5311]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, 0, :10], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :10], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_printed(self):\n@@ -1176,7 +1176,7 @@ def test_inference_printed(self):\n                 device=torch_device,\n             )\n \n-        self.assertTrue(torch.allclose(logits[0, 0, :10], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :10], expected_slice, rtol=1e-4, atol=1e-4)\n \n \n @require_vision\n@@ -1272,7 +1272,7 @@ def test_inference_docvqa(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([24.3873, -6.4491, 32.5394]).to(torch_device)\n-        self.assertTrue(torch.allclose(logits[0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # step 2: generation\n         task_prompt = \"<s_docvqa><s_question>{user_input}</s_question><s_answer>\"\n@@ -1336,7 +1336,7 @@ def test_inference_cordv2(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([-27.4344, -3.2686, -19.3524], device=torch_device)\n-        self.assertTrue(torch.allclose(logits[0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # step 2: generation\n         task_prompt = \"<s_cord-v2>\"\n@@ -1398,7 +1398,7 @@ def test_inference_rvlcdip(self):\n         self.assertEqual(outputs.logits.shape, expected_shape)\n \n         expected_slice = torch.tensor([-17.6490, -4.8381, -15.7577], device=torch_device)\n-        self.assertTrue(torch.allclose(logits[0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # step 2: generation\n         task_prompt = \"<s_rvlcdip>\"\n@@ -1475,7 +1475,7 @@ def test_forward_pass(self):\n             [1.6253, -4.2179, 5.8532, -2.7911, -5.0609, -4.7397, -4.2890, -5.1073, -4.8908, -4.9729]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(logits[0, 0, :10], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :10], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_generation(self):\n         processor = self.default_processor"
        },
        {
            "sha": "ab4adeb5d466c0ab52ffcfea677094ee10c0eb32",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_vision_text_dual_encoder.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -515,4 +515,4 @@ def test_inference(self):\n \n         expected_logits = torch.tensor([[1.2284727, 0.3104122]])\n \n-        self.assertTrue(torch.allclose(outputs.logits_per_image, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "5517f3e22ead6a824b54751937f5034957dbbc9a",
            "filename": "tests/models/visual_bert/test_modeling_visual_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvisual_bert%2Ftest_modeling_visual_bert.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -605,14 +605,14 @@ def test_inference_vqa_coco_pre(self):\n             [[[-5.1858, -5.1903, -4.9142], [-6.2214, -5.9238, -5.8381], [-6.3027, -5.9939, -5.9297]]]\n         )\n \n-        self.assertTrue(torch.allclose(output.prediction_logits[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output.prediction_logits[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         expected_shape_2 = torch.Size((1, 2))\n         self.assertEqual(output.seq_relationship_logits.shape, expected_shape_2)\n \n         expected_slice_2 = torch.tensor([[0.7393, 0.1754]])\n \n-        self.assertTrue(torch.allclose(output.seq_relationship_logits, expected_slice_2, atol=1e-4))\n+        torch.testing.assert_close(output.seq_relationship_logits, expected_slice_2, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_vqa(self):\n@@ -644,7 +644,7 @@ def test_inference_vqa(self):\n             [[-8.9898, 3.0803, -1.8016, 2.4542, -8.3420, -2.0224, -3.3124, -4.4139, -3.1491, -3.8997]]\n         )\n \n-        self.assertTrue(torch.allclose(output.logits[:, :10], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output.logits[:, :10], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_nlvr(self):\n@@ -674,7 +674,7 @@ def test_inference_nlvr(self):\n \n         expected_slice = torch.tensor([[-1.1436, 0.8900]])\n \n-        self.assertTrue(torch.allclose(output.logits, expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output.logits, expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_vcr(self):\n@@ -705,4 +705,4 @@ def test_inference_vcr(self):\n \n         expected_slice = torch.tensor([[-7.7697, -7.7697, -7.7697, -7.7697]])\n \n-        self.assertTrue(torch.allclose(output.logits, expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output.logits, expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "aeb38f73f2966fe50091ca44cf4ffc49d5792fb6",
            "filename": "tests/models/vit/test_modeling_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -285,7 +285,7 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([-0.2744, 0.8215, -0.0836]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -312,7 +312,7 @@ def test_inference_interpolate_pos_encoding(self):\n             [[4.2340, 4.3906, -6.6692], [4.5463, 1.8928, -6.7257], [4.4429, 0.8496, -5.8585]]\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     @require_accelerate"
        },
        {
            "sha": "fb312a17e48afcbc9dfdfddc5d9e58171a098cac",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -331,7 +331,7 @@ def test_inference_for_pretraining(self):\n             [[-0.0548, -1.7023, -0.9325], [0.3721, -0.5670, -0.2233], [0.8235, -1.3878, -0.3524]]\n         )\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice.to(torch_device), atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice.to(torch_device), rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "bfee2d81de27749c2db109dc88f0d8befd43c336",
            "filename": "tests/models/vit_msn/test_modeling_vit_msn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_msn%2Ftest_modeling_vit_msn.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -230,4 +230,4 @@ def test_inference_image_classification_head(self):\n \n         expected_slice = torch.tensor([0.5588, 0.6853, -0.5929]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "d52cc38f7dba2a3fac1cdf4358b988b8a260e671",
            "filename": "tests/models/vitmatte/test_modeling_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_modeling_vitmatte.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -292,4 +292,4 @@ def test_inference(self):\n         expected_slice = torch.tensor(\n             [[0.9977, 0.9987, 0.9990], [0.9980, 0.9998, 0.9998], [0.9983, 0.9998, 0.9998]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(alphas[0, 0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(alphas[0, 0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "f7acf4ca1d57d5c5c2a58b1023c210f76c0bd41a",
            "filename": "tests/models/vitpose/test_modeling_vitpose.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_modeling_vitpose.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -275,9 +275,9 @@ def test_inference_pose_estimation(self):\n         expected_scores = torch.tensor([8.7529e-01, 8.4315e-01, 9.2678e-01])\n \n         self.assertEqual(len(pose_results), 2)\n-        self.assertTrue(torch.allclose(pose_results[1][\"bbox\"].cpu(), expected_bbox, atol=1e-4))\n-        self.assertTrue(torch.allclose(pose_results[1][\"keypoints\"][:3].cpu(), expected_keypoints, atol=1e-2))\n-        self.assertTrue(torch.allclose(pose_results[1][\"scores\"][:3].cpu(), expected_scores, atol=1e-4))\n+        torch.testing.assert_close(pose_results[1][\"bbox\"].cpu(), expected_bbox, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(pose_results[1][\"keypoints\"][:3].cpu(), expected_keypoints, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(pose_results[1][\"scores\"][:3].cpu(), expected_scores, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_batched_inference(self):\n@@ -323,6 +323,6 @@ def test_batched_inference(self):\n \n         self.assertEqual(len(pose_results), 2)\n         self.assertEqual(len(pose_results[0]), 2)\n-        self.assertTrue(torch.allclose(pose_results[0][1][\"bbox\"].cpu(), expected_bbox, atol=1e-4))\n-        self.assertTrue(torch.allclose(pose_results[0][1][\"keypoints\"][:3].cpu(), expected_keypoints, atol=1e-2))\n-        self.assertTrue(torch.allclose(pose_results[0][1][\"scores\"][:3].cpu(), expected_scores, atol=1e-4))\n+        torch.testing.assert_close(pose_results[0][1][\"bbox\"].cpu(), expected_bbox, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(pose_results[0][1][\"keypoints\"][:3].cpu(), expected_keypoints, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(pose_results[0][1][\"scores\"][:3].cpu(), expected_scores, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "42524ebf8b1db5208fb5cb8607630475b7fa5bbe",
            "filename": "tests/models/vits/test_modeling_vits.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvits%2Ftest_modeling_vits.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -434,7 +434,7 @@ def test_forward(self):\n             ]\n         )\n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.waveform[0, 10000:10030].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.waveform[0, 10000:10030].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @require_torch_fp16\n     def test_forward_fp16(self):\n@@ -465,4 +465,4 @@ def test_forward_fp16(self):\n             ]\n         ).to(torch.float16)\n         # fmt: on\n-        self.assertTrue(torch.allclose(outputs.waveform[0, 10000:10030].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(outputs.waveform[0, 10000:10030].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "5cab10700b73db5b67940aa2fcf129a96d260672",
            "filename": "tests/models/vivit/test_modeling_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_modeling_vivit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -357,7 +357,7 @@ def test_inference_for_video_classification(self):\n         # taken from original model\n         expected_slice = torch.tensor([-0.9498, 2.7971, -1.4049, 0.1024, -1.8353]).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits[0, :5], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :5], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "10ca9a22e43c2f69e2d6c877910a5b87b2970627",
            "filename": "tests/models/wav2vec2/test_modeling_wav2vec2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_wav2vec2.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1166,7 +1166,7 @@ def get_logits(model, input_features):\n \n         logits_2 = get_logits(model_2, input_features)\n \n-        self.assertTrue(torch.allclose(logits, logits_2, atol=1e-3))\n+        torch.testing.assert_close(logits, logits_2, rtol=1e-3, atol=1e-3)\n \n     # test that loading adapter weights with mismatched vocab sizes can be loaded\n     def test_load_target_lang_with_mismatched_size(self):\n@@ -1203,7 +1203,7 @@ def get_logits(model, input_features):\n \n         logits_2 = get_logits(model_2, input_features)\n \n-        self.assertTrue(torch.allclose(logits, logits_2, atol=1e-3))\n+        torch.testing.assert_close(logits, logits_2, rtol=1e-3, atol=1e-3)\n \n     def test_load_attn_adapter(self):\n         processor = Wav2Vec2Processor.from_pretrained(\n@@ -1250,7 +1250,7 @@ def get_logits(model, input_features):\n                 model.load_adapter(\"ita\", use_safetensors=True)\n             logits_2 = get_logits(model, input_features)\n \n-            self.assertTrue(torch.allclose(logits, logits_2, atol=1e-3))\n+            torch.testing.assert_close(logits, logits_2, rtol=1e-3, atol=1e-3)\n \n         with tempfile.TemporaryDirectory() as tempdir:\n             model.save_pretrained(tempdir)\n@@ -1271,7 +1271,7 @@ def get_logits(model, input_features):\n \n             logits_2 = get_logits(model, input_features)\n \n-            self.assertTrue(torch.allclose(logits, logits_2, atol=1e-3))\n+            torch.testing.assert_close(logits, logits_2, rtol=1e-3, atol=1e-3)\n \n         model = Wav2Vec2ForCTC.from_pretrained(\"hf-internal-testing/tiny-random-wav2vec2-adapter\")\n         logits = get_logits(model, input_features)\n@@ -1282,7 +1282,7 @@ def get_logits(model, input_features):\n \n         logits_2 = get_logits(model, input_features)\n \n-        self.assertTrue(torch.allclose(logits, logits_2, atol=1e-3))\n+        torch.testing.assert_close(logits, logits_2, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_model_from_pretrained(self):\n@@ -1595,7 +1595,7 @@ def test_inference_integration(self):\n         ], device=torch_device)\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(cosine_sim_masked, expected_cosine_sim_masked, atol=1e-3))\n+        torch.testing.assert_close(cosine_sim_masked, expected_cosine_sim_masked, rtol=1e-3, atol=1e-3)\n \n     def test_inference_pretrained(self):\n         model = Wav2Vec2ForPreTraining.from_pretrained(\"facebook/wav2vec2-base\")\n@@ -1734,7 +1734,7 @@ def test_inference_keyword_spotting(self):\n         expected_logits = torch.tensor([6.1186, 11.8961, 10.2931, 6.0898], device=torch_device)\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=1e-2))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_inference_intent_classification(self):\n         model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-ic\").to(torch_device)\n@@ -1762,9 +1762,9 @@ def test_inference_intent_classification(self):\n         self.assertListEqual(predicted_ids_object.tolist(), expected_labels_object)\n         self.assertListEqual(predicted_ids_location.tolist(), expected_labels_location)\n \n-        self.assertTrue(torch.allclose(predicted_logits_action, expected_logits_action, atol=1e-2))\n-        self.assertTrue(torch.allclose(predicted_logits_object, expected_logits_object, atol=1e-2))\n-        self.assertTrue(torch.allclose(predicted_logits_location, expected_logits_location, atol=1e-2))\n+        torch.testing.assert_close(predicted_logits_action, expected_logits_action, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(predicted_logits_object, expected_logits_object, rtol=1e-2, atol=1e-2)\n+        torch.testing.assert_close(predicted_logits_location, expected_logits_location, rtol=1e-2, atol=1e-2)\n \n     def test_inference_speaker_identification(self):\n         model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-sid\").to(torch_device)\n@@ -1785,7 +1785,7 @@ def test_inference_speaker_identification(self):\n         expected_logits = torch.tensor([37.5627, 71.6362, 64.2419, 31.7778], device=torch_device)\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=1e-2))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_inference_emotion_recognition(self):\n         model = Wav2Vec2ForSequenceClassification.from_pretrained(\"superb/wav2vec2-base-superb-er\").to(torch_device)\n@@ -1804,7 +1804,7 @@ def test_inference_emotion_recognition(self):\n         expected_logits = torch.tensor([2.1722, 3.0779, 8.0287, 6.6797], device=torch_device)\n \n         self.assertListEqual(predicted_ids.tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(predicted_logits, expected_logits, atol=1e-2))\n+        torch.testing.assert_close(predicted_logits, expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_phoneme_recognition(self):\n         model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\").to(torch_device)\n@@ -1936,7 +1936,7 @@ def test_inference_diarization(self):\n         )\n         self.assertEqual(labels[0, :, 0].sum(), 555)\n         self.assertEqual(labels[0, :, 1].sum(), 299)\n-        self.assertTrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))\n+        torch.testing.assert_close(outputs.logits[:, :4], expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_inference_speaker_verification(self):\n         model = Wav2Vec2ForXVector.from_pretrained(\"anton-l/wav2vec2-base-superb-sv\").to(torch_device)"
        },
        {
            "sha": "eaea550ee9762b284e9e11dfb2e599f6678d193f",
            "filename": "tests/models/wav2vec2_with_lm/test_processor_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processor_wav2vec2_with_lm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -506,5 +506,5 @@ def test_word_time_stamp_integration(self):\n         expected_end_tensor = torch.tensor([0.7800, 1.1000, 1.6600, 1.9200, 2.0400, 2.8000, 3.3000, 3.8800, 4.2800])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(start_times, expected_start_tensor, atol=0.01))\n-        self.assertTrue(torch.allclose(end_times, expected_end_tensor, atol=0.01))\n+        torch.testing.assert_close(start_times, expected_start_tensor, rtol=0.01, atol=0.01)\n+        torch.testing.assert_close(end_times, expected_end_tensor, rtol=0.01, atol=0.01)"
        },
        {
            "sha": "ed02c6aa141989a1567dc8b620e68ea631d6d440",
            "filename": "tests/models/wavlm/test_modeling_wavlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwavlm%2Ftest_modeling_wavlm.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -525,7 +525,7 @@ def test_inference_base(self):\n         EXPECTED_HIDDEN_STATES_SLICE = torch.tensor(\n             [[[0.0577, 0.1161], [0.0579, 0.1165]], [[0.0199, 0.1237], [0.0059, 0.0605]]]\n         )\n-        self.assertTrue(torch.allclose(hidden_states_slice, EXPECTED_HIDDEN_STATES_SLICE, atol=5e-2))\n+        torch.testing.assert_close(hidden_states_slice, EXPECTED_HIDDEN_STATES_SLICE, rtol=5e-2, atol=5e-2)\n \n     def test_inference_large(self):\n         model = WavLMModel.from_pretrained(\"microsoft/wavlm-large\").to(torch_device)\n@@ -549,7 +549,7 @@ def test_inference_large(self):\n             [[[0.2122, 0.0500], [0.2118, 0.0563]], [[0.1353, 0.1818], [0.2453, 0.0595]]]\n         )\n \n-        self.assertTrue(torch.allclose(hidden_states_slice, EXPECTED_HIDDEN_STATES_SLICE, rtol=5e-2))\n+        torch.testing.assert_close(hidden_states_slice, EXPECTED_HIDDEN_STATES_SLICE, rtol=5e-2)\n \n     def test_inference_diarization(self):\n         model = WavLMForAudioFrameClassification.from_pretrained(\"microsoft/wavlm-base-plus-sd\").to(torch_device)\n@@ -576,7 +576,7 @@ def test_inference_diarization(self):\n         )\n         self.assertEqual(labels[0, :, 0].sum(), 258)\n         self.assertEqual(labels[0, :, 1].sum(), 647)\n-        self.assertTrue(torch.allclose(outputs.logits[:, :4], expected_logits, atol=1e-2))\n+        torch.testing.assert_close(outputs.logits[:, :4], expected_logits, rtol=1e-2, atol=1e-2)\n \n     def test_inference_speaker_verification(self):\n         model = WavLMForXVector.from_pretrained(\"microsoft/wavlm-base-plus-sv\").to(torch_device)"
        },
        {
            "sha": "ec2e29a41e0c91d91cf62e32bfc82cd267b469d2",
            "filename": "tests/models/whisper/test_feature_extraction_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_feature_extraction_whisper.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -240,7 +240,7 @@ def test_torch_integration(self):\n         input_features = feature_extractor(input_speech, return_tensors=\"pt\").input_features\n \n         self.assertEqual(input_features.shape, (1, 80, 3000))\n-        self.assertTrue(torch.allclose(input_features[0, 0, :30], EXPECTED_INPUT_FEATURES, atol=1e-4))\n+        torch.testing.assert_close(input_features[0, 0, :30], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)\n \n     @unittest.mock.patch(\"transformers.models.whisper.feature_extraction_whisper.is_torch_available\", lambda: False)\n     def test_numpy_integration(self):\n@@ -302,4 +302,4 @@ def test_torch_integration_batch(self):\n         feature_extractor = WhisperFeatureExtractor()\n         input_features = feature_extractor(input_speech, return_tensors=\"pt\").input_features\n         self.assertEqual(input_features.shape, (3, 80, 3000))\n-        self.assertTrue(torch.allclose(input_features[:, 0, :30], EXPECTED_INPUT_FEATURES, atol=1e-4))\n+        torch.testing.assert_close(input_features[:, 0, :30], EXPECTED_INPUT_FEATURES, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "80c4025c259c21e32292206a6a5310fe97ea4a2c",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -499,7 +499,7 @@ def test_encoder_sinusoidal_embed_positions(self):\n         for model_class in self.all_model_classes:\n             model = model_class(config)\n             embeds = model.get_encoder().embed_positions.weight\n-            self.assertTrue(torch.allclose(embeds, sinusoids(*embeds.shape)))\n+            torch.testing.assert_close(embeds, sinusoids(*embeds.shape))\n \n     def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n@@ -924,7 +924,7 @@ def test_flash_attn_2_inference_equivalence(self):\n                 logits_fa = outputs_fa.decoder_hidden_states[-1]\n \n                 # whisper FA2 needs very high tolerance\n-                self.assertTrue(torch.allclose(logits_fa, logits, atol=4e-1))\n+                torch.testing.assert_close(logits_fa, logits, rtol=4e-1, atol=4e-1)\n \n                 # check with inference + dropout\n                 model.train()\n@@ -969,7 +969,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n                 logits_fa = outputs_fa.decoder_hidden_states[-1]\n \n                 # whisper FA2 needs very high tolerance\n-                self.assertTrue(torch.allclose(logits_fa, logits, atol=4e-1))\n+                torch.testing.assert_close(logits_fa, logits, rtol=4e-1, atol=4e-1)\n \n                 other_inputs = {\n                     \"decoder_input_ids\": decoder_input_ids,\n@@ -984,7 +984,7 @@ def test_flash_attn_2_inference_equivalence_right_padding(self):\n                 logits_fa = outputs_fa.decoder_hidden_states[-1]\n \n                 # whisper FA2 needs very high tolerance\n-                self.assertTrue(torch.allclose(logits_fa[:, -2:], logits[:, -2:], atol=4e-1))\n+                torch.testing.assert_close(logits_fa[:, -2:], logits[:, -2:], rtol=4e-1, atol=4e-1)\n \n     def _create_and_check_torchscript(self, config, inputs_dict):\n         if not self.test_torchscript:\n@@ -1663,7 +1663,7 @@ def test_tiny_logits_librispeech(self):\n             ]\n         )\n         # fmt: on\n-        self.assertTrue(torch.allclose(logits[0][0, 0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0][0, 0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n         # fmt: off\n         EXPECTED_GENERATION = torch.tensor(\n@@ -1677,7 +1677,7 @@ def test_tiny_logits_librispeech(self):\n         # fmt: on\n \n         head_logits = logits[0] @ model.decoder.embed_tokens.weight.T\n-        self.assertTrue(torch.allclose(head_logits[0, 0, :30].cpu(), EXPECTED_GENERATION, atol=1e-4))\n+        torch.testing.assert_close(head_logits[0, 0, :30].cpu(), EXPECTED_GENERATION, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_small_en_logits_librispeech(self):\n@@ -1712,7 +1712,7 @@ def test_small_en_logits_librispeech(self):\n             ]\n         )\n         # fmt: on\n-        self.assertTrue(torch.allclose(logits[0, 0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_large_logits_librispeech(self):\n@@ -1756,7 +1756,7 @@ def test_large_logits_librispeech(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(logits[0, 0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0, 0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_tiny_en_generation(self):\n@@ -1868,7 +1868,7 @@ def test_large_batched_generation(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generated_ids.cpu(), EXPECTED_LOGITS))\n+        torch.testing.assert_close(generated_ids.cpu(), EXPECTED_LOGITS)\n \n         # fmt: off\n         EXPECTED_TRANSCRIPT = [\n@@ -1942,7 +1942,7 @@ def test_tiny_en_batched_generation(self):\n         )\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generated_ids, EXPECTED_LOGITS))\n+        torch.testing.assert_close(generated_ids, EXPECTED_LOGITS)\n \n         # fmt: off\n         EXPECTED_TRANSCRIPT = [\n@@ -1975,7 +1975,7 @@ def test_tiny_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generated_ids, EXPECTED_OUTPUT))\n+        torch.testing.assert_close(generated_ids, EXPECTED_OUTPUT)\n \n         EXPECTED_TRANSCRIPT = [\n             {\n@@ -2216,7 +2216,7 @@ def test_large_timestamp_generation(self):\n             50365, 2221, 13, 2326, 388, 391, 307, 264, 50244, 295, 264, 2808, 5359, 11, 293, 321, 366, 5404, 281, 2928, 702, 14943, 13, 50629, 50682, 6966, 307, 2221, 13, 2326, 388, 391, 311, 9060, 1570, 1880, 813, 702, 1871, 13, 50870, 50911, 634, 5112, 505, 300, 412, 341, 42729, 3196, 295, 264, 1064, 11, 365, 5272, 293, 12904, 9256, 450, 10539, 949, 505, 11, 51245, 51287, 1034, 4680, 10117, 490, 3936, 293, 1080, 3542, 5160, 881, 26336, 281, 264, 1575, 13, 51494, 51523, 634, 575, 12525, 22618, 1968, 6144, 35617, 1456, 397, 266, 311, 589, 307, 534, 10281, 934, 439, 11, 51799, 51815, 50365, 293, 393, 4411, 50430\n         ])\n         # fmt: on\n-        self.assertTrue(torch.allclose(generated_ids, EXPECTED_OUTPUT))\n+        torch.testing.assert_close(generated_ids, EXPECTED_OUTPUT)\n \n         EXPECTED_TRANSCRIPT = [\n             {\n@@ -2292,7 +2292,7 @@ def test_tiny_token_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT))\n+        torch.testing.assert_close(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT)\n \n     @slow\n     def test_small_token_timestamp_generation(self):\n@@ -2322,7 +2322,7 @@ def test_small_token_timestamp_generation(self):\n         ])\n         # fmt: on\n \n-        self.assertTrue(torch.allclose(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT))\n+        torch.testing.assert_close(generate_outputs[\"token_timestamps\"].to(\"cpu\"), EXPECTED_OUTPUT)\n \n     @slow\n     def test_tiny_token_timestamp_batch_generation(self):\n@@ -2403,7 +2403,7 @@ def test_tiny_token_timestamp_generation_longform(self):\n         # fmt: on\n \n         for segment, exp_segment in zip(generate_outputs[\"segments\"][0], EXPECTED_OUTPUT):\n-            self.assertTrue(torch.allclose(segment[\"token_timestamps\"], exp_segment))\n+            torch.testing.assert_close(segment[\"token_timestamps\"], exp_segment)\n \n     @slow\n     def test_tiny_specaugment_librispeech(self):\n@@ -2438,7 +2438,7 @@ def test_tiny_specaugment_librispeech(self):\n             ]\n         )\n         # fmt: on\n-        self.assertTrue(torch.allclose(logits[0][0, 0, :30].cpu(), EXPECTED_LOGITS, atol=1e-4))\n+        torch.testing.assert_close(logits[0][0, 0, :30].cpu(), EXPECTED_LOGITS, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_generate_with_prompt_ids(self):"
        },
        {
            "sha": "80ee63fb15c1030704ce43552974425f64546192",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -737,7 +737,7 @@ def test_inference(self):\n \n         expected_logits = torch.tensor([[14.0181, 20.2771, 14.4776]], device=torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.logits_per_video, expected_logits, atol=1e-3))\n+        torch.testing.assert_close(outputs.logits_per_video, expected_logits, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):\n@@ -771,6 +771,6 @@ def test_inference_interpolate_pos_encoding(self):\n             [[0.0126, 0.2109, 0.0609], [0.0448, 0.5862, -0.1688], [-0.0881, 0.8525, -0.3044]]\n         ).to(torch_device)\n \n-        self.assertTrue(\n-            torch.allclose(outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4)\n+        torch.testing.assert_close(\n+            outputs.vision_model_output.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4\n         )"
        },
        {
            "sha": "d9aac95781692cad2f1a90a4c6391b42ab0a9786",
            "filename": "tests/models/xlm_roberta/test_modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_xlm_roberta.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -53,7 +53,7 @@ def test_xlm_roberta_base(self):\n             output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @require_torch_sdpa\n     def test_xlm_roberta_base_sdpa(self):\n@@ -70,7 +70,7 @@ def test_xlm_roberta_base_sdpa(self):\n             output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_xlm_roberta_large(self):\n@@ -89,4 +89,4 @@ def test_xlm_roberta_large(self):\n             output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "9fac147c6176f56fbf06651350dd8f31f1d3718a",
            "filename": "tests/models/xlm_roberta_xl/test_modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta_xl%2Ftest_modeling_xlm_roberta_xl.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -542,7 +542,7 @@ def test_xlm_roberta_xl(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @unittest.skip(reason=\"Model is too large to be tested on the CI\")\n     def test_xlm_roberta_xxl(self):\n@@ -561,4 +561,4 @@ def test_xlm_roberta_xxl(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "80a3ac13b06c2a8016bd441ed9f33379b8d089f5",
            "filename": "tests/models/xmod/test_modeling_xmod.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxmod%2Ftest_modeling_xmod.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -553,7 +553,7 @@ def test_xmod_base(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n         # language de_DE\n         model.set_default_language(\"de_DE\")\n@@ -569,7 +569,7 @@ def test_xmod_base(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_xmod_large_prenorm(self):\n@@ -589,7 +589,7 @@ def test_xmod_large_prenorm(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n         # language de_DE\n         model.set_default_language(\"de_DE\")\n@@ -605,7 +605,7 @@ def test_xmod_large_prenorm(self):\n         output = model(input_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_multilingual_batch(self):\n@@ -631,7 +631,7 @@ def test_multilingual_batch(self):\n         output = model(input_ids, lang_ids=lang_ids)[\"last_hidden_state\"].detach()\n         self.assertEqual(output.shape, expected_output_shape)\n         # compare the actual values for a slice of last dim\n-        self.assertTrue(torch.allclose(output[:, :, -1], expected_output_values_last_dim, atol=1e-3))\n+        torch.testing.assert_close(output[:, :, -1], expected_output_values_last_dim, rtol=1e-3, atol=1e-3)\n \n     @slow\n     def test_end_to_end_mask_fill(self):"
        },
        {
            "sha": "e3b4dc409c4150e1c9764ae966a0170434f5aeae",
            "filename": "tests/models/yolos/test_image_processing_yolos.py",
            "status": "modified",
            "additions": 26,
            "deletions": 26,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_image_processing_yolos.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -185,8 +185,8 @@ def test_equivalence_padding(self):\n         encoded_images_with_method = image_processing_1.pad(image_inputs, return_tensors=\"pt\")\n         encoded_images = image_processing_2(image_inputs, return_tensors=\"pt\")\n \n-        self.assertTrue(\n-            torch.allclose(encoded_images_with_method[\"pixel_values\"], encoded_images[\"pixel_values\"], atol=1e-4)\n+        torch.testing.assert_close(\n+            encoded_images_with_method[\"pixel_values\"], encoded_images[\"pixel_values\"], rtol=1e-4, atol=1e-4\n         )\n \n     @parameterized.expand(\n@@ -234,31 +234,31 @@ def test_call_pytorch_with_coco_detection_annotations(self):\n         self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n         expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify area\n         expected_area = torch.tensor([5832.7256, 11144.6689, 484763.2500, 829269.8125, 146579.4531, 164177.6250])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n         # verify boxes\n         expected_boxes_shape = torch.Size([6, 4])\n         self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n         expected_boxes_slice = torch.tensor([0.5503, 0.2765, 0.0604, 0.2215])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n         # verify image_id\n         expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n         # verify is_crowd\n         expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n         # verify class_labels\n         expected_class_labels = torch.tensor([75, 75, 63, 65, 17, 17])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n         # verify orig_size\n         expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n         # verify size\n         expected_size = torch.tensor([800, 1056])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     @slow\n     def test_call_pytorch_with_coco_panoptic_annotations(self):\n@@ -280,34 +280,34 @@ def test_call_pytorch_with_coco_panoptic_annotations(self):\n         self.assertEqual(encoding[\"pixel_values\"].shape, expected_shape)\n \n         expected_slice = torch.tensor([0.2796, 0.3138, 0.3481])\n-        self.assertTrue(torch.allclose(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(encoding[\"pixel_values\"][0, 0, 0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n         # verify area\n         expected_area = torch.tensor([146591.5000, 163974.2500, 480092.2500, 11187.0000, 5824.5000, 7562.5000])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"area\"], expected_area))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"area\"], expected_area)\n         # verify boxes\n         expected_boxes_shape = torch.Size([6, 4])\n         self.assertEqual(encoding[\"labels\"][0][\"boxes\"].shape, expected_boxes_shape)\n         expected_boxes_slice = torch.tensor([0.2625, 0.5437, 0.4688, 0.8625])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"][0], expected_boxes_slice, rtol=1e-3, atol=1e-3)\n         # verify image_id\n         expected_image_id = torch.tensor([39769])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"image_id\"], expected_image_id))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"image_id\"], expected_image_id)\n         # verify is_crowd\n         expected_is_crowd = torch.tensor([0, 0, 0, 0, 0, 0])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"iscrowd\"], expected_is_crowd)\n         # verify class_labels\n         expected_class_labels = torch.tensor([17, 17, 63, 75, 75, 93])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"class_labels\"], expected_class_labels)\n         # verify masks\n         expected_masks_sum = 815161\n         self.assertEqual(encoding[\"labels\"][0][\"masks\"].sum().item(), expected_masks_sum)\n         # verify orig_size\n         expected_orig_size = torch.tensor([480, 640])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"orig_size\"], expected_orig_size)\n         # verify size\n         expected_size = torch.tensor([800, 1056])\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"size\"], expected_size))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"size\"], expected_size)\n \n     # Output size is slight different from DETR as yolos takes mod of 16\n     @slow\n@@ -373,8 +373,8 @@ def test_batched_coco_detection_annotations(self):\n                 [0.5845, 0.4115, 0.3462, 0.7161],\n             ]\n         )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n \n         # Check the masks have also been padded\n         self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n@@ -425,8 +425,8 @@ def test_batched_coco_detection_annotations(self):\n                 unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n             ]\n         ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1, atol=1)\n+        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1, atol=1)\n \n     # Output size is slight different from DETR as yolos takes mod of 16\n     def test_batched_coco_panoptic_annotations(self):\n@@ -495,8 +495,8 @@ def test_batched_coco_panoptic_annotations(self):\n                 [0.3026, 0.2994, 0.6051, 0.5987],\n             ]\n         )\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1e-3))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1e-3))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1e-3, atol=1e-3)\n+        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1e-3, atol=1e-3)\n \n         # Check the masks have also been padded\n         self.assertEqual(encoding[\"labels\"][0][\"masks\"].shape, torch.Size([6, 800, 1056]))\n@@ -548,8 +548,8 @@ def test_batched_coco_panoptic_annotations(self):\n                 unnormalized_boxes_1[:, 1] + unnormalized_boxes_1[:, 3] / 2,\n             ]\n         ).T\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, rtol=1))\n-        self.assertTrue(torch.allclose(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, rtol=1))\n+        torch.testing.assert_close(encoding[\"labels\"][0][\"boxes\"], expected_boxes_0, atol=1, rtol=1)\n+        torch.testing.assert_close(encoding[\"labels\"][1][\"boxes\"], expected_boxes_1, atol=1, rtol=1)\n \n     # Copied from tests.models.detr.test_image_processing_detr.DetrImageProcessingTest.test_max_width_max_height_resizing_and_pad_strategy with Detr->Yolos\n     def test_max_width_max_height_resizing_and_pad_strategy(self):"
        },
        {
            "sha": "e5857c8a338d133bce70f60106a856a85df36e33",
            "filename": "tests/models/yolos/test_modeling_yolos.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyolos%2Ftest_modeling_yolos.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -361,8 +361,8 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor(\n             [[0.2536, 0.5449, 0.4643], [0.2037, 0.7735, 0.3672], [0.7692, 0.4056, 0.4549]], device=torch_device\n         )\n-        self.assertTrue(torch.allclose(outputs.logits[0, :3, :3], expected_slice_logits, atol=1e-4))\n-        self.assertTrue(torch.allclose(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, atol=1e-4))\n+        torch.testing.assert_close(outputs.logits[0, :3, :3], expected_slice_logits, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.pred_boxes[0, :3, :3], expected_slice_boxes, rtol=1e-4, atol=1e-4)\n \n         # verify postprocessing\n         results = image_processor.post_process_object_detection(\n@@ -373,6 +373,6 @@ def test_inference_object_detection_head(self):\n         expected_slice_boxes = torch.tensor([331.8438, 80.5440, 369.9546, 188.0579]).to(torch_device)\n \n         self.assertEqual(len(results[\"scores\"]), 5)\n-        self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-4))\n+        torch.testing.assert_close(results[\"scores\"], expected_scores, rtol=1e-4, atol=1e-4)\n         self.assertSequenceEqual(results[\"labels\"].tolist(), expected_labels)\n-        self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes))\n+        torch.testing.assert_close(results[\"boxes\"][0, :], expected_slice_boxes)"
        },
        {
            "sha": "2f13e91fc558af3a75463f1de8a14a3f750ca252",
            "filename": "tests/models/yoso/test_modeling_yoso.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fyoso%2Ftest_modeling_yoso.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -375,7 +375,7 @@ def test_inference_no_head(self):\n             [[[-0.0611, 0.1242, 0.0840], [0.0280, -0.0048, 0.1125], [0.0106, 0.0226, 0.0751]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_masked_lm(self):\n@@ -394,7 +394,7 @@ def test_inference_masked_lm(self):\n             [[[-2.1313, -3.7285, -2.2407], [-2.7047, -3.3314, -2.6408], [0.0629, -2.5166, -0.3356]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_inference_masked_lm_long_input(self):\n@@ -413,4 +413,4 @@ def test_inference_masked_lm_long_input(self):\n             [[[-2.3914, -4.3742, -5.0956], [-4.0988, -4.2384, -7.0406], [-3.1427, -3.7192, -6.6800]]]\n         )\n \n-        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(output[:, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)"
        },
        {
            "sha": "662a4d060b743ecb8934c079fbb8758dc5171cb4",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -341,10 +341,14 @@ def test_initialization(self):\n                 if param.requires_grad:\n                     if \"A_log\" in name:\n                         A = torch.arange(1, config.mamba_d_state + 1, dtype=torch.float32)[None, :]\n-                        self.assertTrue(torch.allclose(param.data, torch.log(A), atol=1e-5, rtol=1e-5))\n+                        intermediate_dim = config.mamba_expand * config.hidden_size\n+                        A = A.expand(intermediate_dim, -1).reshape(\n+                            config.n_mamba_heads, intermediate_dim // config.n_mamba_heads, -1\n+                        )\n+                        torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n                     elif \"D\" in name:\n                         # check if it's a ones like\n-                        self.assertTrue(torch.allclose(param.data, torch.ones_like(param.data), atol=1e-5, rtol=1e-5))\n+                        torch.testing.assert_close(param.data, torch.ones_like(param.data), rtol=1e-5, atol=1e-5)\n                     elif \"x_proj\" in name or \"dt_proj_weight\" in name:\n                         self.assertIn(\n                             ((param.data.mean() * 1e2).round() / 1e2).item(),\n@@ -498,7 +502,7 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             next_logits_with_padding = model(**model_kwargs).logits[:, -1, :]\n \n             # They should result in very similar logits\n-            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=3e-3))\n+            torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=3e-3, atol=3e-3)\n \n     @require_flash_attn\n     @require_torch_gpu"
        },
        {
            "sha": "aef49c4752c2877cc800f15da8653ca4d7f46dcc",
            "filename": "tests/models/zoedepth/test_modeling_zoedepth.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -253,7 +253,7 @@ def test_inference_depth_estimation(self):\n             [[1.0020, 1.0219, 1.0389], [1.0349, 1.0816, 1.1000], [1.0576, 1.1094, 1.1249]],\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_inference_depth_estimation_multiple_heads(self):\n         image_processor = ZoeDepthImageProcessor.from_pretrained(\"Intel/zoedepth-nyu-kitti\")\n@@ -275,7 +275,7 @@ def test_inference_depth_estimation_multiple_heads(self):\n             [[1.1571, 1.1438, 1.1783], [1.2163, 1.2036, 1.2320], [1.2688, 1.2461, 1.2734]],\n         ).to(torch_device)\n \n-        self.assertTrue(torch.allclose(outputs.predicted_depth[0, :3, :3], expected_slice, atol=1e-4))\n+        torch.testing.assert_close(outputs.predicted_depth[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def check_target_size(\n         self,\n@@ -301,7 +301,7 @@ def check_target_size(\n                 out_l.unsqueeze(0).unsqueeze(1), size=img.size[::-1], mode=\"bicubic\", align_corners=False\n             )\n             self.assertTrue((np.array(out_l.shape)[::-1] == np.array(img.size) * 2).all())\n-            self.assertTrue(torch.allclose(out, out_l_reduced, rtol=2e-2))\n+            torch.testing.assert_close(out, out_l_reduced, rtol=2e-2)\n \n     def check_post_processing_test(self, image_processor, images, model, pad_input=True, flip_aug=True):\n         inputs = image_processor(images=images, return_tensors=\"pt\", do_pad=pad_input).to(torch_device)\n@@ -323,7 +323,7 @@ def check_post_processing_test(self, image_processor, images, model, pad_input=T\n         for img, out, expected_slice in zip(images, outputs, expected_slices):\n             out = out[\"predicted_depth\"]\n             self.assertTrue(img.size == out.shape[::-1])\n-            self.assertTrue(torch.allclose(expected_slice, out[:3, :3], rtol=1e-3))\n+            torch.testing.assert_close(expected_slice, out[:3, :3], rtol=1e-3)\n \n         self.check_target_size(image_processor, pad_input, images, outputs, raw_outputs, raw_outputs_flipped)\n "
        },
        {
            "sha": "61b60901ca1a9a87b54812d77f8f1c8f3f7af6dc",
            "filename": "tests/peft_integration/test_peft_integration.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fpeft_integration%2Ftest_peft_integration.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpeft_integration%2Ftest_peft_integration.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -166,7 +166,7 @@ def test_peft_enable_disable_adapters(self):\n \n                 peft_logits_enabled = peft_model(dummy_input).logits\n \n-                self.assertTrue(torch.allclose(peft_logits, peft_logits_enabled, atol=1e-12, rtol=1e-12))\n+                torch.testing.assert_close(peft_logits, peft_logits_enabled, rtol=1e-12, atol=1e-12)\n                 self.assertFalse(torch.allclose(peft_logits_enabled, peft_logits_disabled, atol=1e-12, rtol=1e-12))\n \n     def test_peft_add_adapter(self):"
        },
        {
            "sha": "f7e3c83829801f9f1f3aa1d8118749f9439341bf",
            "filename": "tests/quantization/bnb/test_4bit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fquantization%2Fbnb%2Ftest_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fbnb%2Ftest_4bit.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -684,7 +684,7 @@ def test_serialization(self, quant_type=\"nf4\", double_quant=True, safe_serializa\n         encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)\n         out_0 = model_0(**encoded_input)\n         out_1 = model_1(**encoded_input)\n-        self.assertTrue(torch.allclose(out_0[\"logits\"], out_1[\"logits\"], atol=0.05))\n+        torch.testing.assert_close(out_0[\"logits\"], out_1[\"logits\"], rtol=0.05, atol=0.05)\n \n         # comparing generate() outputs\n         encoded_input = tokenizer(self.input_text, return_tensors=\"pt\").to(torch_device)"
        },
        {
            "sha": "3ccf9508de778a6575553d7a7b2538270d810cab",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -784,7 +784,7 @@ def test_mamba_weights_conversion_fp16(self):\n                 if \"mixer.A_log\" in layer_name:\n                     # we should increase tolerance after exponential reversing\n                     # and performing np.log(-weights) operation as numbers are slightly different\n-                    torch.testing.assert_close(original_params, converted_state_dict[layer_name], atol=1e-3, rtol=1e-3)\n+                    torch.testing.assert_close(original_params, converted_state_dict[layer_name], rtol=1e-3, atol=1e-3)\n                 else:\n                     torch.testing.assert_close(original_params, converted_state_dict[layer_name])\n             else:"
        },
        {
            "sha": "b722624564bf4443f1851c4fac95a766392b6549",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -181,10 +181,7 @@ def test_slow_fast_equivalence(self):\n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n-        self.assertLessEqual(\n-            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n-        )\n+        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, rtol=1e-1, atol=1e-2)\n \n     @require_vision\n     @require_torch\n@@ -493,7 +490,7 @@ def test_can_compile_fast_image_processor(self):\n         image_processor = torch.compile(image_processor, mode=\"reduce-overhead\")\n         output_compiled = image_processor(input_image, device=torch_device, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(output_eager.pixel_values, output_compiled.pixel_values, atol=1e-4))\n+        torch.testing.assert_close(output_eager.pixel_values, output_compiled.pixel_values, rtol=1e-4, atol=1e-4)\n \n \n class AnnotationFormatTestMixin:\n@@ -549,7 +546,7 @@ def _compare(a, b) -> None:\n                 for idx in range(len(a)):\n                     _compare(a[idx], b[idx])\n             elif isinstance(a, torch.Tensor):\n-                self.assertTrue(torch.allclose(a, b, atol=1e-3))\n+                torch.testing.assert_close(a, b, rtol=1e-3, atol=1e-3)\n             elif isinstance(a, str):\n                 self.assertEqual(a, b)\n "
        },
        {
            "sha": "98709ba3b84a5bcc3bc5ebd0783b8155da06952a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 31,
            "deletions": 19,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -1771,7 +1771,7 @@ def test_feed_forward_chunking(self):\n             model.eval()\n \n             hidden_states_with_chunk = model(**self._prepare_for_class(inputs_dict, model_class))[0]\n-            self.assertTrue(torch.allclose(hidden_states_no_chunk, hidden_states_with_chunk, atol=1e-3))\n+            torch.testing.assert_close(hidden_states_no_chunk, hidden_states_with_chunk, rtol=1e-3, atol=1e-3)\n \n     def test_resize_position_vector_embeddings(self):\n         if not self.test_resize_position_embeddings:\n@@ -1898,7 +1898,7 @@ def test_resize_tokens_embeddings(self):\n             else:\n                 old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n                 new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n-            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, rtol=1e-3, atol=1e-3)\n \n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             if not is_deepspeed_zero3_enabled():\n@@ -2006,7 +2006,7 @@ def test_resize_tokens_embeddings(self):\n             else:\n                 old_embeddings_mean = torch.mean(model_embed.weight.data[:-10, :], axis=0)\n                 new_embeddings_mean = torch.mean(model_embed.weight.data[-10:, :], axis=0)\n-            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, rtol=1e-3, atol=1e-3)\n \n     @require_deepspeed\n     @require_torch_accelerator\n@@ -2081,7 +2081,7 @@ def test_resize_embeddings_untied(self):\n             else:\n                 old_embeddings_mean = torch.mean(output_embeds.weight.data[:-10, :], axis=0)\n                 new_embeddings_mean = torch.mean(output_embeds.weight.data[-10:, :], axis=0)\n-            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, atol=1e-3, rtol=1e-1)\n+            torch.testing.assert_close(old_embeddings_mean, new_embeddings_mean, rtol=1e-3, atol=1e-3)\n             # check if the old bias mean close to added bias mean.\n             if output_embeds.bias is not None:\n                 if is_deepspeed_zero3_enabled():\n@@ -2092,7 +2092,7 @@ def test_resize_embeddings_untied(self):\n                     old_bias_mean = torch.mean(output_embeds.bias.data[:-10], axis=0)\n                     new_bias_mean = torch.mean(output_embeds.bias.data[-10:], axis=0)\n \n-                torch.testing.assert_close(old_bias_mean, new_bias_mean, atol=1e-5, rtol=1e-2)\n+                torch.testing.assert_close(old_bias_mean, new_bias_mean, rtol=1e-5, atol=1e-5)\n \n             # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n             model.resize_token_embeddings(model_vocab_size - 15)\n@@ -3049,7 +3049,7 @@ def test_inputs_embeds_matches_input_ids(self):\n                     out_embeds = model(\n                         inputs_embeds=inputs_embeds, decoder_inputs_embeds=decoder_inputs_embeds, **inputs\n                     )[0]\n-            self.assertTrue(torch.allclose(out_embeds, out_ids))\n+            torch.testing.assert_close(out_embeds, out_ids)\n \n     @require_non_xpu\n     @require_torch_multi_gpu\n@@ -3170,10 +3170,10 @@ def cast_to_device(dictionary, device):\n \n             for value, parallel_value in zip(output, parallel_output):\n                 if isinstance(value, torch.Tensor):\n-                    self.assertTrue(torch.allclose(value, parallel_value.to(\"cpu\"), atol=1e-7))\n+                    torch.testing.assert_close(value, parallel_value.to(\"cpu\"), rtol=1e-7, atol=1e-7)\n                 elif isinstance(value, (Tuple, List)):\n                     for value_, parallel_value_ in zip(value, parallel_value):\n-                        self.assertTrue(torch.allclose(value_, parallel_value_.to(\"cpu\"), atol=1e-7))\n+                        torch.testing.assert_close(value_, parallel_value_.to(\"cpu\"), rtol=1e-7, atol=1e-7)\n \n     def check_device_map_is_respected(self, model, device_map):\n         for param_name, param in model.named_parameters():\n@@ -3229,9 +3229,12 @@ def test_disk_offload_bin(self):\n                 new_output = new_model(**inputs_dict_class)\n \n                 if isinstance(base_output[0], tuple) and isinstance(new_output[0], tuple):\n-                    self.assertTrue(torch.allclose(a, b, atol=1e-5) for a, b in zip(base_output[0], new_output[0]))\n+                    [\n+                        torch.testing.assert_close(a, b, rtol=1e-5, atol=1e-5)\n+                        for a, b in zip(base_output[0], new_output[0])\n+                    ]\n                 else:\n-                    self.assertTrue(torch.allclose(base_output[0], new_output[0], atol=1e-5))\n+                    torch.testing.assert_close(base_output[0], new_output[0], rtol=1e-5, atol=1e-5)\n \n     @require_accelerate\n     @mark.accelerate_tests\n@@ -3264,9 +3267,12 @@ def test_disk_offload_safetensors(self):\n                 new_output = new_model(**inputs_dict_class)\n \n                 if isinstance(base_output[0], tuple) and isinstance(new_output[0], tuple):\n-                    self.assertTrue(torch.allclose(a, b, atol=1e-5) for a, b in zip(base_output[0], new_output[0]))\n+                    [\n+                        torch.testing.assert_close(a, b, rtol=1e-5, atol=1e-5)\n+                        for a, b in zip(base_output[0], new_output[0])\n+                    ]\n                 else:\n-                    self.assertTrue(torch.allclose(base_output[0], new_output[0], atol=1e-5))\n+                    torch.testing.assert_close(base_output[0], new_output[0], rtol=1e-5, atol=1e-5)\n \n     @require_accelerate\n     @mark.accelerate_tests\n@@ -3303,9 +3309,12 @@ def test_cpu_offload(self):\n                     new_output = new_model(**inputs_dict_class)\n \n                     if isinstance(base_output[0], tuple) and isinstance(new_output[0], tuple):\n-                        self.assertTrue(torch.allclose(a, b, atol=1e-5) for a, b in zip(base_output[0], new_output[0]))\n+                        [\n+                            torch.testing.assert_close(a, b, rtol=1e-5, atol=1e-5)\n+                            for a, b in zip(base_output[0], new_output[0])\n+                        ]\n                     else:\n-                        self.assertTrue(torch.allclose(base_output[0], new_output[0], atol=1e-5))\n+                        torch.testing.assert_close(base_output[0], new_output[0], rtol=1e-5, atol=1e-5)\n \n     @require_accelerate\n     @mark.accelerate_tests\n@@ -3341,9 +3350,12 @@ def test_model_parallelism(self):\n                     new_output = new_model(**inputs_dict_class)\n \n                     if isinstance(base_output[0], tuple) and isinstance(new_output[0], tuple):\n-                        self.assertTrue(torch.allclose(a, b, atol=1e-5) for a, b in zip(base_output[0], new_output[0]))\n+                        [\n+                            torch.testing.assert_close(a, b, rtol=1e-5, atol=1e-5)\n+                            for a, b in zip(base_output[0], new_output[0])\n+                        ]\n                     else:\n-                        self.assertTrue(torch.allclose(base_output[0], new_output[0], atol=1e-5))\n+                        torch.testing.assert_close(base_output[0], new_output[0], rtol=1e-5, atol=1e-5)\n \n     def test_problem_types(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n@@ -4555,10 +4567,10 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n                 logits_padfree = res_padfree.logits[0]\n \n-                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), atol=0, rtol=0)\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n                 # acceptable numerical instability\n                 tol = torch.finfo(torch.float16).eps\n-                torch.testing.assert_close(logits_padded, logits_padfree, atol=tol, rtol=tol)\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n     @is_pt_tf_cross_test\n     def test_tf_from_pt_safetensors(self):\n@@ -4780,7 +4792,7 @@ def test_forward_with_logits_to_keep(self):\n             self.assertEqual(tuple(last_token_logits.shape), (batch_size, 1, vocab_size))\n \n             # Assert the last tokens are actually the same (except for the natural fluctuation due to order of FP ops)\n-            self.assertTrue(torch.allclose(all_logits[:, -1:, :], last_token_logits, atol=1e-5))\n+            torch.testing.assert_close(all_logits[:, -1:, :], last_token_logits, rtol=1e-5, atol=1e-5)\n \n     @require_torch_gpu\n     def test_flex_attention_with_grads(self):"
        },
        {
            "sha": "3de94511fb8ea19e5411079f414affe3c618e05f",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -609,8 +609,8 @@ def check_best_model_has_been_loaded(\n                 state_dict = safetensors.torch.load_file(os.path.join(checkpoint, SAFE_WEIGHTS_NAME))\n             best_model.load_state_dict(state_dict)\n             best_model.to(trainer.args.device)\n-        self.assertTrue(torch.allclose(best_model.a, trainer.model.a))\n-        self.assertTrue(torch.allclose(best_model.b, trainer.model.b))\n+        torch.testing.assert_close(best_model.a, trainer.model.a)\n+        torch.testing.assert_close(best_model.b, trainer.model.b)\n \n         metrics = trainer.evaluate()\n         self.assertEqual(metrics[metric], best_value)\n@@ -698,8 +698,8 @@ def setUp(self):\n     def check_trained_model(self, model, alternate_seed=False):\n         # Checks a training seeded with learning_rate = 0.1\n         (a, b) = self.alternate_trained_model if alternate_seed else self.default_trained_model\n-        self.assertTrue(torch.allclose(model.a, a))\n-        self.assertTrue(torch.allclose(model.b, b))\n+        torch.testing.assert_close(model.a, a)\n+        torch.testing.assert_close(model.b, b)\n \n     def test_reproducible_training(self):\n         # Checks that training worked, model trained and seed made a reproducible training.\n@@ -1567,8 +1567,7 @@ def test_neftune(self):\n         # Check that we get identical embeddings just in case\n         emb1 = trainer.model.get_input_embeddings()(dummy_input)\n         emb2 = trainer.model.get_input_embeddings()(dummy_input)\n-\n-        self.assertTrue(torch.allclose(emb1, emb2), \"Neftune noise is still applied!\")\n+        torch.testing.assert_close(emb1, emb2)\n \n     def test_logging_inf_nan_filter(self):\n         config = GPT2Config(vocab_size=100, n_positions=128, n_embd=32, n_layer=3, n_head=4)"
        },
        {
            "sha": "4a525d9faa81617ef0b10b1d21cc1ab0d08a5aaa",
            "filename": "tests/trainer/test_trainer_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_utils.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -162,7 +162,7 @@ def test_label_smoothing(self):\n         label_smoothed_loss = LabelSmoother(0.1)(model_output, random_labels)\n         log_probs = -nn.functional.log_softmax(random_logits, dim=-1)\n         expected_loss = (1 - epsilon) * loss + epsilon * log_probs.mean()\n-        self.assertTrue(torch.allclose(label_smoothed_loss, expected_loss))\n+        torch.testing.assert_close(label_smoothed_loss, expected_loss)\n \n         # With a few -100 labels\n         random_labels[0, 1] = -100\n@@ -178,7 +178,7 @@ def test_label_smoothing(self):\n         log_probs[2, 1] = 0.0\n         log_probs[2, 3] = 0.0\n         expected_loss = (1 - epsilon) * loss + epsilon * log_probs.sum() / (num_labels * 17)\n-        self.assertTrue(torch.allclose(label_smoothed_loss, expected_loss))\n+        torch.testing.assert_close(label_smoothed_loss, expected_loss)\n \n     def test_group_by_length(self):\n         # Get some inputs of random lengths"
        },
        {
            "sha": "e19b575d1523a3d2dadb8f1678a0a125ee93a869",
            "filename": "tests/utils/test_activations.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Futils%2Ftest_activations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Futils%2Ftest_activations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_activations.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -29,7 +29,7 @@ class TestActivations(unittest.TestCase):\n     def test_gelu_versions(self):\n         x = torch.tensor([-100, -1, -0.1, 0, 0.1, 1.0, 100])\n         torch_builtin = get_activation(\"gelu\")\n-        self.assertTrue(torch.allclose(gelu_python(x), torch_builtin(x)))\n+        torch.testing.assert_close(gelu_python(x), torch_builtin(x))\n         self.assertFalse(torch.allclose(gelu_python(x), gelu_new(x)))\n \n     def test_gelu_10(self):\n@@ -43,7 +43,7 @@ def test_gelu_10(self):\n         clipped_mask = torch.where(y_gelu_10 < 10.0, 1, 0)\n \n         self.assertTrue(torch.max(y_gelu_10).item() == 10.0)\n-        self.assertTrue(torch.allclose(y_gelu * clipped_mask, y_gelu_10 * clipped_mask))\n+        torch.testing.assert_close(y_gelu * clipped_mask, y_gelu_10 * clipped_mask)\n \n     def test_get_activation(self):\n         get_activation(\"gelu\")"
        },
        {
            "sha": "dfc31100509601778b0889f08ef05530aa1003d6",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 21,
            "deletions": 21,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b912f5ee438a1644247da13d789166ec77bb2304/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=b912f5ee438a1644247da13d789166ec77bb2304",
            "patch": "@@ -742,14 +742,14 @@ def test_checkpoint_sharding_local_bin(self):\n                 # Finally, check the model can be reloaded\n                 new_model = BertModel.from_pretrained(tmp_dir)\n                 for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-                    self.assertTrue(torch.allclose(p1, p2))\n+                    torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_sharding_from_hub(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert-sharded\")\n         # the model above is the same as the model below, just a sharded version.\n         ref_model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n         for p1, p2 in zip(model.parameters(), ref_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_variant_local_bin(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n@@ -769,7 +769,7 @@ def test_checkpoint_variant_local_bin(self):\n             new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\")\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_variant_local_sharded_bin(self):\n         model = BertModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n@@ -793,7 +793,7 @@ def test_checkpoint_variant_local_sharded_bin(self):\n             new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\")\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     @require_safetensors\n     def test_checkpoint_variant_local_safe(self):\n@@ -814,7 +814,7 @@ def test_checkpoint_variant_local_safe(self):\n             new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\")\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     @require_safetensors\n     def test_checkpoint_variant_local_sharded_safe(self):\n@@ -839,7 +839,7 @@ def test_checkpoint_variant_local_sharded_safe(self):\n             new_model = BertModel.from_pretrained(tmp_dir, variant=\"v2\")\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_loading_only_safetensors_available(self):\n         # Test that the loading behaviour is as expected when only safetensor checkpoints are available\n@@ -872,7 +872,7 @@ def test_checkpoint_loading_only_safetensors_available(self):\n             new_model = BertModel.from_pretrained(tmp_dir)\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_loading_only_pytorch_bin_available(self):\n         # Test that the loading behaviour is as expected when only pytorch checkpoints are available\n@@ -905,7 +905,7 @@ def test_checkpoint_loading_only_pytorch_bin_available(self):\n             new_model = BertModel.from_pretrained(tmp_dir)\n \n         for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_checkpoint_variant_hub(self):\n         with tempfile.TemporaryDirectory() as tmp_dir:\n@@ -1068,7 +1068,7 @@ def test_from_pretrained_disk_offload_task_model(self):\n             )\n             outputs2 = new_model_with_offload(inputs)\n \n-            self.assertTrue(torch.allclose(outputs1.logits.cpu(), outputs2.logits.cpu()))\n+            torch.testing.assert_close(outputs1.logits.cpu(), outputs2.logits.cpu())\n \n             # With state dict temp offload\n             new_model_with_offload = AutoModelForCausalLM.from_pretrained(\n@@ -1078,7 +1078,7 @@ def test_from_pretrained_disk_offload_task_model(self):\n                 offload_state_dict=True,\n             )\n             outputs2 = new_model_with_offload(inputs)\n-            self.assertTrue(torch.allclose(outputs1.logits.cpu(), outputs2.logits.cpu()))\n+            torch.testing.assert_close(outputs1.logits.cpu(), outputs2.logits.cpu())\n \n     @require_accelerate\n     @mark.accelerate_tests\n@@ -1108,7 +1108,7 @@ def test_from_pretrained_disk_offload_derived_to_base_model(self):\n                 tmp_dir, device_map=device_map, offload_folder=offload_folder\n             )\n             outputs2 = base_model_with_offload(inputs)\n-            self.assertTrue(torch.allclose(outputs1[0].cpu(), outputs2[0].cpu()))\n+            torch.testing.assert_close(outputs1[0].cpu(), outputs2[0].cpu())\n \n             # With state dict temp offload\n             new_model_with_offload = AutoModel.from_pretrained(\n@@ -1118,7 +1118,7 @@ def test_from_pretrained_disk_offload_derived_to_base_model(self):\n                 offload_state_dict=True,\n             )\n             outputs2 = new_model_with_offload(inputs)\n-            self.assertTrue(torch.allclose(outputs1[0].cpu(), outputs2[0].cpu()))\n+            torch.testing.assert_close(outputs1[0].cpu(), outputs2[0].cpu())\n \n     @slow\n     @require_torch\n@@ -1169,7 +1169,7 @@ def test_save_model_with_device_map_cpu(self):\n             saved_model = AutoModelForCausalLM.from_pretrained(tmp_dir, device_map=\"cpu\")\n             saved_model_output = saved_model(inputs)[0]\n \n-        self.assertTrue(torch.allclose(output, saved_model_output))\n+        torch.testing.assert_close(output, saved_model_output)\n \n     @require_accelerate\n     @mark.accelerate_tests\n@@ -1205,8 +1205,8 @@ def test_save_offloaded_model(self):\n             saved_model = AutoModelForCausalLM.from_pretrained(tmp_dir, device_map=device_map)\n             postsaved_output = saved_model(inputs)[0]\n \n-        self.assertTrue(torch.allclose(output, presaved_output, atol=1e-4))\n-        self.assertTrue(torch.allclose(presaved_output, postsaved_output))\n+        torch.testing.assert_close(output, presaved_output, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(presaved_output, postsaved_output)\n \n     @require_safetensors\n     def test_use_safetensors(self):\n@@ -1278,7 +1278,7 @@ def test_safetensors_save_and_load(self):\n \n             # Check models are equal\n             for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-                self.assertTrue(torch.allclose(p1, p2))\n+                torch.testing.assert_close(p1, p2)\n \n     @require_safetensors\n     def test_safetensors_load_from_hub(self):\n@@ -1287,7 +1287,7 @@ def test_safetensors_load_from_hub(self):\n \n         # Check models are equal\n         for p1, p2 in zip(safetensors_model.parameters(), pytorch_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     @require_safetensors\n     def test_safetensors_save_and_load_sharded(self):\n@@ -1305,7 +1305,7 @@ def test_safetensors_save_and_load_sharded(self):\n \n             # Check models are equal\n             for p1, p2 in zip(model.parameters(), new_model.parameters()):\n-                self.assertTrue(torch.allclose(p1, p2))\n+                torch.testing.assert_close(p1, p2)\n \n     @require_safetensors\n     def test_safetensors_load_from_hub_sharded(self):\n@@ -1314,7 +1314,7 @@ def test_safetensors_load_from_hub_sharded(self):\n \n         # Check models are equal\n         for p1, p2 in zip(safetensors_model.parameters(), pytorch_model.parameters()):\n-            self.assertTrue(torch.allclose(p1, p2))\n+            torch.testing.assert_close(p1, p2)\n \n     def test_base_model_to_head_model_load(self):\n         base_model = BaseModel(PretrainedConfig())\n@@ -1324,7 +1324,7 @@ def test_base_model_to_head_model_load(self):\n             # Can load a base model in a model with head\n             model = ModelWithHead.from_pretrained(tmp_dir)\n             for p1, p2 in zip(model.base.parameters(), base_model.parameters()):\n-                self.assertTrue(torch.allclose(p1, p2))\n+                torch.testing.assert_close(p1, p2)\n \n             # It doesn't work if the state dict has a mix of keys of the head and base without prefix though.\n             base_state_dict = base_model.state_dict()\n@@ -1615,7 +1615,7 @@ def test_model_from_pretrained_from_mlx(self):\n         with torch.no_grad():\n             outputs = model(input_ids)\n             outputs_from_saved = new_model(input_ids)\n-            self.assertTrue(torch.allclose(outputs_from_saved[\"logits\"], outputs[\"logits\"]))\n+            torch.testing.assert_close(outputs_from_saved[\"logits\"], outputs[\"logits\"])\n \n     def test_warning_for_beta_gamma_parameters(self):\n         class TestGammaBetaNorm(torch.nn.Module):"
        }
    ],
    "stats": {
        "total": 2017,
        "additions": 1048,
        "deletions": 969
    }
}