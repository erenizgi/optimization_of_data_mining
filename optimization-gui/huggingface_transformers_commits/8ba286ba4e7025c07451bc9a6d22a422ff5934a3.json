{
    "author": "arrdel",
    "message": "fix: correct typos in code comments (#42577)\n\nFix three typos found in code comments:\n- 'avaoid' → 'avoid' in modeling_utils.py\n- 'weigth' → 'weight' in trainer_utils.py\n- 'Templace' → 'Template' in convert_slow_tokenizer.py\n\nThese typos appeared in TODO comments and inline documentation.",
    "sha": "8ba286ba4e7025c07451bc9a6d22a422ff5934a3",
    "files": [
        {
            "sha": "4da1d66615a038183f4445bfd718d7f40ce2334a",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=8ba286ba4e7025c07451bc9a6d22a422ff5934a3",
            "patch": "@@ -1171,7 +1171,7 @@ def converted(self) -> Tokenizer:\n         )\n         tokenizer.decoder = decoders.ByteLevel()\n \n-        # Hack to have a ByteLevel and TemplaceProcessor\n+        # Hack to have a ByteLevel and TemplateProcessor\n         tokenizer.post_processor = processors.RobertaProcessing(\n             sep=(self.original_tokenizer.eos_token, self.original_tokenizer.eos_token_id),\n             cls=(self.original_tokenizer.bos_token, self.original_tokenizer.bos_token_id),"
        },
        {
            "sha": "7a4db11bb86caad316b98fa99055a6445b0e4dd3",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8ba286ba4e7025c07451bc9a6d22a422ff5934a3",
            "patch": "@@ -4180,7 +4180,7 @@ def _load_pretrained_model(\n             tp_device = list(device_map.values())[0]\n             # This is needed for the RotaryEmbedding, which was not initialized on the correct device as it is\n             # not part of the state_dict (persistent=False)\n-            for buffer in model.buffers():  # TODO to avaoid this buffer could be added to the ckpt\n+            for buffer in model.buffers():  # TODO to avoid this buffer could be added to the ckpt\n                 if buffer.device != tp_device:\n                     buffer.data = buffer.to(tp_device)\n "
        },
        {
            "sha": "399f44d3899a074c03c53a5def663a3bdf46b8e9",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8ba286ba4e7025c07451bc9a6d22a422ff5934a3/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=8ba286ba4e7025c07451bc9a6d22a422ff5934a3",
            "patch": "@@ -924,7 +924,7 @@ def load_sharded_checkpoint(model, folder, strict=True, prefer_safe=True):\n     shard_files = list(set(index[\"weight_map\"].values()))\n \n     # If strict=True, error before loading any of the state dicts.\n-    # TODO: Here, update the weigth map with the config.dynamic_weight_conversion\n+    # TODO: Here, update the weight map with the config.dynamic_weight_conversion\n     loaded_keys = index[\"weight_map\"].keys()\n     model_keys = model.state_dict().keys()\n     missing_keys = [key for key in model_keys if key not in loaded_keys]"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}