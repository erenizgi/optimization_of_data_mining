{
    "author": "avchauzov",
    "message": "Fix label smoothing incompatibility with multi-label classification (#40296)\n\n* Fix label smoothing incompatibility with multi-label classification (#40258)\n\n* Improve label smoothing multi-label check based on reviewer feedback\n\n- Move check from LabelSmoother to Trainer.__init__() for better architecture\n- Use model.config.problem_type instead of tensor inference for robustness\n- Warn and disable smoothing instead of raising error for better UX\n- Update test to verify warning behavior",
    "sha": "04c2bae3a86debd85c7dacd954c1f57a41e93a48",
    "files": [
        {
            "sha": "0a976e5e890328bc9970f159aadd6dee9cea1def",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/04c2bae3a86debd85c7dacd954c1f57a41e93a48/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04c2bae3a86debd85c7dacd954c1f57a41e93a48/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=04c2bae3a86debd85c7dacd954c1f57a41e93a48",
            "patch": "@@ -770,6 +770,16 @@ def __init__(\n         else:\n             self.label_smoother = None\n \n+        # Check for multi-label classification incompatibility\n+        if self.args.label_smoothing_factor > 0:\n+            if getattr(self.model.config, \"problem_type\", None) == \"multi_label_classification\":\n+                warnings.warn(\n+                    \"Label smoothing is not compatible with multi-label classification. \"\n+                    \"Disabling label smoothing for this training run.\",\n+                    UserWarning,\n+                )\n+                self.label_smoother = None\n+\n         self.control = TrainerControl()\n \n         self.state = TrainerState("
        },
        {
            "sha": "bc8e6e4f18e2a506f597523bf4d7a86d08c9e876",
            "filename": "tests/trainer/test_trainer_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/04c2bae3a86debd85c7dacd954c1f57a41e93a48/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/04c2bae3a86debd85c7dacd954c1f57a41e93a48/tests%2Ftrainer%2Ftest_trainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer_utils.py?ref=04c2bae3a86debd85c7dacd954c1f57a41e93a48",
            "patch": "@@ -14,9 +14,11 @@\n \n import copy\n import unittest\n+import warnings\n \n import numpy as np\n \n+from transformers import Trainer, TrainingArguments\n from transformers.data.data_collator import default_data_collator\n from transformers.testing_utils import require_accelerate, require_torch\n from transformers.trainer_utils import RemoveColumnsCollator, find_executable_batch_size\n@@ -613,3 +615,41 @@ def test_eval_loop_container(self):\n         self.assertEqual(len(arrays[2]), 2)\n         self.assertEqual(arrays[2][0].shape, (8, 2, 3))\n         self.assertEqual(arrays[2][1].shape, (8, 2))\n+\n+    def test_label_smoothing_multi_label_incompatibility(self):\n+        \"\"\"Test that Trainer warns and disables label smoothing for multi-label classification\"\"\"\n+\n+        # Mock model config with multi-label classification\n+        class MockConfig:\n+            problem_type = \"multi_label_classification\"\n+\n+        class MockModel(nn.Module):\n+            def __init__(self):\n+                super().__init__()\n+                self.config = MockConfig()\n+                self.linear = nn.Linear(10, 3)\n+\n+            def forward(self, **kwargs):\n+                return {\"logits\": torch.randn(2, 3)}\n+\n+        model = MockModel()\n+\n+        # Create training args with label smoothing\n+        training_args = TrainingArguments(\n+            output_dir=\"./test-trainer\",\n+            label_smoothing_factor=0.1,\n+            per_device_train_batch_size=2,\n+            num_train_epochs=1,\n+        )\n+\n+        # Should warn and disable label smoothing\n+        with warnings.catch_warnings(record=True) as w:\n+            warnings.simplefilter(\"always\")\n+            trainer = Trainer(model=model, args=training_args)\n+\n+            # Check warning was issued\n+            self.assertEqual(len(w), 1)\n+            self.assertIn(\"Label smoothing is not compatible with multi-label classification\", str(w[0].message))\n+\n+            # Check label_smoother was disabled\n+            self.assertIsNone(trainer.label_smoother)"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 50,
        "deletions": 0
    }
}