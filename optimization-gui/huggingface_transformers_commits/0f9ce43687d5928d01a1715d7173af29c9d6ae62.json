{
    "author": "nemitha2005",
    "message": "Standardize BertGeneration model card (#40250)\n\n* Standardize BertGeneration model card: new format, usage examples, quantization\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/bert-generation.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Apply reviewer feedback: update code examples\n\n* Add missing code example\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "0f9ce43687d5928d01a1715d7173af29c9d6ae62",
    "files": [
        {
            "sha": "e5f7fbf69ddc20a71ef36272f5a3508f946bad60",
            "filename": "docs/source/en/model_doc/bert-generation.md",
            "status": "modified",
            "additions": 104,
            "deletions": 58,
            "changes": 162,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9ce43687d5928d01a1715d7173af29c9d6ae62/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9ce43687d5928d01a1715d7173af29c9d6ae62/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbert-generation.md?ref=0f9ce43687d5928d01a1715d7173af29c9d6ae62",
            "patch": "@@ -13,84 +13,130 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on 2019-07-29 and added to Hugging Face Transformers on 2020-11-16.*\n \n-# BertGeneration\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The BertGeneration model is a BERT model that can be leveraged for sequence-to-sequence tasks using\n-[`EncoderDecoderModel`] as proposed in [Leveraging Pre-trained Checkpoints for Sequence Generation\n-Tasks](https://huggingface.co/papers/1907.12461) by Sascha Rothe, Shashi Narayan, Aliaksei Severyn.\n+# BertGeneration\n \n-The abstract from the paper is the following:\n+[BertGeneration](https://huggingface.co/papers/1907.12461) leverages pretrained BERT checkpoints for sequence-to-sequence tasks with the [`EncoderDecoderModel`] architecture. BertGeneration adapts the [`BERT`] for generative tasks.\n \n-*Unsupervised pretraining of large neural models has recently revolutionized Natural Language Processing. By\n-warm-starting from the publicly released checkpoints, NLP practitioners have pushed the state-of-the-art on multiple\n-benchmarks while saving significant amounts of compute time. So far the focus has been mainly on the Natural Language\n-Understanding tasks. In this paper, we demonstrate the efficacy of pre-trained checkpoints for Sequence Generation. We\n-developed a Transformer-based sequence-to-sequence model that is compatible with publicly available pre-trained BERT,\n-GPT-2 and RoBERTa checkpoints and conducted an extensive empirical study on the utility of initializing our model, both\n-encoder and decoder, with these checkpoints. Our models result in new state-of-the-art results on Machine Translation,\n-Text Summarization, Sentence Splitting, and Sentence Fusion.*\n+You can find all the original BERT checkpoints under the [BERT](https://huggingface.co/collections/google/bert-release-64ff5e7a4be99045d1896dbc) collection.\n \n-This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten). The original code can be\n-found [here](https://tfhub.dev/s?module-type=text-generation&subtype=module,placeholder).\n+> [!TIP]\n+> This model was contributed by [patrickvonplaten](https://huggingface.co/patrickvonplaten).\n+>\n+> Click on the BertGeneration models in the right sidebar for more examples of how to apply BertGeneration to different sequence generation tasks.\n \n-## Usage examples and tips\n+The example below demonstrates how to use BertGeneration with [`EncoderDecoderModel`] for sequence-to-sequence tasks.\n \n-The model can be used in combination with the [`EncoderDecoderModel`] to leverage two pretrained BERT checkpoints for \n-subsequent fine-tuning:\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n ```python\n->>> # leverage checkpoints for Bert2Bert model...\n->>> # use BERT's cls token as BOS token and sep token as EOS token\n->>> encoder = BertGenerationEncoder.from_pretrained(\"google-bert/bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n->>> # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n->>> decoder = BertGenerationDecoder.from_pretrained(\n-...     \"google-bert/bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n-... )\n->>> bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n-\n->>> # create tokenizer...\n->>> tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")\n-\n->>> input_ids = tokenizer(\n-...     \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n-... ).input_ids\n->>> labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n-\n->>> # train...\n->>> loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n->>> loss.backward()\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"text2text-generation\",\n+    model=\"google/roberta2roberta_L-24_discofuse\",\n+    torch_dtype=torch.float16,\n+    device=0\n+)\n+pipeline(\"Plants create energy through \")\n ```\n \n-Pretrained [`EncoderDecoderModel`] are also directly available in the model hub, e.g.:\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n \n ```python\n->>> # instantiate sentence fusion model\n->>> sentence_fuser = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n->>> tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n+import torch\n+from transformers import EncoderDecoderModel, AutoTokenizer\n+\n+model = EncoderDecoderModel.from_pretrained(\"google/roberta2roberta_L-24_discofuse\", torch_dtype=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n \n->>> input_ids = tokenizer(\n-...     \"This is the first sentence. This is the second sentence.\", add_special_tokens=False, return_tensors=\"pt\"\n-... ).input_ids\n+input_ids = tokenizer(\n+    \"Plants create energy through \", add_special_tokens=False, return_tensors=\"pt\"\n+).input_ids\n \n->>> outputs = sentence_fuser.generate(input_ids)\n+outputs = model.generate(input_ids)\n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n \n->>> print(tokenizer.decode(outputs[0]))\n+```bash\n+echo -e \"Plants create energy through \" | transformers run --task text2text-generation --model \"google/roberta2roberta_L-24_discofuse\" --device 0\n ```\n \n-Tips:\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [BitsAndBytesConfig](../quantizationbitsandbytes) to quantize the weights to 4-bit.\n+\n+```python\n+import torch\n+from transformers import EncoderDecoderModel, AutoTokenizer, BitsAndBytesConfig\n+\n+# Configure 4-bit quantization\n+quantization_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_compute_dtype=torch.float16\n+)\n+\n+model = EncoderDecoderModel.from_pretrained(\n+    \"google/roberta2roberta_L-24_discofuse\",\n+    quantization_config=quantization_config,\n+    torch_dtype=\"auto\"\n+)\n+tokenizer = AutoTokenizer.from_pretrained(\"google/roberta2roberta_L-24_discofuse\")\n+\n+input_ids = tokenizer(\n+    \"Plants create energy through \", add_special_tokens=False, return_tensors=\"pt\"\n+).input_ids\n+\n+outputs = model.generate(input_ids)\n+print(tokenizer.decode(outputs[0]))\n+```\n+\n+## Notes\n+\n+- [`BertGenerationEncoder`] and [`BertGenerationDecoder`] should be used in combination with [`EncoderDecoderModel`] for sequence-to-sequence tasks.\n+\n+   ```python\n+   from transformers import BertGenerationEncoder, BertGenerationDecoder, BertTokenizer, EncoderDecoderModel\n+   \n+   # leverage checkpoints for Bert2Bert model\n+   # use BERT's cls token as BOS token and sep token as EOS token\n+   encoder = BertGenerationEncoder.from_pretrained(\"google-bert/bert-large-uncased\", bos_token_id=101, eos_token_id=102)\n+   # add cross attention layers and use BERT's cls token as BOS token and sep token as EOS token\n+   decoder = BertGenerationDecoder.from_pretrained(\n+       \"google-bert/bert-large-uncased\", add_cross_attention=True, is_decoder=True, bos_token_id=101, eos_token_id=102\n+   )\n+   bert2bert = EncoderDecoderModel(encoder=encoder, decoder=decoder)\n+\n+   # create tokenizer\n+   tokenizer = BertTokenizer.from_pretrained(\"google-bert/bert-large-uncased\")\n+\n+   input_ids = tokenizer(\n+       \"This is a long article to summarize\", add_special_tokens=False, return_tensors=\"pt\"\n+   ).input_ids\n+   labels = tokenizer(\"This is a short summary\", return_tensors=\"pt\").input_ids\n+\n+   # train\n+   loss = bert2bert(input_ids=input_ids, decoder_input_ids=labels, labels=labels).loss\n+   loss.backward()\n+   ```\n \n-- [`BertGenerationEncoder`] and [`BertGenerationDecoder`] should be used in\n-  combination with [`EncoderDecoder`].\n - For summarization, sentence splitting, sentence fusion and translation, no special tokens are required for the input.\n-  Therefore, no EOS token should be added to the end of the input.\n+- No EOS token should be added to the end of the input for most generation tasks.\n \n ## BertGenerationConfig\n \n@@ -109,4 +155,4 @@ Tips:\n ## BertGenerationDecoder\n \n [[autodoc]] BertGenerationDecoder\n-    - forward\n+    - forward\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 104,
        "deletions": 58
    }
}