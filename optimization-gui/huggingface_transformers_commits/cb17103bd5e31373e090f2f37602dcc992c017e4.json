{
    "author": "MekkCyber",
    "message": "Uninstallling Flash attention from quantization docker (#39078)\n\n* update\n\n* revert",
    "sha": "cb17103bd5e31373e090f2f37602dcc992c017e4",
    "files": [
        {
            "sha": "ad9cf891e258a000c9bd7a005b7707b2778080e6",
            "filename": "docker/transformers-quantization-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cb17103bd5e31373e090f2f37602dcc992c017e4/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/cb17103bd5e31373e090f2f37602dcc992c017e4/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-quantization-latest-gpu%2FDockerfile?ref=cb17103bd5e31373e090f2f37602dcc992c017e4",
            "patch": "@@ -93,6 +93,9 @@ RUN python3 -m pip install --no-cache-dir -e ./transformers[dev-torch]\n # `kernels` may give different outputs (within 1e-5 range) even with the same model (weights) and the same inputs\n RUN python3 -m pip uninstall -y kernels\n \n+# Uninstall flash-attn installed by autoawq, it causes issues here : https://github.com/huggingface/transformers/actions/runs/15915442841/job/44892146131\n+RUN python3 -m pip uninstall -y flash-attn\n+\n # When installing in editable mode, `transformers` is not recognized as a package.\n # this line must be added in order for python to be aware of transformers.\n RUN cd transformers && python3 setup.py develop"
        }
    ],
    "stats": {
        "total": 3,
        "additions": 3,
        "deletions": 0
    }
}