{
    "author": "druvdub",
    "message": "Remove merge conflict artifacts in Albert model doc (#38849)",
    "sha": "e61160c5dbd470c4644e6c248d2acb64f763b6d5",
    "files": [
        {
            "sha": "49d207fe579cbd2b0a75742235fd7c8cc6f55dd7",
            "filename": "docs/source/en/model_doc/albert.md",
            "status": "modified",
            "additions": 25,
            "deletions": 70,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/e61160c5dbd470c4644e6c248d2acb64f763b6d5/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e61160c5dbd470c4644e6c248d2acb64f763b6d5/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falbert.md?ref=e61160c5dbd470c4644e6c248d2acb64f763b6d5",
            "patch": "@@ -27,20 +27,13 @@ rendered properly in your Markdown viewer.\n \n [ALBERT](https://huggingface.co/papers/1909.11942) is designed to address memory limitations of scaling and training of [BERT](./bert). It adds two parameter reduction techniques. The first, factorized embedding parametrization, splits the larger vocabulary embedding matrix into two smaller matrices so you can grow the hidden size without adding a lot more parameters. The second, cross-layer parameter sharing, allows layer to share parameters which keeps the number of learnable parameters lower.\n \n-<<<<<<< HEAD\n-=======\n-\n-<<<<<<< HEAD\n ALBERT was created to address problems like -- GPU/TPU memory limitations, longer training times, and unexpected model degradation in BERT. ALBERT uses two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT:\n \n - **Factorized embedding parameterization:** The large vocabulary embedding matrix is decomposed into two smaller matrices, reducing memory consumption.\n - **Cross-layer parameter sharing:** Instead of learning separate parameters for each transformer layer, ALBERT shares parameters across layers, further reducing the number of learnable weights.\n \n-ALBERT uses absolute position embeddings (like BERT) so padding is applied at right. Size of embeddings is 128 While BERT uses 768. ALBERT can processes maximum 512 token at a time. \n->>>>>>> 7ba1110083 (Update docs/source/en/model_doc/albert.md\r)\n+ALBERT uses absolute position embeddings (like BERT) so padding is applied at right. Size of embeddings is 128 While BERT uses 768. ALBERT can processes maximum 512 token at a time.\n \n-=======\n->>>>>>> 155b733538 (Update albert.md)\n You can find all the original ALBERT checkpoints under the [ALBERT community](https://huggingface.co/albert) organization.\n \n > [!TIP]\n@@ -51,7 +44,7 @@ The example below demonstrates how to predict the `[MASK]` token with [`Pipeline\n <hfoptions id=\"usage\">\n <hfoption id=\"Pipeline\">\n \n-```py \n+```py\n import torch\n from transformers import pipeline\n \n@@ -80,7 +73,7 @@ model = AutoModelForMaskedLM.from_pretrained(\n )\n \n prompt = \"Plants create energy through a process known as [MASK].\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device) \n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model(**inputs)\n@@ -103,41 +96,30 @@ echo -e \"Plants create [MASK] through a process known as photosynthesis.\" | tran\n \n </hfoptions>\n \n-\n ## Notes\n \n - Inputs should be padded on the right because BERT uses absolute position embeddings.\n - The embedding size `E` is different from the hidden size `H` because the embeddings are context independent (one embedding vector represents one token) and the hidden states are context dependent (one hidden state represents a sequence of tokens). The embedding matrix is also larger because `V x E` where `V` is the vocabulary size. As a result, it's more logical if `H >> E`. If `E < H`, the model has less parameters.\n \n-\n ## Resources\n \n-\n The resources provided in the following sections consist of a list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with AlBERT. If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n \n-\n <PipelineTag pipeline=\"text-classification\"/>\n \n-\n - [`AlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification).\n \n-\n - [`TFAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification).\n \n - [`FlaxAlbertForSequenceClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/text-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification_flax.ipynb).\n - Check the [Text classification task guide](../tasks/sequence_classification) on how to use the model.\n \n-\n <PipelineTag pipeline=\"token-classification\"/>\n \n-\n - [`AlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification).\n \n-\n - [`TFAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb).\n \n-\n-\n - [`FlaxAlbertForTokenClassification`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/flax/token-classification).\n - [Token classification](https://huggingface.co/course/chapter7/2?fw=pt) chapter of the ðŸ¤— Hugging Face Course.\n - Check the [Token classification task guide](../tasks/token_classification) on how to use the model.\n@@ -163,20 +145,15 @@ The resources provided in the following sections consist of a list of official H\n - [`AlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/pytorch/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice.ipynb).\n - [`TFAlbertForMultipleChoice`] is supported by this [example script](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/multiple-choice) and [notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/multiple_choice-tf.ipynb).\n \n-- Check the  [Multiple choice task guide](../tasks/multiple_choice) on how to use the model.\n-\n+- Check the [Multiple choice task guide](../tasks/multiple_choice) on how to use the model.\n \n ## AlbertConfig\n \n [[autodoc]] AlbertConfig\n \n ## AlbertTokenizer\n \n-[[autodoc]] AlbertTokenizer\n-    - build_inputs_with_special_tokens\n-    - get_special_tokens_mask\n-    - create_token_type_ids_from_sequences\n-    - save_vocabulary\n+[[autodoc]] AlbertTokenizer - build_inputs_with_special_tokens - get_special_tokens_mask - create_token_type_ids_from_sequences - save_vocabulary\n \n ## AlbertTokenizerFast\n \n@@ -193,116 +170,94 @@ The resources provided in the following sections consist of a list of official H\n \n ## AlbertModel\n \n-[[autodoc]] AlbertModel\n-    - forward\n+[[autodoc]] AlbertModel - forward\n \n ## AlbertForPreTraining\n \n-[[autodoc]] AlbertForPreTraining\n-    - forward\n+[[autodoc]] AlbertForPreTraining - forward\n \n ## AlbertForMaskedLM\n \n-[[autodoc]] AlbertForMaskedLM\n-    - forward\n+[[autodoc]] AlbertForMaskedLM - forward\n \n ## AlbertForSequenceClassification\n \n-[[autodoc]] AlbertForSequenceClassification\n-    - forward\n+[[autodoc]] AlbertForSequenceClassification - forward\n \n ## AlbertForMultipleChoice\n \n [[autodoc]] AlbertForMultipleChoice\n \n ## AlbertForTokenClassification\n \n-[[autodoc]] AlbertForTokenClassification\n-    - forward\n+[[autodoc]] AlbertForTokenClassification - forward\n \n ## AlbertForQuestionAnswering\n \n-[[autodoc]] AlbertForQuestionAnswering\n-    - forward\n+[[autodoc]] AlbertForQuestionAnswering - forward\n \n </pt>\n \n <tf>\n \n ## TFAlbertModel\n \n-[[autodoc]] TFAlbertModel\n-    - call\n+[[autodoc]] TFAlbertModel - call\n \n ## TFAlbertForPreTraining\n \n-[[autodoc]] TFAlbertForPreTraining\n-    - call\n+[[autodoc]] TFAlbertForPreTraining - call\n \n ## TFAlbertForMaskedLM\n \n-[[autodoc]] TFAlbertForMaskedLM\n-    - call\n+[[autodoc]] TFAlbertForMaskedLM - call\n \n ## TFAlbertForSequenceClassification\n \n-[[autodoc]] TFAlbertForSequenceClassification\n-    - call\n+[[autodoc]] TFAlbertForSequenceClassification - call\n \n ## TFAlbertForMultipleChoice\n \n-[[autodoc]] TFAlbertForMultipleChoice\n-    - call\n+[[autodoc]] TFAlbertForMultipleChoice - call\n \n ## TFAlbertForTokenClassification\n \n-[[autodoc]] TFAlbertForTokenClassification\n-    - call\n+[[autodoc]] TFAlbertForTokenClassification - call\n \n ## TFAlbertForQuestionAnswering\n \n-[[autodoc]] TFAlbertForQuestionAnswering\n-    - call\n+[[autodoc]] TFAlbertForQuestionAnswering - call\n \n </tf>\n <jax>\n \n ## FlaxAlbertModel\n \n-[[autodoc]] FlaxAlbertModel\n-    - __call__\n+[[autodoc]] FlaxAlbertModel - **call**\n \n ## FlaxAlbertForPreTraining\n \n-[[autodoc]] FlaxAlbertForPreTraining\n-    - __call__\n+[[autodoc]] FlaxAlbertForPreTraining - **call**\n \n ## FlaxAlbertForMaskedLM\n \n-[[autodoc]] FlaxAlbertForMaskedLM\n-    - __call__\n+[[autodoc]] FlaxAlbertForMaskedLM - **call**\n \n ## FlaxAlbertForSequenceClassification\n \n-[[autodoc]] FlaxAlbertForSequenceClassification\n-    - __call__\n+[[autodoc]] FlaxAlbertForSequenceClassification - **call**\n \n ## FlaxAlbertForMultipleChoice\n \n-[[autodoc]] FlaxAlbertForMultipleChoice\n-    - __call__\n+[[autodoc]] FlaxAlbertForMultipleChoice - **call**\n \n ## FlaxAlbertForTokenClassification\n \n-[[autodoc]] FlaxAlbertForTokenClassification\n-    - __call__\n+[[autodoc]] FlaxAlbertForTokenClassification - **call**\n \n ## FlaxAlbertForQuestionAnswering\n \n-[[autodoc]] FlaxAlbertForQuestionAnswering\n-    - __call__\n+[[autodoc]] FlaxAlbertForQuestionAnswering - **call**\n \n </jax>\n </frameworkcontent>\n-\n-"
        }
    ],
    "stats": {
        "total": 95,
        "additions": 25,
        "deletions": 70
    }
}