{
    "author": "zucchini-nlp",
    "message": "Load generation config from nested configs (#42922)\n\n* fix\n\n* add comment\n\n* add a test\n\n* wording\n\n* this is it!",
    "sha": "f2c6d2ad358f2190cffdfdf40c6c81e724a2b431",
    "files": [
        {
            "sha": "abac00dbba16bb372e780bd329ed406b3304445c",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 10,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=f2c6d2ad358f2190cffdfdf40c6c81e724a2b431",
            "patch": "@@ -1150,20 +1150,25 @@ def from_model_config(cls, model_config: Union[\"PreTrainedConfig\", dict]) -> \"Ge\n \n         # Removes all `None` from the model config dict -- this lets the generation config defaults to take hold\n         config_dict = {key: value for key, value in config_dict.items() if value is not None}\n-\n         generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n \n         # Special case: some models have generation attributes set in the decoder. Use them if still unset in the\n         # generation config (which in turn is defined from the outer attributes of model config).\n-        if not isinstance(model_config, dict):\n-            decoder_config = model_config.get_text_config(decoder=True)\n-            if decoder_config is not model_config:\n-                default_generation_config = GenerationConfig()\n-                decoder_config_dict = decoder_config.to_dict()\n-                for attr in generation_config.to_dict():\n-                    is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n-                    if attr in decoder_config_dict and is_unset:\n-                        setattr(generation_config, attr, decoder_config_dict[attr])\n+        if isinstance(model_config, dict):\n+            decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n+            for text_config_name in decoder_possible_text_config_names:\n+                if text_config := model_config.get(text_config_name):\n+                    model_config = text_config\n+                    break\n+        else:\n+            model_config = model_config.get_text_config(decoder=True)\n+            model_config = model_config.to_dict()\n+\n+        default_generation_config = GenerationConfig()\n+        for attr in generation_config.to_dict():\n+            is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n+            if attr in model_config and is_unset:\n+                setattr(generation_config, attr, model_config[attr])\n \n         # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n         if not generation_config.return_dict_in_generate:"
        },
        {
            "sha": "2896ed0742d0cc20e03eedab77df1fd4ece522cb",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f2c6d2ad358f2190cffdfdf40c6c81e724a2b431",
            "patch": "@@ -406,6 +406,9 @@ def adjust_generation_fn(\n                     **repo_loading_kwargs,\n                 )\n             except OSError:\n+                # `self` already has a generation config created from model config, but model config will\n+                # not contain any generation-specific params. These are popped at config's `__init__`.\n+                # Thus we have to load from `config.json` and create a generation config from it (for BART)\n                 logger.info(\n                     \"Generation config file not found, using a generation config created from the model config.\"\n                 )\n@@ -417,6 +420,7 @@ def adjust_generation_fn(\n                     _from_model_config=True,\n                     **repo_loading_kwargs,\n                 )\n+\n             # Load custom generate function if `pretrained_model_name_or_path` defines it (and override `generate`)\n             if hasattr(self, \"load_custom_generate\") and trust_remote_code:\n                 try:"
        },
        {
            "sha": "47e8af4b7dc601d37f0dfb83b637f95ef4d80b0b",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f2c6d2ad358f2190cffdfdf40c6c81e724a2b431/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=f2c6d2ad358f2190cffdfdf40c6c81e724a2b431",
            "patch": "@@ -67,7 +67,6 @@\n         AutoModelForImageTextToText,\n         AutoModelForSeq2SeqLM,\n         AutoModelForSpeechSeq2Seq,\n-        AutoModelForVision2Seq,\n         BartForConditionalGeneration,\n         BartTokenizer,\n         DataCollatorWithFlattening,\n@@ -4427,7 +4426,7 @@ def test_generate_vision2text_conditioning(self):\n         \"\"\"Test that `decoder_input_ids` can be used to condition the generation in vision-to-text models\"\"\"\n         pixel_values = floats_tensor((2, 3, 30, 30))\n         conditioning_input = torch.tensor([[10], [10]])  # this should be the 2nd output token, after the BOS token\n-        model = AutoModelForVision2Seq.from_pretrained(\n+        model = AutoModelForImageTextToText.from_pretrained(\n             \"hf-internal-testing/tiny-random-VisionEncoderDecoderModel-vit-gpt2\"\n         )\n         pixel_values = pixel_values.to(torch_device)\n@@ -4447,6 +4446,24 @@ def test_generate_vision2text_conditioning(self):\n         self.assertTrue(np.array_equal(output_sequences_decoder_input_ids, output_sequences_input_ids))\n         self.assertTrue(np.array_equal(output_sequences_decoder_input_ids[:, 1:2], conditioning_input))\n \n+    @pytest.mark.generate\n+    def test_load_generation_config_from_text_subconfig(self):\n+        \"\"\"\n+        Tests that generation config is be loaded from model's `text_config` when not present\n+        in the model repo. We should infer the text config correctly and re-use special tokens\n+        for generation. See https://github.com/huggingface/transformers/issues/42794\n+        \"\"\"\n+        model = AutoModelForImageTextToText.from_pretrained(\n+            \"hf-internal-testing/tiny-random-LlavaForConditionalGeneration-no-generation-config\",\n+            device_map=torch_device,\n+        )\n+        self.assertTrue(model.generation_config.eos_token_id is not None)\n+        self.assertTrue(model.generation_config.bos_token_id is not None)\n+        self.assertTrue(model.generation_config.pad_token_id is not None)\n+\n+        # test that we can generate without inputs, i.e. from BOS\n+        _ = model.generate()\n+\n     @require_read_token\n     @slow\n     @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 38,
        "deletions": 12
    }
}