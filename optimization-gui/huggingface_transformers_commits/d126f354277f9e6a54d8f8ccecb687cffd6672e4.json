{
    "author": "ArthurZucker",
    "message": "Proper_flex (#36643)\n\n* proper performant flex attention implementation\n\n* wrapper for flex attention to compile only when triggered\n\n* wrapper for flex attention to compile only when triggered\n\n* attention mask type detection\n\n* Update src/transformers/integrations/flex_attention.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* nit\n\n* nit\n\n* nit\n\n* nit\n\n* gemma2 support\n\n* add citation for torchtune\n\n* Update src/transformers/models/llama/modeling_llama.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* Update flex_attention.py\n\n* nit\n\n* nit\n\n* nit\n\n* reset gemma2 modifications\n\n* nit\n\n* nit\n\n* nit\n\n* licencing\n\n* apply changes to other models\n\n* safe import\n\n---------\n\nCo-authored-by: Sung Ching Liu <sunny19981005@outlook.com>\nCo-authored-by: Sung Ching Liu <22844540+bursteratom@users.noreply.github.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "d126f354277f9e6a54d8f8ccecb687cffd6672e4",
    "files": [
        {
            "sha": "b5fd8a1e9dfde2745541093baffb355ad8598936",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -139,6 +139,15 @@\n         \"SUPPORTED_TP_STYLES\",\n         \"translate_to_torch_parallel_style\",\n     ]\n+try:\n+    if not is_torch_greater_or_equal(\"2.5\"):\n+        raise OptionalDependencyNotAvailable()\n+except OptionalDependencyNotAvailable:\n+    pass\n+else:\n+    _import_structure[\"flex_attention\"] = [\n+        \"make_flex_block_causal_mask\",\n+    ]\n \n if TYPE_CHECKING:\n     from .aqlm import replace_with_aqlm_linear\n@@ -255,6 +264,13 @@\n             translate_to_torch_parallel_style,\n         )\n \n+    try:\n+        if not is_torch_greater_or_equal(\"2.5\"):\n+            raise OptionalDependencyNotAvailable()\n+    except OptionalDependencyNotAvailable:\n+        pass\n+    else:\n+        from .flex_attention import make_flex_block_causal_mask\n else:\n     import sys\n "
        },
        {
            "sha": "aff1eb93af77d3de998775cc032ec8e9bcb8e83c",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 151,
            "deletions": 9,
            "changes": 160,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -1,43 +1,185 @@\n-from typing import Optional, Tuple\n+\"\"\"\n+Partially inspired by torchtune's flex attention implementation\n+\n+Citation:\n+@software{torchtune,\n+  title = {torchtune: PyTorch's finetuning library},\n+  author = {torchtune maintainers and contributors},\n+  url = {https//github.com/pytorch/torchtune},\n+  license = {BSD-3-Clause},\n+  month = apr,\n+  year = {2024}\n+}\n+\"\"\"\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Optional, Tuple, Union\n \n import torch\n \n from ..utils import is_torch_flex_attn_available\n \n \n if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import flex_attention\n+    from torch.nn.attention.flex_attention import (\n+        BlockMask,\n+        flex_attention,\n+    )\n+    from torch.nn.attention.flex_attention import (\n+        create_block_mask as create_block_causal_mask_flex,\n+    )\n+\n+\n+class WrappedFlexAttention:\n+    \"\"\"\n+    We are doing a singleton class so that flex attention is compiled once when it's first called.\n+    \"\"\"\n+\n+    _instance = None\n+    _is_flex_compiled = False\n+    _compiled_flex_attention = None\n+\n+    def __new__(cls, *args, **kwargs):\n+        if cls._instance is None:\n+            # Create a new instance if one doesn't already exist\n+            cls._instance = super().__new__(cls)\n+        return cls._instance\n+\n+    @torch.compiler.disable(recursive=False)\n+    def __init__(self):\n+        \"\"\"\n+        Initialize or update the singleton instance.\n+        \"\"\"\n+        if self._is_flex_compiled is False:\n+            self._compiled_flex_attention = torch.compile(flex_attention, dynamic=False)\n+            self._is_flex_compiled = True\n+\n+    def __call__(self):\n+        return self._compiled_flex_attention\n+\n+\n+def make_flex_block_causal_mask(attention_mask_2d: torch.Tensor) -> BlockMask:\n+    \"\"\"\n+    Create a block causal document mask for a batch of sequences, both packed and unpacked.\n+    Create Block causal logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n+    The resultant BlockMask is a compressed representation of the full block causal\n+    mask. BlockMask is essential for performant computation of flex attention.\n+    See: https://pytorch.org/blog/flexattention/\n+\n+    Args:\n+        attention_mask_2d (torch.Tensor): Attention mask for packed and padded sequences\n+        of shape (batch_size, total_seq_len). e.g.\n+\n+        For unpacked sequence:\n+        [[1, 1, 1, 1, 0, 0, 0],\n+         [1, 1, 1, 1, 1, 0, 0]]\n+\n+        For packed sequence:\n+        [[1, 1, 1, 2, 2, 2, 0],\n+         [1, 1, 2, 2, 2, 3, 3]]\n+\n+    Returns:\n+        BlockMask\n+    \"\"\"\n+    device = attention_mask_2d.device\n+\n+    document_ids = attention_mask_2d\n+    batch_size, total_seq_len = document_ids.shape\n+\n+    # Instead of passing a tensor mask, flex attention requires a mask_mod function\n+    # that determines which elements of QK^T should be included in the attention\n+    # computation prior to the softmax. For sample packing, we need both the\n+    # logic for both causal mask and document mask. See PyTorch's official\n+    # blog post for more details: https://pytorch.org/blog/flexattention/#mask-mods\n+    def causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n+        \"\"\"\n+        Defines the logic of a block causal mask by combining both a standard causal mask\n+        and a block diagonal document mask.\n+\n+        See :func:`~torchtune.modules.attention_utils.create_block_causal_mask`\n+        for an illustration.\n+        \"\"\"\n+        causal_mask = q_idx >= kv_idx\n+        document_mask = document_ids[batch_idx, q_idx] == document_ids[batch_idx, kv_idx]\n+        padding_mask = document_ids[batch_idx, q_idx] > 0\n+        return causal_mask & document_mask & padding_mask\n+\n+    return create_block_causal_mask_flex(\n+        mask_mod=causal_mask_mod,\n+        B=batch_size,\n+        H=None,  # attention head\n+        Q_LEN=total_seq_len,\n+        KV_LEN=total_seq_len,\n+        device=device,\n+    )\n+\n+\n+@torch.compiler.disable(recursive=False)\n+def compile_friendly_flex_attention(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    **kwargs,\n+) -> torch.Tensor:\n+    # First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\n+    flex_attention_compiled = WrappedFlexAttention()()\n+    return flex_attention_compiled(\n+        query,\n+        key,\n+        value,\n+        **kwargs,\n+    )\n \n \n def flex_attention_forward(\n     module: torch.nn.Module,\n     query: torch.Tensor,\n     key: torch.Tensor,\n     value: torch.Tensor,\n-    attention_mask: Optional[torch.Tensor],\n+    attention_mask: Union[torch.Tensor, BlockMask],\n     scaling: Optional[float] = None,\n     softcap: Optional[float] = None,\n     head_mask: Optional[torch.Tensor] = None,\n     **kwargs,\n ) -> Tuple[torch.Tensor, torch.Tensor]:\n-    causal_mask = attention_mask\n+    block_mask = None\n+    causal_mask = None\n+    if isinstance(attention_mask, BlockMask):\n+        block_mask = attention_mask\n+    else:\n+        causal_mask = attention_mask\n+\n     if causal_mask is not None:\n         causal_mask = causal_mask[:, :, :, : key.shape[-2]]\n \n-    def causal_mod(score, b, h, q_idx, kv_idx):\n+    def score_mod(score, batch_idx, head_idx, q_idx, kv_idx):\n         if softcap is not None:\n             score = softcap * torch.tanh(score / softcap)\n         if causal_mask is not None:\n-            score = score + causal_mask[b][0][q_idx][kv_idx]\n+            score = score + causal_mask[batch_idx][0][q_idx][kv_idx]\n         if head_mask is not None:\n-            score = score + head_mask[b][h][0][0]\n+            score = score + head_mask[batch_idx][head_idx][0][0]\n         return score\n \n-    attn_output, attention_weights = flex_attention(\n+    attn_output, attention_weights = compile_friendly_flex_attention(\n         query,\n         key,\n         value,\n-        score_mod=causal_mod,\n+        score_mod=score_mod,\n+        block_mask=block_mask,\n         enable_gqa=True,\n         scale=scaling,\n         # Last time checked on PyTorch == 2.5.1: Flex Attention always computes the lse regardless."
        },
        {
            "sha": "05d5ba899792331413c75e19f5c533c1701b5534",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -34,6 +34,7 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -48,6 +49,12 @@\n     from torch import nn\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"AriaTextConfig\"\n \n@@ -1014,6 +1021,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "504220c788b51c74543429082e5e2d4cae268482",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -36,10 +36,20 @@\n     TokenClassifierOutput,\n )\n from ...modeling_utils import PreTrainedModel\n-from ...utils import is_torchdynamo_compiling, logging\n+from ...utils import (\n+    is_torch_flex_attn_available,\n+    is_torchdynamo_compiling,\n+    logging,\n+)\n from .configuration_bloom import BloomConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"bigscience/bloom-560m\"\n@@ -743,6 +753,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3c3bcb4d0117272002201e4cb584dbb2592089a6",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -41,13 +41,20 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_chameleon import ChameleonConfig, ChameleonVQVAEConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from flash_attn.bert_padding import index_first_axis, pad_input, unpad_input  # noqa\n \n@@ -1389,6 +1396,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "89e46a5523d9b08e32f2f95fb71215f04ff14499",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -26,10 +26,22 @@\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n-from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ...utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_codegen import CodeGenConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"Salesforce/codegen-2B-mono\"\n@@ -586,6 +598,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "018fc3b8622c6edbbe5d61a99fb967fb7715964e",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -45,13 +45,20 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere import CohereConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"CohereConfig\"\n@@ -664,6 +671,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "44665289488006e8b2315c366683a627532bc9eb",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -32,13 +32,20 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_dbrx import DbrxConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -1118,6 +1125,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "f490525d999484d0fcf60ac1072585751e11c716",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -48,13 +48,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_diffllama import DiffLlamaConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"kajuma/DiffLlama-0.3B-handcut\"\n@@ -903,6 +910,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "8ad29c02ad56c2aaec08fa2b02671e4c46a08585",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -41,13 +41,20 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1482,6 +1489,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "08305274725093a7c8e83d938d5fc3ef1fb33571",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -43,13 +43,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma import GemmaConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"google/gemma-7b\"\n@@ -636,6 +643,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "53f488116dae237659316aed2e0e54f6704dd7b8",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -43,13 +43,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_glm import GlmConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"THUDM/glm-4-9b\"\n@@ -645,6 +652,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "e72fff18e00f9c6871cd93ce6440f36491d59750",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -42,12 +42,19 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     is_torch_fx_available,\n     logging,\n )\n from .configuration_gpt_neo import GPTNeoConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -795,6 +802,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "9bbd94d798ad56a0ea0c7b4e31c1702f6ad5dcd4",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -29,12 +29,19 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_gpt_neox import GPTNeoXConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -639,6 +646,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3e13ce09c593e1572f264a762d9a1d3ef30966f5",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 15,
            "deletions": 1,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -29,10 +29,19 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n-from ...utils import logging\n+from ...utils import (\n+    is_torch_flex_attn_available,\n+    logging,\n+)\n from .configuration_gpt_neox_japanese import GPTNeoXJapaneseConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"abeja/gpt-neox-japanese-2.7b\"\n@@ -665,6 +674,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "24f224ad1bb293e0d185d016e19f5365e10c450b",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -40,13 +40,20 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     logging,\n )\n from ...utils.model_parallel_utils import assert_device_map, get_device_map\n from .configuration_gptj import GPTJConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -894,6 +901,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "1822bd627d1d47b18da7806f7d527f0675467c48",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -37,13 +37,20 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_granite import GraniteConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"GraniteConfig\"\n \n@@ -648,6 +655,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "73086f958c7541f81dd3f1b196e4e14edb6a09e3",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -37,12 +37,19 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_granitemoe import GraniteMoeConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"GraniteMoeConfig\"\n@@ -1121,6 +1128,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3dc86991c73182c99520a0b8ddd0b7fe0e21836b",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -37,12 +37,19 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_granitemoeshared import GraniteMoeSharedConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1066,6 +1073,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "c3f57149d8024d85f4e7b2f78afcd333a8629259",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -44,13 +44,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_helium import HeliumConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"google/helium-7b\"\n@@ -632,6 +639,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "3ca196936c3e93bd7e0e90e5ed56ef5c20cf739b",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -38,6 +38,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n@@ -46,6 +47,12 @@\n from .vision import IdeficsVisionTransformer\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"IdeficsConfig\"\n@@ -1366,6 +1373,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "814c62a5fd9627b2aa91cc1147f797aa6389c373",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -38,13 +38,20 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_jetmoe import JetMoeConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -1127,6 +1134,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "159f41b3cecbe957a47f3ceb15b18119155a76ff",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -44,13 +44,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_llama import LlamaConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"meta-llama/Llama-2-7b-hf\"\n@@ -634,6 +641,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "9dce316236033947d5da6b49dae5fd9dae762e26",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -40,6 +40,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -48,6 +49,12 @@\n from .configuration_longt5 import LongT5Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"LongT5Config\"\n@@ -1603,6 +1610,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "a86d00ed8be7d454302a23048c2f02e2cf5fba6e",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -32,6 +32,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n     replace_return_docstrings,\n@@ -40,6 +41,12 @@\n from .configuration_mllama import MllamaConfig, MllamaTextConfig, MllamaVisionConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -1081,6 +1088,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "963b7e7aa25e22cc3e7af45854179855193c97de",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -46,12 +46,19 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_moonshine import MoonshineConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"MoonshineConfig\"\n \n@@ -998,6 +1005,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "9c2d23b7afbd2c6f5f32475f1b24ffb648f0a3ec",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -44,6 +44,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -53,6 +54,11 @@\n from .configuration_mt5 import MT5Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"MT5Config\"\n@@ -1195,6 +1201,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "ef73c55e0fd590e62cc81597e27fd872e857974b",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -43,13 +43,20 @@\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_nemotron import NemotronConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"nvidia/nemotron-3-8b-base-4k-hf\"\n@@ -882,6 +889,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "2f6dc1fa9c9b21afc6d83dc5763b93da0d8d14d4",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -23,13 +23,20 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo import OlmoConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"OlmoConfig\"\n \n@@ -610,6 +617,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "05b8d17223836acf2312f54d4ef47f7cd75c9077",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -22,13 +22,20 @@\n     LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_olmo2 import Olmo2Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n _CONFIG_FOR_DOC = \"Olmo2Config\"\n \n@@ -611,6 +618,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "a306e84b23a86595852da769cf7edb75bd7bb5f7",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -40,12 +40,19 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_opt import OPTConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -642,6 +649,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "df6153c81185626b7472acd044a2e8fe06437c1a",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -42,13 +42,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_persimmon import PersimmonConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"adept/persimmon-8b-base\"\n@@ -682,6 +689,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "cac12d59b00ecaead6b03fd16a13899ae47071e0",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -28,13 +28,20 @@\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_phi import PhiConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"microsoft/phi-1\"\n@@ -608,6 +615,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "42e190d2e44da32e33ae4c3763d383a857741f75",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -39,6 +39,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -47,6 +48,12 @@\n from .configuration_pix2struct import Pix2StructConfig, Pix2StructTextConfig, Pix2StructVisionConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n # General docstring\n@@ -1590,6 +1597,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "0c4a2eda978696e3cf6ac4ef6fe7ac8cfdd288ee",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -38,6 +38,7 @@\n from ...utils import (\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -46,6 +47,12 @@\n from .configuration_pop2piano import Pop2PianoConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _load_pop2piano_layer_norm = True\n@@ -1003,6 +1010,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "4621851fea88304b34031c3799619d885d8163b7",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -44,13 +44,20 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_stablelm import StableLmConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -937,6 +944,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "a347654a4902f098c7a572a71d7c83e9b180387e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -40,6 +40,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -48,6 +49,12 @@\n from .configuration_switch_transformers import SwitchTransformersConfig\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"SwitchTransformersConfig\"\n@@ -1139,6 +1146,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "ba96c10ed0c7d091a746b304c94abdac13709be5",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -44,6 +44,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -53,6 +54,12 @@\n from .configuration_t5 import T5Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"T5Config\"\n@@ -1208,6 +1215,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "2b0e27f45e185efaabd449e2e38ec0092b5f90b7",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -43,11 +43,18 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     replace_return_docstrings,\n )\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n+\n logger = logging.getLogger(__name__)\n \n \n@@ -1541,6 +1548,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "7b868696f640583480c5543237d79888dec01e9f",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -41,6 +41,7 @@\n     DUMMY_MASK,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n+    is_torch_flex_attn_available,\n     is_torch_fx_proxy,\n     is_torchdynamo_compiling,\n     logging,\n@@ -49,6 +50,11 @@\n from .configuration_umt5 import UMT5Config\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n logger = logging.get_logger(__name__)\n \n _CONFIG_FOR_DOC = \"UMT5Config\"\n@@ -852,6 +858,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        },
        {
            "sha": "2bcf4026a35ad58ff5d5ba1bee272d25a2261f34",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d126f354277f9e6a54d8f8ccecb687cffd6672e4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=d126f354277f9e6a54d8f8ccecb687cffd6672e4",
            "patch": "@@ -41,13 +41,19 @@\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n     is_flash_attn_greater_or_equal_2_10,\n+    is_torch_flex_attn_available,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_whisper import WhisperConfig\n from .generation_whisper import WhisperGenerationMixin\n \n \n+if is_torch_flex_attn_available():\n+    from torch.nn.attention.flex_attention import BlockMask\n+\n+    from ...integrations.flex_attention import make_flex_block_causal_mask\n+\n if is_flash_attn_2_available():\n     from ...modeling_flash_attention_utils import _flash_attention_forward\n \n@@ -1378,6 +1384,11 @@ def _update_causal_mask(\n             if attention_mask is not None and (attention_mask == 0.0).any():\n                 return attention_mask\n             return None\n+        if self.config._attn_implementation == \"flex_attention\":\n+            if isinstance(attention_mask, torch.Tensor):\n+                attention_mask = make_flex_block_causal_mask(attention_mask)\n+            if isinstance(attention_mask, BlockMask):\n+                return attention_mask\n \n         # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n         # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail"
        }
    ],
    "stats": {
        "total": 657,
        "additions": 645,
        "deletions": 12
    }
}