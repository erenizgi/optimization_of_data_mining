{
    "author": "Wauplin",
    "message": "Migrate transformers cli to Typer (#41487)\n\n* Add typer-slim as explicit dependency\n\n* Migrate CLI to Typer\n\n* code quality\n\n* bump release candidate\n\n* adapt test_cli.py\n\n* Remove ./commands + adapt tests\n\n* fix quality\n\n* consistency\n\n* doctested\n\n* do not serve model in chat\n\n* style\n\n* will it fix them?\n\n* fix test\n\n* capitalize classes\n\n* Rebase\n\n* Rebase\n\n* tests + fixup\n\ntests + fixup\n\n* csutom error message\n\n* fix ?\n\n* should be good\n\n* fix caplog globally\n\n* inner caplog\n\n* last attempt\n\n* Retry\n\n* Let's try with capsys disabled\n\n---------\n\nCo-authored-by: Lysandre <hi@lysand.re>",
    "sha": "af2a66ced94b97e6e435d42be2ac67bc8c980446",
    "files": [
        {
            "sha": "56728b1be57c9f35fd5c7d3f98a0b9399ccf619f",
            "filename": ".gitignore",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/.gitignore",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/.gitignore",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.gitignore?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -98,6 +98,7 @@ celerybeat-schedule\n # Environments\n .env\n .venv\n+.venv*\n env/\n venv/\n ENV/"
        },
        {
            "sha": "4b5141867da169572b9ec796b10fc33dfa1941ee",
            "filename": "setup.py",
            "status": "modified",
            "additions": 4,
            "deletions": 6,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -114,7 +114,7 @@\n     \"GitPython<3.1.19\",\n     \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\",\n-    \"huggingface-hub==1.0.0.rc5\",\n+    \"huggingface-hub==1.0.0.rc6\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n     \"jinja2>=3.1.0\",\n@@ -175,6 +175,7 @@\n     \"torchvision\",\n     \"pyctcdecode>=0.4.0\",\n     \"tqdm>=4.27\",\n+    \"typer-slim\",\n     \"unidic>=1.0.2\",\n     \"unidic_lite>=1.0.7\",\n     \"urllib3<2.0.0\",\n@@ -406,6 +407,7 @@ def run(self):\n     deps[\"regex\"],  # for OpenAI GPT\n     deps[\"requests\"],  # for downloading models over HTTPS\n     deps[\"tokenizers\"],\n+    deps[\"typer-slim\"],  # CLI utilities. In practice, already a dependency of huggingface_hub\n     deps[\"safetensors\"],\n     deps[\"tqdm\"],  # progress bars in model download and training scripts\n ]\n@@ -427,11 +429,7 @@ def run(self):\n     package_data={\"\": [\"**/*.cu\", \"**/*.cpp\", \"**/*.cuh\", \"**/*.h\", \"**/*.pyx\", \"py.typed\"]},\n     zip_safe=False,\n     extras_require=extras,\n-    entry_points={\n-        \"console_scripts\": [\n-            \"transformers=transformers.commands.transformers_cli:main\",\n-        ]\n-    },\n+    entry_points={\"console_scripts\": [\"transformers=transformers.cli.transformers:main\"]},\n     python_requires=\">=3.10.0\",\n     install_requires=list(install_requires),\n     classifiers=["
        },
        {
            "sha": "0a44dc6d0c3dc6eab37f6bea449de056d91d6f83",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -18,7 +18,7 @@\n # to defer the actual importing for when the objects are requested. This way `import transformers` provides the names\n # in the namespace without actually importing anything (and especially none of the backends).\n \n-__version__ = \"4.57.0.dev0\"\n+__version__ = \"5.0.0.dev0\"\n \n from pathlib import Path\n from typing import TYPE_CHECKING\n@@ -58,7 +58,6 @@\n # Base objects, independent of any specific backend\n _import_structure = {\n     \"audio_utils\": [],\n-    \"commands\": [],\n     \"configuration_utils\": [\"PreTrainedConfig\", \"PretrainedConfig\"],\n     \"convert_slow_tokenizers_checkpoints_to_fast\": [],\n     \"data\": ["
        },
        {
            "sha": "8568c82be1c638c0ccd34d460fd8b0f73dcbec4e",
            "filename": "src/transformers/cli/__init__.py",
            "status": "added",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2F__init__.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,13 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License."
        },
        {
            "sha": "c1aaa5bdaaed8a2e49a65c510b18cb53f157b0bd",
            "filename": "src/transformers/cli/add_fast_image_processor.py",
            "status": "renamed",
            "additions": 72,
            "deletions": 87,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_fast_image_processor.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -11,15 +11,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n import os\n import re\n-from argparse import ArgumentParser, Namespace\n from datetime import date\n from pathlib import Path\n+from typing import Annotated\n+\n+import typer\n \n from ..utils import logging\n-from . import BaseTransformersCLICommand\n \n \n logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n@@ -29,6 +29,75 @@\n TRANSFORMERS_PATH = Path(__file__).parent.parent\n REPO_PATH = TRANSFORMERS_PATH.parent.parent\n \n+### Entrypoint\n+\n+\n+def add_fast_image_processor(\n+    model_name: Annotated[str, typer.Argument(help=\"The name of the folder containing the model's implementation.\")],\n+):\n+    \"\"\"\n+    Add a fast image processor to a model.\n+\n+    Adds the necessary references to the fast image processor in the transformers package, and create the fast image processor file in the model's folder.\n+    \"\"\"\n+    model_module = TRANSFORMERS_PATH / \"models\" / model_name\n+    image_processing_module_file = list(model_module.glob(\"image_processing*.py\"))\n+    if not image_processing_module_file:\n+        raise ValueError(f\"No image processing module found in {model_module}\")\n+    elif len(image_processing_module_file) > 1:\n+        for file_name in image_processing_module_file:\n+            if not str(file_name).endswith(\"_fast.py\"):\n+                image_processing_module_file = str(file_name)\n+                break\n+    else:\n+        image_processing_module_file = str(image_processing_module_file[0])\n+\n+    with open(image_processing_module_file, \"r\", encoding=\"utf-8\") as f:\n+        content_base_file = f.read()\n+\n+    # regex to find object starting with \"class \" and ending with \"ImageProcessor\", including \"ImageProcessor\" in the match\n+    image_processor_name = re.findall(r\"class (\\w*ImageProcessor)\", content_base_file)\n+    if not image_processor_name:\n+        raise ValueError(f\"No ImageProcessor class found in {image_processing_module_file}\")\n+    elif len(image_processor_name) > 1:\n+        raise ValueError(f\"Multiple ImageProcessor classes found in {image_processing_module_file}\")\n+\n+    image_processor_name = image_processor_name[0]\n+    fast_image_processor_name = image_processor_name + \"Fast\"\n+    fast_image_processing_module_file = image_processing_module_file.replace(\".py\", \"_fast.py\")\n+\n+    print(f\"Adding {fast_image_processor_name} to {fast_image_processing_module_file}\")\n+\n+    add_fast_image_processor_to_model_init(\n+        fast_image_processing_module_file=fast_image_processing_module_file,\n+        fast_image_processor_name=fast_image_processor_name,\n+        model_name=model_name,\n+    )\n+\n+    add_fast_image_processor_to_auto(\n+        image_processor_name=image_processor_name,\n+        fast_image_processor_name=fast_image_processor_name,\n+    )\n+\n+    add_fast_image_processor_to_doc(\n+        fast_image_processor_name=fast_image_processor_name,\n+        model_name=model_name,\n+    )\n+\n+    add_fast_image_processor_to_tests(\n+        fast_image_processor_name=fast_image_processor_name,\n+        model_name=model_name,\n+    )\n+\n+    add_fast_image_processor_file(\n+        fast_image_processing_module_file=fast_image_processing_module_file,\n+        fast_image_processor_name=fast_image_processor_name,\n+        content_base_file=content_base_file,\n+    )\n+\n+\n+### Core logic\n+\n \n def add_fast_image_processor_to_model_init(\n     fast_image_processing_module_file: str, fast_image_processor_name, model_name: str\n@@ -444,87 +513,3 @@ def add_fast_image_processor_file(\n \n     with open(fast_image_processing_module_file, \"w\", encoding=\"utf-8\") as f:\n         f.write(content)\n-\n-\n-def add_fast_image_processor(model_name: str):\n-    \"\"\"\n-    Add the necessary references to the fast image processor in the transformers package,\n-    and create the fast image processor file in the model's folder.\n-    \"\"\"\n-    model_module = TRANSFORMERS_PATH / \"models\" / model_name\n-    image_processing_module_file = list(model_module.glob(\"image_processing*.py\"))\n-    if not image_processing_module_file:\n-        raise ValueError(f\"No image processing module found in {model_module}\")\n-    elif len(image_processing_module_file) > 1:\n-        for file_name in image_processing_module_file:\n-            if not str(file_name).endswith(\"_fast.py\"):\n-                image_processing_module_file = str(file_name)\n-                break\n-    else:\n-        image_processing_module_file = str(image_processing_module_file[0])\n-\n-    with open(image_processing_module_file, \"r\", encoding=\"utf-8\") as f:\n-        content_base_file = f.read()\n-\n-    # regex to find object starting with \"class \" and ending with \"ImageProcessor\", including \"ImageProcessor\" in the match\n-    image_processor_name = re.findall(r\"class (\\w*ImageProcessor)\", content_base_file)\n-    if not image_processor_name:\n-        raise ValueError(f\"No ImageProcessor class found in {image_processing_module_file}\")\n-    elif len(image_processor_name) > 1:\n-        raise ValueError(f\"Multiple ImageProcessor classes found in {image_processing_module_file}\")\n-\n-    image_processor_name = image_processor_name[0]\n-    fast_image_processor_name = image_processor_name + \"Fast\"\n-    fast_image_processing_module_file = image_processing_module_file.replace(\".py\", \"_fast.py\")\n-\n-    print(f\"Adding {fast_image_processor_name} to {fast_image_processing_module_file}\")\n-\n-    add_fast_image_processor_to_model_init(\n-        fast_image_processing_module_file=fast_image_processing_module_file,\n-        fast_image_processor_name=fast_image_processor_name,\n-        model_name=model_name,\n-    )\n-\n-    add_fast_image_processor_to_auto(\n-        image_processor_name=image_processor_name,\n-        fast_image_processor_name=fast_image_processor_name,\n-    )\n-\n-    add_fast_image_processor_to_doc(\n-        fast_image_processor_name=fast_image_processor_name,\n-        model_name=model_name,\n-    )\n-\n-    add_fast_image_processor_to_tests(\n-        fast_image_processor_name=fast_image_processor_name,\n-        model_name=model_name,\n-    )\n-\n-    add_fast_image_processor_file(\n-        fast_image_processing_module_file=fast_image_processing_module_file,\n-        fast_image_processor_name=fast_image_processor_name,\n-        content_base_file=content_base_file,\n-    )\n-\n-\n-def add_new_model_like_command_factory(args: Namespace):\n-    return AddFastImageProcessorCommand(model_name=args.model_name)\n-\n-\n-class AddFastImageProcessorCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        add_fast_image_processor_parser = parser.add_parser(\"add-fast-image-processor\")\n-        add_fast_image_processor_parser.add_argument(\n-            \"--model-name\",\n-            type=str,\n-            required=True,\n-            help=\"The name of the folder containing the model's implementation.\",\n-        )\n-        add_fast_image_processor_parser.set_defaults(func=add_new_model_like_command_factory)\n-\n-    def __init__(self, model_name: str, *args):\n-        self.model_name = model_name\n-\n-    def run(self):\n-        add_fast_image_processor(model_name=self.model_name)",
            "previous_filename": "src/transformers/commands/add_fast_image_processor.py"
        },
        {
            "sha": "8c884d970b974ce228a43c6b0c997cd82a129781",
            "filename": "src/transformers/cli/add_new_model_like.py",
            "status": "renamed",
            "additions": 71,
            "deletions": 72,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fadd_new_model_like.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -16,11 +16,12 @@\n import re\n import subprocess\n import textwrap\n-from argparse import ArgumentParser, Namespace\n from collections.abc import Callable\n from datetime import date\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Annotated, Any, Optional, Union\n+\n+import typer\n \n from ..models.auto.configuration_auto import CONFIG_MAPPING_NAMES, MODEL_NAMES_MAPPING\n from ..models.auto.feature_extraction_auto import FEATURE_EXTRACTOR_MAPPING_NAMES\n@@ -29,7 +30,6 @@\n from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES\n from ..models.auto.video_processing_auto import VIDEO_PROCESSOR_MAPPING_NAMES\n from ..utils import is_libcst_available\n-from . import BaseTransformersCLICommand\n from .add_fast_image_processor import add_fast_image_processor\n \n \n@@ -71,8 +71,7 @@ def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):\n \n \n CURRENT_YEAR = date.today().year\n-TRANSFORMERS_PATH = Path(__file__).parent.parent\n-REPO_PATH = TRANSFORMERS_PATH.parent.parent\n+REPO_PATH = Path(__file__).parents[3]\n \n COPYRIGHT = f\"\"\"\n # coding=utf-8\n@@ -91,6 +90,37 @@ def visit_SimpleStatementLine(self, node: cst.SimpleStatementLine):\n # limitations under the License.\n \"\"\".lstrip()\n \n+### Entrypoint\n+\n+\n+def add_new_model_like(\n+    repo_path: Annotated[\n+        Optional[str], typer.Argument(help=\"When not using an editable install, the path to the Transformers repo.\")\n+    ] = None,\n+):\n+    \"\"\"\n+    Add a new model to the library, based on an existing one.\n+    \"\"\"\n+    (\n+        old_model_infos,\n+        new_lowercase_name,\n+        new_model_paper_name,\n+        filenames_to_add,\n+        create_fast_image_processor,\n+    ) = get_user_input()\n+\n+    _add_new_model_like_internal(\n+        repo_path=Path(repo_path) if repo_path is not None else REPO_PATH,\n+        old_model_infos=old_model_infos,\n+        new_lowercase_name=new_lowercase_name,\n+        new_model_paper_name=new_model_paper_name,\n+        filenames_to_add=filenames_to_add,\n+        create_fast_image_processor=create_fast_image_processor,\n+    )\n+\n+\n+### Core logic\n+\n \n class ModelInfos:\n     \"\"\"\n@@ -149,6 +179,7 @@ def add_content_to_file(file_name: Union[str, os.PathLike], new_content: str, ad\n \n \n def add_model_to_auto_mappings(\n+    repo_path: Path,\n     old_model_infos: ModelInfos,\n     new_lowercase_name: str,\n     new_model_paper_name: str,\n@@ -185,12 +216,12 @@ def add_model_to_auto_mappings(\n \n     # Add the config mappings directly as the handling for config is a bit different\n     add_content_to_file(\n-        TRANSFORMERS_PATH / \"models\" / \"auto\" / \"configuration_auto.py\",\n+        repo_path / \"src\" / \"transformers\" / \"models\" / \"auto\" / \"configuration_auto.py\",\n         new_content=f'        (\"{new_lowercase_name}\", \"{new_cased_name}Config\"),\\n',\n         add_after=\"CONFIG_MAPPING_NAMES = OrderedDict[str, str](\\n    [\\n        # Add configs here\\n\",\n     )\n     add_content_to_file(\n-        TRANSFORMERS_PATH / \"models\" / \"auto\" / \"configuration_auto.py\",\n+        repo_path / \"src\" / \"transformers\" / \"models\" / \"auto\" / \"configuration_auto.py\",\n         new_content=f'        (\"{new_lowercase_name}\", \"{new_model_paper_name}\"),\\n',\n         add_after=\"MODEL_NAMES_MAPPING = OrderedDict[str, str](\\n    [\\n        # Add full (and cased) model names here\\n\",\n     )\n@@ -199,15 +230,14 @@ def add_model_to_auto_mappings(\n         if to_add:\n             # The auto mapping\n             filename = filename.replace(\"_fast.py\", \".py\")\n-            with open(TRANSFORMERS_PATH / \"models\" / \"auto\" / filename) as f:\n-                file = f.read()\n+            file = (repo_path / \"src\" / \"transformers\" / \"models\" / \"auto\" / filename).read_text()\n             # The regex has to be a bit complex like this as the tokenizer mapping has new lines everywhere\n             matching_lines = re.findall(\n                 rf'( {{8,12}}\\(\\s*\"{old_lowercase_name}\",.*?\\),\\n)(?: {{4,12}}\\(|\\])', file, re.DOTALL\n             )\n             for match in matching_lines:\n                 add_content_to_file(\n-                    TRANSFORMERS_PATH / \"models\" / \"auto\" / filename,\n+                    repo_path / \"src\" / \"transformers\" / \"models\" / \"auto\" / filename,\n                     new_content=match.replace(old_lowercase_name, new_lowercase_name).replace(\n                         old_cased_name, new_cased_name\n                     ),\n@@ -271,7 +301,9 @@ def create_doc_file(new_paper_name: str, public_classes: list[str]):\n     return copyright_for_markdown + doc_template + class_doc\n \n \n-def insert_model_in_doc_toc(old_lowercase_name: str, new_lowercase_name: str, new_model_paper_name: str):\n+def insert_model_in_doc_toc(\n+    repo_path: Path, old_lowercase_name: str, new_lowercase_name: str, new_model_paper_name: str\n+):\n     \"\"\"\n     Insert the new model in the doc `_toctree.yaml`, in the same section as the old model.\n \n@@ -283,14 +315,14 @@ def insert_model_in_doc_toc(old_lowercase_name: str, new_lowercase_name: str, ne\n         new_model_paper_name (`str`):\n             The fully cased name (as in the official paper name) of the new model.\n     \"\"\"\n-    toc_file = REPO_PATH / \"docs\" / \"source\" / \"en\" / \"_toctree.yml\"\n+    toc_file = repo_path / \"docs\" / \"source\" / \"en\" / \"_toctree.yml\"\n     with open(toc_file, \"r\") as f:\n         content = f.read()\n \n     old_model_toc = re.search(rf\"- local: model_doc/{old_lowercase_name}\\n {{8}}title: .*?\\n\", content).group(0)\n     new_toc = f\"      - local: model_doc/{new_lowercase_name}\\n        title: {new_model_paper_name}\\n\"\n     add_content_to_file(\n-        REPO_PATH / \"docs\" / \"source\" / \"en\" / \"_toctree.yml\", new_content=new_toc, add_after=old_model_toc\n+        repo_path / \"docs\" / \"source\" / \"en\" / \"_toctree.yml\", new_content=new_toc, add_after=old_model_toc\n     )\n \n \n@@ -374,6 +406,7 @@ def find_modular_structure(\n \n \n def create_modular_file(\n+    repo_path: Path,\n     old_model_infos: ModelInfos,\n     new_lowercase_name: str,\n     filenames_to_add: list[tuple[str, bool]],\n@@ -393,7 +426,7 @@ def create_modular_file(\n     \"\"\"\n     new_cased_name = \"\".join(x.title() for x in new_lowercase_name.replace(\"-\", \"_\").split(\"_\"))\n     old_lowercase_name = old_model_infos.lowercase_name\n-    old_folder_root = TRANSFORMERS_PATH / \"models\" / old_lowercase_name\n+    old_folder_root = repo_path / \"src\" / \"transformers\" / \"models\" / old_lowercase_name\n \n     # Construct the modular file from the original (old) model, by subclassing each class\n     all_imports = \"\"\n@@ -425,7 +458,9 @@ def create_modular_file(\n     return modular_file, all_public_classes\n \n \n-def create_test_files(old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]):\n+def create_test_files(\n+    repo_path: Path, old_model_infos: ModelInfos, new_lowercase_name, filenames_to_add: list[tuple[str, bool]]\n+):\n     \"\"\"\n     Create the test files for the new model. It basically copies over the old test files and adjust the class names.\n \n@@ -458,7 +493,7 @@ def create_test_files(old_model_infos: ModelInfos, new_lowercase_name, filenames\n     for new_file, to_add in corrected_filenames_to_add:\n         if to_add:\n             original_test_file = new_file.replace(new_lowercase_name, old_lowercase_name)\n-            original_test_path = REPO_PATH / \"tests\" / \"models\" / old_lowercase_name / original_test_file\n+            original_test_path = repo_path / \"tests\" / \"models\" / old_lowercase_name / original_test_file\n             # Sometimes, tests may not exist\n             if not original_test_path.is_file():\n                 continue\n@@ -475,7 +510,8 @@ def create_test_files(old_model_infos: ModelInfos, new_lowercase_name, filenames\n     return test_files\n \n \n-def create_new_model_like(\n+def _add_new_model_like_internal(\n+    repo_path: Path,\n     old_model_infos: ModelInfos,\n     new_lowercase_name: str,\n     new_model_paper_name: str,\n@@ -486,6 +522,8 @@ def create_new_model_like(\n     Creates a new model module like a given model of the Transformers library.\n \n     Args:\n+        repo_path (`Path`):\n+            The path to the root of the Transformers repository.\n         old_model_infos (`ModelInfos`):\n             The structure containing the class information of the old model.\n         new_lowercase_name (`str`):\n@@ -505,11 +543,13 @@ def create_new_model_like(\n     old_lowercase_name = old_model_infos.lowercase_name\n \n     # 1. We create the folder for our new model\n-    new_module_folder = TRANSFORMERS_PATH / \"models\" / new_lowercase_name\n+    new_module_folder = repo_path / \"src\" / \"transformers\" / \"models\" / new_lowercase_name\n     os.makedirs(new_module_folder, exist_ok=True)\n \n     # 2. Create and add the modular file\n-    modular_file, public_classes = create_modular_file(old_model_infos, new_lowercase_name, filenames_to_add)\n+    modular_file, public_classes = create_modular_file(\n+        repo_path, old_model_infos, new_lowercase_name, filenames_to_add\n+    )\n     with open(new_module_folder / f\"modular_{new_lowercase_name}.py\", \"w\") as f:\n         f.write(modular_file)\n \n@@ -520,55 +560,55 @@ def create_new_model_like(\n \n     # 4. Add new model to the models init\n     add_content_to_file(\n-        TRANSFORMERS_PATH / \"models\" / \"__init__.py\",\n+        repo_path / \"src\" / \"transformers\" / \"models\" / \"__init__.py\",\n         new_content=f\"    from .{new_lowercase_name} import *\\n\",\n         add_after=\"if TYPE_CHECKING:\\n\",\n     )\n \n     # 5. Add model to auto mappings\n-    add_model_to_auto_mappings(old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add)\n+    add_model_to_auto_mappings(repo_path, old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add)\n \n     # 6. Add test files\n-    tests_folder = REPO_PATH / \"tests\" / \"models\" / new_lowercase_name\n+    tests_folder = repo_path / \"tests\" / \"models\" / new_lowercase_name\n     os.makedirs(tests_folder, exist_ok=True)\n     # Add empty __init__.py\n     with open(tests_folder / \"__init__.py\", \"w\"):\n         pass\n-    test_files = create_test_files(old_model_infos, new_lowercase_name, filenames_to_add)\n+    test_files = create_test_files(repo_path, old_model_infos, new_lowercase_name, filenames_to_add)\n     for filename, content in test_files.items():\n         with open(tests_folder / filename, \"w\") as f:\n             f.write(content)\n \n     # 7. Add doc file\n     doc_file = create_doc_file(new_model_paper_name, public_classes)\n-    with open(REPO_PATH / \"docs\" / \"source\" / \"en\" / \"model_doc\" / f\"{new_lowercase_name}.md\", \"w\") as f:\n+    with open(repo_path / \"docs\" / \"source\" / \"en\" / \"model_doc\" / f\"{new_lowercase_name}.md\", \"w\") as f:\n         f.write(doc_file)\n-    insert_model_in_doc_toc(old_lowercase_name, new_lowercase_name, new_model_paper_name)\n+    insert_model_in_doc_toc(repo_path, old_lowercase_name, new_lowercase_name, new_model_paper_name)\n \n     # 8. Add additional fast image processor if necessary\n     if create_fast_image_processor:\n         add_fast_image_processor(model_name=new_lowercase_name)\n \n     # 9. Run linters\n-    model_init_file = TRANSFORMERS_PATH / \"models\" / \"__init__.py\"\n+    model_init_file = repo_path / \"src\" / \"transformers\" / \"models\" / \"__init__.py\"\n     subprocess.run(\n         [\"ruff\", \"check\", new_module_folder, tests_folder, model_init_file, \"--fix\"],\n-        cwd=REPO_PATH,\n+        cwd=repo_path,\n         stdout=subprocess.DEVNULL,\n     )\n     subprocess.run(\n         [\"ruff\", \"format\", new_module_folder, tests_folder, model_init_file],\n-        cwd=REPO_PATH,\n+        cwd=repo_path,\n         stdout=subprocess.DEVNULL,\n     )\n     subprocess.run(\n-        [\"python\", \"utils/check_doc_toc.py\", \"--fix_and_overwrite\"], cwd=REPO_PATH, stdout=subprocess.DEVNULL\n+        [\"python\", \"utils/check_doc_toc.py\", \"--fix_and_overwrite\"], cwd=repo_path, stdout=subprocess.DEVNULL\n     )\n-    subprocess.run([\"python\", \"utils/sort_auto_mappings.py\"], cwd=REPO_PATH, stdout=subprocess.DEVNULL)\n+    subprocess.run([\"python\", \"utils/sort_auto_mappings.py\"], cwd=repo_path, stdout=subprocess.DEVNULL)\n \n     # 10. Run the modular conversion\n     subprocess.run(\n-        [\"python\", \"utils/modular_model_converter.py\", new_lowercase_name], cwd=REPO_PATH, stdout=subprocess.DEVNULL\n+        [\"python\", \"utils/modular_model_converter.py\", new_lowercase_name], cwd=repo_path, stdout=subprocess.DEVNULL\n     )\n \n \n@@ -741,44 +781,3 @@ def get_user_input():\n         )\n \n     return old_model_infos, new_lowercase_name, new_model_paper_name, filenames_to_add, create_fast_image_processor\n-\n-\n-def add_new_model_like_command_factory(args: Namespace):\n-    return AddNewModelLikeCommand(path_to_repo=args.path_to_repo)\n-\n-\n-class AddNewModelLikeCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        add_new_model_like_parser = parser.add_parser(\"add-new-model-like\")\n-        add_new_model_like_parser.add_argument(\n-            \"--path_to_repo\", type=str, help=\"When not using an editable install, the path to the Transformers repo.\"\n-        )\n-        add_new_model_like_parser.set_defaults(func=add_new_model_like_command_factory)\n-\n-    def __init__(self, path_to_repo=None, **kwargs):\n-        (\n-            self.old_model_infos,\n-            self.new_lowercase_name,\n-            self.new_model_paper_name,\n-            self.filenames_to_add,\n-            self.create_fast_image_processor,\n-        ) = get_user_input()\n-        self.path_to_repo = path_to_repo\n-\n-    def run(self):\n-        if self.path_to_repo is not None:\n-            # Adapt constants\n-            global TRANSFORMERS_PATH\n-            global REPO_PATH\n-\n-            REPO_PATH = Path(self.path_to_repo)\n-            TRANSFORMERS_PATH = REPO_PATH / \"src\" / \"transformers\"\n-\n-        create_new_model_like(\n-            old_model_infos=self.old_model_infos,\n-            new_lowercase_name=self.new_lowercase_name,\n-            new_model_paper_name=self.new_model_paper_name,\n-            filenames_to_add=self.filenames_to_add,\n-            create_fast_image_processor=self.create_fast_image_processor,\n-        )",
            "previous_filename": "src/transformers/commands/add_new_model_like.py"
        },
        {
            "sha": "7944f58d4864b0764e95bac4ef2b4aab1756e634",
            "filename": "src/transformers/cli/chat.py",
            "status": "added",
            "additions": 546,
            "deletions": 0,
            "changes": 546,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fchat.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,546 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import asyncio\n+import json\n+import os\n+import platform\n+import re\n+import string\n+import time\n+from collections.abc import AsyncIterator\n+from typing import Annotated, Optional\n+\n+import click\n+import typer\n+import yaml\n+from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n+\n+from transformers import GenerationConfig\n+from transformers.utils import is_rich_available\n+\n+\n+try:\n+    import readline  # noqa importing this enables GNU readline capabilities\n+except ImportError:\n+    # some platforms may not support readline: https://docs.python.org/3/library/readline.html\n+    pass\n+\n+if platform.system() != \"Windows\":\n+    import pwd\n+\n+if is_rich_available():\n+    from rich.console import Console\n+    from rich.live import Live\n+    from rich.markdown import Markdown\n+\n+ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n+ALLOWED_VALUE_CHARS = set(\n+    string.ascii_letters + string.digits + string.whitespace + r\".!\\\"#$%&'()*+,\\-/:<=>?@[]^_`{|}~\"\n+)\n+\n+DEFAULT_EXAMPLES = {\n+    \"llama\": {\"text\": \"There is a Llama in my lawn, how can I get rid of it?\"},\n+    \"code\": {\n+        \"text\": (\n+            \"Write a Python function that integrates any Python function f(x) numerically over an arbitrary \"\n+            \"interval [x_start, x_end].\"\n+        ),\n+    },\n+    \"helicopter\": {\"text\": \"How many helicopters can a human eat in one sitting?\"},\n+    \"numbers\": {\"text\": \"Count to 10 but skip every number ending with an 'e'\"},\n+    \"birds\": {\"text\": \"Why aren't birds real?\"},\n+    \"socks\": {\"text\": \"Why is it important to eat socks after meditating?\"},\n+    \"numbers2\": {\"text\": \"Which number is larger, 9.9 or 9.11?\"},\n+}\n+\n+# Printed at the start of a chat session\n+HELP_STRING_MINIMAL = \"\"\"\n+\n+**TRANSFORMERS CHAT INTERFACE**\n+\n+Chat interface to try out a model. Besides chatting with the model, here are some basic commands:\n+- **!help**: shows all available commands (set generation settings, save chat, etc.)\n+- **!status**: shows the current status of the model and generation settings\n+- **!clear**: clears the current conversation and starts a new one\n+- **!exit**: closes the interface\n+\"\"\"\n+\n+\n+# Printed when the user types `help` in the chat session\n+HELP_STRING = f\"\"\"\n+\n+**TRANSFORMERS CHAT INTERFACE HELP**\n+\n+Full command list:\n+- **!help**: shows this help message\n+- **!clear**: clears the current conversation and starts a new one\n+- **!status**: shows the current status of the model and generation settings\n+- **!example {{NAME}}**: loads example named `{{NAME}}` from the config and uses it as the user input.\n+Available example names: `{\"`, `\".join(DEFAULT_EXAMPLES.keys())}`\n+- **!set {{ARG_1}}={{VALUE_1}} {{ARG_2}}={{VALUE_2}}** ...: changes the system prompt or generation settings (multiple\n+settings are separated by a space). Accepts the same flags and format as the `generate_flags` CLI argument.\n+If you're a new user, check this basic flag guide: https://huggingface.co/docs/transformers/llm_tutorial#common-options\n+- **!save {{SAVE_NAME}} (optional)**: saves the current chat and settings to file by default to\n+`./chat_history/{{MODEL_ID}}/chat_{{DATETIME}}.yaml` or `{{SAVE_NAME}}` if provided\n+- **!exit**: closes the interface\n+\"\"\"\n+\n+\n+class RichInterface:\n+    def __init__(self, model_id: str, user_id: str):\n+        self._console = Console()\n+        self.model_id = model_id\n+        self.user_id = user_id\n+\n+    async def stream_output(self, stream: AsyncIterator[ChatCompletionStreamOutput]) -> tuple[str, int]:\n+        self._console.print(f\"[bold blue]<{self.model_id}>:\")\n+        with Live(console=self._console, refresh_per_second=4) as live:\n+            text = \"\"\n+            async for token in await stream:\n+                outputs = token.choices[0].delta.content\n+\n+                if not outputs:\n+                    continue\n+\n+                # Escapes single words encased in <>, e.g. <think> -> \\<think\\>, for proper rendering in Markdown.\n+                # It only escapes single words that may have `_`, optionally following a `/` (e.g. </think>)\n+                outputs = re.sub(r\"<(/*)(\\w*)>\", r\"\\<\\1\\2\\>\", outputs)\n+\n+                text += outputs\n+                # Render the accumulated text as Markdown\n+                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n+                #  in rich. The chatbots output treat \"\\n\" as a new line for\n+                #  better compatibility with real-world text. However, rendering\n+                #  in markdown would break the format. It is because standard markdown\n+                #  treat a single \"\\n\" in normal text as a space.\n+                #  Our workaround is adding two spaces at the end of each line.\n+                #  This is not a perfect solution, as it would\n+                #  introduce trailing spaces (only) in code block, but it works well\n+                #  especially for console output, because in general the console does not\n+                #  care about trailing spaces.\n+\n+                lines = []\n+                for line in text.splitlines():\n+                    lines.append(line)\n+                    if line.startswith(\"```\"):\n+                        # Code block marker - do not add trailing spaces, as it would\n+                        #  break the syntax highlighting\n+                        lines.append(\"\\n\")\n+                    else:\n+                        lines.append(\"  \\n\")\n+\n+                markdown = Markdown(\"\".join(lines).strip(), code_theme=\"github-dark\")\n+\n+                # Update the Live console output\n+                live.update(markdown, refresh=True)\n+\n+        self._console.print()\n+\n+        return text\n+\n+    def input(self) -> str:\n+        \"\"\"Gets user input from the console.\"\"\"\n+        input = self._console.input(f\"[bold red]<{self.user_id}>:\\n\")\n+        self._console.print()\n+        return input\n+\n+    def clear(self):\n+        \"\"\"Clears the console.\"\"\"\n+        self._console.clear()\n+\n+    def print_user_message(self, text: str):\n+        \"\"\"Prints a user message to the console.\"\"\"\n+        self._console.print(f\"[bold red]<{self.user_id}>:[/ bold red]\\n{text}\")\n+        self._console.print()\n+\n+    def print_color(self, text: str, color: str):\n+        \"\"\"Prints text in a given color to the console.\"\"\"\n+        self._console.print(f\"[bold {color}]{text}\")\n+        self._console.print()\n+\n+    def print_help(self, minimal: bool = False):\n+        \"\"\"Prints the help message to the console.\"\"\"\n+        self._console.print(Markdown(HELP_STRING_MINIMAL if minimal else HELP_STRING))\n+        self._console.print()\n+\n+    def print_status(self, config: GenerationConfig):\n+        \"\"\"Prints the status of the model and generation settings to the console.\"\"\"\n+        self._console.print(f\"[bold blue]Model: {self.model_id}\\n\")\n+        self._console.print(f\"[bold blue]{config}\")\n+        self._console.print()\n+\n+\n+class ChatCommand(typer.core.TyperCommand):\n+    \"\"\"Custom Click command to override missing parameter error message.\n+\n+    Transformers v5 introduced a breaking change in the `transformers chat` command: the `model_id` parameter\n+    is now required, and the command can no longer starts a server. This class overrides the default error message\n+    to provide a more helpful message to users who may be used to the old behavior.\n+    \"\"\"\n+\n+    def parse_args(self, ctx, args):\n+        try:\n+            return super().parse_args(ctx, args)\n+        except click.MissingParameter as e:\n+            if e.param and e.param.name == \"model_id\":\n+                typer.echo(\"Error: Missing argument 'MODEL_ID'.\\n\")\n+                typer.echo(\n+                    \"Launching a server directly from the `transformers chat` command is no longer supported. \"\n+                    \"Please use `transformers serve` to launch a server. \"\n+                    \"Use --help for more information.\",\n+                )\n+                ctx.exit(1)\n+            raise\n+\n+\n+class Chat:\n+    \"\"\"Chat with a model from the command line.\"\"\"\n+\n+    # Defining a class to help with internal state but in practice it's just a method to call\n+    # TODO: refactor into a proper module with helpers + 1 main method\n+    def __init__(\n+        self,\n+        base_url: Annotated[str, typer.Argument(help=\"Base url to connect to (e.g. http://localhost:8000/v1).\")],\n+        model_id: Annotated[str, typer.Argument(help=\"ID of the model to use (e.g. 'HuggingFaceTB/SmolLM3-3B').\")],\n+        generate_flags: Annotated[\n+            Optional[list[str]],\n+            typer.Argument(\n+                help=(\n+                    \"Flags to pass to `generate`, using a space as a separator between flags. Accepts booleans, numbers, \"\n+                    \"and lists of integers, more advanced parameterization should be set through --generation-config. \"\n+                    \"Example: `transformers chat <base_url> <model_id> max_new_tokens=100 do_sample=False eos_token_id=[1,2]`. \"\n+                    \"If you're a new user, check this basic flag guide: \"\n+                    \"https://huggingface.co/docs/transformers/llm_tutorial#common-options\"\n+                )\n+            ),\n+        ] = None,\n+        # General settings\n+        user: Annotated[\n+            Optional[str],\n+            typer.Option(help=\"Username to display in chat interface. Defaults to the current user's name.\"),\n+        ] = None,\n+        system_prompt: Annotated[Optional[str], typer.Option(help=\"System prompt.\")] = None,\n+        save_folder: Annotated[str, typer.Option(help=\"Folder to save chat history.\")] = \"./chat_history/\",\n+        examples_path: Annotated[Optional[str], typer.Option(help=\"Path to a yaml file with examples.\")] = None,\n+        # Generation settings\n+        generation_config: Annotated[\n+            Optional[str],\n+            typer.Option(\n+                help=\"Path to a local generation config file or to a HuggingFace repo containing a `generation_config.json` file. Other generation settings passed as CLI arguments will be applied on top of this generation config.\"\n+            ),\n+        ] = None,\n+    ) -> None:\n+        \"\"\"Chat with a model from the command line.\"\"\"\n+        self.base_url = base_url\n+        self.model_id = model_id\n+        self.system_prompt = system_prompt\n+        self.save_folder = save_folder\n+\n+        # Generation settings\n+        config = load_generation_config(generation_config)\n+        config.update(**{\"do_sample\": True, \"max_new_tokens\": 256})  # some default values\n+        config.update(**parse_generate_flags(generate_flags))\n+        self.config = config\n+\n+        self.settings = {\"base_url\": base_url, \"model_id\": model_id, \"config\": self.config.to_dict()}\n+\n+        # User settings\n+        self.user = user if user is not None else get_username()\n+\n+        # Load examples\n+        if examples_path:\n+            with open(examples_path) as f:\n+                self.examples = yaml.safe_load(f)\n+        else:\n+            self.examples = DEFAULT_EXAMPLES\n+\n+        # Check requirements\n+        if not is_rich_available():\n+            raise ImportError(\"You need to install rich to use the chat interface. (`pip install rich`)\")\n+\n+        # Run chat session\n+        asyncio.run(self._inner_run())\n+\n+    # -----------------------------------------------------------------------------------------------------------------\n+    # User commands\n+    def handle_non_exit_user_commands(\n+        self,\n+        user_input: str,\n+        interface: RichInterface,\n+        examples: dict[str, dict[str, str]],\n+        config: GenerationConfig,\n+        chat: list[dict],\n+    ) -> tuple[list[dict], GenerationConfig]:\n+        \"\"\"\n+        Handles all user commands except for `!exit`. May update the chat history (e.g. reset it) or the\n+        generation config (e.g. set a new flag).\n+        \"\"\"\n+        valid_command = True\n+\n+        if user_input == \"!clear\":\n+            chat = new_chat_history(self.system_prompt)\n+            interface.clear()\n+\n+        elif user_input == \"!help\":\n+            interface.print_help()\n+\n+        elif user_input.startswith(\"!save\") and len(user_input.split()) < 2:\n+            split_input = user_input.split()\n+            filename = (\n+                split_input[1]\n+                if len(split_input) == 2\n+                else os.path.join(self.save_folder, self.model_id, f\"chat_{time.strftime('%Y-%m-%d_%H-%M-%S')}.json\")\n+            )\n+            save_chat(filename=filename, chat=chat, settings=self.settings)\n+            interface.print_color(text=f\"Chat saved to {filename}!\", color=\"green\")\n+\n+        elif user_input.startswith(\"!set\"):\n+            # splits the new args into a list of strings, each string being a `flag=value` pair (same format as\n+            # `generate_flags`)\n+            new_generate_flags = user_input[4:].strip()\n+            new_generate_flags = new_generate_flags.split()\n+            # sanity check: each member in the list must have an =\n+            for flag in new_generate_flags:\n+                if \"=\" not in flag:\n+                    interface.print_color(\n+                        text=(\n+                            f\"Invalid flag format, missing `=` after `{flag}`. Please use the format \"\n+                            \"`arg_1=value_1 arg_2=value_2 ...`.\"\n+                        ),\n+                        color=\"red\",\n+                    )\n+                    break\n+            else:\n+                # Update config from user flags\n+                config.update(**parse_generate_flags(new_generate_flags))\n+\n+        elif user_input.startswith(\"!example\") and len(user_input.split()) == 2:\n+            example_name = user_input.split()[1]\n+            if example_name in examples:\n+                interface.clear()\n+                chat = []\n+                interface.print_user_message(examples[example_name][\"text\"])\n+                chat.append({\"role\": \"user\", \"content\": examples[example_name][\"text\"]})\n+            else:\n+                example_error = (\n+                    f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n+                )\n+                interface.print_color(text=example_error, color=\"red\")\n+\n+        elif user_input == \"!status\":\n+            interface.print_status(config=config)\n+\n+        else:\n+            valid_command = False\n+            interface.print_color(text=f\"'{user_input}' is not a valid command. Showing help message.\", color=\"red\")\n+            interface.print_help()\n+\n+        return chat, valid_command, config\n+\n+    # -----------------------------------------------------------------------------------------------------------------\n+    # Main logic\n+\n+    async def _inner_run(self):\n+        interface = RichInterface(model_id=self.model_id, user_id=self.user)\n+        interface.clear()\n+        chat = new_chat_history(self.system_prompt)\n+\n+        # Starts the session with a minimal help message at the top, so that a user doesn't get stuck\n+        interface.print_help(minimal=True)\n+\n+        config = self.config\n+\n+        async with AsyncInferenceClient(base_url=self.base_url) as client:\n+            while True:\n+                try:\n+                    user_input = interface.input()\n+\n+                    # User commands\n+                    if user_input == \"!exit\":\n+                        break\n+\n+                    elif user_input == \"!clear\":\n+                        chat = new_chat_history(self.system_prompt)\n+                        interface.clear()\n+                        continue\n+\n+                    elif user_input == \"!help\":\n+                        interface.print_help()\n+                        continue\n+\n+                    elif user_input.startswith(\"!save\") and len(user_input.split()) < 2:\n+                        split_input = user_input.split()\n+                        filename = (\n+                            split_input[1]\n+                            if len(split_input) == 2\n+                            else os.path.join(\n+                                self.save_folder, self.model_id, f\"chat_{time.strftime('%Y-%m-%d_%H-%M-%S')}.json\"\n+                            )\n+                        )\n+                        save_chat(filename=filename, chat=chat, settings=self.settings)\n+                        interface.print_color(text=f\"Chat saved to {filename}!\", color=\"green\")\n+                        continue\n+\n+                    elif user_input.startswith(\"!set\"):\n+                        # splits the new args into a list of strings, each string being a `flag=value` pair (same format as\n+                        # `generate_flags`)\n+                        new_generate_flags = user_input[4:].strip()\n+                        new_generate_flags = new_generate_flags.split()\n+                        # sanity check: each member in the list must have an =\n+                        for flag in new_generate_flags:\n+                            if \"=\" not in flag:\n+                                interface.print_color(\n+                                    text=(\n+                                        f\"Invalid flag format, missing `=` after `{flag}`. Please use the format \"\n+                                        \"`arg_1=value_1 arg_2=value_2 ...`.\"\n+                                    ),\n+                                    color=\"red\",\n+                                )\n+                                break\n+                        else:\n+                            # Update config from user flags\n+                            config.update(**parse_generate_flags(new_generate_flags))\n+                        continue\n+\n+                    elif user_input.startswith(\"!example\") and len(user_input.split()) == 2:\n+                        example_name = user_input.split()[1]\n+                        if example_name in self.examples:\n+                            interface.clear()\n+                            chat = []\n+                            interface.print_user_message(self.examples[example_name][\"text\"])\n+                            chat.append({\"role\": \"user\", \"content\": self.examples[example_name][\"text\"]})\n+                        else:\n+                            example_error = f\"Example {example_name} not found in list of available examples: {list(self.examples.keys())}.\"\n+                            interface.print_color(text=example_error, color=\"red\")\n+\n+                    elif user_input == \"!status\":\n+                        interface.print_status(config=config)\n+                        continue\n+\n+                    elif user_input.startswith(\"!\"):\n+                        interface.print_color(\n+                            text=f\"'{user_input}' is not a valid command. Showing help message.\", color=\"red\"\n+                        )\n+                        interface.print_help()\n+                        continue\n+\n+                    else:\n+                        chat.append({\"role\": \"user\", \"content\": user_input})\n+\n+                    stream = client.chat_completion(\n+                        chat,\n+                        stream=True,\n+                        model=self.model_id,\n+                        extra_body={\n+                            \"generation_config\": config.to_json_string(),\n+                            \"model\": self.model_id,\n+                        },\n+                    )\n+\n+                    model_output = await interface.stream_output(stream)\n+\n+                    chat.append({\"role\": \"assistant\", \"content\": model_output})\n+                except KeyboardInterrupt:\n+                    break\n+\n+\n+def load_generation_config(generation_config: Optional[str]) -> GenerationConfig:\n+    if generation_config is None:\n+        return GenerationConfig()\n+\n+    if \".json\" in generation_config:  # is a local file\n+        dirname = os.path.dirname(generation_config)\n+        filename = os.path.basename(generation_config)\n+        return GenerationConfig.from_pretrained(dirname, filename)\n+    else:\n+        return GenerationConfig.from_pretrained(generation_config)\n+\n+\n+def parse_generate_flags(generate_flags: Optional[list[str]]) -> dict:\n+    \"\"\"Parses the generate flags from the user input into a dictionary of `generate` kwargs.\"\"\"\n+    if generate_flags is None or len(generate_flags) == 0:\n+        return {}\n+\n+    # Assumption: `generate_flags` is a list of strings, each string being a `flag=value` pair, that can be parsed\n+    # into a json string if we:\n+    # 1. Add quotes around each flag name\n+    generate_flags_as_dict = {'\"' + flag.split(\"=\")[0] + '\"': flag.split(\"=\")[1] for flag in generate_flags}\n+\n+    # 2. Handle types:\n+    # 2. a. booleans should be lowercase, None should be null\n+    generate_flags_as_dict = {\n+        k: v.lower() if v.lower() in [\"true\", \"false\"] else v for k, v in generate_flags_as_dict.items()\n+    }\n+    generate_flags_as_dict = {k: \"null\" if v == \"None\" else v for k, v in generate_flags_as_dict.items()}\n+\n+    # 2. b. strings should be quoted\n+    def is_number(s: str) -> bool:\n+        # handle negative numbers\n+        s = s.removeprefix(\"-\")\n+        return s.replace(\".\", \"\", 1).isdigit()\n+\n+    generate_flags_as_dict = {k: f'\"{v}\"' if not is_number(v) else v for k, v in generate_flags_as_dict.items()}\n+    # 2. c. [no processing needed] lists are lists of ints because `generate` doesn't take lists of strings :)\n+    # We also mention in the help message that we only accept lists of ints for now.\n+\n+    # 3. Join the result into a comma separated string\n+    generate_flags_string = \", \".join([f\"{k}: {v}\" for k, v in generate_flags_as_dict.items()])\n+\n+    # 4. Add the opening/closing brackets\n+    generate_flags_string = \"{\" + generate_flags_string + \"}\"\n+\n+    # 5. Remove quotes around boolean/null and around lists\n+    generate_flags_string = generate_flags_string.replace('\"null\"', \"null\")\n+    generate_flags_string = generate_flags_string.replace('\"true\"', \"true\")\n+    generate_flags_string = generate_flags_string.replace('\"false\"', \"false\")\n+    generate_flags_string = generate_flags_string.replace('\"[', \"[\")\n+    generate_flags_string = generate_flags_string.replace(']\"', \"]\")\n+\n+    # 6. Replace the `=` with `:`\n+    generate_flags_string = generate_flags_string.replace(\"=\", \":\")\n+\n+    try:\n+        processed_generate_flags = json.loads(generate_flags_string)\n+    except json.JSONDecodeError:\n+        raise ValueError(\n+            \"Failed to convert `generate_flags` into a valid JSON object.\"\n+            \"\\n`generate_flags` = {generate_flags}\"\n+            \"\\nConverted JSON string = {generate_flags_string}\"\n+        )\n+    return processed_generate_flags\n+\n+\n+def new_chat_history(system_prompt: Optional[str] = None) -> list[dict]:\n+    \"\"\"Returns a new chat conversation.\"\"\"\n+    return [{\"role\": \"system\", \"content\": system_prompt}] if system_prompt else []\n+\n+\n+def save_chat(filename: str, chat: list[dict], settings: dict) -> str:\n+    \"\"\"Saves the chat history to a file.\"\"\"\n+    os.makedirs(os.path.dirname(filename), exist_ok=True)\n+    with open(filename, \"w\") as f:\n+        json.dump({\"settings\": settings, \"chat_history\": chat}, f, indent=4)\n+    return os.path.abspath(filename)\n+\n+\n+def get_username() -> str:\n+    \"\"\"Returns the username of the current user.\"\"\"\n+    if platform.system() == \"Windows\":\n+        return os.getlogin()\n+    else:\n+        return pwd.getpwuid(os.getuid()).pw_name\n+\n+\n+if __name__ == \"__main__\":\n+    Chat(model_id=\"meta-llama/Llama-3.2-3b-Instruct\")"
        },
        {
            "sha": "3ae078c9542426069e000b2d648a17134392744f",
            "filename": "src/transformers/cli/download.py",
            "status": "added",
            "additions": 40,
            "deletions": 0,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fdownload.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fdownload.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fdownload.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,40 @@\n+# Copyright 2020 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Annotated, Optional\n+\n+import typer\n+\n+\n+def download(\n+    model_id: Annotated[str, typer.Argument(help=\"The model ID to download\")],\n+    cache_dir: Annotated[Optional[str], typer.Option(help=\"Directory where to save files.\")] = None,\n+    force_download: Annotated[\n+        bool, typer.Option(help=\"If set, the files will be downloaded even if they are already cached locally.\")\n+    ] = False,\n+    trust_remote_code: Annotated[\n+        bool,\n+        typer.Option(\n+            help=\"Whether or not to allow for custom models defined on the Hub in their own modeling files. Use only if you've reviewed the code as it will execute on your local machine\"\n+        ),\n+    ] = False,\n+):\n+    \"\"\"Download a model and its tokenizer from the Hub.\"\"\"\n+    from ..models.auto import AutoModel, AutoTokenizer\n+\n+    AutoModel.from_pretrained(\n+        model_id, cache_dir=cache_dir, force_download=force_download, trust_remote_code=trust_remote_code\n+    )\n+    AutoTokenizer.from_pretrained(\n+        model_id, cache_dir=cache_dir, force_download=force_download, trust_remote_code=trust_remote_code\n+    )"
        },
        {
            "sha": "b365039d0d49b713373c1cbf11e13102af6dd415",
            "filename": "src/transformers/cli/run.py",
            "status": "added",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Frun.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Frun.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Frun.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,101 @@\n+# Copyright 2020 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from enum import Enum\n+from typing import Annotated, Optional\n+\n+import typer\n+\n+from ..pipelines import PipelineDataFormat, get_supported_tasks, pipeline\n+from ..utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+TaskEnum = Enum(\"TaskEnum\", {task.upper(): task for task in get_supported_tasks()}, type=str)\n+FormatEnum = Enum(\"FormatEnum\", {fmt.upper(): fmt for fmt in PipelineDataFormat.SUPPORTED_FORMATS}, type=str)\n+\n+\n+def run(\n+    task: Annotated[TaskEnum, typer.Argument(help=\"Task to run\", case_sensitive=False)],  # type: ignore\n+    input: Annotated[Optional[str], typer.Option(help=\"Path to the file to use for inference\")] = None,\n+    output: Annotated[\n+        Optional[str], typer.Option(help=\"Path to the file that will be used post to write results.\")\n+    ] = None,\n+    model: Annotated[\n+        Optional[str],\n+        typer.Option(\n+            help=\"Name or path to the model to instantiate. If not provided, will use the default model for that task.\"\n+        ),\n+    ] = None,\n+    config: Annotated[\n+        Optional[str],\n+        typer.Option(\n+            help=\"Name or path to the model's config to instantiate. If not provided, will use the model's one.\"\n+        ),\n+    ] = None,\n+    tokenizer: Annotated[\n+        Optional[str], typer.Option(help=\"Name of the tokenizer to use. If not provided, will use the model's one.\")\n+    ] = None,\n+    column: Annotated[\n+        Optional[str],\n+        typer.Option(help=\"Name of the column to use as input. For multi columns input use 'column1,columns2'\"),\n+    ] = None,\n+    format: Annotated[FormatEnum, typer.Option(help=\"Input format to read from\", case_sensitive=False)] = \"infer\",  # type: ignore\n+    device: Annotated[\n+        int, typer.Option(help=\"Indicate the device to run onto, -1 indicates CPU, >= 0 indicates GPU.\")\n+    ] = -1,\n+    overwrite: Annotated[bool, typer.Option(help=\"Allow overwriting the output file.\")] = False,\n+):\n+    \"\"\"Run a pipeline on a given input file.\"\"\"\n+    # Initialize pipeline\n+    pipe = pipeline(task=task, model=model, config=config, tokenizer=tokenizer, device=device)\n+\n+    # Initialize reader\n+    reader = PipelineDataFormat.from_str(\n+        format=_try_infer_format_from_ext(input) if format == \"infer\" else format,\n+        output_path=output,\n+        input_path=input,\n+        column=column if column else pipe.default_input_names,\n+        overwrite=overwrite,\n+    )\n+\n+    # Run\n+    outputs = []\n+    for entry in reader:\n+        output = pipe(**entry) if reader.is_multi_columns else pipe(entry)\n+        if isinstance(output, dict):\n+            outputs.append(output)\n+        else:\n+            outputs += output\n+\n+    # Saving data\n+    if pipe.binary_output:\n+        binary_path = reader.save_binary(outputs)\n+        logger.warning(f\"Current pipeline requires output to be in binary format, saving at {binary_path}\")\n+    else:\n+        reader.save(outputs)\n+\n+\n+def _try_infer_format_from_ext(path: str) -> str:\n+    if not path:\n+        return \"pipe\"\n+\n+    for ext in PipelineDataFormat.SUPPORTED_FORMATS:\n+        if path.endswith(ext):\n+            return ext\n+\n+    raise Exception(\n+        f\"Unable to determine file format from file extension {path}. \"\n+        f\"Please provide the format through --format {PipelineDataFormat.SUPPORTED_FORMATS}\"\n+    )"
        },
        {
            "sha": "7a31b797b34dd9a53c7c8c6e10aad15d29fbfaf2",
            "filename": "src/transformers/cli/serve.py",
            "status": "renamed",
            "additions": 245,
            "deletions": 284,
            "changes": 529,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -25,14 +25,13 @@\n import threading\n import time\n import uuid\n-from argparse import ArgumentParser, Namespace\n from collections.abc import Generator, Iterable\n from contextlib import asynccontextmanager\n-from dataclasses import dataclass, field\n from io import BytesIO\n from threading import Thread\n-from typing import Optional, TypedDict, Union\n+from typing import Annotated, Optional, TypedDict, Union\n \n+import typer\n from huggingface_hub import model_info\n from huggingface_hub.constants import HF_HUB_OFFLINE\n from openai.types.chat.chat_completion import Choice\n@@ -60,7 +59,6 @@\n     TextIteratorStreamer,\n )\n from ..utils import is_torch_available, logging\n-from . import BaseTransformersCLICommand\n \n \n if is_torch_available():\n@@ -224,15 +222,6 @@ class Modality(enum.Enum):\n     TTS = \"TTS\"\n \n \n-def serve_command_factory(args: Namespace):\n-    \"\"\"\n-    Factory function used to instantiate serving server from provided command line arguments.\n-\n-    Returns: ServeCommand\n-    \"\"\"\n-    return ServeCommand(args)\n-\n-\n def create_generation_config_from_req(\n     req: dict,\n     model_generation_config: \"GenerationConfig\",\n@@ -359,177 +348,237 @@ def is_deleted(self):\n         return not hasattr(self, \"model\") or self.model is None\n \n \n-@dataclass\n-class ServeArguments:\n-    r\"\"\"\n-    Arguments for the serve CLI.\n-\n-    See the metadata arg for each argument's description -- the metadata will be printed with\n-    `transformers serve --help`\n-    \"\"\"\n-\n-    continuous_batching: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to use continuous batching for chat completions.\"},\n-    )\n-    device: str = field(\n-        default=\"auto\",\n-        metadata={\n-            \"help\": \"Device to use for inference; will default to `auto` and\"\n-            \"place the model on an accelerator if available.\"\n-        },\n-    )\n-    torch_dtype: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"`torch_dtype` is deprecated! Please use `dtype` argument instead.\",\n-            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n-        },\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"auto\",\n-        metadata={\n-            \"help\": \"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, \"\n-            \"the dtype will be automatically derived from the model's weights.\",\n-            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False, metadata={\"help\": \"Whether to trust remote code when loading a model.\"}\n-    )\n-    attn_implementation: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in \"\n-            \"which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n-        },\n-    )\n-    load_in_8bit: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to use 8 bit precision for the base model - works only with LoRA.\"},\n-    )\n-    load_in_4bit: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to use 4 bit precision for the base model - works only with LoRA.\"},\n-    )\n-    bnb_4bit_quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization type.\", \"choices\": [\"fp4\", \"nf4\"]})\n-    use_bnb_nested_quant: bool = field(default=False, metadata={\"help\": \"Whether to use nested quantization.\"})\n-\n-    # Serving settings\n-    host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to.\"})\n-    port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-    model_timeout: Optional[int] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"Time in seconds after which a model will be removed from memory; defaults to 300 unless \"\n-            \"`force_model` is set, in which case the model will not be removed from memory unless a value\"\n-            \"is specified here.\"\n-        },\n-    )\n-\n-    # Other settings\n-    log_level: str = field(\n-        default=\"info\", metadata={\"help\": \"Logging level as a string. Example: 'info' or 'warning'.\"}\n-    )\n-    default_seed: Optional[int] = field(\n-        default=None, metadata={\"help\": \"The default seed for torch, should be an integer.\"}\n-    )\n-    enable_cors: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\n-                \"Whether to enable CORS. Some apps that make requests from external domains (e.g. Cursor) require \"\n-                \"CORS to be enabled.\"\n+class Serve:\n+    # Defining a class to help with internal state but in practice it's just a method to call\n+    # TODO: refactor into a proper module with helpers + 1 main method\n+    def __init__(\n+        self,\n+        continuous_batching: Annotated[\n+            bool, typer.Option(help=\"Whether to use continuous batching for chat completions.\")\n+        ] = False,\n+        device: Annotated[\n+            str,\n+            typer.Option(\n+                help=\"Device to use for inference; will default to `auto` and place the model on an accelerator if available.\"\n             ),\n-        },\n-    )\n-\n-    # TODO\n-    # Testing\n-    # As of 2025-07-11, testing on https://github.com/openai/openai-responses-starter-app/, validation on the\n-    # Response input is failing. The app works well without validation. Enable at some point in the future.\n-    input_validation: bool = field(\n-        default=False,\n-        metadata={\n-            \"help\": (\"Whether to turn on strict input validation.\"),\n-        },\n-    )\n-    force_model: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Name of the model to be forced on all requests. This is useful for testing Apps that don't allow \"\n-                \"changing models in the request.\"\n+        ] = \"auto\",\n+        dtype: Annotated[\n+            Optional[str],\n+            typer.Option(\n+                help=\"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, the dtype will be automatically derived from the model's weights.\"\n             ),\n-        },\n-    )\n-\n-    def __post_init__(self):\n-        \"\"\"Only used for BC `torch_dtype` argument.\"\"\"\n-        # In this case only the BC torch_dtype was given\n-        if self.torch_dtype is not None:\n-            if self.dtype is None:\n-                self.dtype = self.torch_dtype\n-            elif self.torch_dtype != self.dtype:\n-                raise ValueError(\n-                    f\"`torch_dtype` {self.torch_dtype} and `dtype` {self.dtype} have different values. `torch_dtype` is deprecated and \"\n-                    \"will be removed in 4.59.0, please set `dtype` instead.\"\n-                )\n-\n-\n-class ServeCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        \"\"\"\n-        Register this command to argparse so it's available for the transformer-cli\n-\n-        Args:\n-            parser: Root parser to register command-specific arguments\n-        \"\"\"\n-        dataclass_types = (ServeArguments,)\n-        serve_parser = parser.add_parser(\"serve\", dataclass_types=dataclass_types)\n-        serve_parser.set_defaults(func=serve_command_factory)\n-\n-    def __init__(self, args: ServeArguments):\n+        ] = \"auto\",\n+        trust_remote_code: Annotated[\n+            bool, typer.Option(help=\"Whether to trust remote code when loading a model.\")\n+        ] = False,\n+        attn_implementation: Annotated[\n+            Optional[str],\n+            typer.Option(\n+                help=\"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n+            ),\n+        ] = None,\n+        load_in_8bit: Annotated[\n+            bool, typer.Option(help=\"Whether to use 8 bit precision for the base model - works only with LoRA.\")\n+        ] = False,\n+        load_in_4bit: Annotated[\n+            bool, typer.Option(help=\"Whether to use 4 bit precision for the base model - works only with LoRA.\")\n+        ] = False,\n+        bnb_4bit_quant_type: Annotated[str, typer.Option(help=\"Quantization type.\")] = \"nf4\",\n+        use_bnb_nested_quant: Annotated[bool, typer.Option(help=\"Whether to use nested quantization.\")] = False,\n+        host: Annotated[str, typer.Option(help=\"Interface the server will listen to.\")] = \"localhost\",\n+        port: Annotated[int, typer.Option(help=\"Port the server will listen to.\")] = 8000,\n+        model_timeout: Annotated[\n+            int, typer.Option(help=\"Time in seconds after which a model will be removed from memory.\")\n+        ] = 300,\n+        log_level: Annotated[\n+            str, typer.Option(help=\"Logging level as a string. Example: 'info' or 'warning'.\")\n+        ] = \"info\",\n+        default_seed: Annotated[\n+            Optional[int], typer.Option(help=\"The default seed for torch, should be an integer.\")\n+        ] = None,\n+        enable_cors: Annotated[\n+            bool,\n+            typer.Option(\n+                help=\"Whether to enable CORS. Some apps that make requests from external domains (e.g. Cursor) require CORS to be enabled.\"\n+            ),\n+        ] = False,\n+        input_validation: Annotated[bool, typer.Option(help=\"Whether to turn on strict input validation.\")] = False,\n+        force_model: Annotated[\n+            Optional[str],\n+            typer.Option(\n+                help=\"Name of the model to be forced on all requests. This is useful for testing Apps that don't allow changing models in the request.\"\n+            ),\n+        ] = None,\n+        non_blocking: Annotated[\n+            bool, typer.Option(hidden=True, help=\"Whether to run the server in a separate thread.\")\n+        ] = False,\n+    ) -> None:\n         if not serve_dependencies_available:\n             raise ImportError(\n                 \"Missing dependencies for the serving CLI. Please install with `pip install transformers[serving]`\"\n             )\n \n-        # Store and process input arguments\n-        self.args = args\n-        self.use_continuous_batching = self.args.continuous_batching\n-        self.enable_cors = self.args.enable_cors\n-\n-        if self.args.default_seed is not None:\n-            torch.manual_seed(self.args.default_seed)\n+        # Save input arguments\n+        self.continuous_batching = continuous_batching\n+        self.device = device\n+        self.dtype = dtype\n+        self.trust_remote_code = trust_remote_code\n+        self.attn_implementation = attn_implementation\n+        self.load_in_8bit = load_in_8bit\n+        self.load_in_4bit = load_in_4bit\n+        self.bnb_4bit_quant_type = bnb_4bit_quant_type\n+        self.use_bnb_nested_quant = use_bnb_nested_quant\n+        self.host = host\n+        self.port = port\n+        self.model_timeout = model_timeout\n+        self.log_level = log_level\n+        self.default_seed = default_seed\n+        self.enable_cors = enable_cors\n+        self.input_validation = input_validation\n+        self.force_model = force_model\n+        self.non_blocking = non_blocking\n+\n+        # Seed\n+        if default_seed is not None:\n+            torch.manual_seed(default_seed)\n \n         # Set up logging\n         transformers_logger = logging.get_logger(\"transformers\")\n-        transformers_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n+        transformers_logger.setLevel(logging.log_levels[log_level.lower()])\n \n         cb_logger = logging.get_logger(\"transformers.generation.continuous_batching\")\n-        cb_logger.setLevel(logging.log_levels[self.args.log_level.lower()])\n+        cb_logger.setLevel(logging.log_levels[log_level.lower()])\n \n         # Internal state:\n         # 1. Tracks models in memory, to prevent reloading the model unnecessarily\n         self.loaded_models: dict[str, TimedModel] = {}\n         self.running_continuous_batching_manager: Optional[ContinuousBatchingManager] = None\n \n         # 2. preserves information about the last call and last KV cache, to determine whether we can reuse the KV\n-        # cache and avoid re-running prefil\n+        # cache and avoid re-running prefill\n         self.last_messages = None\n         self.last_kv_cache = None\n         self.last_model = None\n \n-        if self.args.model_timeout is None:\n-            self.args.model_timeout = -1 if self.args.force_model else 300\n+        if self.model_timeout is None:\n+            self.model_timeout = -1 if self.force_model else 300\n \n-        if self.args.force_model:\n-            model_id_and_revision = self.process_model_name(self.args.force_model)\n+        if self.force_model:\n+            model_id_and_revision = self.process_model_name(self.force_model)\n             self.last_model = model_id_and_revision\n             self.load_model_and_processor(model_id_and_revision)\n \n+        @asynccontextmanager\n+        async def lifespan(app: \"FastAPI\"):\n+            yield\n+            for model in self.loaded_models.values():\n+                model.delete_model()\n+            if self.running_continuous_batching_manager is not None:\n+                self.running_continuous_batching_manager.stop(block=True, timeout=5)\n+\n+        app = FastAPI(lifespan=lifespan)\n+\n+        # Some apps that make requests from external domains (e.g. Cursor) require CORS to be enabled. However, for\n+        # security purposes, it's disabled by default\n+        if self.enable_cors:\n+            app.add_middleware(\n+                CORSMiddleware,\n+                allow_origins=[\"*\"],\n+                allow_credentials=True,\n+                allow_methods=[\"*\"],\n+                allow_headers=[\"*\"],\n+            )\n+            logger.warning_once(\n+                \"CORS allow origin is set to `*`. This is not recommended for production environments.\"\n+            )\n+\n+        from fastapi import Request\n+\n+        @app.post(\"/v1/chat/completions\")\n+        def chat_completion(request: Request, body: dict):\n+            self.validate_chat_completion_request(request=body)\n+\n+            if self.continuous_batching:\n+                return self.continuous_batching_chat_completion(body, request.state.request_id)\n+            else:\n+                return self.generate_chat_completion(body)\n+\n+        @app.post(\"/v1/responses\")\n+        def responses(request: dict):\n+            self.validate_response_request(request=request)\n+            # Support non-streaming mode when `stream=false` is provided\n+            stream = request.get(\"stream\", True)\n+            if not stream:\n+                response_obj = self.generate_response_non_streaming(request)\n+                return JSONResponse(response_obj)\n+\n+            output = self.generate_response(request)\n+            return StreamingResponse(output, media_type=\"text/event-stream\")\n+\n+        @app.post(\"/v1/audio/transcriptions\")\n+        async def audio_transcriptions(request: Request):\n+            # Parses the multipart/form-data request into the request format used by other endpoints\n+            async with request.form() as form:\n+                parsed_request = TransformersTranscriptionCreateParams(\n+                    file=await form[\"file\"].read(),\n+                    model=form[\"model\"],\n+                    # TODO: add other fields\n+                )\n+                logger.debug(\n+                    f\"Received file: {form['file'].filename}; MIME type: {form['file'].content_type}; \"\n+                    f\"size: {form['file'].size / 1024:.2f} KiB\"\n+                )\n+            self.validate_transcription_request(request=parsed_request)\n+\n+            output = self.generate_transcription(parsed_request)\n+            return StreamingResponse(output, media_type=\"text/event-stream\")\n+\n+        @app.options(\"/v1/models\")\n+        @app.get(\"/v1/models\")\n+        def get_all_models():\n+            return JSONResponse({\"object\": \"list\", \"data\": self.get_gen_models()})\n+\n+        @app.get(\"/health\")\n+        def healthcheck():\n+            return JSONResponse({\"status\": \"ok\"})\n+\n+        @app.middleware(\"http\")\n+        async def get_or_set_request_id(request: Request, call_next):\n+            request_id = request.headers.get(X_REQUEST_ID) or str(uuid.uuid4())\n+            request.state.request_id = request_id\n+            response = await call_next(request)\n+            response.headers[X_REQUEST_ID] = request_id\n+            return response\n+\n+        config = uvicorn.Config(app, host=self.host, port=self.port, log_level=self.log_level)\n+        self.server = uvicorn.Server(config)\n+\n+        if self.non_blocking:\n+            self.start_server()\n+        else:\n+            self.server.run()\n+\n+    def start_server(self):\n+        def _run():\n+            self._loop = asyncio.new_event_loop()\n+            asyncio.set_event_loop(self._loop)\n+            # serve() is a coroutine; it exits when server.should_exit becomes True\n+            self._loop.run_until_complete(self.server.serve())\n+\n+        self._thread = threading.Thread(target=_run, name=\"uvicorn-thread\", daemon=False)\n+        self._thread.start()\n+\n+    def kill_server(self):\n+        if not self._thread:\n+            raise ValueError(\"The server cannot be killed as it was not launched in a separate thread.\")\n+\n+        if not self._thread.is_alive():\n+            raise ValueError(\"The server is already killed.\")\n+\n+        self.server.should_exit = True\n+        if self._thread and self._thread.is_alive():\n+            self._thread.join(timeout=2)\n+\n     def _validate_request(\n         self,\n         request: dict,\n@@ -563,7 +612,7 @@ def _validate_request(\n             logger.error(f\"Unexpected keys in the request: {unexpected_keys}\")\n             raise HTTPException(status_code=422, detail=f\"Unexpected keys in the request: {unexpected_keys}\")\n \n-        if self.args.input_validation:\n+        if self.input_validation:\n             # Validate expected keys\n             try:\n                 validator.validate_python(request)\n@@ -678,106 +727,6 @@ def chunk_to_sse_element(chunk: ChatCompletionChunk | BaseModel) -> str:\n         \"\"\"\n         return f\"data: {chunk.model_dump_json(exclude_none=True)}\\n\\n\"\n \n-    def run(self):\n-        \"\"\"\n-        Setup and run the FastAPI server for transformers serve.\n-\n-        Models will be loaded and unloaded automatically based on usage and a timeout.\n-\n-        The server will expose the following endpoints:\n-        - POST /v1/chat/completions: Generates chat completions.\n-        - POST /v1/responses: Generates responses.\n-        - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n-        - GET /v1/models: Lists available models for 3rd party tools.\n-        - GET /health: Health check.\n-\n-        Requires FastAPI and Uvicorn to be installed.\n-        \"\"\"\n-\n-        @asynccontextmanager\n-        async def lifespan(app: FastAPI):\n-            yield\n-            for model in self.loaded_models.values():\n-                model.delete_model()\n-            if self.running_continuous_batching_manager is not None:\n-                self.running_continuous_batching_manager.stop(block=True, timeout=5)\n-\n-        app = FastAPI(lifespan=lifespan)\n-\n-        # Some apps that make requests from external domains (e.g. Cursor) require CORS to be enabled. However, for\n-        # security purposes, it's disabled by default\n-        if self.enable_cors:\n-            app.add_middleware(\n-                CORSMiddleware,\n-                allow_origins=[\"*\"],\n-                allow_credentials=True,\n-                allow_methods=[\"*\"],\n-                allow_headers=[\"*\"],\n-            )\n-            logger.warning_once(\n-                \"CORS allow origin is set to `*`. This is not recommended for production environments.\"\n-            )\n-\n-        from fastapi import Request\n-\n-        @app.post(\"/v1/chat/completions\")\n-        def chat_completion(request: Request, body: dict):\n-            self.validate_chat_completion_request(request=body)\n-\n-            if self.use_continuous_batching:\n-                return self.continuous_batching_chat_completion(body, request.state.request_id)\n-            else:\n-                return self.generate_chat_completion(body)\n-\n-        @app.post(\"/v1/responses\")\n-        def responses(request: dict):\n-            self.validate_response_request(request=request)\n-            # Support non-streaming mode when `stream=false` is provided\n-            stream = request.get(\"stream\", True)\n-            if not stream:\n-                response_obj = self.generate_response_non_streaming(request)\n-                return JSONResponse(response_obj)\n-\n-            output = self.generate_response(request)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n-\n-        @app.post(\"/v1/audio/transcriptions\")\n-        async def audio_transcriptions(request: Request):\n-            # Parses the multipart/form-data request into the request format used by other endpoints\n-            async with request.form() as form:\n-                parsed_request = TransformersTranscriptionCreateParams(\n-                    file=await form[\"file\"].read(),\n-                    model=form[\"model\"],\n-                    # TODO: add other fields\n-                )\n-                logger.debug(\n-                    f\"Received file: {form['file'].filename}; MIME type: {form['file'].content_type}; \"\n-                    f\"size: {form['file'].size / 1024:.2f} KiB\"\n-                )\n-            self.validate_transcription_request(request=parsed_request)\n-\n-            output = self.generate_transcription(parsed_request)\n-            return StreamingResponse(output, media_type=\"text/event-stream\")\n-\n-        @app.options(\"/v1/models\")\n-        @app.get(\"/v1/models\")\n-        def get_all_models():\n-            return JSONResponse({\"object\": \"list\", \"data\": self.get_gen_models()})\n-\n-        @app.get(\"/health\")\n-        def healthcheck():\n-            return JSONResponse({\"status\": \"ok\"})\n-\n-        @app.middleware(\"http\")\n-        async def get_or_set_request_id(request: Request, call_next):\n-            request_id = request.headers.get(X_REQUEST_ID) or str(uuid.uuid4())\n-            request.state.request_id = request_id\n-            response = await call_next(request)\n-            response.headers[X_REQUEST_ID] = request_id\n-            return response\n-\n-        uvicorn.run(app, host=self.args.host, port=self.args.port, log_level=self.args.log_level)\n-\n     @functools.cache\n     def get_gen_models(self) -> list[dict[str, any]]:\n         \"\"\"\n@@ -1025,8 +974,10 @@ def generate_chat_completion(self, req: dict) -> StreamingResponse | JSONRespons\n         Returns:\n             `Generator[str, None, None]`: A generator that yields the OpenAI Chat Completion chunks.\n         \"\"\"\n-        if self.args.force_model is not None:\n-            req[\"model\"] = self.args.force_model\n+\n+        # TODO: This should throw an error in case the specified model in the request is different to the forced model.\n+        if self.force_model is not None:\n+            req[\"model\"] = self.force_model\n \n         messages: Iterable[ChatCompletionMessageParam] = req[\"messages\"]\n \n@@ -1730,27 +1681,23 @@ def is_continuation(self, req: dict) -> bool:\n         self.last_messages = messages\n         return req_continues_last_messages\n \n-    @staticmethod\n-    def get_quantization_config(args: ServeArguments) -> Optional[\"BitsAndBytesConfig\"]:\n+    def get_quantization_config(self) -> Optional[\"BitsAndBytesConfig\"]:\n         \"\"\"\n         Returns the quantization config for the given CLI arguments.\n \n-        Args:\n-            args (`ServeArguments`): The serve arguments. May contain quantization settings, device, etc.\n-\n         Returns:\n             `Optional[BitsAndBytesConfig]`: The quantization config.\n         \"\"\"\n-        if args.load_in_4bit:\n+        if self.load_in_4bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_4bit=True,\n                 # For consistency with model weights, we use the same value as `dtype`\n-                bnb_4bit_compute_dtype=args.dtype,\n-                bnb_4bit_quant_type=args.bnb_4bit_quant_type,\n-                bnb_4bit_use_double_quant=args.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=args.dtype,\n+                bnb_4bit_compute_dtype=self.dtype,\n+                bnb_4bit_quant_type=self.bnb_4bit_quant_type,\n+                bnb_4bit_use_double_quant=self.use_bnb_nested_quant,\n+                bnb_4bit_quant_storage=self.dtype,\n             )\n-        elif args.load_in_8bit:\n+        elif self.load_in_8bit:\n             quantization_config = BitsAndBytesConfig(\n                 load_in_8bit=True,\n             )\n@@ -1770,8 +1717,8 @@ def process_model_name(self, model_id: str) -> str:\n         Returns:\n             `str`: The canonicalized model name to be used\n         \"\"\"\n-        if self.args.force_model is not None:\n-            model_id = self.args.force_model\n+        if self.force_model is not None:\n+            model_id = self.force_model\n         if \"@\" in model_id:\n             return model_id\n         return f\"{model_id}@main\"\n@@ -1791,7 +1738,6 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n             `tuple[PreTrainedModel, Union[ProcessorMixin, PreTrainedTokenizerFast]]`: The loaded model and\n             data processor (tokenizer, audio processor, etc.).\n         \"\"\"\n-        args = self.args\n         logger.info(f\"Loading {model_id_and_revision}\")\n \n         if \"@\" in model_id_and_revision:\n@@ -1802,18 +1748,18 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n         data_processor = AutoProcessor.from_pretrained(\n             model_id,\n             revision=revision,\n-            trust_remote_code=args.trust_remote_code,\n+            trust_remote_code=self.trust_remote_code,\n         )\n \n-        dtype = args.dtype if args.dtype in [\"auto\", None] else getattr(torch, args.dtype)\n-        quantization_config = self.get_quantization_config(args)\n+        dtype = self.dtype if self.dtype in [\"auto\", None] else getattr(torch, self.dtype)\n+        quantization_config = self.get_quantization_config()\n \n         model_kwargs = {\n             \"revision\": revision,\n-            \"attn_implementation\": args.attn_implementation,\n+            \"attn_implementation\": self.attn_implementation,\n             \"dtype\": dtype,\n             \"device_map\": \"auto\",\n-            \"trust_remote_code\": args.trust_remote_code,\n+            \"trust_remote_code\": self.trust_remote_code,\n         }\n         if quantization_config is not None:\n             model_kwargs[\"quantization_config\"] = quantization_config\n@@ -1823,7 +1769,7 @@ def _load_model_and_data_processor(self, model_id_and_revision: str):\n         model = architecture.from_pretrained(model_id, **model_kwargs)\n \n         if getattr(model, \"hf_device_map\", None) is None:\n-            model = model.to(args.device)\n+            model = model.to(self.device)\n \n         has_default_max_length = (\n             model.generation_config.max_new_tokens is None and model.generation_config.max_length == 20\n@@ -1854,7 +1800,7 @@ def load_model_and_processor(\n             model, processor = self._load_model_and_data_processor(model_id_and_revision)\n             self.loaded_models[model_id_and_revision] = TimedModel(\n                 model,\n-                timeout_seconds=self.args.model_timeout,\n+                timeout_seconds=self.model_timeout,\n                 processor=processor,\n             )\n         else:\n@@ -1879,7 +1825,7 @@ def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"P\n             audio_model, audio_processor = self._load_model_and_data_processor(model_id_and_revision)\n             self.loaded_models[model_id_and_revision] = TimedModel(\n                 audio_model,\n-                timeout_seconds=self.args.model_timeout,\n+                timeout_seconds=self.model_timeout,\n                 processor=audio_processor,\n             )\n         else:\n@@ -1890,6 +1836,21 @@ def load_audio_model_and_processor(self, model_id_and_revision: str) -> tuple[\"P\n         return audio_model, audio_processor\n \n \n+# set docstring separately to make it look nice (Typer doesn't play well with the class command)\n+Serve.__doc__ = \"\"\"\n+Run a FastAPI server to serve models on-demand with an OpenAI compatible API.\n+\n+Models will be loaded and unloaded automatically based on usage and a timeout.\n+\n+\\b\n+The server will expose the following endpoints:\n+    - POST /v1/chat/completions: Generates chat completions.\n+    - POST /v1/responses: Generates responses.\n+    - POST /v1/audio/transcriptions: Generates transcriptions from audio.\n+    - GET /v1/models: Lists available models for 3rd party tools.\n+\n+Requires FastAPI and Uvicorn to be installed.\n+\"\"\"\n+\n if __name__ == \"__main__\":\n-    serve = ServeCommand()\n-    serve.run()\n+    serve = Serve()",
            "previous_filename": "src/transformers/commands/serving.py"
        },
        {
            "sha": "dc6d1203448d708e20a29644cd689db49062ab5b",
            "filename": "src/transformers/cli/system.py",
            "status": "added",
            "additions": 137,
            "deletions": 0,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fsystem.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Fsystem.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fsystem.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,137 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Contains commands to print information about the environment and version.\n+\n+Usage:\n+    transformers env\n+    transformers version\n+\"\"\"\n+\n+import contextlib\n+import io\n+import os\n+import platform\n+from typing import Annotated, Optional\n+\n+import huggingface_hub\n+import typer\n+\n+from .. import __version__\n+from ..integrations.deepspeed import is_deepspeed_available\n+from ..utils import (\n+    is_accelerate_available,\n+    is_torch_available,\n+    is_torch_hpu_available,\n+    is_torch_npu_available,\n+    is_torch_xpu_available,\n+)\n+\n+\n+def env(\n+    accelerate_config_file: Annotated[\n+        Optional[str],\n+        typer.Argument(help=\"The accelerate config file to use for the default values in the launching script.\"),\n+    ] = None,\n+) -> None:\n+    \"\"\"Print information about the environment.\"\"\"\n+    import safetensors\n+\n+    safetensors_version = safetensors.__version__\n+\n+    accelerate_version = \"not installed\"\n+    accelerate_config = accelerate_config_str = \"not found\"\n+\n+    if is_accelerate_available():\n+        import accelerate\n+        from accelerate.commands.config import default_config_file, load_config_from_file\n+\n+        accelerate_version = accelerate.__version__\n+        # Get the default from the config file.\n+        if accelerate_config_file is not None or os.path.isfile(default_config_file):\n+            accelerate_config = load_config_from_file(accelerate_config_file).to_dict()\n+\n+        accelerate_config_str = (\n+            \"\\n\".join([f\"\\t- {prop}: {val}\" for prop, val in accelerate_config.items()])\n+            if isinstance(accelerate_config, dict)\n+            else f\"\\t{accelerate_config}\"\n+        )\n+\n+    pt_version = \"not installed\"\n+    pt_cuda_available = \"NA\"\n+    pt_accelerator = \"NA\"\n+    if is_torch_available():\n+        import torch\n+\n+        pt_version = torch.__version__\n+        pt_cuda_available = torch.cuda.is_available()\n+        pt_xpu_available = is_torch_xpu_available()\n+        pt_npu_available = is_torch_npu_available()\n+        pt_hpu_available = is_torch_hpu_available()\n+\n+        if pt_cuda_available:\n+            pt_accelerator = \"CUDA\"\n+        elif pt_xpu_available:\n+            pt_accelerator = \"XPU\"\n+        elif pt_npu_available:\n+            pt_accelerator = \"NPU\"\n+        elif pt_hpu_available:\n+            pt_accelerator = \"HPU\"\n+\n+    deepspeed_version = \"not installed\"\n+    if is_deepspeed_available():\n+        # Redirect command line output to silence deepspeed import output.\n+        with contextlib.redirect_stdout(io.StringIO()):\n+            import deepspeed\n+        deepspeed_version = deepspeed.__version__\n+\n+    info = {\n+        \"`transformers` version\": __version__,\n+        \"Platform\": platform.platform(),\n+        \"Python version\": platform.python_version(),\n+        \"Huggingface_hub version\": huggingface_hub.__version__,\n+        \"Safetensors version\": f\"{safetensors_version}\",\n+        \"Accelerate version\": f\"{accelerate_version}\",\n+        \"Accelerate config\": f\"{accelerate_config_str}\",\n+        \"DeepSpeed version\": f\"{deepspeed_version}\",\n+        \"PyTorch version (accelerator?)\": f\"{pt_version} ({pt_accelerator})\",\n+        \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n+    }\n+    if is_torch_available():\n+        if pt_cuda_available:\n+            info[\"Using GPU in script?\"] = \"<fill in>\"\n+            info[\"GPU type\"] = torch.cuda.get_device_name()\n+        elif pt_xpu_available:\n+            info[\"Using XPU in script?\"] = \"<fill in>\"\n+            info[\"XPU type\"] = torch.xpu.get_device_name()\n+        elif pt_hpu_available:\n+            info[\"Using HPU in script?\"] = \"<fill in>\"\n+            info[\"HPU type\"] = torch.hpu.get_device_name()\n+        elif pt_npu_available:\n+            info[\"Using NPU in script?\"] = \"<fill in>\"\n+            info[\"NPU type\"] = torch.npu.get_device_name()\n+            info[\"CANN version\"] = torch.version.cann\n+\n+    print(\"\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n\")\n+    print(_format_dict(info))\n+\n+    return info\n+\n+\n+def version() -> None:\n+    \"\"\"Print CLI version.\"\"\"\n+    print(__version__)\n+\n+\n+def _format_dict(d: dict) -> str:\n+    return \"\\n\".join([f\"- {prop}: {val}\" for prop, val in d.items()]) + \"\\n\""
        },
        {
            "sha": "8c710fdd8cff404e30e61000ca3fa91155678051",
            "filename": "src/transformers/cli/transformers.py",
            "status": "added",
            "additions": 44,
            "deletions": 0,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fcli%2Ftransformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Ftransformers.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,44 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Transformers CLI.\"\"\"\n+\n+from huggingface_hub import typer_factory\n+\n+from transformers.cli.add_fast_image_processor import add_fast_image_processor\n+from transformers.cli.add_new_model_like import add_new_model_like\n+from transformers.cli.chat import Chat, ChatCommand\n+from transformers.cli.download import download\n+from transformers.cli.run import run\n+from transformers.cli.serve import Serve\n+from transformers.cli.system import env, version\n+\n+\n+app = typer_factory(help=\"Transformers CLI\")\n+\n+app.command()(add_fast_image_processor)\n+app.command()(add_new_model_like)\n+app.command(name=\"chat\", cls=ChatCommand)(Chat)\n+app.command()(download)\n+app.command()(env)\n+app.command()(run)\n+app.command(name=\"serve\")(Serve)\n+app.command()(version)\n+\n+\n+def main():\n+    app()\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "6ddf90164ba71814111ee3f9e0716e598e154553",
            "filename": "src/transformers/commands/chat.py",
            "status": "removed",
            "additions": 0,
            "deletions": 760,
            "changes": 760,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,760 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import asyncio\n-import copy\n-import json\n-import os\n-import platform\n-import re\n-import string\n-import time\n-from argparse import ArgumentParser, Namespace\n-from collections.abc import AsyncIterator\n-from dataclasses import dataclass, field\n-from threading import Thread\n-from typing import Optional\n-\n-import yaml\n-from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n-\n-from transformers import (\n-    AutoTokenizer,\n-    GenerationConfig,\n-    PreTrainedTokenizer,\n-)\n-from transformers.commands import BaseTransformersCLICommand\n-from transformers.commands.serving import ServeArguments, ServeCommand\n-from transformers.utils import is_rich_available, is_torch_available\n-\n-\n-try:\n-    import readline  # noqa importing this enables GNU readline capabilities\n-except ImportError:\n-    # some platforms may not support readline: https://docs.python.org/3/library/readline.html\n-    pass\n-\n-if platform.system() != \"Windows\":\n-    import pwd\n-\n-if is_rich_available():\n-    from rich.console import Console\n-    from rich.live import Live\n-    from rich.markdown import Markdown\n-\n-if is_torch_available():\n-    import torch\n-\n-    from transformers import (\n-        AutoModelForCausalLM,\n-        BitsAndBytesConfig,\n-    )\n-\n-ALLOWED_KEY_CHARS = set(string.ascii_letters + string.whitespace)\n-ALLOWED_VALUE_CHARS = set(\n-    string.ascii_letters + string.digits + string.whitespace + r\".!\\\"#$%&'()*+,\\-/:<=>?@[]^_`{|}~\"\n-)\n-\n-DEFAULT_EXAMPLES = {\n-    \"llama\": {\"text\": \"There is a Llama in my lawn, how can I get rid of it?\"},\n-    \"code\": {\n-        \"text\": (\n-            \"Write a Python function that integrates any Python function f(x) numerically over an arbitrary \"\n-            \"interval [x_start, x_end].\"\n-        ),\n-    },\n-    \"helicopter\": {\"text\": \"How many helicopters can a human eat in one sitting?\"},\n-    \"numbers\": {\"text\": \"Count to 10 but skip every number ending with an 'e'\"},\n-    \"birds\": {\"text\": \"Why aren't birds real?\"},\n-    \"socks\": {\"text\": \"Why is it important to eat socks after meditating?\"},\n-    \"numbers2\": {\"text\": \"Which number is larger, 9.9 or 9.11?\"},\n-}\n-\n-# Printed at the start of a chat session\n-HELP_STRING_MINIMAL = \"\"\"\n-\n-**TRANSFORMERS CHAT INTERFACE**\n-\n-Chat interface to try out a model. Besides chatting with the model, here are some basic commands:\n-- **!help**: shows all available commands (set generation settings, save chat, etc.)\n-- **!status**: shows the current status of the model and generation settings\n-- **!clear**: clears the current conversation and starts a new one\n-- **!exit**: closes the interface\n-\"\"\"\n-\n-\n-# Printed when the user types `help` in the chat session\n-HELP_STRING = f\"\"\"\n-\n-**TRANSFORMERS CHAT INTERFACE HELP**\n-\n-Full command list:\n-- **!help**: shows this help message\n-- **!clear**: clears the current conversation and starts a new one\n-- **!status**: shows the current status of the model and generation settings\n-- **!example {{NAME}}**: loads example named `{{NAME}}` from the config and uses it as the user input.\n-Available example names: `{\"`, `\".join(DEFAULT_EXAMPLES.keys())}`\n-- **!set {{ARG_1}}={{VALUE_1}} {{ARG_2}}={{VALUE_2}}** ...: changes the system prompt or generation settings (multiple\n-settings are separated by a space). Accepts the same flags and format as the `generate_flags` CLI argument.\n-If you're a new user, check this basic flag guide: https://huggingface.co/docs/transformers/llm_tutorial#common-options\n-- **!save {{SAVE_NAME}} (optional)**: saves the current chat and settings to file by default to\n-`./chat_history/{{MODEL_NAME}}/chat_{{DATETIME}}.yaml` or `{{SAVE_NAME}}` if provided\n-- **!exit**: closes the interface\n-\"\"\"\n-\n-\n-class RichInterface:\n-    def __init__(self, model_name: Optional[str] = None, user_name: Optional[str] = None):\n-        self._console = Console()\n-        if model_name is None:\n-            self.model_name = \"assistant\"\n-        else:\n-            self.model_name = model_name\n-        if user_name is None:\n-            self.user_name = \"user\"\n-        else:\n-            self.user_name = user_name\n-\n-    async def stream_output(self, stream: AsyncIterator[ChatCompletionStreamOutput]) -> tuple[str, int]:\n-        self._console.print(f\"[bold blue]<{self.model_name}>:\")\n-        with Live(console=self._console, refresh_per_second=4) as live:\n-            text = \"\"\n-            async for token in await stream:\n-                outputs = token.choices[0].delta.content\n-\n-                if not outputs:\n-                    continue\n-\n-                # Escapes single words encased in <>, e.g. <think> -> \\<think\\>, for proper rendering in Markdown.\n-                # It only escapes single words that may have `_`, optionally following a `/` (e.g. </think>)\n-                outputs = re.sub(r\"<(/*)(\\w*)>\", r\"\\<\\1\\2\\>\", outputs)\n-\n-                text += outputs\n-                # Render the accumulated text as Markdown\n-                # NOTE: this is a workaround for the rendering \"unstandard markdown\"\n-                #  in rich. The chatbots output treat \"\\n\" as a new line for\n-                #  better compatibility with real-world text. However, rendering\n-                #  in markdown would break the format. It is because standard markdown\n-                #  treat a single \"\\n\" in normal text as a space.\n-                #  Our workaround is adding two spaces at the end of each line.\n-                #  This is not a perfect solution, as it would\n-                #  introduce trailing spaces (only) in code block, but it works well\n-                #  especially for console output, because in general the console does not\n-                #  care about trailing spaces.\n-\n-                lines = []\n-                for line in text.splitlines():\n-                    lines.append(line)\n-                    if line.startswith(\"```\"):\n-                        # Code block marker - do not add trailing spaces, as it would\n-                        #  break the syntax highlighting\n-                        lines.append(\"\\n\")\n-                    else:\n-                        lines.append(\"  \\n\")\n-\n-                markdown = Markdown(\"\".join(lines).strip(), code_theme=\"github-dark\")\n-\n-                # Update the Live console output\n-                live.update(markdown, refresh=True)\n-\n-        self._console.print()\n-\n-        return text\n-\n-    def input(self) -> str:\n-        \"\"\"Gets user input from the console.\"\"\"\n-        input = self._console.input(f\"[bold red]<{self.user_name}>:\\n\")\n-        self._console.print()\n-        return input\n-\n-    def clear(self):\n-        \"\"\"Clears the console.\"\"\"\n-        self._console.clear()\n-\n-    def print_user_message(self, text: str):\n-        \"\"\"Prints a user message to the console.\"\"\"\n-        self._console.print(f\"[bold red]<{self.user_name}>:[/ bold red]\\n{text}\")\n-        self._console.print()\n-\n-    def print_color(self, text: str, color: str):\n-        \"\"\"Prints text in a given color to the console.\"\"\"\n-        self._console.print(f\"[bold {color}]{text}\")\n-        self._console.print()\n-\n-    def print_help(self, minimal: bool = False):\n-        \"\"\"Prints the help message to the console.\"\"\"\n-        self._console.print(Markdown(HELP_STRING_MINIMAL if minimal else HELP_STRING))\n-        self._console.print()\n-\n-    def print_status(self, model_name: str, generation_config: GenerationConfig, model_kwargs: dict):\n-        \"\"\"Prints the status of the model and generation settings to the console.\"\"\"\n-        self._console.print(f\"[bold blue]Model: {model_name}\\n\")\n-        if model_kwargs:\n-            self._console.print(f\"[bold blue]Model kwargs: {model_kwargs}\")\n-        self._console.print(f\"[bold blue]{generation_config}\")\n-        self._console.print()\n-\n-\n-@dataclass\n-class ChatArguments:\n-    r\"\"\"\n-    Arguments for the chat CLI.\n-\n-    See the metadata arg for each argument's description -- the medatata will be printed with\n-    `transformers chat --help`\n-    \"\"\"\n-\n-    # General settings\n-    model_name_or_path: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"Name of the pre-trained model. The positional argument will take precedence if both are passed.\"\n-        },\n-    )\n-    user: Optional[str] = field(\n-        default=None,\n-        metadata={\"help\": \"Username to display in chat interface. Defaults to the current user's name.\"},\n-    )\n-    system_prompt: Optional[str] = field(default=None, metadata={\"help\": \"System prompt.\"})\n-    save_folder: str = field(default=\"./chat_history/\", metadata={\"help\": \"Folder to save chat history.\"})\n-    examples_path: Optional[str] = field(default=None, metadata={\"help\": \"Path to a yaml file with examples.\"})\n-    verbose: bool = field(default=False, metadata={\"help\": \"Whether to show runtime warnings in the chat interface.\"})\n-\n-    # Generation settings\n-    generation_config: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": (\n-                \"Path to a local generation config file or to a HuggingFace repo containing a \"\n-                \"`generation_config.json` file. Other generation settings passed as CLI arguments will be applied on \"\n-                \"top of this generation config.\"\n-            ),\n-        },\n-    )\n-\n-    # Model loading\n-    model_revision: str = field(\n-        default=\"main\",\n-        metadata={\"help\": \"Specific model version to use (can be a branch name, tag name or commit id).\"},\n-    )\n-    device: str = field(default=\"auto\", metadata={\"help\": \"Device to use for inference.\"})\n-    torch_dtype: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"`torch_dtype` is deprecated! Please use `dtype` argument instead.\",\n-            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n-        },\n-    )\n-    dtype: Optional[str] = field(\n-        default=\"auto\",\n-        metadata={\n-            \"help\": \"Override the default `torch.dtype` and load the model under this dtype. If `'auto'` is passed, \"\n-            \"the dtype will be automatically derived from the model's weights.\",\n-            \"choices\": [\"auto\", \"bfloat16\", \"float16\", \"float32\"],\n-        },\n-    )\n-    trust_remote_code: bool = field(\n-        default=False, metadata={\"help\": \"Whether to trust remote code when loading a model.\"}\n-    )\n-    attn_implementation: Optional[str] = field(\n-        default=None,\n-        metadata={\n-            \"help\": \"Which attention implementation to use; you can run --attn_implementation=flash_attention_2, in \"\n-            \"which case you must install this manually by running `pip install flash-attn --no-build-isolation`.\"\n-        },\n-    )\n-    load_in_8bit: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to use 8 bit precision for the base model - works only with LoRA.\"},\n-    )\n-    load_in_4bit: bool = field(\n-        default=False,\n-        metadata={\"help\": \"Whether to use 4 bit precision for the base model - works only with LoRA.\"},\n-    )\n-    bnb_4bit_quant_type: str = field(default=\"nf4\", metadata={\"help\": \"Quantization type.\", \"choices\": [\"fp4\", \"nf4\"]})\n-    use_bnb_nested_quant: bool = field(default=False, metadata={\"help\": \"Whether to use nested quantization.\"})\n-\n-    # Serving settings\n-    host: str = field(default=\"localhost\", metadata={\"help\": \"Interface the server will listen to..\"})\n-    port: int = field(default=8000, metadata={\"help\": \"Port the server will listen to.\"})\n-\n-    def __post_init__(self):\n-        \"\"\"Only used for BC `torch_dtype` argument.\"\"\"\n-        # In this case only the BC torch_dtype was given\n-        if self.torch_dtype is not None:\n-            if self.dtype is None:\n-                self.dtype = self.torch_dtype\n-            elif self.torch_dtype != self.dtype:\n-                raise ValueError(\n-                    f\"`torch_dtype` {self.torch_dtype} and `dtype` {self.dtype} have different values. `torch_dtype` is deprecated and \"\n-                    \"will be removed in 4.59.0, please set `dtype` instead.\"\n-                )\n-\n-\n-def chat_command_factory(args: Namespace):\n-    \"\"\"\n-    Factory function used to chat with a local model.\n-    \"\"\"\n-    return ChatCommand(args)\n-\n-\n-class ChatCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        \"\"\"\n-        Register this command to argparse so it's available for the transformer-cli\n-\n-        Args:\n-            parser: Root parser to register command-specific arguments\n-        \"\"\"\n-        dataclass_types = (ChatArguments,)\n-        chat_parser = parser.add_parser(\"chat\", dataclass_types=dataclass_types)\n-\n-        group = chat_parser.add_argument_group(\"Positional arguments\")\n-        group.add_argument(\n-            \"model_name_or_path_or_address\",\n-            type=str,\n-            default=None,\n-            help=\"Name of the pre-trained model or address to connect to.\",\n-        )\n-        group.add_argument(\n-            \"generate_flags\",\n-            type=str,\n-            default=None,\n-            help=(\n-                \"Flags to pass to `generate`, using a space as a separator between flags. Accepts booleans, numbers, \"\n-                \"and lists of integers, more advanced parameterization should be set through --generation-config. \"\n-                \"Example: `transformers chat <model_repo> max_new_tokens=100 do_sample=False eos_token_id=[1,2]`. \"\n-                \"If you're a new user, check this basic flag guide: \"\n-                \"https://huggingface.co/docs/transformers/llm_tutorial#common-options\"\n-            ),\n-            nargs=\"*\",\n-        )\n-        chat_parser.set_defaults(func=chat_command_factory)\n-\n-    def __init__(self, args):\n-        if args.model_name_or_path_or_address is not None:\n-            name = args.model_name_or_path_or_address\n-            if name.startswith(\"http\") or name.startswith(\"https\") or name.startswith(\"localhost\"):\n-                self.spawn_backend = False\n-\n-                if args.host != \"localhost\" or args.port != 8000:\n-                    raise ValueError(\n-                        \"Looks like youve set both a server address and a custom host/port. \"\n-                        \"Please pick just one way to specify the server.\"\n-                    )\n-\n-                args.host, args.port = args.model_name_or_path_or_address.rsplit(\":\", 1)\n-\n-                if args.model_name_or_path is None:\n-                    raise ValueError(\n-                        \"When connecting to a server, please specify a model name with the --model_name_or_path flag.\"\n-                    )\n-            else:\n-                self.spawn_backend = True\n-                args.model_name_or_path = args.model_name_or_path_or_address\n-\n-        if not is_rich_available() and (not is_torch_available() and self.spawn_backend):\n-            raise ImportError(\n-                \"You need to install rich to use the chat interface. Additionally, you have not specified a remote \"\n-                \"endpoint and are therefore spawning a backend. Torch is required for this: (`pip install rich torch`)\"\n-            )\n-        elif not is_rich_available():\n-            raise ImportError(\"You need to install rich to use the chat interface. (`pip install rich`)\")\n-        elif not is_torch_available() and self.spawn_backend:\n-            raise ImportError(\n-                \"You have not specified a remote endpoint and are therefore spawning a backend. Torch is required \"\n-                \"for this: (`pip install rich torch`)\"\n-            )\n-\n-        self.args = args\n-\n-    # -----------------------------------------------------------------------------------------------------------------\n-    # Chat session methods\n-    @staticmethod\n-    def get_username() -> str:\n-        \"\"\"Returns the username of the current user.\"\"\"\n-        if platform.system() == \"Windows\":\n-            return os.getlogin()\n-        else:\n-            return pwd.getpwuid(os.getuid()).pw_name\n-\n-    @staticmethod\n-    def save_chat(chat, args: ChatArguments, filename: Optional[str] = None) -> str:\n-        \"\"\"Saves the chat history to a file.\"\"\"\n-        output_dict = {}\n-        output_dict[\"settings\"] = vars(args)\n-        output_dict[\"chat_history\"] = chat\n-\n-        folder = args.save_folder\n-\n-        if filename is None:\n-            time_str = time.strftime(\"%Y-%m-%d_%H-%M-%S\")\n-            filename = f\"{args.model_name_or_path_or_address}/chat_{time_str}.json\"\n-            filename = os.path.join(folder, filename)\n-\n-        os.makedirs(os.path.dirname(filename), exist_ok=True)\n-        with open(filename, \"w\") as f:\n-            json.dump(output_dict, f, indent=4)\n-        return os.path.abspath(filename)\n-\n-    @staticmethod\n-    def clear_chat_history(system_prompt: Optional[str] = None) -> list[dict]:\n-        \"\"\"Clears the chat history.\"\"\"\n-        if system_prompt is None:\n-            chat = []\n-        else:\n-            chat = [{\"role\": \"system\", \"content\": system_prompt}]\n-        return chat\n-\n-    # -----------------------------------------------------------------------------------------------------------------\n-    # Input parsing methods\n-    def parse_generate_flags(self, generate_flags: list[str]) -> dict:\n-        \"\"\"Parses the generate flags from the user input into a dictionary of `generate` kwargs.\"\"\"\n-        if len(generate_flags) == 0:\n-            return {}\n-\n-        # Assumption: `generate_flags` is a list of strings, each string being a `flag=value` pair, that can be parsed\n-        # into a json string if we:\n-        # 1. Add quotes around each flag name\n-        generate_flags_as_dict = {'\"' + flag.split(\"=\")[0] + '\"': flag.split(\"=\")[1] for flag in generate_flags}\n-\n-        # 2. Handle types:\n-        # 2. a. booleans should be lowercase, None should be null\n-        generate_flags_as_dict = {\n-            k: v.lower() if v.lower() in [\"true\", \"false\"] else v for k, v in generate_flags_as_dict.items()\n-        }\n-        generate_flags_as_dict = {k: \"null\" if v == \"None\" else v for k, v in generate_flags_as_dict.items()}\n-\n-        # 2. b. strings should be quoted\n-        def is_number(s: str) -> bool:\n-            # handle negative numbers\n-            s = s.removeprefix(\"-\")\n-            return s.replace(\".\", \"\", 1).isdigit()\n-\n-        generate_flags_as_dict = {k: f'\"{v}\"' if not is_number(v) else v for k, v in generate_flags_as_dict.items()}\n-        # 2. c. [no processing needed] lists are lists of ints because `generate` doesn't take lists of strings :)\n-        # We also mention in the help message that we only accept lists of ints for now.\n-\n-        # 3. Join the result into a comma separated string\n-        generate_flags_string = \", \".join([f\"{k}: {v}\" for k, v in generate_flags_as_dict.items()])\n-\n-        # 4. Add the opening/closing brackets\n-        generate_flags_string = \"{\" + generate_flags_string + \"}\"\n-\n-        # 5. Remove quotes around boolean/null and around lists\n-        generate_flags_string = generate_flags_string.replace('\"null\"', \"null\")\n-        generate_flags_string = generate_flags_string.replace('\"true\"', \"true\")\n-        generate_flags_string = generate_flags_string.replace('\"false\"', \"false\")\n-        generate_flags_string = generate_flags_string.replace('\"[', \"[\")\n-        generate_flags_string = generate_flags_string.replace(']\"', \"]\")\n-\n-        # 6. Replace the `=` with `:`\n-        generate_flags_string = generate_flags_string.replace(\"=\", \":\")\n-\n-        try:\n-            processed_generate_flags = json.loads(generate_flags_string)\n-        except json.JSONDecodeError:\n-            raise ValueError(\n-                \"Failed to convert `generate_flags` into a valid JSON object.\"\n-                \"\\n`generate_flags` = {generate_flags}\"\n-                \"\\nConverted JSON string = {generate_flags_string}\"\n-            )\n-        return processed_generate_flags\n-\n-    def get_generation_parameterization(\n-        self, args: ChatArguments, model_generation_config: GenerationConfig\n-    ) -> tuple[GenerationConfig, dict]:\n-        \"\"\"\n-        Returns a GenerationConfig object holding the generation parameters for the CLI command.\n-        \"\"\"\n-        # No generation config arg provided -> use model's default generation config, then apply CLI defaults\n-        if args.generation_config is not None:\n-            if \".json\" in args.generation_config:  # is a local file\n-                dirname = os.path.dirname(args.generation_config)\n-                filename = os.path.basename(args.generation_config)\n-                generation_config = GenerationConfig.from_pretrained(dirname, filename)\n-            else:\n-                generation_config = GenerationConfig.from_pretrained(args.generation_config)\n-        else:\n-            # !!!!!!!!!\n-            # This is a chat session, so we have a few non-standard defaults\n-            # !!!!!!!!!\n-            generation_config = copy.deepcopy(model_generation_config)\n-            generation_config.update(**{\"do_sample\": True, \"max_new_tokens\": 256})\n-\n-        # Finally: parse and apply `generate_flags`\n-        parsed_generate_flags = self.parse_generate_flags(args.generate_flags)\n-        model_kwargs = generation_config.update(**parsed_generate_flags)\n-        # `model_kwargs` contain non-generation flags in `parsed_generate_flags` that should be passed directly to\n-        # `generate`\n-        return generation_config, model_kwargs\n-\n-    @staticmethod\n-    def parse_eos_tokens(\n-        tokenizer: PreTrainedTokenizer,\n-        generation_config: GenerationConfig,\n-        eos_tokens: Optional[str],\n-        eos_token_ids: Optional[str],\n-    ) -> tuple[int, list[int]]:\n-        \"\"\"Retrieves the pad token ID and all possible EOS token IDs.\"\"\"\n-        if generation_config.pad_token_id is None:\n-            pad_token_id = generation_config.eos_token_id\n-        else:\n-            pad_token_id = generation_config.pad_token_id\n-\n-        all_eos_token_ids = []\n-\n-        if eos_tokens is not None:\n-            all_eos_token_ids.extend(tokenizer.convert_tokens_to_ids(eos_tokens.split(\",\")))\n-\n-        if eos_token_ids is not None:\n-            all_eos_token_ids.extend([int(token_id) for token_id in eos_token_ids.split(\",\")])\n-\n-        if len(all_eos_token_ids) == 0:\n-            all_eos_token_ids.append(generation_config.eos_token_id)\n-\n-        return pad_token_id, all_eos_token_ids\n-\n-    # -----------------------------------------------------------------------------------------------------------------\n-    # Model loading and performance automation methods\n-    @staticmethod\n-    def get_quantization_config(model_args: ChatArguments) -> Optional[BitsAndBytesConfig]:\n-        if model_args.load_in_4bit:\n-            quantization_config = BitsAndBytesConfig(\n-                load_in_4bit=True,\n-                # For consistency with model weights, we use the same value as `dtype`\n-                bnb_4bit_compute_dtype=model_args.dtype,\n-                bnb_4bit_quant_type=model_args.bnb_4bit_quant_type,\n-                bnb_4bit_use_double_quant=model_args.use_bnb_nested_quant,\n-                bnb_4bit_quant_storage=model_args.dtype,\n-            )\n-        elif model_args.load_in_8bit:\n-            quantization_config = BitsAndBytesConfig(\n-                load_in_8bit=True,\n-            )\n-        else:\n-            quantization_config = None\n-\n-        return quantization_config\n-\n-    def load_model_and_tokenizer(self, args: ChatArguments) -> tuple[\"AutoModelForCausalLM\", AutoTokenizer]:\n-        tokenizer = AutoTokenizer.from_pretrained(\n-            args.model_name_or_path_positional,\n-            revision=args.model_revision,\n-            trust_remote_code=args.trust_remote_code,\n-        )\n-\n-        dtype = args.dtype if args.dtype in [\"auto\", None] else getattr(torch, args.dtype)\n-        quantization_config = self.get_quantization_config(args)\n-        model_kwargs = {\n-            \"revision\": args.model_revision,\n-            \"attn_implementation\": args.attn_implementation,\n-            \"dtype\": dtype,\n-            \"device_map\": \"auto\",\n-            \"quantization_config\": quantization_config,\n-        }\n-        model = AutoModelForCausalLM.from_pretrained(\n-            args.model_name_or_path_positional, trust_remote_code=args.trust_remote_code, **model_kwargs\n-        )\n-\n-        if getattr(model, \"hf_device_map\", None) is None:\n-            model = model.to(args.device)\n-\n-        return model, tokenizer\n-\n-    # -----------------------------------------------------------------------------------------------------------------\n-    # User commands\n-    def handle_non_exit_user_commands(\n-        self,\n-        user_input: str,\n-        args: ChatArguments,\n-        interface: RichInterface,\n-        examples: dict[str, dict[str, str]],\n-        generation_config: GenerationConfig,\n-        model_kwargs: dict,\n-        chat: list[dict],\n-    ) -> tuple[list[dict], GenerationConfig, dict]:\n-        \"\"\"\n-        Handles all user commands except for `!exit`. May update the chat history (e.g. reset it) or the\n-        generation config (e.g. set a new flag).\n-        \"\"\"\n-        valid_command = True\n-\n-        if user_input == \"!clear\":\n-            chat = self.clear_chat_history(args.system_prompt)\n-            interface.clear()\n-\n-        elif user_input == \"!help\":\n-            interface.print_help()\n-\n-        elif user_input.startswith(\"!save\") and len(user_input.split()) < 2:\n-            split_input = user_input.split()\n-\n-            if len(split_input) == 2:\n-                filename = split_input[1]\n-            else:\n-                filename = None\n-            filename = self.save_chat(chat, args, filename)\n-            interface.print_color(text=f\"Chat saved in {filename}!\", color=\"green\")\n-\n-        elif user_input.startswith(\"!set\"):\n-            # splits the new args into a list of strings, each string being a `flag=value` pair (same format as\n-            # `generate_flags`)\n-            new_generate_flags = user_input[4:].strip()\n-            new_generate_flags = new_generate_flags.split()\n-            # sanity check: each member in the list must have an =\n-            for flag in new_generate_flags:\n-                if \"=\" not in flag:\n-                    interface.print_color(\n-                        text=(\n-                            f\"Invalid flag format, missing `=` after `{flag}`. Please use the format \"\n-                            \"`arg_1=value_1 arg_2=value_2 ...`.\"\n-                        ),\n-                        color=\"red\",\n-                    )\n-                    break\n-            else:\n-                # parses the new args into a dictionary of `generate` kwargs, and updates the corresponding variables\n-                parsed_new_generate_flags = self.parse_generate_flags(new_generate_flags)\n-                new_model_kwargs = generation_config.update(**parsed_new_generate_flags)\n-                model_kwargs.update(**new_model_kwargs)\n-\n-        elif user_input.startswith(\"!example\") and len(user_input.split()) == 2:\n-            example_name = user_input.split()[1]\n-            if example_name in examples:\n-                interface.clear()\n-                chat = []\n-                interface.print_user_message(examples[example_name][\"text\"])\n-                chat.append({\"role\": \"user\", \"content\": examples[example_name][\"text\"]})\n-            else:\n-                example_error = (\n-                    f\"Example {example_name} not found in list of available examples: {list(examples.keys())}.\"\n-                )\n-                interface.print_color(text=example_error, color=\"red\")\n-\n-        elif user_input == \"!status\":\n-            interface.print_status(\n-                model_name=args.model_name_or_path,\n-                generation_config=generation_config,\n-                model_kwargs=model_kwargs,\n-            )\n-\n-        else:\n-            valid_command = False\n-            interface.print_color(text=f\"'{user_input}' is not a valid command. Showing help message.\", color=\"red\")\n-            interface.print_help()\n-\n-        return chat, valid_command, generation_config, model_kwargs\n-\n-    # -----------------------------------------------------------------------------------------------------------------\n-    # Main logic\n-    def run(self):\n-        asyncio.run(self._inner_run())\n-\n-    async def _inner_run(self):\n-        if self.spawn_backend:\n-            serve_args = ServeArguments(\n-                device=self.args.device,\n-                dtype=self.args.dtype,\n-                trust_remote_code=self.args.trust_remote_code,\n-                attn_implementation=self.args.attn_implementation,\n-                load_in_8bit=self.args.load_in_8bit,\n-                load_in_4bit=self.args.load_in_4bit,\n-                bnb_4bit_quant_type=self.args.bnb_4bit_quant_type,\n-                use_bnb_nested_quant=self.args.use_bnb_nested_quant,\n-                host=self.args.host,\n-                port=self.args.port,\n-                log_level=\"error\",\n-            )\n-            serve_command = ServeCommand(serve_args)\n-\n-            thread = Thread(target=serve_command.run)\n-            thread.daemon = True\n-            thread.start()\n-\n-        model = self.args.model_name_or_path + \"@\" + self.args.model_revision\n-        host = \"http://localhost\" if self.args.host == \"localhost\" else self.args.host\n-\n-        args = self.args\n-        if args.examples_path is None:\n-            examples = DEFAULT_EXAMPLES\n-        else:\n-            with open(args.examples_path) as f:\n-                examples = yaml.safe_load(f)\n-\n-        if args.user is None:\n-            user = self.get_username()\n-        else:\n-            user = args.user\n-\n-        model_generation_config = GenerationConfig.from_pretrained(args.model_name_or_path)\n-        generation_config, model_kwargs = self.get_generation_parameterization(args, model_generation_config)\n-\n-        interface = RichInterface(model_name=args.model_name_or_path, user_name=user)\n-        interface.clear()\n-        chat = self.clear_chat_history(args.system_prompt)\n-\n-        # Starts the session with a minimal help message at the top, so that a user doesn't get stuck\n-        interface.print_help(minimal=True)\n-\n-        async with AsyncInferenceClient(f\"{host}:{self.args.port}\") as client:\n-            while True:\n-                try:\n-                    user_input = interface.input()\n-\n-                    # User commands\n-                    if user_input.startswith(\"!\"):\n-                        # `!exit` is special, it breaks the loop\n-                        if user_input == \"!exit\":\n-                            break\n-                        else:\n-                            chat, valid_command, generation_config, model_kwargs = self.handle_non_exit_user_commands(\n-                                user_input=user_input,\n-                                args=args,\n-                                interface=interface,\n-                                examples=examples,\n-                                generation_config=generation_config,\n-                                model_kwargs=model_kwargs,\n-                                chat=chat,\n-                            )\n-                        # `!example` sends a user message to the model\n-                        if not valid_command or not user_input.startswith(\"!example\"):\n-                            continue\n-                    else:\n-                        chat.append({\"role\": \"user\", \"content\": user_input})\n-\n-                    stream = client.chat_completion(\n-                        chat,\n-                        stream=True,\n-                        extra_body={\n-                            \"generation_config\": generation_config.to_json_string(),\n-                            \"model\": model,\n-                        },\n-                    )\n-\n-                    model_output = await interface.stream_output(stream)\n-\n-                    chat.append({\"role\": \"assistant\", \"content\": model_output})\n-                except KeyboardInterrupt:\n-                    break\n-\n-\n-if __name__ == \"__main__\":\n-    args = ChatArguments()\n-    args.model_name_or_path_or_address = \"meta-llama/Llama-3.2-3b-Instruct\"\n-    args.model_name_or_path_or_address = \"http://localhost:8000\"\n-    chat = ChatCommand(args)\n-    chat.run()"
        },
        {
            "sha": "8af3c6397b442f1016640c51b4c54cfd9921fd6a",
            "filename": "src/transformers/commands/download.py",
            "status": "removed",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fdownload.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fdownload.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fdownload.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,56 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from argparse import ArgumentParser\n-\n-from . import BaseTransformersCLICommand\n-\n-\n-def download_command_factory(args):\n-    return DownloadCommand(args.model, args.cache_dir, args.force, args.trust_remote_code)\n-\n-\n-class DownloadCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        download_parser = parser.add_parser(\"download\")\n-        download_parser.add_argument(\n-            \"--cache-dir\", type=str, default=None, help=\"Path to location to store the models\"\n-        )\n-        download_parser.add_argument(\n-            \"--force\", action=\"store_true\", help=\"Force the model to be download even if already in cache-dir\"\n-        )\n-        download_parser.add_argument(\n-            \"--trust-remote-code\",\n-            action=\"store_true\",\n-            help=\"Whether or not to allow for custom models defined on the Hub in their own modeling files. Use only if you've reviewed the code as it will execute on your local machine\",\n-        )\n-        download_parser.add_argument(\"model\", type=str, help=\"Name of the model to download\")\n-        download_parser.set_defaults(func=download_command_factory)\n-\n-    def __init__(self, model: str, cache: str, force: bool, trust_remote_code: bool):\n-        self._model = model\n-        self._cache = cache\n-        self._force = force\n-        self._trust_remote_code = trust_remote_code\n-\n-    def run(self):\n-        from ..models.auto import AutoModel, AutoTokenizer\n-\n-        AutoModel.from_pretrained(\n-            self._model, cache_dir=self._cache, force_download=self._force, trust_remote_code=self._trust_remote_code\n-        )\n-        AutoTokenizer.from_pretrained(\n-            self._model, cache_dir=self._cache, force_download=self._force, trust_remote_code=self._trust_remote_code\n-        )"
        },
        {
            "sha": "328e42fb2ea986606289205966442ae9c415317c",
            "filename": "src/transformers/commands/env.py",
            "status": "removed",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fenv.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Fenv.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fenv.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,144 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import contextlib\n-import io\n-import os\n-import platform\n-from argparse import ArgumentParser\n-\n-import huggingface_hub\n-\n-from .. import __version__ as version\n-from ..integrations.deepspeed import is_deepspeed_available\n-from ..utils import (\n-    is_accelerate_available,\n-    is_torch_available,\n-    is_torch_hpu_available,\n-    is_torch_npu_available,\n-    is_torch_xpu_available,\n-)\n-from . import BaseTransformersCLICommand\n-\n-\n-def info_command_factory(_):\n-    return EnvironmentCommand()\n-\n-\n-def download_command_factory(args):\n-    return EnvironmentCommand(args.accelerate_config_file)\n-\n-\n-class EnvironmentCommand(BaseTransformersCLICommand):\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        download_parser = parser.add_parser(\"env\")\n-        download_parser.set_defaults(func=info_command_factory)\n-        download_parser.add_argument(\n-            \"--accelerate-config_file\",\n-            default=None,\n-            help=\"The accelerate config file to use for the default values in the launching script.\",\n-        )\n-        download_parser.set_defaults(func=download_command_factory)\n-\n-    def __init__(self, accelerate_config_file, *args) -> None:\n-        self._accelerate_config_file = accelerate_config_file\n-\n-    def run(self):\n-        import safetensors\n-\n-        safetensors_version = safetensors.__version__\n-\n-        accelerate_version = \"not installed\"\n-        accelerate_config = accelerate_config_str = \"not found\"\n-\n-        if is_accelerate_available():\n-            import accelerate\n-            from accelerate.commands.config import default_config_file, load_config_from_file\n-\n-            accelerate_version = accelerate.__version__\n-            # Get the default from the config file.\n-            if self._accelerate_config_file is not None or os.path.isfile(default_config_file):\n-                accelerate_config = load_config_from_file(self._accelerate_config_file).to_dict()\n-\n-            accelerate_config_str = (\n-                \"\\n\".join([f\"\\t- {prop}: {val}\" for prop, val in accelerate_config.items()])\n-                if isinstance(accelerate_config, dict)\n-                else f\"\\t{accelerate_config}\"\n-            )\n-\n-        pt_version = \"not installed\"\n-        pt_cuda_available = \"NA\"\n-        pt_accelerator = \"NA\"\n-        if is_torch_available():\n-            import torch\n-\n-            pt_version = torch.__version__\n-            pt_cuda_available = torch.cuda.is_available()\n-            pt_xpu_available = is_torch_xpu_available()\n-            pt_npu_available = is_torch_npu_available()\n-            pt_hpu_available = is_torch_hpu_available()\n-\n-            if pt_cuda_available:\n-                pt_accelerator = \"CUDA\"\n-            elif pt_xpu_available:\n-                pt_accelerator = \"XPU\"\n-            elif pt_npu_available:\n-                pt_accelerator = \"NPU\"\n-            elif pt_hpu_available:\n-                pt_accelerator = \"HPU\"\n-\n-        deepspeed_version = \"not installed\"\n-        if is_deepspeed_available():\n-            # Redirect command line output to silence deepspeed import output.\n-            with contextlib.redirect_stdout(io.StringIO()):\n-                import deepspeed\n-            deepspeed_version = deepspeed.__version__\n-\n-        info = {\n-            \"`transformers` version\": version,\n-            \"Platform\": platform.platform(),\n-            \"Python version\": platform.python_version(),\n-            \"Huggingface_hub version\": huggingface_hub.__version__,\n-            \"Safetensors version\": f\"{safetensors_version}\",\n-            \"Accelerate version\": f\"{accelerate_version}\",\n-            \"Accelerate config\": f\"{accelerate_config_str}\",\n-            \"DeepSpeed version\": f\"{deepspeed_version}\",\n-            \"PyTorch version (accelerator?)\": f\"{pt_version} ({pt_accelerator})\",\n-            \"Using distributed or parallel set-up in script?\": \"<fill in>\",\n-        }\n-        if is_torch_available():\n-            if pt_cuda_available:\n-                info[\"Using GPU in script?\"] = \"<fill in>\"\n-                info[\"GPU type\"] = torch.cuda.get_device_name()\n-            elif pt_xpu_available:\n-                info[\"Using XPU in script?\"] = \"<fill in>\"\n-                info[\"XPU type\"] = torch.xpu.get_device_name()\n-            elif pt_hpu_available:\n-                info[\"Using HPU in script?\"] = \"<fill in>\"\n-                info[\"HPU type\"] = torch.hpu.get_device_name()\n-            elif pt_npu_available:\n-                info[\"Using NPU in script?\"] = \"<fill in>\"\n-                info[\"NPU type\"] = torch.npu.get_device_name()\n-                info[\"CANN version\"] = torch.version.cann\n-\n-        print(\"\\nCopy-and-paste the text below in your GitHub issue and FILL OUT the two last points.\\n\")\n-        print(self.format_dict(info))\n-\n-        return info\n-\n-    @staticmethod\n-    def format_dict(d):\n-        return \"\\n\".join([f\"- {prop}: {val}\" for prop, val in d.items()]) + \"\\n\""
        },
        {
            "sha": "dbf067ae4d95088a1e3a46deb02825ebe0d147d8",
            "filename": "src/transformers/commands/run.py",
            "status": "removed",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Frun.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Frun.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Frun.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,110 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from argparse import ArgumentParser\n-\n-from ..pipelines import Pipeline, PipelineDataFormat, get_supported_tasks, pipeline\n-from ..utils import logging\n-from . import BaseTransformersCLICommand\n-\n-\n-logger = logging.get_logger(__name__)  # pylint: disable=invalid-name\n-\n-\n-def try_infer_format_from_ext(path: str):\n-    if not path:\n-        return \"pipe\"\n-\n-    for ext in PipelineDataFormat.SUPPORTED_FORMATS:\n-        if path.endswith(ext):\n-            return ext\n-\n-    raise Exception(\n-        f\"Unable to determine file format from file extension {path}. \"\n-        f\"Please provide the format through --format {PipelineDataFormat.SUPPORTED_FORMATS}\"\n-    )\n-\n-\n-def run_command_factory(args):\n-    nlp = pipeline(\n-        task=args.task,\n-        model=args.model if args.model else None,\n-        config=args.config,\n-        tokenizer=args.tokenizer,\n-        device=args.device,\n-    )\n-    format = try_infer_format_from_ext(args.input) if args.format == \"infer\" else args.format\n-    reader = PipelineDataFormat.from_str(\n-        format=format,\n-        output_path=args.output,\n-        input_path=args.input,\n-        column=args.column if args.column else nlp.default_input_names,\n-        overwrite=args.overwrite,\n-    )\n-    return RunCommand(nlp, reader)\n-\n-\n-class RunCommand(BaseTransformersCLICommand):\n-    def __init__(self, nlp: Pipeline, reader: PipelineDataFormat):\n-        self._nlp = nlp\n-        self._reader = reader\n-\n-    @staticmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        run_parser = parser.add_parser(\"run\", help=\"Run a pipeline through the CLI\")\n-        run_parser.add_argument(\"--task\", choices=get_supported_tasks(), help=\"Task to run\")\n-        run_parser.add_argument(\"--input\", type=str, help=\"Path to the file to use for inference\")\n-        run_parser.add_argument(\"--output\", type=str, help=\"Path to the file that will be used post to write results.\")\n-        run_parser.add_argument(\"--model\", type=str, help=\"Name or path to the model to instantiate.\")\n-        run_parser.add_argument(\"--config\", type=str, help=\"Name or path to the model's config to instantiate.\")\n-        run_parser.add_argument(\n-            \"--tokenizer\", type=str, help=\"Name of the tokenizer to use. (default: same as the model name)\"\n-        )\n-        run_parser.add_argument(\n-            \"--column\",\n-            type=str,\n-            help=\"Name of the column to use as input. (For multi columns input as QA use column1,columns2)\",\n-        )\n-        run_parser.add_argument(\n-            \"--format\",\n-            type=str,\n-            default=\"infer\",\n-            choices=PipelineDataFormat.SUPPORTED_FORMATS,\n-            help=\"Input format to read from\",\n-        )\n-        run_parser.add_argument(\n-            \"--device\",\n-            type=int,\n-            default=-1,\n-            help=\"Indicate the device to run onto, -1 indicates CPU, >= 0 indicates GPU (default: -1)\",\n-        )\n-        run_parser.add_argument(\"--overwrite\", action=\"store_true\", help=\"Allow overwriting the output file.\")\n-        run_parser.set_defaults(func=run_command_factory)\n-\n-    def run(self):\n-        nlp, outputs = self._nlp, []\n-\n-        for entry in self._reader:\n-            output = nlp(**entry) if self._reader.is_multi_columns else nlp(entry)\n-            if isinstance(output, dict):\n-                outputs.append(output)\n-            else:\n-                outputs += output\n-\n-        # Saving data\n-        if self._nlp.binary_output:\n-            binary_path = self._reader.save_binary(outputs)\n-            logger.warning(f\"Current pipeline requires output to be in binary format, saving at {binary_path}\")\n-        else:\n-            self._reader.save(outputs)"
        },
        {
            "sha": "7d5f3b6fb3837e75b8ec8cf2dcd33500a338781f",
            "filename": "src/transformers/commands/transformers_cli.py",
            "status": "removed",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Ftransformers_cli.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,52 +0,0 @@\n-#!/usr/bin/env python\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from transformers import HfArgumentParser\n-from transformers.commands.add_fast_image_processor import AddFastImageProcessorCommand\n-from transformers.commands.add_new_model_like import AddNewModelLikeCommand\n-from transformers.commands.chat import ChatCommand\n-from transformers.commands.download import DownloadCommand\n-from transformers.commands.env import EnvironmentCommand\n-from transformers.commands.run import RunCommand\n-from transformers.commands.serving import ServeCommand\n-\n-\n-def main():\n-    parser = HfArgumentParser(prog=\"Transformers CLI tool\", usage=\"transformers <command> [<args>]\")\n-    commands_parser = parser.add_subparsers(help=\"transformers command helpers\")\n-\n-    # Register commands\n-    ChatCommand.register_subcommand(commands_parser)\n-    DownloadCommand.register_subcommand(commands_parser)\n-    EnvironmentCommand.register_subcommand(commands_parser)\n-    RunCommand.register_subcommand(commands_parser)\n-    ServeCommand.register_subcommand(commands_parser)\n-    AddNewModelLikeCommand.register_subcommand(commands_parser)\n-    AddFastImageProcessorCommand.register_subcommand(commands_parser)\n-\n-    # Let's go\n-    args = parser.parse_args()\n-\n-    if not hasattr(args, \"func\"):\n-        parser.print_help()\n-        exit(1)\n-\n-    # Run\n-    service = args.func(args)\n-    service.run()\n-\n-\n-if __name__ == \"__main__\":\n-    main()"
        },
        {
            "sha": "0551abfc6aaa669b91750b60c12ae5f7b0eb941d",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -23,7 +23,7 @@\n     \"GitPython\": \"GitPython<3.1.19\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\": \"hf_xet\",\n-    \"huggingface-hub\": \"huggingface-hub==1.0.0.rc5\",\n+    \"huggingface-hub\": \"huggingface-hub==1.0.0.rc6\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"jinja2\": \"jinja2>=3.1.0\",\n@@ -81,6 +81,7 @@\n     \"torchvision\": \"torchvision\",\n     \"pyctcdecode\": \"pyctcdecode>=0.4.0\",\n     \"tqdm\": \"tqdm>=4.27\",\n+    \"typer-slim\": \"typer-slim\",\n     \"unidic\": \"unidic>=1.0.2\",\n     \"unidic_lite\": \"unidic_lite>=1.0.7\",\n     \"urllib3\": \"urllib3<2.0.0\","
        },
        {
            "sha": "05fe393240bc6d7a17a66c70f855a75b5c226b74",
            "filename": "tests/cli/conftest.py",
            "status": "renamed",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Fconftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Fconftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Fconftest.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -1,4 +1,4 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n #\n # Licensed under the Apache License, Version 2.0 (the \"License\");\n # you may not use this file except in compliance with the License.\n@@ -11,17 +11,16 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import pytest\n+from typer.testing import CliRunner\n \n-from abc import ABC, abstractmethod\n-from argparse import ArgumentParser\n+import transformers.cli.transformers\n \n \n-class BaseTransformersCLICommand(ABC):\n-    @staticmethod\n-    @abstractmethod\n-    def register_subcommand(parser: ArgumentParser):\n-        raise NotImplementedError()\n+@pytest.fixture\n+def cli():\n+    def _cli_invoke(*args):\n+        runner = CliRunner()\n+        return runner.invoke(transformers.cli.transformers.app, list(args), catch_exceptions=False)\n \n-    @abstractmethod\n-    def run(self):\n-        raise NotImplementedError()\n+    return _cli_invoke",
            "previous_filename": "src/transformers/commands/__init__.py"
        },
        {
            "sha": "70613fd4dfa0fd37c7326b6947cdd32cca6acb18",
            "filename": "tests/cli/test_chat.py",
            "status": "added",
            "additions": 46,
            "deletions": 0,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_chat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_chat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_chat.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,46 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import json\n+import os\n+import tempfile\n+\n+from transformers.cli.chat import new_chat_history, parse_generate_flags, save_chat\n+\n+\n+def test_help(cli):\n+    output = cli(\"chat\", \"--help\")\n+    assert output.exit_code == 0\n+    assert \"Chat with a model from the command line.\" in output.output\n+\n+\n+def test_save_and_clear_chat():\n+    with tempfile.TemporaryDirectory() as tmp_path:\n+        filename = os.path.join(tmp_path, \"chat.json\")\n+        save_chat(filename, [{\"role\": \"user\", \"content\": \"hi\"}], {\"foo\": \"bar\"})\n+        assert os.path.isfile(filename)\n+        with open(filename, \"r\") as f:\n+            data = json.load(f)\n+            assert data[\"chat_history\"] == [{\"role\": \"user\", \"content\": \"hi\"}]\n+            assert data[\"settings\"] == {\"foo\": \"bar\"}\n+\n+\n+def test_new_chat_history():\n+    assert new_chat_history() == []\n+    assert new_chat_history(\"prompt\") == [{\"role\": \"system\", \"content\": \"prompt\"}]\n+\n+\n+def test_parse_generate_flags():\n+    parsed = parse_generate_flags([\"temperature=0.5\", \"max_new_tokens=10\"])\n+    assert parsed[\"temperature\"] == 0.5\n+    assert parsed[\"max_new_tokens\"] == 10"
        },
        {
            "sha": "cd26bdc2fb378678ed035a405103585d54066920",
            "filename": "tests/cli/test_download.py",
            "status": "added",
            "additions": 54,
            "deletions": 0,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_download.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_download.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_download.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,54 @@\n+# Copyright 2025-present, the HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import os\n+import tempfile\n+\n+from transformers.testing_utils import require_torch\n+\n+\n+@require_torch\n+def test_cli_download(cli):\n+    with tempfile.TemporaryDirectory() as tmpdir:\n+        output = cli(\"download\", \"hf-internal-testing/tiny-random-gptj\", \"--cache-dir\", tmpdir)\n+        assert output.exit_code == 0\n+\n+        # check if the model files are downloaded correctly\n+        model_dir = os.path.join(tmpdir, \"models--hf-internal-testing--tiny-random-gptj\")\n+        assert os.path.exists(os.path.join(model_dir, \"blobs\"))\n+        assert os.path.exists(os.path.join(model_dir, \"refs\"))\n+        assert os.path.exists(os.path.join(model_dir, \"snapshots\"))\n+\n+\n+@require_torch\n+def test_cli_download_trust_remote(cli, caplog, capsys):\n+    caplog.set_level(100000)\n+    # ^ hack to avoid an issue happening only in CI. We don't check logs anyway so it's fine.\n+    #   Source: https://github.com/pallets/click/issues/824#issuecomment-562581313\n+\n+    with capsys.disabled():\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            output = cli(\n+                \"download\",\n+                \"hf-internal-testing/test_dynamic_model_with_tokenizer\",\n+                \"--trust-remote-code\",\n+                \"--cache-dir\",\n+                tmpdir,\n+            )\n+            assert output.exit_code == 0\n+\n+            # check if the model files are downloaded correctly\n+            model_dir = os.path.join(tmpdir, \"models--hf-internal-testing--test_dynamic_model_with_tokenizer\")\n+            assert os.path.exists(os.path.join(model_dir, \"blobs\"))\n+            assert os.path.exists(os.path.join(model_dir, \"refs\"))\n+            assert os.path.exists(os.path.join(model_dir, \"snapshots\"))"
        },
        {
            "sha": "b000dcaea7a4f7f81a55b67a3590e66eb5a1ab4c",
            "filename": "tests/cli/test_serve.py",
            "status": "renamed",
            "additions": 190,
            "deletions": 194,
            "changes": 384,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_serve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_serve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_serve.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -11,22 +11,19 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-import asyncio\n import os\n import time\n import unittest\n from threading import Thread\n-from unittest.mock import patch\n+from unittest.mock import Mock, patch\n \n-import aiohttp.client_exceptions\n import httpx\n-from huggingface_hub import AsyncInferenceClient, ChatCompletionStreamOutput\n+from huggingface_hub import ChatCompletionStreamOutput, InferenceClient\n from parameterized import parameterized\n \n-import transformers.commands.transformers_cli as cli\n from transformers import GenerationConfig\n-from transformers.commands.serving import Modality, ServeArguments, ServeCommand\n-from transformers.testing_utils import CaptureStd, require_openai, slow\n+from transformers.cli.serve import Modality, Serve\n+from transformers.testing_utils import require_openai, slow\n from transformers.utils.import_utils import is_openai_available\n \n \n@@ -48,132 +45,170 @@\n \n \n @require_openai\n-class ServeCLITest(unittest.TestCase):\n-    def test_help(self):\n-        \"\"\"Minimal test: we can invoke the help command.\"\"\"\n-        with patch(\"sys.argv\", [\"transformers\", \"serve\", \"--help\"]), CaptureStd() as cs:\n-            with self.assertRaises(SystemExit):\n-                cli.main()\n-        self.assertIn(\"serve\", cs.out.lower())\n-\n-    def test_parsed_args(self):\n-        \"\"\"Minimal test: we can set arguments through the CLI.\"\"\"\n-        with (\n-            patch.object(ServeCommand, \"__init__\", return_value=None) as init_mock,\n-            patch.object(ServeCommand, \"run\") as run_mock,\n-            patch(\"sys.argv\", [\"transformers\", \"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"9000\"]),\n-        ):\n-            cli.main()\n-        init_mock.assert_called_once()\n-        run_mock.assert_called_once()\n-        parsed_args = init_mock.call_args[0][0]\n-        self.assertEqual(parsed_args.host, \"0.0.0.0\")\n-        self.assertEqual(parsed_args.port, 9000)\n-\n-    def test_build_chat_completion_chunk(self):\n-        \"\"\"\n-        Tests that the chunks are correctly built for the Chat Completion API. The `choices` checks implicitly\n-        confirm that empty fields are not emitted.\n-        \"\"\"\n-        dummy = ServeCommand.__new__(ServeCommand)\n-        dummy.args = type(\"Args\", (), {})()\n-\n-        # The keys for these fields must be present in every chunk\n-        MANDATORY_FIELDS = [\"data\", \"id\", \"choices\", \"created\", \"model\", \"object\", \"system_fingerprint\"]\n-\n-        # Case 1: most fields are provided\n-        chunk = ServeCommand.build_chat_completion_chunk(\n-            dummy, request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n-        )\n-        chunk = ServeCommand.chunk_to_sse_element(chunk)\n-        for field in MANDATORY_FIELDS:\n-            self.assertIn(field, chunk)\n-        self.assertIn(\n-            '\"choices\":[{\"delta\":{\"content\":\"hello\",\"role\":\"user\"},\"finish_reason\":\"stop\",\"index\":0}]', chunk\n-        )\n+def test_help(cli):\n+    \"\"\"Minimal test: we can invoke the help command.\"\"\"\n+    output = cli(\"serve\", \"--help\")\n+    assert output.exit_code == 0\n+    assert \"serve\" in output.output\n \n-        # Case 2: only the role is provided -- other fields in 'choices' are omitted\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n-        chunk = ServeCommand.chunk_to_sse_element(chunk)\n-        for field in MANDATORY_FIELDS:\n-            self.assertIn(field, chunk)\n-        self.assertIn('\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]', chunk)\n-\n-        # Case 3: only the content is provided -- other fields in 'choices' are omitted\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n-        chunk = ServeCommand.chunk_to_sse_element(chunk)\n-        for field in MANDATORY_FIELDS:\n-            self.assertIn(field, chunk)\n-        self.assertIn('\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]', chunk)\n-\n-        # Case 4: tool calls support a list of ChoiceDeltaToolCall objects\n-        tool_call = ChoiceDeltaToolCall(\n-            index=0,\n-            function=ChoiceDeltaToolCallFunction(name=\"foo_bar\", arguments='{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}'),\n-            type=\"function\",\n-        )\n-        chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n-        chunk = ServeCommand.chunk_to_sse_element(chunk)\n-        for field in MANDATORY_FIELDS:\n-            self.assertIn(field, chunk)\n-        expected_choices_content = (\n-            'choices\":[{\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\\\"foo1\\\\\": \\\\\"bar1\\\\\", '\n-            '\\\\\"foo2\\\\\": \\\\\"bar2\\\\\"}\",\"name\":\"foo_bar\"},\"type\":\"function\"}]},\"index\":0}]'\n-        )\n-        self.assertIn(expected_choices_content, chunk)\n-\n-    def test_build_response_event(self):\n-        \"\"\"\n-        Tests that the events are correctly built for the Response API.\n-\n-        Contrarily to the Chat Completion API, the Response API has a wide set of possible output objects. This test\n-        only checks a few basic assumptions -- we rely on OpenAI's pydantic models to enforce the correct schema.\n-        \"\"\"\n-        dummy = ServeCommand.__new__(ServeCommand)\n-        dummy.args = type(\"Args\", (), {})()\n-\n-        response_created = ResponseCreatedEvent(\n-            type=\"response.created\",\n-            sequence_number=0,\n-            response=Response(\n-                id=\"resp_0\",\n-                created_at=time.time(),\n-                status=\"queued\",\n-                model=\"dummy_model@main\",\n-                instructions=None,  # <--- is set to None = should NOT be in the output.\n-                text={\"format\": {\"type\": \"text\"}},\n-                object=\"response\",\n-                tools=[],  # <--- empty lists should be in the output (they are often mandatory fields)\n-                output=[],\n-                parallel_tool_calls=False,\n-                tool_choice=\"auto\",\n-                metadata=None,\n-            ),\n-        )\n \n-        event = dummy.chunk_to_sse_element(response_created)\n-        self.assertTrue(event.startswith(\"data: \"))  # Sanity check: event formatting\n-        self.assertIn('\"model\":\"dummy_model@main\"', event)  # Sanity check: set field\n-        self.assertIn('\"status\":\"queued\"', event)\n-        self.assertIn(\"tools\", event)  # empty lists should be in the output\n-        self.assertIn(\"output\", event)\n-        self.assertNotIn(\"instructions\", event)  # None fields should NOT be in the output\n-        self.assertNotIn(\"metadata\", event)\n-        self.assertNotIn(\"error\", event)  # Unset optional fields should NOT be in the output\n-        self.assertNotIn(\"top_p\", event)\n+@require_openai\n+def test_host_port_blocking(cli):\n+    \"\"\"Minimal test: we can set arguments through the CLI - blocking\"\"\"\n+    with (\n+        patch(\"uvicorn.Config\") as ConfigMock,\n+        patch(\"uvicorn.Server\") as ServerMock,\n+    ):\n+        server_instance = Mock()\n+        ServerMock.return_value = server_instance\n+\n+        # Call the serve CLI with host/port\n+        out = cli(\"serve\", \"--host\", \"0.0.0.0\", \"--port\", \"9000\")\n+        _, kwargs = ConfigMock.call_args\n+\n+        assert out.exit_code == 0\n+        assert kwargs[\"host\"] == \"0.0.0.0\"\n+        assert kwargs[\"port\"] == 9000\n+\n+        ServerMock.assert_called_once_with(ConfigMock.return_value)\n+        server_instance.run.assert_called_once()\n+\n+\n+@require_openai\n+def test_host_port_non_blocking(cli, caplog):\n+    \"\"\"Minimal test: we can set arguments through the CLI - non-blocking\"\"\"\n+    caplog.set_level(100000)\n+    # ^ hack to avoid an issue happening only in CI. We don't check logs anyway so it's fine.\n+    #   Source: https://github.com/pallets/click/issues/824#issuecomment-562581313\n+\n+    with (\n+        patch(\"uvicorn.Config\") as ConfigMock,\n+        patch(\"uvicorn.Server\") as ServerMock,\n+        patch.object(Serve, \"start_server\") as start_mock,\n+    ):\n+        server_instance = Mock()\n+        ServerMock.return_value = server_instance\n+\n+        out = cli(\"serve\", \"--host\", \"0.5.0.0\", \"--port\", \"9002\", \"--non-blocking\")\n+        assert out.exit_code == 0\n+\n+        # Config got the CLI args\n+        _, kwargs = ConfigMock.call_args\n+        assert kwargs[\"host\"] == \"0.5.0.0\"\n+        assert kwargs[\"port\"] == 9002\n+\n+        # Non-blocking path uses start_server(), not server.run()\n+        start_mock.assert_called_once()\n+        server_instance.run.assert_not_called()\n+\n+\n+@require_openai\n+def test_build_chat_completion_chunk():\n+    \"\"\"\n+    Tests that the chunks are correctly built for the Chat Completion API. The `choices` checks implicitly\n+    confirm that empty fields are not emitted.\n+    \"\"\"\n+    dummy = Serve.__new__(Serve)\n+\n+    # The keys for these fields must be present in every chunk\n+    MANDATORY_FIELDS = [\"data\", \"id\", \"choices\", \"created\", \"model\", \"object\", \"system_fingerprint\"]\n+\n+    # Case 1: most fields are provided\n+    chunk = dummy.build_chat_completion_chunk(\n+        request_id=\"req0\", content=\"hello\", finish_reason=\"stop\", role=\"user\", model=\"dummy_model@main\"\n+    )\n+    chunk = dummy.chunk_to_sse_element(chunk)\n+    for field in MANDATORY_FIELDS:\n+        assert field in chunk\n+    assert '\"choices\":[{\"delta\":{\"content\":\"hello\",\"role\":\"user\"},\"finish_reason\":\"stop\",\"index\":0}]' in chunk\n+\n+    # Case 2: only the role is provided -- other fields in 'choices' are omitted\n+    chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", role=\"user\", model=\"dummy_model@main\")\n+    chunk = dummy.chunk_to_sse_element(chunk)\n+    for field in MANDATORY_FIELDS:\n+        assert field in chunk\n+    assert '\"choices\":[{\"delta\":{\"role\":\"user\"},\"index\":0}]' in chunk\n+\n+    # Case 3: only the content is provided -- other fields in 'choices' are omitted\n+    chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", content=\"hello\", model=\"dummy_model@main\")\n+    chunk = dummy.chunk_to_sse_element(chunk)\n+    for field in MANDATORY_FIELDS:\n+        assert field in chunk\n+    assert '\"choices\":[{\"delta\":{\"content\":\"hello\"},\"index\":0}]' in chunk\n+\n+    # Case 4: tool calls support a list of ChoiceDeltaToolCall objects\n+    tool_call = ChoiceDeltaToolCall(\n+        index=0,\n+        function=ChoiceDeltaToolCallFunction(name=\"foo_bar\", arguments='{\"foo1\": \"bar1\", \"foo2\": \"bar2\"}'),\n+        type=\"function\",\n+    )\n+    chunk = dummy.build_chat_completion_chunk(request_id=\"req0\", tool_calls=[tool_call], model=\"dummy_model@main\")\n+    chunk = dummy.chunk_to_sse_element(chunk)\n+    for field in MANDATORY_FIELDS:\n+        assert field in chunk\n+    expected_choices_content = (\n+        'choices\":[{\"delta\":{\"tool_calls\":[{\"index\":0,\"function\":{\"arguments\":\"{\\\\\"foo1\\\\\": \\\\\"bar1\\\\\", '\n+        '\\\\\"foo2\\\\\": \\\\\"bar2\\\\\"}\",\"name\":\"foo_bar\"},\"type\":\"function\"}]},\"index\":0}]'\n+    )\n+    assert expected_choices_content in chunk\n+\n+\n+@require_openai\n+def test_build_response_event():\n+    \"\"\"\n+    Tests that the events are correctly built for the Response API.\n+\n+    Contrarily to the Chat Completion API, the Response API has a wide set of possible output objects. This test\n+    only checks a few basic assumptions -- we rely on OpenAI's pydantic models to enforce the correct schema.\n+    \"\"\"\n+    dummy = Serve.__new__(Serve)\n+\n+    response_created = ResponseCreatedEvent(\n+        type=\"response.created\",\n+        sequence_number=0,\n+        response=Response(\n+            id=\"resp_0\",\n+            created_at=time.time(),\n+            status=\"queued\",\n+            model=\"dummy_model@main\",\n+            instructions=None,  # <--- is set to None = should NOT be in the output.\n+            text={\"format\": {\"type\": \"text\"}},\n+            object=\"response\",\n+            tools=[],  # <--- empty lists should be in the output (they are often mandatory fields)\n+            output=[],\n+            parallel_tool_calls=False,\n+            tool_choice=\"auto\",\n+            metadata=None,\n+        ),\n+    )\n+\n+    event = dummy.chunk_to_sse_element(response_created)\n+    assert event.startswith(\"data: \")  # Sanity check: event formatting\n+    assert '\"model\":\"dummy_model@main\"' in event  # Sanity check: set field\n+    assert '\"status\":\"queued\"' in event\n+    assert \"tools\" in event  # empty lists should be in the output\n+    assert \"output\" in event\n+    assert \"instructions\" not in event  # None fields should NOT be in the output\n+    assert \"metadata\" not in event\n+    assert \"error\" not in event  # Unset optional fields should NOT be in the output\n+    assert \"top_p\" not in event\n \n \n-def async_retry(fn, max_attempts=5, delay=2):\n+def retry(fn, max_attempts=5, delay=2):\n     \"\"\"\n     Retry a function up to `max_attempts` times with a `delay` between attempts.\n-    Useful for testing async functions that may fail due to server not being ready.\n+    Useful for testing functions that may fail due to server not being ready.\n     \"\"\"\n \n-    async def wrapper(*args, **kwargs):\n-        for _ in range(max_attempts):\n+    def wrapper(*args, **kwargs):\n+        nb_attempts = 0\n+        while True:\n+            nb_attempts += 1\n             try:\n-                return await fn(*args, **kwargs)\n-            except (aiohttp.client_exceptions.ClientConnectorError, APIConnectionError):\n+                return fn(*args, **kwargs)\n+            except (httpx.HTTPError, APIConnectionError):\n+                if nb_attempts >= max_attempts:\n+                    raise\n                 time.sleep(delay)\n \n     return wrapper\n@@ -185,17 +220,10 @@ class ServeCompletionsMixin:\n     (`generate` and `continuous_batching`).\n     \"\"\"\n \n-    @async_retry\n-    async def run_server(self, request):\n-        client = AsyncInferenceClient(f\"http://localhost:{self.port}\")\n-        stream = client.chat_completion(**request)\n-\n-        all_payloads = []\n-        async for payload in await stream:\n-            all_payloads.append(payload)\n-\n-        await client.close()\n-        return all_payloads\n+    @retry\n+    def run_server(self, request):\n+        with InferenceClient(f\"http://localhost:{self.port}\") as client:\n+            return list(client.chat_completion(**request))\n \n     @parameterized.expand(\n         [\n@@ -229,10 +257,10 @@ def test_requests(self, test_name: str, request_flags: dict):\n             \"max_tokens\": 5,  # Small generation by default\n         }\n         request.update(request_flags)\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         # If a request is successful, the returned payload needs to follow the schema, which we test here.\n-        # NOTE: the output of our server is wrapped by `AsyncInferenceClient`, which sends fields even when they\n+        # NOTE: the output of our server is wrapped by `InferenceClient`, which sends fields even when they\n         # are empty.\n \n         # Finish reason: the last payload should have a finish reason of \"stop\", all others should be empty\n@@ -265,7 +293,7 @@ def test_generation_config_in_request(self):\n                 \"generation_config\": generation_config.to_json_string(),\n             },\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n         contents = [payload.choices[0].delta.content for payload in all_payloads]\n         output_text = \"\".join([text for text in contents if text is not None])\n         # The generation config sets greedy decoding, so the output is reproducible. By default, `Qwen/Qwen3-0.6B`\n@@ -284,7 +312,7 @@ def test_processor_inputs_from_inbound_messages_llm(self):\n             {\"role\": \"assistant\", \"content\": \"I'm doing great, thank you for asking! How can I assist you today?\"},\n             {\"role\": \"user\", \"content\": \"Can you help me write tests?\"},\n         ]\n-        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+        outputs = Serve.get_processor_inputs_from_inbound_messages(messages, modality)\n         self.assertListEqual(expected_outputs, outputs)\n \n         messages_with_type = [\n@@ -297,7 +325,7 @@ def test_processor_inputs_from_inbound_messages_llm(self):\n             },\n             {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Can you help me write tests?\"}]},\n         ]\n-        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages_with_type, modality)\n+        outputs = Serve.get_processor_inputs_from_inbound_messages(messages_with_type, modality)\n         self.assertListEqual(expected_outputs, outputs)\n \n         messages_multiple_text = [\n@@ -315,7 +343,7 @@ def test_processor_inputs_from_inbound_messages_llm(self):\n                 \"content\": \"How are you doing? I'm doing great, thank you for asking! How can I assist you today?\",\n             },\n         ]\n-        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages_multiple_text, modality)\n+        outputs = Serve.get_processor_inputs_from_inbound_messages(messages_multiple_text, modality)\n         self.assertListEqual(expected_outputs_multiple_text, outputs)\n \n     def test_processor_inputs_from_inbound_messages_vlm_text_only(self):\n@@ -337,7 +365,7 @@ def test_processor_inputs_from_inbound_messages_vlm_text_only(self):\n             {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Can you help me write tests?\"}]},\n         ]\n \n-        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+        outputs = Serve.get_processor_inputs_from_inbound_messages(messages, modality)\n         self.assertListEqual(expected_outputs, outputs)\n \n     def test_processor_inputs_from_inbound_messages_vlm_text_and_image_in_base_64(self):\n@@ -382,7 +410,7 @@ def test_processor_inputs_from_inbound_messages_vlm_text_and_image_in_base_64(se\n             {\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"Alright\"}]},\n         ]\n \n-        outputs = ServeCommand.get_processor_inputs_from_inbound_messages(messages, modality)\n+        outputs = Serve.get_processor_inputs_from_inbound_messages(messages, modality)\n \n         for expected_output, output in zip(expected_outputs, outputs):\n             expected_output_content = expected_output[\"content\"]\n@@ -414,19 +442,11 @@ class ServeCompletionsGenerateIntegrationTest(ServeCompletionsMixin, unittest.Te\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8001\n-        args = ServeArguments(port=cls.port)\n-        cls.serve_command = ServeCommand(args)\n-        cls.thread = Thread(target=cls.serve_command.run)\n-        cls.thread.daemon = True\n-        cls.thread.start()\n+        cls.server = Serve(port=cls.port, non_blocking=True)\n \n     @classmethod\n     def tearDownClass(cls):\n-        cls.thread.join(timeout=1)\n-\n-    def setUp(self):\n-        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n-        _call_healthcheck(f\"http://localhost:{self.port}\")\n+        cls.server.kill_server()\n \n     @slow\n     def test_tool_call(self):\n@@ -477,7 +497,7 @@ def test_tool_call(self):\n                 }\n             ],\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         # The first payload should contain the role\n         roles = [payload.choices[0].delta.role for payload in all_payloads]\n@@ -560,19 +580,13 @@ class ServeCompletionsContinuousBatchingIntegrationTest(ServeCompletionsMixin, u\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8002\n-        args = ServeArguments(port=cls.port, continuous_batching=True, default_seed=42)\n-        cls.serve_command = ServeCommand(args)\n-        cls.thread = Thread(target=cls.serve_command.run)\n-        cls.thread.daemon = True\n-        cls.thread.start()\n+        cls.server = Serve(\n+            port=cls.port, continuous_batching=True, attn_implementation=\"sdpa\", default_seed=42, non_blocking=True\n+        )\n \n     @classmethod\n     def tearDownClass(cls):\n-        cls.thread.join(timeout=1)\n-\n-    def setUp(self):\n-        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n-        _call_healthcheck(f\"http://localhost:{self.port}\")\n+        cls.server.kill_server()\n \n     def test_full_request(self):\n         \"\"\"Tests that an inference using the Responses API and Continuous Batching works\"\"\"\n@@ -586,7 +600,7 @@ def test_full_request(self):\n             \"stream\": True,\n             \"max_tokens\": 30,\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         full_text = \"\"\n         for token in all_payloads:\n@@ -610,7 +624,7 @@ def test_max_tokens_not_set_in_req(self):\n             ],\n             \"stream\": True,\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         full_text = \"\"\n         for token in all_payloads:\n@@ -638,7 +652,7 @@ def test_request_cancellation(self):\n \n         _open_stream_and_cancel(base_url, request_id)\n \n-        scheduler = _get_scheduler(self.serve_command)\n+        scheduler = _get_scheduler(self.server)\n \n         # Because cancellation is non-blocking, poll for a short, bounded time.\n         deadline = time.time() + 8.0  # generous but still CI-friendly\n@@ -665,8 +679,8 @@ class ServeResponsesMixin:\n     (`generate` and `continuous_batching`).\n     \"\"\"\n \n-    @async_retry\n-    async def run_server(self, request):\n+    @retry\n+    def run_server(self, request):\n         client = OpenAI(base_url=f\"http://localhost:{self.port}/v1\", api_key=\"<KEY>\")\n         stream = client.responses.create(**request)\n \n@@ -686,7 +700,7 @@ def test_request(self):\n             \"stream\": True,\n             \"max_output_tokens\": 1,\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         # Allow variable number of delta events depending on tokenizer/streamer behavior\n         self.assertGreaterEqual(len(all_payloads), 8)\n@@ -719,19 +733,11 @@ class ServeResponsesIntegrationTest(ServeResponsesMixin, unittest.TestCase):\n     def setUpClass(cls):\n         \"\"\"Starts a server for tests to connect to.\"\"\"\n         cls.port = 8003\n-        args = ServeArguments(port=cls.port, default_seed=42)\n-        serve_command = ServeCommand(args)\n-        cls.thread = Thread(target=serve_command.run)\n-        cls.thread.daemon = True\n-        cls.thread.start()\n+        cls.server = Serve(port=cls.port, default_seed=42, non_blocking=True)\n \n     @classmethod\n     def tearDownClass(cls):\n-        cls.thread.join(timeout=1)\n-\n-    def setUp(self):\n-        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n-        _call_healthcheck(f\"http://localhost:{self.port}\")\n+        cls.server.kill_server()\n \n     @slow\n     def test_full_request(self):\n@@ -746,7 +752,7 @@ def test_full_request(self):\n             # Disable sampling for deterministic output\n             \"temperature\": 0,\n         }\n-        all_payloads = asyncio.run(self.run_server(request))\n+        all_payloads = self.run_server(request)\n \n         full_text = \"\"\n         for token in all_payloads:\n@@ -791,19 +797,9 @@ class ServeInfrastructureTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):\n         cls.port = 8042\n-        args = ServeArguments(port=cls.port)\n-        serve_command = ServeCommand(args)\n-        cls.thread = Thread(target=serve_command.run)\n-        cls.thread.daemon = True\n-        cls.thread.start()\n-\n-    @classmethod\n-    def tearDownClass(cls):\n-        cls.thread.join(timeout=1)\n-\n-    def setUp(self):\n-        \"\"\"Ensures that the healthcheck works before each test.\"\"\"\n-        _call_healthcheck(f\"http://localhost:{self.port}\")\n+        thread = Thread(target=Serve, kwargs={\"port\": cls.port})\n+        thread.daemon = True\n+        thread.start()\n \n     def test_healthcheck(self):\n         \"\"\"Tests that the healthcheck endpoint works.\"\"\"",
            "previous_filename": "tests/commands/test_serving.py"
        },
        {
            "sha": "41135d9d7e25c8086b63a67d862d2830bd3a1041",
            "filename": "tests/cli/test_system.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_system.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Fcli%2Ftest_system.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcli%2Ftest_system.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -0,0 +1,29 @@\n+# Copyright 2025-present, the HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from transformers import __version__\n+\n+\n+def test_cli_env(cli):\n+    output = cli(\"env\")\n+    assert output.exit_code == 0\n+    assert \"Python version\" in output.output\n+    assert \"Platform\" in output.output\n+    assert \"Using distributed or parallel set-up in script?\" in output.output\n+\n+\n+def test_cli_version(cli):\n+    output = cli(\"version\")\n+    assert output.exit_code == 0\n+    assert output.output.strip() == __version__"
        },
        {
            "sha": "e07df4a393893c9436b2818307e4a4580aa75e23",
            "filename": "tests/commands/test_chat.py",
            "status": "removed",
            "additions": 0,
            "deletions": 100,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/tests%2Fcommands%2Ftest_chat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/tests%2Fcommands%2Ftest_chat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fcommands%2Ftest_chat.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,100 +0,0 @@\n-# Copyright 2025 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import os\n-import tempfile\n-import unittest\n-from unittest.mock import patch\n-\n-import transformers.commands.transformers_cli as cli\n-from transformers.commands.chat import ChatArguments, ChatCommand\n-from transformers.testing_utils import CaptureStd\n-\n-\n-class ChatCLITest(unittest.TestCase):\n-    def test_help(self):\n-        with patch(\"sys.argv\", [\"transformers\", \"chat\", \"--help\"]), CaptureStd() as cs:\n-            with self.assertRaises(SystemExit):\n-                cli.main()\n-        self.assertIn(\"chat interface\", cs.out.lower())\n-\n-    @patch.object(ChatCommand, \"run\")\n-    def test_cli_dispatch_model(self, run_mock):\n-        \"\"\"\n-        Running transformers chat with just a model should work & spawn a serve underneath\n-        \"\"\"\n-        args = [\"transformers\", \"chat\", \"hf-internal-testing/tiny-random-gpt2\"]\n-        with patch(\"sys.argv\", args):\n-            cli.main()\n-        run_mock.assert_called_once()\n-\n-    def test_cli_dispatch_url(self):\n-        \"\"\"\n-        Running transformers chat with just a URL should not work as a model should additionally be specified\n-        \"\"\"\n-        args = [\"transformers\", \"chat\", \"localhost:8000\"]\n-        with self.assertRaises(ValueError):\n-            with patch(\"sys.argv\", args):\n-                cli.main()\n-\n-    @patch.object(ChatCommand, \"run\")\n-    def test_cli_dispatch_url_and_model(self, run_mock):\n-        \"\"\"\n-        Running transformers chat with a URL and a model should work\n-        \"\"\"\n-        args = [\"transformers\", \"chat\", \"localhost:8000\", \"--model_name_or_path=hf-internal-testing/tiny-random-gpt2\"]\n-        with patch(\"sys.argv\", args):\n-            cli.main()\n-        run_mock.assert_called_once()\n-\n-    def test_parsed_args(self):\n-        with (\n-            patch.object(ChatCommand, \"__init__\", return_value=None) as init_mock,\n-            patch.object(ChatCommand, \"run\") as run_mock,\n-            patch(\n-                \"sys.argv\",\n-                [\n-                    \"transformers\",\n-                    \"chat\",\n-                    \"test-model\",\n-                    \"max_new_tokens=64\",\n-                ],\n-            ),\n-        ):\n-            cli.main()\n-        init_mock.assert_called_once()\n-        run_mock.assert_called_once()\n-        parsed_args = init_mock.call_args[0][0]\n-        self.assertEqual(parsed_args.model_name_or_path_or_address, \"test-model\")\n-        self.assertEqual(parsed_args.generate_flags, [\"max_new_tokens=64\"])\n-\n-\n-class ChatUtilitiesTest(unittest.TestCase):\n-    def test_save_and_clear_chat(self):\n-        tmp_path = tempfile.mkdtemp()\n-\n-        args = ChatArguments(save_folder=str(tmp_path))\n-        args.model_name_or_path_or_address = \"test-model\"\n-\n-        chat_history = [{\"role\": \"user\", \"content\": \"hi\"}]\n-        filename = ChatCommand.save_chat(chat_history, args)\n-        self.assertTrue(os.path.isfile(filename))\n-\n-        cleared = ChatCommand.clear_chat_history()\n-        self.assertEqual(cleared, [])\n-\n-    def test_parse_generate_flags(self):\n-        dummy = ChatCommand.__new__(ChatCommand)\n-        parsed = ChatCommand.parse_generate_flags(dummy, [\"temperature=0.5\", \"max_new_tokens=10\"])\n-        self.assertEqual(parsed[\"temperature\"], 0.5)\n-        self.assertEqual(parsed[\"max_new_tokens\"], 10)"
        },
        {
            "sha": "c1eeed5b824c2c311298cf52fed127daaec78777",
            "filename": "tests/utils/test_add_new_model_like.py",
            "status": "modified",
            "additions": 8,
            "deletions": 14,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Futils%2Ftest_add_new_model_like.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/tests%2Futils%2Ftest_add_new_model_like.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_add_new_model_like.py?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -19,8 +19,7 @@\n from datetime import date\n from pathlib import Path\n \n-import transformers.commands.add_new_model_like\n-from transformers.commands.add_new_model_like import ModelInfos, create_new_model_like\n+from transformers.cli.add_new_model_like import ModelInfos, _add_new_model_like_internal\n from transformers.testing_utils import require_torch\n \n \n@@ -36,7 +35,8 @@ def setUpClass(cls):\n         \"\"\"\n         Create a temporary repo with the same structure as Transformers, with just 2 models.\n         \"\"\"\n-        cls.FAKE_REPO = tempfile.TemporaryDirectory().name\n+        cls.tmp_dir = tempfile.TemporaryDirectory()\n+        cls.FAKE_REPO = cls.tmp_dir.name\n         os.makedirs(os.path.join(cls.FAKE_REPO, \"src\", \"transformers\", \"models\"), exist_ok=True)\n         os.makedirs(os.path.join(cls.FAKE_REPO, \"tests\", \"models\"), exist_ok=True)\n         os.makedirs(os.path.join(cls.FAKE_REPO, \"docs\", \"source\", \"en\", \"model_doc\"), exist_ok=True)\n@@ -64,22 +64,14 @@ def setUpClass(cls):\n                 doc_src = os.path.join(REPO_PATH, \"docs\", \"source\", \"en\", \"model_doc\", f\"{model}.md\")\n                 shutil.copy(doc_src, doc_src.replace(REPO_PATH, cls.FAKE_REPO))\n \n-        # Replace the globals\n-        cls.ORIGINAL_REPO = transformers.commands.add_new_model_like.REPO_PATH\n-        cls.ORIGINAL_TRANSFORMERS_REPO = transformers.commands.add_new_model_like.TRANSFORMERS_PATH\n-        transformers.commands.add_new_model_like.REPO_PATH = Path(cls.FAKE_REPO)\n-        transformers.commands.add_new_model_like.TRANSFORMERS_PATH = Path(cls.FAKE_REPO) / \"src\" / \"transformers\"\n-\n         # For convenience\n         cls.MODEL_PATH = os.path.join(cls.FAKE_REPO, \"src\", \"transformers\", \"models\")\n         cls.TESTS_MODEL_PATH = os.path.join(cls.FAKE_REPO, \"tests\", \"models\")\n         cls.DOC_PATH = os.path.join(cls.FAKE_REPO, \"docs\", \"source\", \"en\")\n \n     @classmethod\n     def tearDownClass(cls):\n-        transformers.commands.add_new_model_like.REPO_PATH = cls.ORIGINAL_REPO\n-        transformers.commands.add_new_model_like.TRANSFORMERS_PATH = cls.ORIGINAL_TRANSFORMERS_REPO\n-        del cls.FAKE_REPO\n+        cls.tmp_dir.cleanup()\n \n     def assertFileIsEqual(self, text: str, filepath: str):\n         with open(filepath, \"r\") as f:\n@@ -105,7 +97,8 @@ def test_llama_without_tokenizers(self):\n             (\"processing_llama.py\", False),\n         )\n         # Run the command\n-        create_new_model_like(\n+        _add_new_model_like_internal(\n+            repo_path=Path(self.FAKE_REPO),\n             old_model_infos=ModelInfos(\"llama\"),\n             new_lowercase_name=\"my_test\",\n             new_model_paper_name=\"MyTest\",\n@@ -385,7 +378,8 @@ def test_phi4_with_all_processors(self):\n             (\"processing_phi4_multimodal.py\", True),\n         )\n         # Run the command\n-        create_new_model_like(\n+        _add_new_model_like_internal(\n+            repo_path=Path(self.FAKE_REPO),\n             old_model_infos=ModelInfos(\"phi4_multimodal\"),\n             new_lowercase_name=\"my_test2\",\n             new_model_paper_name=\"MyTest2\","
        },
        {
            "sha": "e9467e518d17dd9045b7f42d17ac287b30c602ad",
            "filename": "tests/utils/test_cli.py",
            "status": "removed",
            "additions": 0,
            "deletions": 77,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/tests%2Futils%2Ftest_cli.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a59124e27ea9b5a3110495b2c6737c087c8e3bb0/tests%2Futils%2Ftest_cli.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cli.py?ref=a59124e27ea9b5a3110495b2c6737c087c8e3bb0",
            "patch": "@@ -1,77 +0,0 @@\n-# Copyright 2019-present, the HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import os\n-import shutil\n-import unittest\n-from unittest.mock import patch\n-\n-from transformers.testing_utils import CaptureStd, require_torch\n-\n-\n-class CLITest(unittest.TestCase):\n-    @patch(\"sys.argv\", [\"fakeprogrampath\", \"env\"])\n-    def test_cli_env(self):\n-        # test transformers env\n-        import transformers.commands.transformers_cli\n-\n-        with CaptureStd() as cs:\n-            transformers.commands.transformers_cli.main()\n-        self.assertIn(\"Python version\", cs.out)\n-        self.assertIn(\"Platform\", cs.out)\n-        self.assertIn(\"Using distributed or parallel set-up in script?\", cs.out)\n-\n-    @require_torch\n-    @patch(\"sys.argv\", [\"fakeprogrampath\", \"download\", \"hf-internal-testing/tiny-random-gptj\", \"--cache-dir\", \"/tmp\"])\n-    def test_cli_download(self):\n-        import transformers.commands.transformers_cli\n-\n-        # # remove any previously downloaded model to start clean\n-        shutil.rmtree(\"/tmp/models--hf-internal-testing--tiny-random-gptj\", ignore_errors=True)\n-\n-        # run the command\n-        transformers.commands.transformers_cli.main()\n-\n-        # check if the model files are downloaded correctly on /tmp/models--hf-internal-testing--tiny-random-gptj\n-        self.assertTrue(os.path.exists(\"/tmp/models--hf-internal-testing--tiny-random-gptj/blobs\"))\n-        self.assertTrue(os.path.exists(\"/tmp/models--hf-internal-testing--tiny-random-gptj/refs\"))\n-        self.assertTrue(os.path.exists(\"/tmp/models--hf-internal-testing--tiny-random-gptj/snapshots\"))\n-\n-    @require_torch\n-    @patch(\n-        \"sys.argv\",\n-        [\n-            \"fakeprogrampath\",\n-            \"download\",\n-            \"hf-internal-testing/test_dynamic_model_with_tokenizer\",\n-            \"--trust-remote-code\",\n-            \"--cache-dir\",\n-            \"/tmp\",\n-        ],\n-    )\n-    def test_cli_download_trust_remote(self):\n-        import transformers.commands.transformers_cli\n-\n-        # # remove any previously downloaded model to start clean\n-        shutil.rmtree(\"/tmp/models--hf-internal-testing--test_dynamic_model_with_tokenizer\", ignore_errors=True)\n-\n-        # run the command\n-        transformers.commands.transformers_cli.main()\n-\n-        # check if the model files are downloaded correctly on /tmp/models--hf-internal-testing--test_dynamic_model_with_tokenizer\n-        self.assertTrue(os.path.exists(\"/tmp/models--hf-internal-testing--test_dynamic_model_with_tokenizer/blobs\"))\n-        self.assertTrue(os.path.exists(\"/tmp/models--hf-internal-testing--test_dynamic_model_with_tokenizer/refs\"))\n-        self.assertTrue(\n-            os.path.exists(\"/tmp/models--hf-internal-testing--test_dynamic_model_with_tokenizer/snapshots\")\n-        )"
        },
        {
            "sha": "922134308c25f420ac5c8969075288ff86a58da1",
            "filename": "utils/not_doctested.txt",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/af2a66ced94b97e6e435d42be2ac67bc8c980446/utils%2Fnot_doctested.txt",
            "raw_url": "https://github.com/huggingface/transformers/raw/af2a66ced94b97e6e435d42be2ac67bc8c980446/utils%2Fnot_doctested.txt",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fnot_doctested.txt?ref=af2a66ced94b97e6e435d42be2ac67bc8c980446",
            "patch": "@@ -310,12 +310,14 @@ docs/source/en/training.md\n docs/source/en/troubleshooting.md\n src/transformers/activations.py\n src/transformers/audio_utils.py\n-src/transformers/commands/add_new_model_like.py\n-src/transformers/commands/download.py\n-src/transformers/commands/env.py\n-src/transformers/commands/run.py\n-src/transformers/commands/serving.py\n-src/transformers/commands/transformers_cli.py\n+src/transformers/cli/add_fast_image_processor.py\n+src/transformers/cli/add_new_model_like.py\n+src/transformers/cli/chat.py\n+src/transformers/cli/download.py\n+src/transformers/cli/run.py\n+src/transformers/cli/serve.py\n+src/transformers/cli/system.py\n+src/transformers/cli/transformers.py\n src/transformers/configuration_utils.py\n src/transformers/convert_slow_tokenizer.py\n src/transformers/convert_slow_tokenizers_checkpoints_to_fast.py"
        }
    ],
    "stats": {
        "total": 3598,
        "additions": 1622,
        "deletions": 1976
    }
}