{
    "author": "winglian",
    "message": "schedulefree optimizers (#30079)\n\n* schedulefree optimizers\r\n\r\n* fix train instead of eval for optimizer\r\n\r\n* fixes and update docs\r\n\r\n* chore: lint\r\n\r\n* add tests and drop overly-verbose _32bit suffix\r\n\r\n* chore: lint\r\n\r\n* fix for docs\r\n\r\n* fix code review issues\r\n\r\n* use duck-typing to avoid per-optimizer patches\r\n\r\n* fixup style\r\n\r\n* fixup style\r\n\r\n* warn if incorrect accelerate version with schedule free\r\n\r\nCo-authored-by: Aman Gupta Karmani <aman@tmm1.net>\r\n\r\n---------\r\n\r\nCo-authored-by: Aman Karmani <aman@tmm1.net>",
    "sha": "62aecd85ffa226d9552529acfd388d87a6b4ecb1",
    "files": [
        {
            "sha": "f51833bfb6dfc63ffc1fc683e997a6fde2cbe00d",
            "filename": "docs/source/en/trainer.md",
            "status": "modified",
            "additions": 45,
            "deletions": 0,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/docs%2Fsource%2Fen%2Ftrainer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/docs%2Fsource%2Fen%2Ftrainer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftrainer.md?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -518,6 +518,51 @@ trainer.train()\n \n This script demonstrates how to fine-tune the `google/gemma-2b` model on the IMDB dataset using the GrokAdamW optimizer. The `TrainingArguments` are configured to use GrokAdamW, and the dataset is passed to the `Trainer` for training.\n \n+## Schedule Free Optimizer\n+\n+The Schedule Free optimizers have been introduced in [The Road Less Scheduled](https://hf.co/papers/2405.15682).\n+Schedule-Free learning replaces the momentum of the base optimizer with a combination of averaging and interpolation, to completely remove the need to anneal the learning rate with a traditional schedule.\n+Supported optimizers for SFO are `\"schedule_free_adamw\"` and `\"schedule_free_sgd\"`. First install schedulefree from pypi `pip install schedulefree`.\n+\n+Below is a simple script to demonstrate how to fine-tune [google/gemma-2b](https://huggingface.co/google/gemma-2b) on IMDB dataset in full precision:\n+\n+```python\n+import torch\n+import datasets\n+from transformers import TrainingArguments, AutoTokenizer, AutoModelForCausalLM\n+import trl\n+\n+train_dataset = datasets.load_dataset('imdb', split='train')\n+\n+args = TrainingArguments(\n+    output_dir=\"./test-schedulefree\",\n+    max_steps=1000,\n+    per_device_train_batch_size=4,\n+    optim=\"schedule_free_adamw\",\n+    gradient_checkpointing=True,\n+    logging_strategy=\"steps\",\n+    logging_steps=1,\n+    learning_rate=2e-6,\n+    save_strategy=\"no\",\n+    run_name=\"sfo-imdb\",\n+)\n+\n+model_id = \"google/gemma-2b\"\n+\n+tokenizer = AutoTokenizer.from_pretrained(model_id)\n+model = AutoModelForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True).to(0)\n+\n+trainer = trl.SFTTrainer(\n+    model=model, \n+    args=args,\n+    train_dataset=train_dataset,\n+    dataset_text_field='text',\n+    max_seq_length=1024,\n+)\n+\n+trainer.train()\n+```\n+\n ## Accelerate and Trainer\n \n The [`Trainer`] class is powered by [Accelerate](https://hf.co/docs/accelerate), a library for easily training PyTorch models in distributed environments with support for integrations such as [FullyShardedDataParallel (FSDP)](https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/) and [DeepSpeed](https://www.deepspeed.ai/)."
        },
        {
            "sha": "14a80d3321be8e23cebf5a4757ce76d04a8585c9",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -163,6 +163,7 @@\n     \"sacremoses\",\n     \"safetensors>=0.4.1\",\n     \"sagemaker>=2.31.0\",\n+    \"schedulefree>=1.2.6\",\n     \"scikit-learn\",\n     \"scipy<1.13.0\",  # SciPy >= 1.13.0 is not supported with the current jax pin (`jax>=0.4.1,<=0.4.13`)\n     \"sentencepiece>=0.1.91,!=0.1.92\","
        },
        {
            "sha": "c199884a19603b2bcd5315eb7f6c90718e70078f",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -69,6 +69,7 @@\n     \"sacremoses\": \"sacremoses\",\n     \"safetensors\": \"safetensors>=0.4.1\",\n     \"sagemaker\": \"sagemaker>=2.31.0\",\n+    \"schedulefree\": \"schedulefree>=1.2.6\",\n     \"scikit-learn\": \"scikit-learn\",\n     \"scipy\": \"scipy<1.13.0\",\n     \"sentencepiece\": \"sentencepiece>=0.1.91,!=0.1.92\","
        },
        {
            "sha": "3306f76249fe9f0105e804b61205f863ad390f8d",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -103,6 +103,7 @@\n     is_rjieba_available,\n     is_sacremoses_available,\n     is_safetensors_available,\n+    is_schedulefree_available,\n     is_scipy_available,\n     is_sentencepiece_available,\n     is_seqio_available,\n@@ -370,6 +371,14 @@ def require_grokadamw(test_case):\n     return unittest.skipUnless(is_grokadamw_available(), \"test requires GrokAdamW\")(test_case)\n \n \n+def require_schedulefree(test_case):\n+    \"\"\"\n+    Decorator marking a test that requires schedulefree. These tests are skipped when schedulefree isn't installed.\n+    https://github.com/facebookresearch/schedule_free\n+    \"\"\"\n+    return unittest.skipUnless(is_schedulefree_available(), \"test requires schedulefree\")(test_case)\n+\n+\n def require_cv2(test_case):\n     \"\"\"\n     Decorator marking a test that requires OpenCV."
        },
        {
            "sha": "525708645c2c49289ae0d0af2b304a4afbca280e",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 38,
            "deletions": 0,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -161,6 +161,7 @@\n     is_safetensors_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n+    is_schedulefree_available,\n     is_torch_compile_available,\n     is_torch_mlu_available,\n     is_torch_mps_available,\n@@ -1488,6 +1489,36 @@ def optimizer_hook(param):\n \n             optimizer_cls = AdamW4bit\n             optimizer_kwargs.update(adam_kwargs)\n+        elif args.optim in [\n+            OptimizerNames.SCHEDULE_FREE_ADAMW,\n+            OptimizerNames.SCHEDULE_FREE_SGD,\n+        ]:\n+            if not is_schedulefree_available():\n+                raise ImportError(\n+                    \"You need to install `schedulefree` in order to use schedulefree optimizers\"\n+                    \" install it with `pip install schedulefree`\"\n+                )\n+            if not is_accelerate_available(\"0.30.0\"):\n+                raise ImportError(\"You need to have `accelerate>=0.30.0` to be able to use schedulefree optimizers\")\n+            from schedulefree import AdamWScheduleFree, SGDScheduleFree\n+\n+            additional_optim_kwargs = {}\n+            if args.optim == OptimizerNames.SCHEDULE_FREE_ADAMW:\n+                optimizer_cls = AdamWScheduleFree\n+                additional_optim_kwargs = adam_kwargs\n+            elif args.optim == OptimizerNames.SCHEDULE_FREE_SGD:\n+                optimizer_cls = SGDScheduleFree\n+            else:\n+                raise ValueError(\"Invalid schedulefree optimizer\")\n+            additional_optim_kwargs[\"weight_decay\"] = args.weight_decay\n+            additional_optim_kwargs[\"warmup_steps\"] = args.warmup_steps\n+            additional_optim_kwargs.update(\n+                {\n+                    \"weight_lr_power\": float(optim_args.get(\"weight_lr_power\", 2.0)),\n+                    \"r\": float(optim_args.get(\"r\", 0.0)),\n+                }\n+            )\n+            optimizer_kwargs.update(additional_optim_kwargs)\n         else:\n             raise ValueError(f\"Trainer cannot instantiate unsupported optimizer: {args.optim}\")\n         return optimizer_cls, optimizer_kwargs\n@@ -3410,6 +3441,9 @@ def training_step(self, model: nn.Module, inputs: Dict[str, Union[torch.Tensor,\n             `torch.Tensor`: The tensor with training loss on this batch.\n         \"\"\"\n         model.train()\n+        if hasattr(self.optimizer, \"train\") and callable(self.optimizer.train):\n+            self.optimizer.train()\n+\n         inputs = self._prepare_inputs(inputs)\n         if is_sagemaker_mp_enabled():\n             loss_mb = smp_forward_backward(model, inputs, self.args.gradient_accumulation_steps)\n@@ -3960,6 +3994,8 @@ def evaluation_loop(\n         logger.info(f\"  Batch size = {batch_size}\")\n \n         model.eval()\n+        if hasattr(self.optimizer, \"eval\") and callable(self.optimizer.eval):\n+            self.optimizer.eval()\n \n         self.callback_handler.eval_dataloader = dataloader\n         # Do this before wrapping.\n@@ -4573,6 +4609,8 @@ def prediction_loop(\n             inputs_gatherer = DistributedTensorGatherer(world_size, num_examples, make_multiple_of=make_multiple_of)\n \n         model.eval()\n+        if hasattr(self.optimizer, \"eval\") and callable(self.optimizer.eval):\n+            self.optimizer.eval()\n \n         if args.past_index >= 0:\n             self._past = None"
        },
        {
            "sha": "02413c28583256db79cd3fd1d029460045978a45",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -178,6 +178,8 @@ class OptimizerNames(ExplicitEnum):\n     LOMO = \"lomo\"\n     ADALOMO = \"adalomo\"\n     GROKADAMW = \"grokadamw\"\n+    SCHEDULE_FREE_ADAMW = \"schedule_free_adamw\"\n+    SCHEDULE_FREE_SGD = \"schedule_free_sgd\"\n \n \n # Sometimes users will pass in a `str` repr of a dict in the CLI"
        },
        {
            "sha": "eee350349f5565a2c09903bd3436fefa23f9ff28",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -175,6 +175,7 @@\n     is_safetensors_available,\n     is_sagemaker_dp_enabled,\n     is_sagemaker_mp_enabled,\n+    is_schedulefree_available,\n     is_scipy_available,\n     is_sentencepiece_available,\n     is_seqio_available,"
        },
        {
            "sha": "c9123299b61d7db7981cefe4a377666ea351e45f",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -103,6 +103,7 @@ def _is_package_available(pkg_name: str, return_version: bool = False) -> Union[\n _galore_torch_available = _is_package_available(\"galore_torch\")\n _lomo_available = _is_package_available(\"lomo_optim\")\n _grokadamw_available = _is_package_available(\"grokadamw\")\n+_schedulefree_available = _is_package_available(\"schedulefree\")\n # `importlib.metadata.version` doesn't work with `bs4` but `beautifulsoup4`. For `importlib.util.find_spec`, reversed.\n _bs4_available = importlib.util.find_spec(\"bs4\") is not None\n _coloredlogs_available = _is_package_available(\"coloredlogs\")\n@@ -364,6 +365,10 @@ def is_grokadamw_available():\n     return _grokadamw_available\n \n \n+def is_schedulefree_available():\n+    return _schedulefree_available\n+\n+\n def is_pyctcdecode_available():\n     return _pyctcdecode_available\n "
        },
        {
            "sha": "1837d9890352cd27b187773b4ccf4a0d0dad3502",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/62aecd85ffa226d9552529acfd388d87a6b4ecb1/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/62aecd85ffa226d9552529acfd388d87a6b4ecb1/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=62aecd85ffa226d9552529acfd388d87a6b4ecb1",
            "patch": "@@ -70,6 +70,7 @@\n     require_peft,\n     require_ray,\n     require_safetensors,\n+    require_schedulefree,\n     require_sentencepiece,\n     require_sigopt,\n     require_tensorboard,\n@@ -1442,6 +1443,27 @@ def test_grokadamw():\n             # Check this works\n             _ = trainer.train()\n \n+    @require_schedulefree\n+    @require_torch_gpu\n+    def test_schedulefree_adam(self):\n+        config = LlamaConfig(vocab_size=100, hidden_size=32, num_hidden_layers=3, num_attention_heads=4)\n+        tiny_llama = LlamaForCausalLM(config)\n+        x = torch.randint(0, 100, (128,))\n+        train_dataset = RepeatDataset(x)\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            # Trainer without inf/nan filter\n+            args = TrainingArguments(\n+                tmpdir,\n+                learning_rate=1e-9,\n+                logging_steps=5,\n+                optim=\"schedule_free_adamw\",\n+            )\n+            trainer = Trainer(tiny_llama, args, train_dataset=train_dataset)\n+\n+            # Check this works\n+            _ = trainer.train()\n+\n     def test_galore_matched_modules(self):\n         regex_patterns = [r\".*.attn.*\", r\".*.mlp.*\"]\n "
        }
    ],
    "stats": {
        "total": 124,
        "additions": 124,
        "deletions": 0
    }
}